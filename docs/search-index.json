[
  

    {
      "slug": "404-html",
      "title": "",
      "content"	 : "---layout: pagelang: delang-ref: ./site/404.html------layout: pagelang: delang-ref: ./site/404.html------layout: pagelang: delang-ref: ./site/404.html------layout: pagelang: delang-ref: ./site/404.html------layout: pagelang: delang-ref: ./site/404.html------layout: pagelang: en---  üò≠ Page not found üò¢  ",
      "url": " /404.html"
    },
  

    {
      "slug": "en-spec-changelog-html",
      "title": "",
      "content"	 : "Change LogAll notable changes to the specs will be documented in this file.Versioned according to Semantic Versioning.Unreleased3.4.2 - 2020-01-08Changed:  bagit-profile accepts metadata as non-payload dir, #133  Relaxed the requirement for the mets:fileGrp/@USE syntax, #1383.4.1 - 2020-01-03Added:  No multi-page TIFF, #1323.4.0 - 2019-11-05Fixed  Various typos, #128Changed:  Dockerfile: no CMD, no ENTRYPOINT, #130  Processors should assume 300 dpi if image metadata cannot be trusted, #129Added:  Spec for provenance, #1263.3.0 - 2019-10-23Added:  Draft spec for logging  Draft spec for provenanceChanged:  ocrd-tool: Additional additional category layout/segmentation/text-image  ocrd-tool: Remove syntactical restriction for content-type  ocrd-tool: output_file_grp no longer required  CLI: --mets and --working-dir are optional not required  CLI: --output-file-grp is optional, OCR-D/core#2963.2.1 - 2019-06-25Added:  glossary: ‚ÄúMP‚Äù, #112  glossary: ‚Äúfont family‚Äù, #100 #109  cli: allow JSON strings for -p, OCR-D/core#239 #110Fixed:  bagit: path of OcrdMets must be relative to /data, fix #107, #1133.2.0 - 2019-02-27Added:Convention for columnsFixed:PAGE: link to the page xml docs3.1.0 - 2018-12-20Added:  Consistency check level ‚Äòlax‚ÄôFixed:  Example in ocrd_tool.md is from ocrd_kraken, not ocrd_tesserocr3.0.0 - 2018-12-13Added:  PAGE text result and consistency checks, #82, OCR-D/assets#16Changed:  :fire: Drop recommendation on reusing source file ID for page grouping  :fire: Drop GROUPID and replace with mets:structMap[@TYPE=‚ÄùPHYSICAL‚Äù] throughout  :fire: CLI: Replace -g/-group-id with -g/--page-id  CLI: Mark possible comma-separated multi-value parameters as such  CLI: Update ocrd process example  OCRD-ZIP: Set BagIt-Profile-Version to 1.22.7.0 - 2018-12-04Added:  Font information, #76, #962.6.3 - 2018-11-23Changed:  OCRD-ZIP: Ocrd-Mets and mets:FLocat URI/paths must be relative to /data, #99  OCRD-ZIP: Ocrd-Mets only relevant for extraction  OCRD-ZIP: Filenames MUST be relative to mets.xml  METS: Filenames MAY/SHOULD be relative to mets.xml  OCRD-ZIP: Allow a limited set of files in the bag basedir (readme, build files), #972.6.2 - 2018-11-22Changed:  OCRD-ZIP bagit profile: Add empty list requirement for Tag-Manifest-Required, Tag-Files-Required  OCRD-ZIP bagit profile: Contact info  OCRD-ZIP allow fetch.txt, #982.6.1 - 2018-11-09Fixed:  OCRD-ZIP: typo in bagit-profile: Bagit- ‚Äì&amp;gt; BagIt-  OCRD-ZIP: Require BagIt-Profile-Identifier  OCRD-ZIP: Version number must be a string, bagit-profile/bagit-profile#132.6.0 - 2018-11-06Changed:  Base workspace and workspace serialization mechanics on bagit, #702.5.0 - 2018-10-30Added:  Recording processing information in METS, #89  Input and output file groups can be provided in ocrd-tool.json, #91Changed  :fire: METS: grouping pages by physical structMap not GROUPID, #812.4.0 - 2018-10-19Added:  File parameters, #69  Step for post-correction, #642.3.1 - 2018-10-10Fixed  CLI: Example used repeated options2.3.0 - 2018-09-26Changed:  CLI: filtering by log level required, OCR-D/core#173, #74  CLI: log messages must adhere to uniform pattern, #78Added:  CLI: Convention to prefer comma-separated values over repeated flags, #682.2.2 - 2018-08-14Fixed:  Missed description for parameters2.2.1 - 2018-07-25Changed  spell out parameter properties in ocrd-tool.json schem2.2.0 - 2018-07-23Added:  CLI: Conventions for handling URL on the command line2.1.2 - 2018-07-19Added:  Reference PAGE media type in PAGE conventions, #652.1.1 - 2018-06-18Fixed:  ocrd-tool: regex for version had a YAML error2.1.0 - 2018-06-18Added:  ocrd-tool: Must define version  METS: mets:file must have ID  METS: mets:fileGrp must have consistent MIMETYPE  METS: mets:file GROUPID must be unique with a mets:fileGrp2.0.0 - 2018-06-18Removed:  ‚Äìoutput-mets CLI option1.3.0 - 2018-06-15Added:  Glossary, #56Removed:  drop OCR-D-GT-PAGE, #61Fixed:  explain GT- prefix for fileGrp@USE of ground truth files, #58  various typos1.2.0 - 2018-05-25Fixed:  Fix example for ocrd_tool  Fix TIFF media typeAdded:  -J/‚Äìdump-json, #30Changed  ocrd-tool: tags -&amp;gt; category, #44  ocrd-tool: step -&amp;gt; steps (now an array), #44  ocrd-tool: parameterSchema -&amp;gt; parameters, #48  ocrd-tool: ‚Äòtools‚Äô is an object now, not an array, #431.1.5 - 2018-05-15Added:  ocrd-tool: Steps: preprocessing/optimization/grayscale_normalization and layout/segmentation/word  PAGE conventions1.1.4 - 2018-05-02Added:  PAGE/XML media type, #33  mets:file@GROUPID == pg:pcGtsId, #311.1.3 - 2018-04-28Added:  Add OCR-D-SEG-WORD and OCR-D-SEG-GLYPH as USE attributes1.1.2 - 2018-04-23Changed:  rename repo OCR-D/pyocrd -&amp;gt; OCR-D/core  rename repo OCR-D/ocrd-assets -&amp;gt; OCR-D/assets  renamed docker base image ocrd/pyocrd -&amp;gt; ocrd/coreFixed:  In ocrd_tool example: renamed parameter structure-level -&amp;gt; level-of-operation1.1.1 - 2018-04-19Fixed:  typo: exceutable -&amp;gt; executable  disallow custom properties1.1.0 - 2018-04-19Added  Spec for OCRD-ZIPChanged  Use executable instead of binary to reduce confusionFixed  typos (@stweil)Removed1.0.0 - 2018-04-16Initial Release",
      "url": " /en/spec/CHANGELOG.html"
    },
  

    {
      "slug": "de-spec-changelog-html",
      "title": "",
      "content"	 : "Change LogAll notable changes to the specs will be documented in this file.Versioned according to Semantic Versioning.Unreleased3.4.2 - 2020-01-08Changed:  bagit-profile accepts metadata as non-payload dir, #133  Relaxed the requirement for the mets:fileGrp/@USE syntax, #1383.4.1 - 2020-01-03Added:  No multi-page TIFF, #1323.4.0 - 2019-11-05Fixed  Various typos, #128Changed:  Dockerfile: no CMD, no ENTRYPOINT, #130  Processors should assume 300 dpi if image metadata cannot be trusted, #129Added:  Spec for provenance, #1263.3.0 - 2019-10-23Added:  Draft spec for logging  Draft spec for provenanceChanged:  ocrd-tool: Additional additional category layout/segmentation/text-image  ocrd-tool: Remove syntactical restriction for content-type  ocrd-tool: output_file_grp no longer required  CLI: --mets and --working-dir are optional not required  CLI: --output-file-grp is optional, OCR-D/core#2963.2.1 - 2019-06-25Added:  glossary: ‚ÄúMP‚Äù, #112  glossary: ‚Äúfont family‚Äù, #100 #109  cli: allow JSON strings for -p, OCR-D/core#239 #110Fixed:  bagit: path of OcrdMets must be relative to /data, fix #107, #1133.2.0 - 2019-02-27Added:Convention for columnsFixed:PAGE: link to the page xml docs3.1.0 - 2018-12-20Added:  Consistency check level ‚Äòlax‚ÄôFixed:  Example in ocrd_tool.md is from ocrd_kraken, not ocrd_tesserocr3.0.0 - 2018-12-13Added:  PAGE text result and consistency checks, #82, OCR-D/assets#16Changed:  :fire: Drop recommendation on reusing source file ID for page grouping  :fire: Drop GROUPID and replace with mets:structMap[@TYPE=‚ÄùPHYSICAL‚Äù] throughout  :fire: CLI: Replace -g/-group-id with -g/--page-id  CLI: Mark possible comma-separated multi-value parameters as such  CLI: Update ocrd process example  OCRD-ZIP: Set BagIt-Profile-Version to 1.22.7.0 - 2018-12-04Added:  Font information, #76, #962.6.3 - 2018-11-23Changed:  OCRD-ZIP: Ocrd-Mets and mets:FLocat URI/paths must be relative to /data, #99  OCRD-ZIP: Ocrd-Mets only relevant for extraction  OCRD-ZIP: Filenames MUST be relative to mets.xml  METS: Filenames MAY/SHOULD be relative to mets.xml  OCRD-ZIP: Allow a limited set of files in the bag basedir (readme, build files), #972.6.2 - 2018-11-22Changed:  OCRD-ZIP bagit profile: Add empty list requirement for Tag-Manifest-Required, Tag-Files-Required  OCRD-ZIP bagit profile: Contact info  OCRD-ZIP allow fetch.txt, #982.6.1 - 2018-11-09Fixed:  OCRD-ZIP: typo in bagit-profile: Bagit- ‚Äì&amp;gt; BagIt-  OCRD-ZIP: Require BagIt-Profile-Identifier  OCRD-ZIP: Version number must be a string, bagit-profile/bagit-profile#132.6.0 - 2018-11-06Changed:  Base workspace and workspace serialization mechanics on bagit, #702.5.0 - 2018-10-30Added:  Recording processing information in METS, #89  Input and output file groups can be provided in ocrd-tool.json, #91Changed  :fire: METS: grouping pages by physical structMap not GROUPID, #812.4.0 - 2018-10-19Added:  File parameters, #69  Step for post-correction, #642.3.1 - 2018-10-10Fixed  CLI: Example used repeated options2.3.0 - 2018-09-26Changed:  CLI: filtering by log level required, OCR-D/core#173, #74  CLI: log messages must adhere to uniform pattern, #78Added:  CLI: Convention to prefer comma-separated values over repeated flags, #682.2.2 - 2018-08-14Fixed:  Missed description for parameters2.2.1 - 2018-07-25Changed  spell out parameter properties in ocrd-tool.json schem2.2.0 - 2018-07-23Added:  CLI: Conventions for handling URL on the command line2.1.2 - 2018-07-19Added:  Reference PAGE media type in PAGE conventions, #652.1.1 - 2018-06-18Fixed:  ocrd-tool: regex for version had a YAML error2.1.0 - 2018-06-18Added:  ocrd-tool: Must define version  METS: mets:file must have ID  METS: mets:fileGrp must have consistent MIMETYPE  METS: mets:file GROUPID must be unique with a mets:fileGrp2.0.0 - 2018-06-18Removed:  ‚Äìoutput-mets CLI option1.3.0 - 2018-06-15Added:  Glossary, #56Removed:  drop OCR-D-GT-PAGE, #61Fixed:  explain GT- prefix for fileGrp@USE of ground truth files, #58  various typos1.2.0 - 2018-05-25Fixed:  Fix example for ocrd_tool  Fix TIFF media typeAdded:  -J/‚Äìdump-json, #30Changed  ocrd-tool: tags -&amp;gt; category, #44  ocrd-tool: step -&amp;gt; steps (now an array), #44  ocrd-tool: parameterSchema -&amp;gt; parameters, #48  ocrd-tool: ‚Äòtools‚Äô is an object now, not an array, #431.1.5 - 2018-05-15Added:  ocrd-tool: Steps: preprocessing/optimization/grayscale_normalization and layout/segmentation/word  PAGE conventions1.1.4 - 2018-05-02Added:  PAGE/XML media type, #33  mets:file@GROUPID == pg:pcGtsId, #311.1.3 - 2018-04-28Added:  Add OCR-D-SEG-WORD and OCR-D-SEG-GLYPH as USE attributes1.1.2 - 2018-04-23Changed:  rename repo OCR-D/pyocrd -&amp;gt; OCR-D/core  rename repo OCR-D/ocrd-assets -&amp;gt; OCR-D/assets  renamed docker base image ocrd/pyocrd -&amp;gt; ocrd/coreFixed:  In ocrd_tool example: renamed parameter structure-level -&amp;gt; level-of-operation1.1.1 - 2018-04-19Fixed:  typo: exceutable -&amp;gt; executable  disallow custom properties1.1.0 - 2018-04-19Added  Spec for OCRD-ZIPChanged  Use executable instead of binary to reduce confusionFixed  typos (@stweil)Removed1.0.0 - 2018-04-16Initial Release",
      "url": " /de/spec/CHANGELOG.html"
    },
  

    {
      "slug": "en-ocr-d-20from-20novice-20to-20pro-html",
      "title": "",
      "content"	 : "User Guide for Non-IT Users (without Docker)Prerequisites and PreparationsVirtual environmentBefore starting to work with the OCR-D-software you should activate thevirtualenv. This has either been installed automatically if you installed thesoftware via ocrd_all, or you should have installed it yourself beforeinstalling the OCR-D-software individually.source ~/venv/bin/activateOnce you have activated the virtualenv, you should see (venv) prepended toyour shell prompt.When you are done with your OCR-D-work, you can use deactivate to deactivateyour venv.Preparing a workspaceOCR-D processes digitized images in so-called workspaces, special directorieswhich contain the images to be processed and their corresponding METS file. Anyfiles generated while processing these images with the OCR-D-software will alsobe stored in this directory.How you prepare a workspace depends on whether you already have or don‚Äôt have aMETS file with the paths to the images you want to process. For usage withinOCR-D your METS file should look similar to this example.Already Existing METSIf you already have a METS file as indicated above, you can create a workspaceand load the pictues to be processed with the following command:ocrd workspace clone [URL of your mets.xml]In most cases, METS files indicate several picture formats. For OCR-D you willonly need one format. We strongly recommend using the format with the bestresolution. Optionally, you can specify to only load the filegroup needed atthe end of the command above.You can also optionally specify a particular name for your workspace. If youdon‚Äôt, it will simply generate a name by itself.Non-Existing METSIf you don‚Äôt have a METS file or it doesn‚Äôt suffice the OCR-D-requirements youcan generate it with the following commands. First, you have to create aworkspace:ocrd workspace init [name of your workspace]Then you can go into your workspace and set a unique IDworkspace$ ocrd workspace set-id &#39;unique ID&#39;and copy the folder containing your pictures to be processed into the workspace:cp -r [path to your pictures&#39; folder] .Now you can add your pictures to the METS. When creating the workspace, a blankMETS file was created, too, to which you can add the pictures to be processed.You can do this manually with the following command:ocrd workspace add -g [ID of the physical page, has to start with a letter] -G [name of picture folder in your workspace] -i [ID of the scanned page] -m image/[format of your pictures] [path to your picture]Your command could e.g. look like this:ocrd workspace add -g P_00001 -G OCR-D-IMG -i 00001 -m image/tif OCR-D-IMG/00001.tifIf you have many pictures to be added to the METS, you can do this automatically with a for-loop:for i in [name of picture folder in your workspace].[file ending of your pictures]; do base= `basename ${i} .[file ending of your pictures`; ocrd workspace add -G [name of picture folder in your workspace] -i ${base} -g P_${base} -m image/[format of your pictures] ${i}; doneYour for-loop could e.g. look like this:for i in OCR-D-IMG/*.tif; do base=`basename ${i} .tif`; ocrd workspace add -G OCR-D-IMG -i ${base} -g P_${base} -m image/tif ${i}; doneIn the end, your METS file should look like this example METSUsing the OCR-D-processorsOCR-D-SyntaxThere are several ways for invoking the OCR-D-processors. However, all of thoseways make use of the following syntax:-I Input-Group    # folder of the files to be processed-O Output-Group   # folder for the output of your processor-p parameter.json # indication of parameters for a particular processorFor some processors parameters are purely optional, other processors as e.g. ocrd-tesserocr-recognize won‚Äôt work without one or several parameters.Calling a single processorIf you just want to call a single processor, e.g. for testing purposes, you can go into your workspace and use the following command:ocrd-[processor needed] -I [Input-Group] -O [Output-Group] -p [path to parameter.json]&#39;Your command could e.g. look like this:ocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -p param-tess-fraktur.jsonThe parameter.json file can be created with the following command:echo &#39;{ &quot;[parameter]&quot;: &quot;[specification]&quot; }&#39; &amp;gt; [name of your param.json file]Instead of creating a calling a parameter.json file you can also directlywrite down the parameters when invoking a processor with writing your data to a JSON file, like so:-p &#39;{&quot;[parameter]&quot;: &quot;[specification]&quot;}`Calling several processorsocrd-processIf you quickly want to specify a particular workflow on the CLI, you can useocrd-process, which has a similar syntax as calling single processors.ocrd process   &#39;[processor needed] -I [Input-Group] -O [Output-Group]&#39;   &#39;[processor needed] -I [Input-Group] -O [Output-Group] -p [parameter.json]&#39;Your command could e.g. look like this:ocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -p param-tess-fraktur.json&#39;Note that in contrast to calling a single processor, for ocrd-process you leaveout the prefix ocrd- before the name of a particular processor.TavernaTaverna is a more sophisticated workflow-software which allows you to specify aparticular workflow in a file and call this workflow, or rather its file, onseveral workspaces.Note that Taverna is not included in yourocrd_all installation. Therefore,you still might have to install it following this setup guide.Taverna comes with several predefined workflows which you can help you gettingstarted. These are stored in the /conf directory. For every workflow at leasttwo files are needed: A workflow_configuration file contains a particularworkflow which is invoked by a parameters file.For calling a workflow via Taverna, go into the Taverna folder and use thefollowing command:bash startWorkflow.sh [particular parameters.txt] [path to your workspace]The images in your indicated workspace will be processed and the respectiveoutput will be saved into the same workspace.When you want to adjust a workflow for better results on your particularimages, you should start off by copying the original workflow_configurationand parameters files. To this end, change to the /conf subdirectory ofTaverna and use the following commands:conf$ cp [original workflow_configuration.txt] [name of your new workflow_configuration.txt]conf$ cp [original parameters.txt] [name of your new parameters.txt]Open the new parameters.txt file with an editor like e.g. Nano and change thename of the old workflow_configuration.txt specified in this file to the nameof your new workflow_configuration.txt file:nano [name of your new workflow_configuration.txt]Then open your new workflow_configuration.txt file respectively and adjust it to your needs.üë∑workflow-configüë∑Specifying New OCR-D-Workflowsüë∑When you want to specify a new workflow adapted to the features of particularimages, we recommend using an exisiting workflow as specified in Taverna orworkflow-config as starting point. You can adjust it to your needs byexchanging or adding the specified processors of parameters. For an overview onthe existing processors, their tasks and features, see ???.",
      "url": " /en/OCR-D:%20from%20novice%20to%20pro.html"
    },
  

    {
      "slug": "en-spec-readme-html",
      "title": "",
      "content"	 : "Specification of the technical architecture, interface definitions and data exchange format(s)See https://ocr-d.github.io/.",
      "url": " /en/spec/README.html"
    },
  

    {
      "slug": "de-spec-readme-html",
      "title": "",
      "content"	 : "Specification of the technical architecture, interface definitions and data exchange format(s)See https://ocr-d.github.io/.",
      "url": " /de/spec/README.html"
    },
  

    {
      "slug": "en-advisory-board-html",
      "title": "",
      "content"	 : "Scientific Advisory BoardThe project is counseled by a scientific advisory board, which is consituted by the following persons at the moment:Crane, Gregory (Leipzig University)Kaiser, Max (Austrian National Library)K√∂hler, Joachim (Fraunhofer IAIS)Meyer, Sebastian (SLUB Dresden)M√ºhlberger, G√ºnther (University of Innsbruck)Schulz, Klaus (LMU Munich ‚Äì CIS)Str√∂tgen, Robert (TU Braunschweig)",
      "url": " /en/advisory-board.html"
    },
  

    {
      "slug": "en-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /en/blog.html"
    },
  

    {
      "slug": "de-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /de/blog.html"
    },
  

    {
      "slug": "en-spec-cli-html",
      "title": "",
      "content"	 : "Command Line Interface (CLI)NOTE: Command line options cannot be repeated. Parameters markedMULTI-VALUE specify multiple values, provide a single string withcomma-separated items (e.g. -I group1,group2,group3 instead of -I group1 -Igroup2 -I group3).CLI executable nameAll CLI provided by MP must be standalone executables, installable into $PATH.Every CLI executable‚Äôs name must begin with ocrd-.Examples:  ocrd-kraken-binarize  ocrd-tesserocr-recognizeMandatory parameters-I, --input-file-grp GRPMULTI-VALUEFile group(s) used as input.Optional parameters-O, --output-file-grp GRPMULTI-VALUEFile group(s) used as output.Omit to resort to default output file groups of the processor or for processors that do not produce output files.-m, --mets METS_INInput METS URL. Default: mets.xml-w, --working-dir DIRWorking Directory. Default: current working directory.-g, --page-id IDMULTI-VALUEThe mets:div[@TYPE=&#39;page&#39;]/@ID that contains the mets:fptr/@FILEID pointersto files representing a page. Effectively, only those files in the input filegroup that are referenced in thesemets:div[@TYPE=&quot;page&quot;] will be processed.-p, --parameter PARAM_JSONURL of parameter file in JSON format. If that file is not readable andPARAM_JSON begins with { (opening brace), try to parse PARAM_JSON asJSON. If that also fails, throw an exception.-l, --log-level LOGLEVELSet the global maximum verbosity level. More verbose log entries will beignored. (One of OFF, ERROR, WARN, INFO (default), DEBUG, TRACE).NOTE: Setting the log level via --log-level parameter should override anyother implementation-specific means of logging configuration. For example, with--log-level TRACE no log messages should be filtered globally, whereas--log-level ERROR, only errors should be output globally.-J, --dump-jsonInstead of processing METS, output the ocrd-tool description forthis executable, in particular its parameters.Return valueSuccessful execution should signal 0. Any non-zero return value is considered a failure.LoggingData printed to STDERR and STDOUT is captured linewise and stored as log data.Processors must adjust logging verbosity according to the --log-level parameter.Errors, especially those leading to exceptions, must be printed to STDERR.The log messages must have the format TIME LEVEL LOGGERNAME - MESSAGEn, where  TIME is the current time in the format HH:MM:ss.mmm, e.g. 07:05:31.007  LEVEL is the log level of the message, in uppercase, e.g. INFO  LOGGERNAME is the name of the logging component, such as the class name. Segments of LOGGERNAME should be separated by dot ., e.g. ocrd.fancy_tool.analyze  MESSAGE is the message to log, should not contain new lines.  n is ASCII char 0x0a (newline)URL/file conventionWhenever a URL is expected, it should be possible to use a local file pathinstead and have the implementation interpret as a file:// URL on the fly.Implementations should adhere to this algorithm when resolving a URL u:  If u contains the string ://: Do not modify.  If u is an absolute path according to the mechanics of the underlying file system: Prepend file:// to u.  Otherwise: Resolve u as a path relative to the current working directory, prepend file:// to u.NOTE: This convention is limited to the CLI for convenience of users anddevelopers. In METS and PAGE documents, URLs must be strictly valid andresolvable by common software agents as-is.ExampleThis is how the CLI provided by the MP should work:$&amp;gt; ocrd-kraken-binarize     --mets &quot;file:///path/to/file/mets.xml&quot;     --working-dir &quot;file:///path/to/workingDir/&quot;     --parameters &quot;file:///path/to/file/parameters.json&quot;     --page-id PHYS_0001,PHYS_0002,PHYS_0003     --input-file-grp OCR-D-IMG    --output-file-grp OCR-D-IMG-BIN-KRAKENAnd this is how it will be called with the ocrd process CLI:$&amp;gt; ocrd process     &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN-KRAKEN -p /path/to/file/parameters.json&#39;    -m &quot;file:///path/to/file/mets.xml&quot;     -g PHYS_0001,PHYS_0002,PHYS_0003    preprocessing/binarization/kraken-binarizeMETS input&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;Input JSON parameter file{    &quot;threshold&quot;: 0.05,    &quot;zoom&quot;: 2,    &quot;range&quot;: [5, 10],}METS outputThis is the METS file after being run through the MP CLI:&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div DMDID=&quot;DMDPHYS_0000&quot; ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN-KRAKEN&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0001.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0002.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0003.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /en/spec/cli.html"
    },
  

    {
      "slug": "de-spec-cli-html",
      "title": "",
      "content"	 : "Command Line Interface (CLI)NOTE: Command line options cannot be repeated. Parameters markedMULTI-VALUE specify multiple values, provide a single string withcomma-separated items (e.g. -I group1,group2,group3 instead of -I group1 -Igroup2 -I group3).CLI executable nameAll CLI provided by MP must be standalone executables, installable into $PATH.Every CLI executable‚Äôs name must begin with ocrd-.Examples:  ocrd-kraken-binarize  ocrd-tesserocr-recognizeMandatory parameters-I, --input-file-grp GRPMULTI-VALUEFile group(s) used as input.Optional parameters-O, --output-file-grp GRPMULTI-VALUEFile group(s) used as output.Omit to resort to default output file groups of the processor or for processors that do not produce output files.-m, --mets METS_INInput METS URL. Default: mets.xml-w, --working-dir DIRWorking Directory. Default: current working directory.-g, --page-id IDMULTI-VALUEThe mets:div[@TYPE=&#39;page&#39;]/@ID that contains the mets:fptr/@FILEID pointersto files representing a page. Effectively, only those files in the input filegroup that are referenced in thesemets:div[@TYPE=&quot;page&quot;] will be processed.-p, --parameter PARAM_JSONURL of parameter file in JSON format. If that file is not readable andPARAM_JSON begins with { (opening brace), try to parse PARAM_JSON asJSON. If that also fails, throw an exception.-l, --log-level LOGLEVELSet the global maximum verbosity level. More verbose log entries will beignored. (One of OFF, ERROR, WARN, INFO (default), DEBUG, TRACE).NOTE: Setting the log level via --log-level parameter should override anyother implementation-specific means of logging configuration. For example, with--log-level TRACE no log messages should be filtered globally, whereas--log-level ERROR, only errors should be output globally.-J, --dump-jsonInstead of processing METS, output the ocrd-tool description forthis executable, in particular its parameters.Return valueSuccessful execution should signal 0. Any non-zero return value is considered a failure.LoggingData printed to STDERR and STDOUT is captured linewise and stored as log data.Processors must adjust logging verbosity according to the --log-level parameter.Errors, especially those leading to exceptions, must be printed to STDERR.The log messages must have the format TIME LEVEL LOGGERNAME - MESSAGEn, where  TIME is the current time in the format HH:MM:ss.mmm, e.g. 07:05:31.007  LEVEL is the log level of the message, in uppercase, e.g. INFO  LOGGERNAME is the name of the logging component, such as the class name. Segments of LOGGERNAME should be separated by dot ., e.g. ocrd.fancy_tool.analyze  MESSAGE is the message to log, should not contain new lines.  n is ASCII char 0x0a (newline)URL/file conventionWhenever a URL is expected, it should be possible to use a local file pathinstead and have the implementation interpret as a file:// URL on the fly.Implementations should adhere to this algorithm when resolving a URL u:  If u contains the string ://: Do not modify.  If u is an absolute path according to the mechanics of the underlying file system: Prepend file:// to u.  Otherwise: Resolve u as a path relative to the current working directory, prepend file:// to u.NOTE: This convention is limited to the CLI for convenience of users anddevelopers. In METS and PAGE documents, URLs must be strictly valid andresolvable by common software agents as-is.ExampleThis is how the CLI provided by the MP should work:$&amp;gt; ocrd-kraken-binarize     --mets &quot;file:///path/to/file/mets.xml&quot;     --working-dir &quot;file:///path/to/workingDir/&quot;     --parameters &quot;file:///path/to/file/parameters.json&quot;     --page-id PHYS_0001,PHYS_0002,PHYS_0003     --input-file-grp OCR-D-IMG    --output-file-grp OCR-D-IMG-BIN-KRAKENAnd this is how it will be called with the ocrd process CLI:$&amp;gt; ocrd process     &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN-KRAKEN -p /path/to/file/parameters.json&#39;    -m &quot;file:///path/to/file/mets.xml&quot;     -g PHYS_0001,PHYS_0002,PHYS_0003    preprocessing/binarization/kraken-binarizeMETS input&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;Input JSON parameter file{    &quot;threshold&quot;: 0.05,    &quot;zoom&quot;: 2,    &quot;range&quot;: [5, 10],}METS outputThis is the METS file after being run through the MP CLI:&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div DMDID=&quot;DMDPHYS_0000&quot; ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN-KRAKEN&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0001.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0002.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0003.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /de/spec/cli.html"
    },
  

    {
      "slug": "en-contact-html",
      "title": "",
      "content"	 : "ContactContact person:Elisabeth EnglProject coordination OCR-DDuke August Library Wolfenb√ºttelengl[at]hab.deSubstitution:Andrea OpitzDeputy head of departmentNew Media, Digital LibraryDuke August Libraryopitz[at]hab.deDr. Johannes MangeiDeputy Director of the Herzog August LibraryHead of Department New Media, Digital LibraryDuke August Librarymangei[at]hab.de",
      "url": " /en/contact.html"
    },
  

    {
      "slug": "de-contact-html",
      "title": "",
      "content"	 : "KontaktAnsprechpartnerin:Elisabeth EnglProjektkoordinatorin OCR-DHerzog August Bibliothek Wolfenb√ºttelengl[at]hab.deStellvertreter*innen:Andrea OpitzDeputy head of departmentNew Media, Digital LibraryDuke August Libraryopitz[at]hab.deDr. Johannes MangeiDeputy Director of the Herzog August LibraryHead of Department New Media, Digital LibraryDuke August Librarymangei[at]hab.de",
      "url": " /de/contact.html"
    },
  

    {
      "slug": "en-cookbook-html",
      "title": "",
      "content"	 : "OCR-D Cookbook  A set of examples on how to apply the OCR-D guideIntroductionThe ‚ÄúOCR-D cookbook‚Äù helps developers writing software and usingtools within the OCR-D ecosystem by listing practical examples in addition to the OCR-D guide.From image to transcriptionOCR-D workflowThe workflow consists of several steps, from the image with some additionalmetadata to the textual content of the image. The tools used to generate thetext are divided into the following categories:  Image preprocessing  Layout analysis  Text recognition and optimization  Model training  Long-term preservation  Quality assuranceThe workflow may be divided in the following steps:  preprocessing/characterization  preprocessing/optimization  preprocessing/optimization/cropping  preprocessing/optimization/deskewing  preprocessing/optimization/despeckling  preprocessing/optimization/dewarping  preprocessing/optimization/binarization  preprocessing/optimization/grayscale_normalization  layout/segmentation  layout/segmentation/region  layout/segmentation/line  layout/segmentation/word  layout/segmentation/classification  layout/analysis  recognition/text-recognition  recognition/font-identificationKRAKEN, OLENA, TESSEROCR, OCROPY# Step 0: Check/Install git and dependenciesSee subsection Bootstrapping# Step 1: Clone repositories# Step 1a: KRAKEN$ cd ~/projects/OCR-D$ git clone https://github.com/OCR-D/ocrd_kraken$ cd ocrd_kraken/$ make deps$ make install# Step 1b: Test installation$ ocrd-kraken-binarize --versionVersion 0.0.1, ocrd/core 0.4.0WorkflowsBinarize one image without existing METS file.# Step 0: Create Workspace &amp;amp; METS file# ------------------------# Step 0a: Create directory for workshop$ mkdir -p ~/projects/OCR-D/workshop/2018_06_26/workspaces$ cd ~/projects/OCR-D/workshop/2018_06_26/workspaces# Step 0b: Create workspace including METS file in subdir `./emptyWorkspace`$ ocrd workspace init emptyWorkspace$ cd emptyWorkspace$ ocrd workspace validate$ cd ws1  # Step 0c: Validate workspace&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;METS has no unique identifier&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Step 0d: Add identifier to METS file$ ocrd workspace set-id &#39;http://resolver.staatsbibliothek-berlin.de/SBB0000F29300000000&#39;$ ocrd workspace validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Step 1: Download tiff image# ---------------------------$ wget -O PPN767137728_00000005.tif &quot;http://ngcs.staatsbibliothek-berlin.de/?action=metsImage&amp;amp;format=tif&amp;amp;metsFile=PPN767137728&amp;amp;divID=PHYS_0005&amp;amp;original=true&quot;# Step 2: Add image to METS# -------------------------# Be aware, that the ID and the GROUPID have to identical if the referenced image represents the original image$ ocrd workspace add --file-grp OCR-D-IMG --file-id OCR-D-IMG_0001 --group-id OCR-D-IMG_0001 --mimetype image/tiff PPN767137728_00000005.tif# Step 3: Validate workspace# --------------------------$ ocrd workspace validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;# Step 3a: Clone workspace (optional)# -----------------------------------# Create new directory and clone workspace to this directory$ ocrd workspace clone --download mets.xml ../cloneEmptyWorkspace$ cd ../cloneEmptyWorkspace# Show all files (use --help to see all parameters)$ ocrd workspace findfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/cloneEmptyWorkspace/PPN767137728_00000005.tif# Step 4: Execute binarization of image# -------------------------------------See subsection Bootstrapping# Step 4a: Install KRAKEN see [Installation KRAKEN] (#KRAKEN, OLENA, TESSEROCR, OCROPY)See subsection Install KRAKEN# Step 4b: List all available tools$ ocrd ocrd-tool   ~/projects/OCR-D/ocrd_kraken/ocrd-tool.json list-tools  ocrd-kraken-binarize  ocrd-kraken-ocr  ocrd-kraken-segment# Step 4c: List attributes of &#39;ocrd-kraken-binarize&#39;$ ocrd ocrd-tool   ~/projects/OCR-D/ocrd_kraken/ocrd-tool.json tool ocrd-kraken-binarize description$ ocrd ocrd-tool   ~/projects/OCR-D/ocrd_kraken/ocrd-tool.json tool ocrd-kraken-binarize categoriesBinarize images with kraken  Image preprocessing$ ocrd ocrd-tool   ~/projects/OCR-D/ocrd_kraken/ocrd-tool.json tool ocrd-kraken-binarize steps  preprocessing/optimization/binarization# Step 4d: Binarize Image with KRAKEN# Binarize all images inside fileGrp &#39;OCR-D-IMG&#39;$ ocrd-kraken-binarize --input-file-grp OCR-D-IMG --output-file-grp OCR-D-IMG-KRAKEN-BIN --group-id OCR-D-IMG_0001 --working-dir ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeEmptyWorkspace --mets mets.xml# Check result$ firefox ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeEmptyWorkspace/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.png# That&#39;s itBinarize all images of a METS file.# Step 0: Create Workspace &amp;amp; METS file# ------------------------# Step 0a: Create workspace including METS file$ ocrd workspace init ~/projects/OCR-D/workshop/2018_06_26/workspaces/multipleImages$ cd ~/projects/OCR-D/workshop/2018_06_26/workspaces/multipleImages# Step 0b: Validate workspace$ ocrd workspace validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;METS has no unique identifier&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Step 0c: Add identifier to METS file$ ocrd workspace set-id http://resolver.staatsbibliothek-berlin.de/SBB0000F29300000000# &amp;lt;mods:mods xmlns:mods=&quot;http://www.loc.gov/mods/v3&quot;&amp;gt;#   &amp;lt;mods:identifier type=&quot;purl&quot;&amp;gt;http://resolver.staatsbibliothek-berlin.de/SBB0000F29300000000&amp;lt;/mods:identifier&amp;gt;# &amp;lt;/mods:mods&amp;gt;$ ocrd workspace validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Step 1: Download tiff images# ----------------------------$ wget -O PPN767137728_00000005.tif &quot;http://ngcs.staatsbibliothek-berlin.de/?action=metsImage&amp;amp;format=jpg&amp;amp;metsFile=PPN767137728&amp;amp;divID=PHYS_0005&amp;amp;original=true&quot;$ wget -O PPN767137728_00000006.tif &quot;http://ngcs.staatsbibliothek-berlin.de/?action=metsImage&amp;amp;format=jpg&amp;amp;metsFile=PPN767137728&amp;amp;divID=PHYS_0006&amp;amp;original=true&quot;    # Step 2: Add images to METS# --------------------------# Be aware, that the ID and the GROUPID have to identical if the referenced image represents the original image$ ocrd workspace add --file-grp OCR-D-IMG --file-id OCR-D-IMG_0001 --group-id OCR-D-IMG_0001 --mimetype image/tiff PPN767137728_00000005.tif$ ocrd workspace add --file-grp OCR-D-IMG --file-id OCR-D-IMG_0002 --group-id OCR-D-IMG_0002 --mimetype image/tiff PPN767137728_00000006.tif# Step 3: Validate workspace# --------------------------$ ocrd workspace validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;# Step 3a: Clone workspace (optional)# -----------------------------------# Create new directory and clone workspace to this directory$ ocrd workspace clone --download $OLDPWD/mets.xml workspace3# Show all files (use --help to see all parameters)$ cd workspace3$ ocrd workspace findfile:///path/to/new/workspace/OCR-D-IMG/PPN767137728_00000005.tiffile:///path/to/new/workspace/OCR-D-IMG/PPN767137728_00000006.tif# Step 4: Binarize Image with KRAKEN# ----------------------------------$ ocrd-kraken-binarize --input-file-grp OCR-D-IMG --output-file-grp OCR-D-IMG-KRAKEN-BIN --working-dir ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages --mets /tmp/pyocrd-&#39;xyz&#39;/mets.xml# Check result$ firefox ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.png ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0002.bin.png# That&#39;s itBinarize one image of a METS file.For preparing workspace see subsection Binarize all images of a METS file (Step 0 - 3)# Step 0: Reuse existing workspace# --------------------------------$ cd ~/projects/OCR-D/workshop/2018_06_26/workspaces/multipleImages# Step 0b: Validate workspace# --------------------------$ ocrd workspace validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;# Step 1: Clone workspace (optional)# -----------------------------------# This step creates a temporal directory (/tmp/pyocrd-&#39;xyz&#39;)$ ocrd workspace clone --download mets.xml ../selectOneImage# Change directory$ cd ../selectOneImage# Show all files (use --help to see all parameters)$ ocrd workspace findfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/selectOneImage/OCR-D-IMG/PPN767137728_00000005.tiffile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/selectOneImage/OCR-D-IMG/PPN767137728_00000006.tif# Step 2: Binarize Image with KRAKEN# ----------------------------------# Step 2a: List all GROUPIDs.¬ß ocrd workspace find --output-field groupIdOCR-D-IMG_0001OCR-D-IMG_0002Step 2b: Binarize image from a chosen GROUPID$ ocrd-kraken-binarize --input-file-grp OCR-D-IMG --output-file-grp OCR-D-IMG-KRAKEN-BIN --group-id OCR-D-IMG_0001 --working-dir ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeSelectedImage --mets mets.xml# Check result$ firefox ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeSelectedImage/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.png# That&#39;s itGet Ground Truth from OCR-D# Create data directory$ mkdir -p ~/projects/OCR-D/data/groundTruth$ cd ~/projects/OCR-D/data/groundTruth# Download GT from OCR-D$ wget http://ocr-d.de/sites/all/GTDaten/blumenbach_anatomie_1805.zip$ unzip blumenbach_anatomie_1805.zip# Step 1: Clone workspace from METS$ mkdir -p ~/projects/OCR-D/workshop/2018_06_26/workspaces/; cd ~/projects/OCR-D/workshop/2018_06_26/workspaces/$ ocrd workspace clone --download ~/projects/OCR-D/data/groundTruth/blumenbach_anatomie_1805/blumenbach_anatomie_1805/mets.xml blumenbach_anatomie_1805$ cd blumenbach_anatomie_1805/Installing a MP executable$ mkdir ~/projects/OCR-D/modules$ cd ~/projects/OCR-D/modules$ git clone https://github.com/OCR-D/ocrd_kraken$ cd ocrd_kraken$ sudo make installTools for MPGetting files referenced inside METSThe command ‚Äòocrd workspace find‚Äô supports several options.$ cd ~/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages# List all files.$ ocrd workspace findfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/PPN767137728.00000005.tiffile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/PPN767137728.00000006.tiffile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.pngfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0002.bin.png# List all files inside a fileGrp¬ß ocrd workspace find --file-grp OCR-D-IMG-KRAKEN-BINfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.pngfile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0002.bin.png# List all files of a GROUPID$ ocrd workspace find --group-id  OCR-D-IMG_0001file:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/PPN767137728.00000005.tiffile:///home/ocrd/projects/OCR-D/workshop/2018_06_26/workspaces/binarizeAllImages/OCR-D-IMG-KRAKEN-BIN/OCR-D-IMG-KRAKEN-BIN_0001.bin.png# See &#39;ocrd workspace find --help&#39; for further information",
      "url": " /en/cookbook.html"
    },
  

    {
      "slug": "en-data-html",
      "title": "",
      "content"	 : "DataReference DataThe reference data includes a Ground Truth corpus and other special corpora.The Ground Truth corpus contains pages from publications printed between 1500and 1900. The content of the corpus is based on a particular selection from theholdings of the DFG project ‚ÄúGerman TextArchive‚Äù, the Digitized Collections of theStaatsbibliothek zu Berlin and theWolfenb√ºttel DigitalLibrary ofthe Duke August library. The holdings of projects and digital collections ofother libraries as well as additional Ground Truth data, which are compiledtogether with module projects, can be included in the corpus as specialextensions in concertation with the OCR-D coordination project. If additionalannotations or texts are necessary, these can be created in consultation withthe OCR-D coordination project.Depth of Annotation, Text Accuracy and ArtifactsThe Ground Truth corpus offers three annotation depths:  Structural regions, text lines, word coordinates  Structure regions, text lines  Text linesOverview (The list will be extended continuously.)The special corpora contain:  a corpus of data with lower text accuracy (dirty OCR), which can be used for individual comparisons and evaluations  a corpus of artifacts with objects that show disturbancesOverviewCreation of the Ground TruthThe image data were first subjected to a layout analysis (text region and line recognition) using Transkribus and then processed with the integrated OCR engine (ABBYY FineReader 11 SDK). The automatically recognized text as well as the lines and text regions were corrected manually (if necessary by using existing Ground Truth data) and finally exported as ALTO and PAGE files. Together with the images in TIF format, these form the content of the zip files.If you are interested in further Ground Truth data (e.g. for binarization) please contact us: elisabeth.engl[at]hab.deThe data are subject to a CC-BY-SA license, for the use of the image datadifferent licenses may exist. Please contact the project and/or the owning library.",
      "url": " /en/data.html"
    },
  

    {
      "slug": "de-data-html",
      "title": "",
      "content"	 : "#DatenDie ReferenzdatenDie Referenzdaten umfassen ein Ground-Truth-Korpus und weitere Spezialkorpora.Das Ground-Truth-Korpus umfasst Seiten aus Publikationen aus dem Zeitraum 1500            Der Inhalt des Korpus basiert auf einer gezielten Auswahl aus demBestand des DFG-Projektes ‚ÄûDeutsches Textarchiv‚Äú, der DigitalisiertenSammlungen der Staatsbibliothek zu Berlin und der Wolfenb√ºtteler DigitalenBibliothek der Herzog August Bibliothek. Best√§nde von Projekten und digitalenSammlungen anderer Bibliotheken sowie zus√§tzliche Ground-Truth-Daten, diezusammen mit Modulprojekten erarbeitet werden, k√∂nnen in Abstimmung mit demOCR-D-Koordinierungsgremium in das Korpus als spezielle Erweiterungenaufgenommen werden. Sollten zus√§tzliche Annotationen oder Texte notwendig sein,k√∂nnen diese in Abstimmung erstellt werden.      Annotationstiefe, Textgenauigkeit und ArtefakteDas Ground-Truth-Korpus bietet drei Annotationstiefen an:  Strukturregionen, Textzeilen, Wortkoordinaten  Strukturregionen, Textzeilen  TextzeilenZur √úbersicht (Die Liste wird stetig erweitert.)Die Spezialkorpora umfassen:  Spezialkorpus von Daten geringerer Textgenauigkeit (schmutzige OCR), kann f√ºr einzelne Vergleiche und Evaluationen herangezogen werden.  Spezialkorpus Artefakte: Dieses Korpus beinhaltet ausschlie√ülich Objekte die St√∂rungen aufweisen.Zur √úbersichtErstellung des Ground TruthDie Image-Daten wurden mittels Transkribus zun√§chst einer Layout-Analyse(Textregion- und Zeilenerkennung) unterzogen und anschlie√üend mit derintegrierten OCR-Engine (ABBYY FineReader 11 SDK) prozessiert. Der soautomatisch erkannte Text sowie die Zeilen und Textregionen wurden manuellnachkorrigiert (ggf. unter Verwendung von vorhandenen Ground-Truth-Daten) undschlie√ülich als ALTO- und PAGE-Dateien exportiert. Diese bilden zusammen mitden Images im TIF-Format den Inhalt der zip-Dateien.Wenn Sie Interesse an weiteren Ground-Truth-Daten haben (bspw. zurBinarisierung) schreiben Sie uns bitte: elisabeth.engl[at]hab.deDie Daten unterliegen einer CC-BY-SA-Lizenz, f√ºr die Verwendung der Bilddatenk√∂nnen abweichende Lizenzen vorliegen. Bitte kontaktieren Sie diesbez√ºglich dasProjekt und/oder die besitzende Bibliothek.",
      "url": " /de/data.html"
    },
  

    {
      "slug": "en-developer-guide-html",
      "title": "",
      "content"	 : "OCR-D Developer Guide  A practical guide to the OCR-D frameworkIntroductionThe ‚ÄúOCR-D guide‚Äù helps developers writing software and usingtools within the OCR-D ecosystem.Scope and purpose of the OCR-D guideThe OCR-D guide is a collection of concise recipes that provide pragmatic advise on how to  bootstrap a development environment,  work with the ocrd command line tool,  manipulate METS and PAGE documents,  create spec-compliant softwareNotationLines in code examples  starting with #  are comments;  starting with $  are typed shell input (everything after $  is);  are output otherwise.Words in ALL CAPS with a preprended $ are variable names:  $METS_URL: URL or file path to a mets.xml file, e.g.          https://github.com/OCR-D/assets/raw/master/data/kant_aufklaerung_1784/mets.xml        $WORKSPACE_DIR: File path of the workspace created, e.g.          $WORKSPACE_DIR      /data/ocrd-workspaces/kant-aufklaerung-2018-07-11      When referring to a ‚Äúsomething command‚Äù, it is actually ocrd something onthe command line.Other OCR-D documentation  Specification: Formal specifications  Glossary: A glossary of terms in the OCRdomain as used throughout our documentationBootstrappingUbuntu LinuxOCR-D development is targeted towards Ubuntu Linux &amp;gt;= 18.04 since it is free,widely used and well-documented.Most of the setup will be the same for other Debian-based Linuxes and olderUbuntu versions. You might run into problems with outdated system packagesthough.In particular, it can be tricky at times to install tesseract at the rightversion. Try alex-p‚Äôs PPA or buildtesseract from source.Essential system packagessudo apt install   git   build-essential   python python-pip   python3 python3-pip  git: Version control, OCR-D uses git extensively  build-essential: Installs make and C/C++ compiler  python: Python 2.7 for legacy applications like ocropy  python3: Current version of Python on which the OCR-D software core stack is built  pip/pip3: Python package managementPython API and CLIThe OCR-D toolkit is based on a Python API thatyou can reuse if you are developing software in Python.This API is exposed via a command line tool ocrd. This CLI offers much of thesame functionality of the API without the need to write Python code and can be readilyintegrated into shell scripts and external command callouts in your code.So, If you do not intend to code in Python or want to wrapexisting/legacy tools, a major part of the functionality of the API isavailable as a command line tool ocrd.Python setupCreate virtualenvWe strongly recommend using virtualenv (or similar tools if they are morefamiliar to you) over system-wide installation of python packages. It reducesthe amount of pain supporting multiple Python versions and allows you to testyour software in various configurations while you develop it, spinning up andtearing down environments as necessary.sudo apt install   python3-virtualenv   python-virtualenv # If you require Python2 compatCreate a virtualenv in an easy to remember or easy-to-search-shell-history-for location:$ virtualenv -p python3.6 $HOME/ocrd-venv3$ virtualenv -p python2.7 $HOME/ocrd-venv2 # If you require Python2 compatActivate virtualenvYou need to activate this virtual environment whenever you open a new terminal:$ source $HOME/ocrd-venv3/bin/activateIf you tend to forget sourcing the script before working on your code, addsource $HOME/ocrd-venv3 to the end of your .bashrc/.zshrc file and logout and back in.Install ocrd in virtualenv from pypiMake sure, the virtualenv is activated and install ocrd with pip:$ pip install ocrdGeneric setupIn this variant, you still need to install the ocrd Python package. But sinceit‚Äôs only used for its CLI (and as a depencency for Python-based OCR-Dsoftware), you can install it system-wide:$ sudo pip install ocrdSetup from sourceIf you want to build the ocrd package fromsource to stay up-to-date on unreleased changesor to contribute code, you can clone the repository and build from source:$ git clone https://github.com/OCR-D/core$ cd coreIf you are using the python setup:$ pip install -r requirements.txt$ pip install -e .If you are using the generic setup:$ sudo pip install -r requirements.txt$ sudo pip install .Verify setupAfter setting up, check that these commands do not throw errors and have theminimum version:$ git --version# Version 1.7 or higher?$ make --version# Version 9.0.1 or higher?$ ocrd --version# ocrd, version 0.4.0Anatomy of an OCR-D module project (MP)MP are git repositories with at least a description of the MP andits provided tools (ocrd-tool.json and aMakefile for installing the MP into a suitable OS.ocrd-tool.jsonThis is a JSON file that describes the software of a particular MP. It servesmainly three purposes:  providing a machine-actionable description of MP and the bundled tools andtheir parameters  concise human-targeted descriptions as the foundation for the applicationdocumentation  ensuring compatible definitions and interfaces, which is essential forsustainable, scalable workflowsThis document is mainly focusing on the first point.The structure and syntax of the ocrd-tool.json is defined by a JSONSchema and expects JSON Schemafor the parameter definitions. In addition to the schema, the ocrd commandline tool can help you validate the ocrd-tool.jsonMechanics of the ocrd-tool.json:fire: TODO :fire:  [kba] Wir brauchen einen besseren Namen, ich kann das schon nicht mehrschreiben dauernd, ocrd-tool.json. Vielleicht einfach manifest.json oderpackage.json oder tool-desc odr irgendwas.:fire: TODO :fire:The ocrd-tool.json has two conceptual levels:  Information about the MP as a whole and thepeople and processes involved  Technical metadata on the level of the individual toolsBeyond the ocrd-tool.json file, it is part of the requirements that the toolscan provide the section of the ocrd-tool.json about ‚Äòthemselves‚Äô at runtimewith the -J/--dump-json flags.The reason for this redundancy is to make the tools inspectable at runtime andto prevent ‚Äúfeature drift‚Äù where the  software evolves to the point where thedescription/documentation is out-of-date with the actual implementation.From a developer‚Äôs perspective, the easiest way to handle this is by bundlingthe ocrd-tool.json into your software, e.g. by the following pattern:  Store the ocrd-tool.json at a location where it is easy to deploy andaccess after installation  Symlink it to the root of the repository: ln -sr src/ocrd-tool.json .  Handle --dump-json by parsing the ocrd-tool.json and sending out therelevant section  Validate input and provide defaults basedon the JSON schema mechanicsMetadata about the module projectRequired properties are bold.  version: Version of the tool, adhering to SemanticVersioning  git_url: URL of the Github  tool: See next section  dockerhub: The project‚Äôs DockerHub URL  creators: :rotating_light: TODO  :rotating_light::  institution: :rotating_light: TODO  :rotating_light::  synopsis: :rotating_light: TODO  :rotating_light::Example:{  &quot;version&quot;: &quot;0.0.1&quot;,  &quot;name&quot;: &quot;ocrd-blockissifier&quot;,  &quot;synopsis&quot;: &quot;Tools for reasoning about how these blocks fit on this here page&quot;,  &quot;git_url&quot;: &quot;https://githbub.com/johndoe/ocrd_blocksifier&quot;,  &quot;dockerhub&quot;: &quot;https://hub.docker.com/r/johndoe/ocrd_blocksifier&quot;,  &quot;authors&quot;: [{    &quot;name&quot;: &quot;John Doe&quot;,    &quot;email&quot;: &quot;johndoe@ocr-corp.com&quot;,    &quot;url&quot;: &quot;johndoe.github.io&quot;  }],  &quot;bugs&quot;: {    &quot;url&quot;: &quot;https://github.com/sindresorhus/temp-dir/issues&quot;  },  &quot;tools&quot;: {      /* see next section */    }}Metadata about the toolsThe tools section is an object with the key being the name of the executable described and the value being an object with the following properties (bold means required):  executable: Name of the exceutable. Must match the key and start with ocrd-  parameters: Description of the parameters this tool accepts  description: Concise description what the tool does  categories: Tools belong to these categories, representing modules within the OCR-D project structure, list is part of the specs  steps: This tool can be used at these steps in the OCR-D functional model, list of values in the specsMetadata about parametersRequired properties are bold.  type: What kind of parameter this is, either a string, a number or a boolean  format: Subtype defining the syntax of the value such as float/integer for numbers or uri for string  required: If true, this parameter must be provided by the user  default: Default value if not required  enum: List of possible values if a fixed listrequired: true and setting default are mutually exclusive.MakefileAll MP should provide a Makefile with at least two targets: deps and install.make deps should install any dependencies, such as required python modules.make install should install the executable(s) into $(PREFIX)/bin.make test should start the unit/regression test suite if provided.Makefile for Python MPmake deps should install dependencies with pip.make install should call python setup.py install.See the makefile of the ocrd_kraken project for an example.Makefile for generic MPmake deps should install dependencies either by compiling from source or using apt-get.make install should  Copy the executables to $(PREFIX)/bin, creating $(PREFIX)/bin if necessary.  Copy any required files to $(PREFIX)/share/&amp;lt;name-of-the-package&amp;gt;, creating the latter if necessarySee the makefile of the ocrd_olena project for an example.ocrd workspace - Working with METSMETS is the container format of choice for OCR-D because it is widely used indigitzation workflows in cultural heritage institutions.A METS file references files in file groups and can contain a variety ofmetadata, the details can be found in the specs.From METS to WorkspaceWithin the OCR-D toolkit, we use the term ‚Äúworkspace‚Äù, a folder containing afile mets.xml and any number of the files referenced by the METS.One can think of the mets.xml as the MANIFEST of a JAR or the .git folderof a git repository.The workspace command of the ocrd tool allows various manipulations ofworkspaces and therefore METS files.Git similarity intendedThe workspace command‚Äôs syntax and mechanics are strongly inspired bygit so if you know git, this should be familiar.            git      ocrd workspace                  init      init              clone      clone              add      add              ls-files      find              fetch      find --download              archive      pack      Set the workspace to work onFor most commands, workspace assumes the workspace is the current workingdirectory. If you want to use a different directory, use the -d / --directory option# Listing files in the workspace at $PWD$ ocrd workspace find# Listing files in the workspace at $WORKSPACE_DIR$ ocrd workspace -d $WORKSPACE_DIR findUse another name than mets.xmlAccording to convention, the METS of a workspace is named mets.xml.To select a different basename for that file, use the -M / --mets-basename option:# Assume this workspace structure$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets3000.xml# This will fail in a loud and unpleasant manner$ ocrd workspace -d $WORKSPACE_DIR find# This will not$ ocrd workspace -d $WORKSPACE_DIR -M mets3000.xml findCreating an empty workspaceTo create an empty workspace to which you can add files, use the workspace init command$ ocrd workspace init ws1/home/ocr/ws1Load an existing METS as a workspaceTo create a workspace and save a METS file, use the workspace clone command:$ ocrd workspace clone $METS_URL new-workspace/home/ocr/new-workspace$ find new-workspacenew-workspacenew-workspace/mets.xmlLoad an existing METS and referenced files as a workspaceTo not only clone the METS but alsodownload the contained files, use workspace clone with the --download flag:$ ocrd workspace clone --download $METS_URL $WORKSPACE_DIR$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-GT-ALTO$WORKSPACE_DIR/OCR-D-GT-ALTO/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-GT-PAGE$WORKSPACE_DIR/OCR-D-GT-PAGE/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifNOTE: This will download all files, which can mean hundreds ofhigh-resolution images. If you want more fine-grained control,clone the bare workspaceand then use the workspace find command with the download flagSearching the files in a METSYou can search the files in a METS file with the workspace find command.  All files: ocrd workspace find  All TIFF files: ocrd workspace find --mimetype image/tiff  All TIFF files in the OCR-D-IMG-BIN group: ocrd workspace find --mimetype image/tiff --file-grp OCR-D-IMG-BINSee ocrd workspace --find for the full range of selection optionsDownloading/Copying files to the workspaceTo download remote or copy local files referenced in the mets.xml to theworkspace, append the --download flag to the workspace findcommand:# Clone Bare workspace:$ ocrd workspace clone $METS_URL$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml# Download all files in the `OCR-D-IMG` file group$ ocrd workspace -d $WORKSPACE_DIR find --file-grp OCR-D-IMG --download[...]$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifThe convention is that files will be downloaded to $WORKSPACE_DIR/$FILE_GROUP/$BASENAME where  $FILE_GROUP is the @USE attribute of the mets:fileGrp  $BASENAME is the last URL segment of the @xlink:href attribute of the mets:FLocatNOTE Downloading a file not only copies the file to the $WORKSPACE_DIRbut also changes the URL of the file from its original to the absolute filepath of the downloaded file.Adding files to the workspaceWhen running a module project, new files are created (PAGE XML, images ‚Ä¶). Toregister these new files, they need to be added to the mets.xml as amets:file with a mets:FLocat within a mets:fileGrp, each with the rightattributes. The workspace add command makes this possible:$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$ ocrd workspace -d $WORKSPACE_DIR add   --file-grp OCR-D-IMG-BIN   --file-id PAGE-0013-BIN   --mimetype image/png   --group-id PAGE-0013   page0013binarized.png$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$WORKSPACE_DIR/OCR-D-IMG-BIN/page0013binarized.tifValidating OCR-D compliant METSTo ensure a METS file and the workspace it describes adheres to the OCR-Dspecs, use the workspace validate command:# Create a bare workspaceocrd workspace init $WORKSPACE_DIR# Validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;METS has no unique identifier&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Oops, let&#39;s set the identifier ...$ ocrd workspace -d $WORKSPACE_DIR set-id &#39;scheme://my/identifier/syntax/kant_aufklaerung_1784&#39;# ... and add a file$ ocrd workspace -d $WORKSPACE_DIR add -G OCR-D-IMG-BIN -i PAGE-0013-BIN -m image/png -g PAGE-0013 page0013binarized.png# Validate again&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;ocrd tool ‚Äì Working with ocrd-tool.jsonThis command helps you to explore and validate the information in any ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json SUBCOMMANDocrd-tool validateValidate that an ocrd-tool.json is syntactically valid and adheres to the schema.This is useful while developing to make sure there are no typos and all required properties are set.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;steps&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;categories&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[] &#39;version&#39; is a required property&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;This example shows that the ocrd-wip-xyzzy executable is missing the required steps andcategories properties and the root level object is missing the versionproperty.Adding them should result in$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;Introspect an ocrd-tool.jsonThese commands are used for enumerating the executables contained in anocrd-tool.json and get root level metadata, such as the version.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json version0.0.1# Lists all the tools (executables) one per-line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json list-toolsocrd-wip-xyzzyocrd-wip-frobozzIntrospect individual toolsThis set of commands allows introspection of the metadata on individualtools within an ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json tool EXECUTABLE SUBCOMMAND$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy dump{  &quot;description&quot;: &quot;Nothing happens&quot;,  &quot;categories&quot;: [&quot;Text recognition and optimization&quot;, &quot;Arcane Magic&quot;],  &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],  &quot;exceutable&quot;: &quot;ocrd-wip-xyzzy&quot;}# Description$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy descriptionNothing happens# List categories one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy categoriesText recognition and optimizationArcane Magic# List steps one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy stepsrecognition/text-recognitionParse parametersThe details of how a tool is configured at run-time are determined byparameters. When a parameter file is passed to atool, it should:  ensure it is valid JSON  validate according to the parameter schema  add default values when no explicit values were providedThe ocrd ocrd-tool tool parse-params command does just that and can outputthe resulting default-enriched parameter as either JSON or as shell scriptassignments to evaluate:# Get JSON$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params --json -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;){  &quot;val1&quot;: 42,  &quot;val2&quot;: false,  &quot;val-with-default&quot;: 23}# Get back shell assignments to an associative array &quot;params&quot;$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;)params[&quot;val1&quot;]=&quot;42&quot;params[&quot;val2&quot;]=&quot;true&quot;params[&quot;val-with-default&quot;]=&quot;23&quot;ocrd process - Run a multi-step workflowOCR requires multiple steps, such as binarization, layout recognition, textrecognition etc. These steps are implemented with command line tools thatadhere to the same command line interface whichmakes it straightforward to chain these calls.For example, to run kraken binarization and tesseract block segmentation, one could execute:ocrd-kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BINocrd-tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.jsonThe disadvantage of individual calls is that it requires the user to check whether runs wereactually successful. To remedy this, users can use the ocrd process CLI which  simplifies the CLI syntax for multiple calls  checks for required and expected-to-be-produced file groups  checks for return value  sets logging levels uniformly across toolsThe same calls mentioned before can be passed to ocrd process as follows:ocrd process -l DEBUG   &quot;kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BIN&quot;   &quot;tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.json&quot;Wrapping a CLI using bashThis section describes how you can make an existing tool OCR-Dcompliant, i.e. provide a CLI which implements all the specs and callsout to another executable.For this purpose, the ocrd offers a bash library that handles:  command line option parsing  on-line help  parsing and providing defaults for parametersThe shell library is bundled with the ocrd command line tool and can be accessed with theocrd bashlib command.ocrd bashlibTo get the filename of the shell lib, use ocrd bashlib filename, which youcan employ to source the shell code in a wrapper script. After sourcing this scriptyou will have access to a number of shell functions that begin with ocrd__.The only function you definitely need is ocrd__wrap which parses anocrd-tool.json and scaffolds a spec-compliant CLI, parses command linearguments and parameters and lets the developer then react to the inputs.In combination with the ocrd workspace command this allowsyou to write CLI applications without touching any METS or PAGE/XML files by hand.ocrd__wrapocrd__wrap has this signature:ocrd__wrap OCRD_TOOL_JSON EXECUTABLE_NAME ...ARGSwhere  OCRD_TOOL_JSON is the path to the ocrd-tool.json  EXECUTABLE_NAME is the name of an executable within OCRD_TOOL_JSON  ...ARGS are 0..n command line arguments passed on from the userExample:   ocrd__wrap /usr/share/ocrd-wip/ocrd-tool.json ocrd-wip-xyzzy &quot;$@&quot;",
      "url": " /en/developer-guide.html"
    },
  

    {
      "slug": "en-dita-html",
      "title": "",
      "content"	 : "Only available in german",
      "url": " /en/dita.html"
    },
  

    {
      "slug": "de-dita-html",
      "title": "",
      "content"	 : "OCR-D: Anforderungsprofil f√ºr die Dokumentation der Modul Projekte  Allgemein:  Die Dokumentation der Tools und Schnittstellen betrifft sowohl die Anwendung selbst (Anwendungsdokumentation) als auch deren Anwendung von Nutzern (Benutzerdokumentation).Das OCR-D: Anforderungsprofil f√ºr die Dokumentation der Modul Projekte stellt nicht eine DITA-Einf√ºhrung oder DITA-Dokumentation dar, das gleiche betrifft die Markdown Auszeichnungssprache. In dieser Dokumentation werden erg√§nzende Informationen sowie unmittelbare Anforderungen f√ºr eine OCR-D konforme Dokumentation dargelegt.Adressaten der Dokumentation:Es sollen sowohl Techniker, die vor allem Informationen zur Anwendung ben√∂tigen (Installation, Wartung sowie Integration im Umfeld der eigenen Werkzeuge), als auch Benutzer, die das Werkzeug nutzen m√∂chten bzw. mit der Anwendung ein bestimmtes Ergebnis erzielen m√∂chten im Rahmen der Anwendungsdokumentation und Benutzerdokumentation informiert werden.Stil: Der Stil der Dokumentation sollte verst√§ndlich und in kurzen S√§tzen abgefasst sein. Die Dokumentation muss alle Aspekte der Anwendung und Benutzung umfassend beinhalten.Format: Die Dokumentation ist entweder im xmlbasierten DITA-Format oder im Auszeichnungsformat Markdown (Markdown-DITA-Syntax) abzufassen.Software: Zur Erstellung der Dokumentation wird ein Editor  (empfohlen wird ein Editor, mit XML-Unterst√ºtzung) sowie das DITA-OT (Open Toolkit) ben√∂tigt. N√§here Information finden sich unter: https://www.dita-ot.org/Die Erstellung der DokumentationDie Erstellung der Dokumentation erfolgt stufenweise.Die erste Stufe bildet die Dokumentation des Werkzeuges in Form der ocrd_tool.json (siehe https://ocr-d.github.io/ocrd_tool).In der zweiten Stufe werden manuell u. a. Funktionen, Parameter, Fehlerbehandlungen der Anwendung in Form einzelner Dateien entsprechend den folgenden Formatvorgaben abgefasst.StrukturvorgabenDie Dokumentation sollte wie folgt strukturell aufgebaut sein. Auf Grund der Adressaten der Dokumentation k√∂nnen Schwerpunkte unterschiedlich gesetzt werden.Zum Beispiel wird der Schwerpunkt auf die Benutzerdokumentation gelegt, sollte auf folgende Punkte geachtet werden:  Was kann man mit dem Tool machen? Welches Ergebnis ist von der Anwendung zu erwarten.  Wie wird die Anwendung bedient?  Welche Probleme und Fehlermeldungen k√∂nnen auftreten.Bei der Abfassung ist folgendem allgemeinem Aufbau zu folgen.StrukturvorgabenErstellungVorlagen1. Tool nameInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert2. Release notesmanuell erstellenreleaseNote.md3. Installationmanuell erstelleninstallation.md4. Simple tool descriptionInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert5. Descriptionmanuell erstellenDescription.md6. Optionmanuell erstellenOption.md7. Input format descriptionInhalt der ocrd_tool.jsonInputFormatDescription.md8. ParametersInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert9. Output format descriptionmanuell erstellenOutputFormatDescription.md10. Troubleshootingmanuell erstellenTroubleshooting.dita11. Resourcesmanuell erstellenResources.md12. Glossarmanuell erstellenGlossar.dita13. AuthorsInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert14. ReportingInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert15. CopyrightInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiertDie Dokumentation schreibenDas unmittelbare Schreiben stellt die zweite Stufe der Dokumentation dar. Anhand der Strukturvorgaben ist zu sehen, dass die Dokumentation nicht aus einer homogenen in sich geschlossenen Beschreibung besteht. Sondern einzelne Aspekte u. a. der Name des Werkzeuges, der Installations- und Bedienungsteil, Fehlerbetrachtungen und eventuell weiterf√ºhrende Hinweise Bestandteile oder Themenbereiche (Topics) der Dokumentation sind. Sowohl zur Schreibunterst√ºtzung als auch zum Lesen, der Ver√∂ffentlichung sowie der sp√§teren Pflege werden vom OCR-D Projekt diese spezifischen Formatvorgaben vorgenommen.DITA‚ÄúDie Darwin Information Typing Architecture (DITA) ist ein topic- und xmlbasiertes Dateiformat.‚Äù1 DITA ist ein OASIS-Standard (Organization for the Advancement of Structured Information Standards).2 DITA ist ein Format, das die Dokumentation bei der Erstellung, Verbreitung und (Wieder)verwendung unterst√ºtzen soll.TopicsMit Hilfe von Topics werden in sich inhaltlich geschlossene spezifische Bestandteile der Dokumentation gegliedert und typisiert. Allgemein beinhaltet ein Topic immer die Angabe eines Titels (&amp;lt;title&amp;gt;), den sogenannten Textk√∂rper (u. a. &amp;lt;body&amp;gt;) sowie beispielsweise einzelne Abs√§tze (&amp;lt;p&amp;gt;) oder Listen (&amp;lt;ul&amp;gt;,  &amp;lt;ol&amp;gt;). Das Topic wird in der Regel in einer Datei gespeichert.Folgende Topic-Typen stehen f√ºr die OCR-D Dokumentation zur Verf√ºgung. Die einzelnen Topic-Typen basieren auf eigenen formalen Dokumentspezifikationen. Die kurzen Beschreibungen in der Tabelle basieren auf der DITA-Spezifikation 1.3.[^3][^3]: siehe http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/dita-v1.3-errata01-os-part3-all-inclusive-complete.html#dita_ref_topic            Topic-Typ      Beschreibung      Konkordanz zur OCR-D Strukturvorgaben      Verweis                  General task      Das general task-Topic beinhaltet allgemein abgefasste Handlungsanweisungen. Diese k√∂nnen in einzelnen Abschnitten  angeordnet werden. Im Unterschied zum *strict task-Topic* k√∂nnen in diesem  verwendet werden. Die  beschreiben in einem umfangreichen Absatz den einzelnen Schritt mit dem jeweiligen Kontext.      3. Installation (alternative M√∂glichkeit)      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-generic-task-topic.html#dita_generic_task_topic              Task topic (strict task)      Das task topic (strict task) beinhaltet die Handlungsanweisungen die notwendig sind zur Bedienung des jeweiligen Werkzeuges. Dabei werden die einzelnen notwendigen Schritte klar in einzelnen  dokumentiert. Ein Schritt-Element  beinhaltet immer ein Komanndozeilen-Element .      3. Installation      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-task-topic.html#dita_task_topic              Concept      Das concept-Topic beinhaltet ma√ügebende Informationen, die zur Bedienung des Werkzeuges notwendig sind. Das Topic kann dabei notwendiges Hintergrundwissen f√ºr die Bedienung und den Umgang mit dem Werkzeug bieten sowie Definitionen oder Erkl√§rungen enthalten.      4. Simple tool description      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-concept-topic.html#dita_concept_topic              Reference      Das reference-Topic konzentriert sich auf die unmittelbaren Informationen des Werkzeuges oder einer spezifischen Schnittstelle. Mit dem Reference-Topic soll der Nutzer schnell und pr√§zise informiert werden.      5. Input format description, 6. Input Parameters, 7. Output format description, 8. Setting Parameters      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-reference-topic.html#dita_ref_topic              Troubleshooting      Das troubleshooting-Topic beinhalt Anweisungen zur Fehlerbehandlung. Dabei wird zuerst der Fehler oder die Symtome  beschrieben und im darauf folgenden L√∂sungsteil der Grund  f√ºr den Fehler benannt und abschlie√üend die L√∂sung  des Fehlers dokumentiert.      8. Troubleshooting      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-troubleshooting-topic.html#dita-troubleshooting-topic              Glossary entry      Im glossary entry-Topic wird die Bedeutung eines Begriffes oder Vorgehens definiert. Im  kann der zu definierende Term n√§her beschrieben werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossary-topic.html#glossaryArch              Glossary group      Im glossary group-Topic k√∂nnen die einzelnen Glossary entry-Topic zusammengefasst werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossarygroup-topic.html      Markdown DITA syntaxAlternativ zum DITA-XML-Markup kann Markdown zur Abfassung folgender Topic-Typen f√ºr das Schreiben der Dokumentation genutzt werden. Die einfache Auszeichnungssprache Markdown im Besonderen Markdown-DITA-Syntax ist entsprechend der Dokumentation des DITA-Open Toolkit http://www.dita-ot.org/3.0/topics/markdown-dita-syntax-reference.html zu verwenden.Folgende Topics werden in Markdown unterst√ºtzt:  concept  task (im Besonderen das Task topic: strict task)  referenceDie Topic-Typen:  troubleshooting  glossary group  glossary entrysind ausschlie√ülich in DITA zu schreiben.Beispiele f√ºr Topics in der jeweiligen spezifischen Syntax in Markdown-DITA-Syntax oder DITABeispiel: f√ºr ein Topic concept in Markdown-DITA-Syntax# Simple tool description {#toolDescription .concept}&quot;A command-line interface or command languageinterpreter (CLI), also known as command-line user interface,console user interface and character user interface (CUI), isa means of interacting with a computer program where the user(or client) issues commands to the program in the form ofsuccessive lines of text (command lines).&quot; Source: Wikipediacontributors. (2018, June 5). Command-line interface. InWikipedia, The Free Encyclopedia. Retrieved 12:45, June 6,2018, from [Wikipeadia](https://en.wikipedia.org/w/index.php?title=Command-line_interface&amp;amp;oldid=844566807)Beispiel: f√ºr ein Topic task in Markdown-DITA-Syntax# Installation {#installation .task}1.    erster Schritt2.    zweiter SchrittBeispiel: f√ºr ein Topic reference in Markdown-DITA-Syntax# Release Note {#releaseNote .reference}The Command Line Interface (CLI) is a maintenancerelease that fixes issues reported in OCR-D.## RequirementsThe CL can be used with all operating systems.Beispiel: f√ºr ein Topic troubleshooting in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE troubleshooting PUBLIC &quot;-//OASIS//DTD DITA 1.3 Troubleshooting//EN&quot; &quot;troubleshooting.dtd&quot;&amp;gt;&amp;lt;troubleshooting id=&quot;Troubleshooting&quot;&amp;gt;    &amp;lt;title&amp;gt;Troubleshooting&amp;lt;/title&amp;gt;    &amp;lt;troublebody&amp;gt;        &amp;lt;condition&amp;gt;            &amp;lt;title&amp;gt;Condition&amp;lt;/title&amp;gt;            &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;        &amp;lt;/condition&amp;gt;        &amp;lt;troubleSolution&amp;gt;            &amp;lt;cause&amp;gt;                &amp;lt;title&amp;gt;Cause&amp;lt;/title&amp;gt;                &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;            &amp;lt;/cause&amp;gt;            &amp;lt;remedy&amp;gt;                &amp;lt;title&amp;gt;Remedy&amp;lt;/title&amp;gt;                &amp;lt;responsibleParty&amp;gt;&amp;lt;/responsibleParty&amp;gt;                &amp;lt;steps&amp;gt;                    &amp;lt;step&amp;gt;                        &amp;lt;cmd&amp;gt;&amp;lt;/cmd&amp;gt;                    &amp;lt;/step&amp;gt;                &amp;lt;/steps&amp;gt;            &amp;lt;/remedy&amp;gt;        &amp;lt;/troubleSolution&amp;gt;    &amp;lt;/troublebody&amp;gt;&amp;lt;/troubleshooting&amp;gt;Beispiel: f√ºr ein Topic glossary group in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossgroup PUBLIC &quot;-//OASIS//DTD DITA Glossary Group//EN&quot; &quot;glossgroup.dtd&quot;&amp;gt;&amp;lt;glossgroup id=&quot;Glossar&quot;&amp;gt;    &amp;lt;title&amp;gt;Glossar&amp;lt;/title&amp;gt;    &amp;lt;glossentry id=&quot;txtline&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Textline&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;A TextLine is a block of text without line break.        &amp;lt;/glossdef&amp;gt;    &amp;lt;/glossentry&amp;gt;    &amp;lt;glossentry id=&quot;gt&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are        transcriptions, specific structure descriptions and word lists.        These are essentially available in PAGE XML format in        combination with the original image. Essential parts of         the GT were created manually.        &amp;lt;/glossdef&amp;gt;&amp;lt;/glossgroup&amp;gt;Beispiel: f√ºr ein Topic glossary entry in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossentry PUBLIC &quot;-//OASIS//DTD DITA Glossary//EN&quot; &quot;glossary.dtd&quot;&amp;gt;&amp;lt;glossentry id=&quot;gt&quot;&amp;gt;    &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;    &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are    transcriptions, specific structure descriptions and word lists.    These are essentially available in PAGE XML format in combination    with the original image. Essential parts of the GT were created    manually.    &amp;lt;/glossdef&amp;gt;&amp;lt;/glossentry&amp;gt;Technische Organisation der DokumenationTechnisch organisiert und zusammengefasst wird die DITA-Dokumentation mit einer sogenannten DITA-Map. Die DITA-Map √§hnelt einem Inhaltsverzeichnis, die die Topics auflistet. Die Topics sind in einzelnen Dateien gespeichert.Beispiel DITA-Map ocr-d.ditamap&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE map PUBLIC &quot;-//OASIS//DTD DITA Map//EN&quot; &quot;map.dtd&quot;&amp;gt;&amp;lt;map&amp;gt;&amp;lt;title&amp;gt;Titel der Dokumentation&amp;lt;/title&amp;gt;    &amp;lt;topicref href=&quot;releaseNote.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;installation.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;simpletoolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;toolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Option.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;InputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Parameters.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;OutputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Troubleshooting.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Glossar.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Authors.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Reporting.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Copyright.md&quot;/&amp;gt;&amp;lt;/map&amp;gt;Die Verwendung des DITA-Open Toolkits zur Publikation der DokumentationF√ºr die Generierung der Dokumentation ist die Kommandozeilen-Anwendung des DITA-Open Toolkits (http://www.dita-ot.org/3.0/topics/build-using-dita-command.html) zu verwenden.Mit dieser Anwendung k√∂nnen verschiedene Formate der Dokumentation erstellt werden. F√ºr die finale Dokumentation (Publikation) des OCR-D Moduls ist nur das Format DITA gefordert. Wird die Dokumentation in DITA geschrieben ist die Nutzung der Kommandozeilen-Anwendung nicht notwendig. Bei der Verwendung mit Markdown ist eine Konvertierung mit der Kommandozeilen-Anwendung  notwendig.Aber auch zur Korrektur oder zur Vollst√§ndigkeitskontrolle ist eine Konvertierung in ein Pr√§sentationsformat von Nutzem. Es k√∂nnen u. a. folgende Pr√§sentionsformate erstellt werden:  html5  pdf  troff  xhtmlBeispiel Kommandoaufruf f√ºr DITA-OT  F√ºr die Erstellung einer DITA-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=dita --output=output/dita       F√ºr die Erstellung einer html5-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=html5 --output=output/html5Impressum und und Datenschutzerkl√§rungFolgendes Impressum ist der Dokumentation anzuf√ºgen:Impressum und Datenschutzerkl√§rungNachstehend finden Sie die gesetzlich geregelten Pflichtangaben zur Anbieterkennzeichnung sowie rechtliche Hinweise zur Dokumentation des Modulprojektes: XXX des OCR-D Projektes.AnbieterAnbieter dieser Internetpr√§senz ist im Rechtssinne XXX[es folgt die Adresse][es folgt der Vertreter]Das Modul-Projekt wird vertreten durch XXX.[es folgt der Redaktionsverantwortliche mit Angabe der Persion und Adresse]Lizenz der DokumentationDie Dokumentation liegt unter dem xmlbasierten Format DITA [http://docs.oasis-open.org/dita/dita/v1.3/dita-v1.3-part3-all-inclusive.html] vor und kann unter der Creative Commons-Lizenz CC BY-SA 4.0 DE (https://creativecommons.org/licenses/by-sa/4.0/de/) genutzt werden.            siehe: Seite ‚ÄûDarwin Information Typing Architecture‚Äú. In: Wikipedia, Die freie Enzyklop√§die. Bearbeitungsstand: 5. April 2018, 15:34 UTC. URL: https://de.wikipedia.org/w/index.php?title=Darwin_Information_Typing_Architecture&amp;amp;oldid=175806494 (Abgerufen: 23. Mai 2018, 10:40 UTC)¬†&amp;#8617;              siehe https://de.wikipedia.org/wiki/Organization_for_the_Advancement_of_Structured_Information_Standards¬†&amp;#8617;      ",
      "url": " /de/dita.html"
    },
  

    {
      "slug": "en-spec-docker-html",
      "title": "",
      "content"	 : "Dockerfile provided by MPMP should provide aDockerfile that shouldresult in a container which bundles the tools developed by the MP alongwith all requirements.Based on ocrd:coreDocker containers should be based on the ocrd baseimage which itself is based on Ubuntu18.04. For one, this allows MP to use the ocrd tool to handle recurrent tasksin a spec-conformant way. Besides, it locally installed and containerizedCLI interchangeable.Shell entrypointNo CMD should be provided.No ENTRYPOINT should be provided.If CMD or ENTRYPOINT are provided, they should be empty arrays./data as volumeThe directory /data in the the container should be marked as a volume toallow processing host data in the container in a uniform way.ExampleFROM ocrd:coreVOLUME [&quot;/data&quot;]# RUN-commands to install requirements, build and install# e.g.# apt-get install -y curlENTRYPOINT []",
      "url": " /en/spec/docker.html"
    },
  

    {
      "slug": "de-spec-docker-html",
      "title": "",
      "content"	 : "Dockerfile provided by MPMP should provide aDockerfile that shouldresult in a container which bundles the tools developed by the MP alongwith all requirements.Based on ocrd:coreDocker containers should be based on the ocrd baseimage which itself is based on Ubuntu18.04. For one, this allows MP to use the ocrd tool to handle recurrent tasksin a spec-conformant way. Besides, it locally installed and containerizedCLI interchangeable.Shell entrypointNo CMD should be provided.No ENTRYPOINT should be provided.If CMD or ENTRYPOINT are provided, they should be empty arrays./data as volumeThe directory /data in the the container should be marked as a volume toallow processing host data in the container in a uniform way.ExampleFROM ocrd:coreVOLUME [&quot;/data&quot;]# RUN-commands to install requirements, build and install# e.g.# apt-get install -y curlENTRYPOINT []",
      "url": " /de/spec/docker.html"
    },
  

    {
      "slug": "en-faq-html",
      "title": "",
      "content"	 : "FAQGeneralWhere can I start my journey into the OCR-D ecosphere?Who is the target audience of OCR-D?OCR-D‚Äôs primary target audience are libraries and archives, digitizinghistorical prints at scale.Where can I get support on OCR-D?  Open an issue at the OCR-D/core repository  Chat with OCR-D project members and other OCR-D users in OCR-D‚Äôs chat room.  Send an email to ‚Ä¶What is the difference between OCR-D and ABBYY?ABBYY is a software developer producing the ABBYY Recognition Server whichoffers layout detection and text recognition with a pay-per-page pricing model.OCR-D is a project that integrates a wide variety of solutions for the fullgamut of possible OCR workflow steps. ABBYY is simple to use but offers fewoptions for customization whereas OCR-D workflows can be fine-tuned for bestrecognition of specific corpora. OCR-D has a strong focus on historical prints,trainable layout detection and text recognition and open interfaces toaccomodate future developments, whereas ABBYY performs more strongly for modernprint. Finally, OCR-D is a community effort with a strong focus on transparencyand Free Software.What is the difference between OCR-D and Tesseract?Tesseract is the leading FreeSoftware OCR solution and tightly integrated into OCR-D in both a technical andorganizational sense. Technically, Tesseract has been wrapped asocrd_tesserocr, an OCR-D-compliantprocessor that is more powerful than the command line tool bundled withTesseract. Organizationally, Tesseract maintainers and contributors have beenpart of the OCR-D project from the beginning and the originally OCR-D-developedTesseract training tooltesstrain has been adopted bythe wider Tesseract community.What is the difference between OCR-D and TRANSKRIBUS?TRANSKRIBUS is a software platform and serverinfrastructure to make it easier for Digital Humanities practicioners tocollaborate on Handwriting Text Recognition. Apart from the different use cases  historical prints for OCR-D, historical manuscripts for TRANSKRIBUS - thereare differences in philosophy: All components of OCR-D are freely available asApache-licensed Free Software whereas some core components of TRANSKRIBUS,particularly the recognition engine, are proprietary. TRANSKRIBUS is aserver-client architecture with an Eclipse-based graphical user interface atits core whereas OCR-D‚Äôs focus is on mass digitization and command lineinteraction.What is the difference between OCR-D and ocrd4all?Is OCR-D production-ready?Which formats are supported by OCR-D?OCR-D is primarily based around METS as a container format and PAGE-XML forlayout detection and text recognition results. Other OCR formats such as ALTO,hOCR or ABBYY FineReader XML are supported through conversion withocrd_fileformat.The preferred image format within OCR-D is TIFF but PNG and JPEG are alsosupported. JPEG2000 is not currently supported but can be added in the futureif there is demand for it.Why does OCR-D need METS files? How can I process images without METS?The processes within OCR-D are designed around for the simple reason that it issuch an ubiquitous and well-defined format used in libraries and archivesaround the world. By relying on a container format instead of just images,processors can make use of more information and can store detailed results in awell-defined fashion.If the data to be processed isn‚Äôt already described by a METS file, the ocrd command linetool offers simple ways to create new METS files or augment existing ones.How much does it cost to deploy OCR-D?OCR-D is Free Software, licensed under the terms of the Apache 2.0 license andwill be free to use and adapt in perpetuity.What are the system requirements for OCR-D-software?The OCR-D/core framework is fairly lightcompared with other interoperability platforms. System requirements thereforedepend on the actual processors to be used and the scale of the operation. Itis possible to use OCR-D on commodity hardware such as desktop PCs and laptopsbut can also be deployed to massive servers or even single-board computers.However, OCR workflows can be very memory-intensive, in particular when workingwith large neural network models that have to be loaded into memory. We recommendat least 16 GB of RAM to support even the most demanding workflow steps.Another bottleneck for OCR workflows is input/output. We recommend storing dataon SSD instead of HDD.CLIHow can I find out the version of OCR-D software?To find the version of the OCR-D/core framework installed, run the ocrd CLIwith the --version flag:$ ocrd --versionocrd, version 2.2.2All OCR-D processors also support the --version flag, e.g.:ocrd-tesserocr-recognize --versionVersion 0.7.0, ocrd/core 2.2.2How do I get help on ocrd CLI commands?Every command and subcommand of the ocrd CLI tool supports the --helpoption to print a description, arguments and options:ocrd --helpocrd workspace --helpocrd workspace add --helpHow do I get help on OCR-D processors?All OCR-D-compliant processors support the -h/--help flag as well:$ ocrd-tesserocr-recognize --helpHow can I specify parameters on the command line?Parameters to an OCR-D-compliant processor must be specified in the JSON syntax. The JSON datacan be passed to a processor with the -p CLI option, which can be either the filename of a file containing the JSON data or the JSON data itself:ocrd-tesseract-recognize -I IN -O OUT -p &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39;# same effect:echo  &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39; &amp;gt; /tmp/params.jsonocrd-tesseract-recognize -I IN -O OUT -p /tmp/params.jsonHow do I specify multiple input/output file groups?You can specify multiple file group names for both input and output by joiningthe names with a comma (,).ocrd-tesserocr-recognize -I DEFAULT,REGIONS -O OCR-TESSSERACTThis would instruct ocrd-tesserocr-recognize to take images from theDEFAULT group and region-segmented layout information from the REGIONSgroup.How to configure logging?OCR-D module project softwareWhere can I find official OCR-D module project software?Which third-party OCR-D-compatible software exists?Which processors are available?Workflows and processorsHow can I define workflows?Where can I find sample workflows to experiment with?How to handle failing workflows?Why do some processors have multiple input or output file groups?Where can I learn about the input and output file groups of a processor?How can I validate my workflow is correctly wired?Where can I learn about the parameters of a processor?ocrd_allWhat is ocrd_all?How to update ocrd_all?How to debug ocrd_all problems?I used sudo and now everything is brokenTrainingI want to train a custom OCR model. Where do I start?OCR-D-Ground TruthWhich of the three transcription levels specified in the [Transcription Guidelines] (https://ocr-d.github.io/gt//trans_documentation/transkription.html) was used for the GT of OCR-D?Are the three transcription levels designed hierarchically? Meaning, does level 3 include level 2 and level 1?I want to make some GT myself. Which level should I use? Can I mix levels?I have some transcriptions of early modern books, but I didn‚Äôt stick to the OCR-D GT guidelines. Would my transcriptions still be useful for OCR-D?",
      "url": " /en/faq.html"
    },
  

    {
      "slug": "en-spec-glossary-html",
      "title": "",
      "content"	 : "OCR-D Glossary  Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D frameworkThis section is non-normative.Layout and TypographyBlockA block is a region described by a polygon inside a page.Block typeThe semantics or function of a block such as heading, page number, column, table‚Ä¶Font familyWithin OCR-D, font family refers to grouping elements by font similarity. Thesemantics of a font family are up to the data producer.GlyphWithin OCR-D, a glyph is the atomic unit within a word.Grapheme ClusterSee GlyphLineSee TextLineReading OrderReading order describes the logical sequence of blocks within a document.RegionSee BlockSymbolSee GlyphTextLineA TextLine is a block of text without line break.WordA word is a sequence of glyphs not containing any word-bounding whitespace.DataGround TruthGround truth (GT) in the context of OCR-D aretranscriptions, specific structure descriptions and word lists. These areessentially available in PAGE XML format in combination with the originalimage. Essential parts of the GT were created manually.We distinguish different usage scenarios for GT:Reference dataWith the term reference data, we refer to data that illustratesdifferent stages of an OCR/OLR process on representative materials. They aresupposed to support the assessment of commonly encountered difficulties and challenges whenrunning certain analysis operations and are therefore manually annotatedat all levels.Evaluation dataEvaluation data are used to quantitatively evaluate the performance of OCR toolsand/or algorithms. Parts of these data which correspond to the tool(s) under considerationare guaranteed to be recorded manually.Training dataMany OCR-related tools need to be adapted to the specific domain of the works which are tobe processed. This domain adaptation is called training. Data used to guide this processare called training data. It is essential that those parts of these data which are fedto the training algorithm are captured manually.ActivitiesBinarizationBinarization means converting all color or grayscale pixels in an image to either black or white.Controlled term: binarized (comments of a mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)See Felix‚Äô Niklas interactive demoDewarpingManipulate an image in such a way that all text lines arestraightened and any geometrical distortions have been corrected.Controlled term: preprocessing/optimization/dewarpingSee Matt Zucker‚Äôs entry on Dewarping.DespecklingRemove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to remove ‚Äúsalt-and-pepper‚Äù noise resulting from Binarization.Controlled term: preprocessing/optimization/despecklingDeskewingRotate an image so that all text lines are horizontal.Controlled term: preprocessing/optimization/deskewingFont identificationDetect the font type(s) used in the document, either before or after an OCR run.Controlled term: recognition/font-identificationGrayscale normalization  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled term:  gray_normalized (comments in file)  preprocessing/optimization/cropping (step)Gray normalization is similar to binarization but instead of a purely bitonalimage, the output can also contain shades of gray to avoid inadvertentlycombining glyphs when they are very close together.Document analysisDocument analysis is the detection of structure on the document level to e.g. create a table of contents.Reading order detectionDetect the reading order of blocks.CroppingDetecting the print space in a page, as opposed to the margins. It is a form ofblock segmentation.Controlled term: preprocessing/optimization/cropping.Border removal‚Äì&amp;gt; CroppingSegmentationSegmentation means detecting areas within an image.Specific segmentation algorithms are labelled by the semantics of the regionsthey detect not the semantics of the input, i.e. an algorithm that detectsblocks is called block segmentation.Block segmentationSegment an image into blocks. Also determines whether this is a textor non-text block (e.g. images).Controlled term:  SEG-BLOCK (USE)  layout/segmentation/region (step)Block classificationDetermine the type of a detected block.Line segmentationSegment text blocks into textlines.Controlled term:  SEG-LINE (USE)  layout/segmentation/line (step)MPModule Project, a software project producing one or more tools to OCR-D. Thereare currently eight MP active in the OCR-Dcommunity.OCRMap pixel areas to glyphs and words.Word segmentationSegment a textline into wordsControlled term:  SEG-LINE (USE)  layout/segmentation/word (step)Glyph segmentationSegment a textline into glyphsControlled term: SEG-GLYPHText recognitionSee OCR.Text optimizationText optimization encompasses the manipulations to the text based on the stepsup to and including text recognition. This includes (semi-)automatically correctingrecognition errors, orthographical harmonization, fixing segmentation errors etc.Data PersistenceSoftware repositoryThe software repository contains all OCR-D algorithms and tools developedduring the project including tests. It will also contain the documentation andinstallation instructions for deploying a document analysis workflow.Ground Truth repositoryContains all the ground truth data.Research data repositoryThe research data repository may contain the results of allactivities during document analysis. At least it contains theend results of every processed document and its full provenance. The researchdata repository must be available locally.Model repositoryContains all trained (OCR) models for text recognition. The model repositoryhas to be available at least locally. Ideally, a publicly available model repository willbe developed.OCR-D modulesThe OCR-D project divided the various elements of an OCRworkflow into six modules.Image preprocessingManipulating the input images for subsequent layout analysis and text recognition.Layout analysisDetection of structure within the page.Text recognition and optimizationRecognition of text and post-correction of recognition errors.Model trainingGenerating data files from aligned ground truth text and images to configurethe prediction of text and layout recognition engines.Long-term preservation and persistenceStoring results of OCR and OLR indefinitely, taking into account versioning,multiple runs, provenance/parametrization and providing access to these savedsnapshots in a granular fashion.Quality assuranceProviding measures, algorithms and software to estimate the quality of the individual processes within the OCR-D domain.",
      "url": " /en/spec/glossary.html"
    },
  

    {
      "slug": "de-spec-glossary-html",
      "title": "",
      "content"	 : "OCR-D Glossary  Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D frameworkThis section is non-normative.Layout and TypographyBlockA block is a region described by a polygon inside a page.Block typeThe semantics or function of a block such as heading, page number, column, table‚Ä¶Font familyWithin OCR-D, font family refers to grouping elements by font similarity. Thesemantics of a font family are up to the data producer.GlyphWithin OCR-D, a glyph is the atomic unit within a word.Grapheme ClusterSee GlyphLineSee TextLineReading OrderReading order describes the logical sequence of blocks within a document.RegionSee BlockSymbolSee GlyphTextLineA TextLine is a block of text without line break.WordA word is a sequence of glyphs not containing any word-bounding whitespace.DataGround TruthGround truth (GT) in the context of OCR-D aretranscriptions, specific structure descriptions and word lists. These areessentially available in PAGE XML format in combination with the originalimage. Essential parts of the GT were created manually.We distinguish different usage scenarios for GT:Reference dataWith the term reference data, we refer to data that illustratesdifferent stages of an OCR/OLR process on representative materials. They aresupposed to support the assessment of commonly encountered difficulties and challenges whenrunning certain analysis operations and are therefore manually annotatedat all levels.Evaluation dataEvaluation data are used to quantitatively evaluate the performance of OCR toolsand/or algorithms. Parts of these data which correspond to the tool(s) under considerationare guaranteed to be recorded manually.Training dataMany OCR-related tools need to be adapted to the specific domain of the works which are tobe processed. This domain adaptation is called training. Data used to guide this processare called training data. It is essential that those parts of these data which are fedto the training algorithm are captured manually.ActivitiesBinarizationBinarization means converting all color or grayscale pixels in an image to either black or white.Controlled term: binarized (comments of a mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)See Felix‚Äô Niklas interactive demoDewarpingManipulate an image in such a way that all text lines arestraightened and any geometrical distortions have been corrected.Controlled term: preprocessing/optimization/dewarpingSee Matt Zucker‚Äôs entry on Dewarping.DespecklingRemove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to remove ‚Äúsalt-and-pepper‚Äù noise resulting from Binarization.Controlled term: preprocessing/optimization/despecklingDeskewingRotate an image so that all text lines are horizontal.Controlled term: preprocessing/optimization/deskewingFont identificationDetect the font type(s) used in the document, either before or after an OCR run.Controlled term: recognition/font-identificationGrayscale normalization  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled term:  gray_normalized (comments in file)  preprocessing/optimization/cropping (step)Gray normalization is similar to binarization but instead of a purely bitonalimage, the output can also contain shades of gray to avoid inadvertentlycombining glyphs when they are very close together.Document analysisDocument analysis is the detection of structure on the document level to e.g. create a table of contents.Reading order detectionDetect the reading order of blocks.CroppingDetecting the print space in a page, as opposed to the margins. It is a form ofblock segmentation.Controlled term: preprocessing/optimization/cropping.Border removal‚Äì&amp;gt; CroppingSegmentationSegmentation means detecting areas within an image.Specific segmentation algorithms are labelled by the semantics of the regionsthey detect not the semantics of the input, i.e. an algorithm that detectsblocks is called block segmentation.Block segmentationSegment an image into blocks. Also determines whether this is a textor non-text block (e.g. images).Controlled term:  SEG-BLOCK (USE)  layout/segmentation/region (step)Block classificationDetermine the type of a detected block.Line segmentationSegment text blocks into textlines.Controlled term:  SEG-LINE (USE)  layout/segmentation/line (step)MPModule Project, a software project producing one or more tools to OCR-D. Thereare currently eight MP active in the OCR-Dcommunity.OCRMap pixel areas to glyphs and words.Word segmentationSegment a textline into wordsControlled term:  SEG-LINE (USE)  layout/segmentation/word (step)Glyph segmentationSegment a textline into glyphsControlled term: SEG-GLYPHText recognitionSee OCR.Text optimizationText optimization encompasses the manipulations to the text based on the stepsup to and including text recognition. This includes (semi-)automatically correctingrecognition errors, orthographical harmonization, fixing segmentation errors etc.Data PersistenceSoftware repositoryThe software repository contains all OCR-D algorithms and tools developedduring the project including tests. It will also contain the documentation andinstallation instructions for deploying a document analysis workflow.Ground Truth repositoryContains all the ground truth data.Research data repositoryThe research data repository may contain the results of allactivities during document analysis. At least it contains theend results of every processed document and its full provenance. The researchdata repository must be available locally.Model repositoryContains all trained (OCR) models for text recognition. The model repositoryhas to be available at least locally. Ideally, a publicly available model repository willbe developed.OCR-D modulesThe OCR-D project divided the various elements of an OCRworkflow into six modules.Image preprocessingManipulating the input images for subsequent layout analysis and text recognition.Layout analysisDetection of structure within the page.Text recognition and optimizationRecognition of text and post-correction of recognition errors.Model trainingGenerating data files from aligned ground truth text and images to configurethe prediction of text and layout recognition engines.Long-term preservation and persistenceStoring results of OCR and OLR indefinitely, taking into account versioning,multiple runs, provenance/parametrization and providing access to these savedsnapshots in a granular fashion.Quality assuranceProviding measures, algorithms and software to estimate the quality of the individual processes within the OCR-D domain.",
      "url": " /de/spec/glossary.html"
    },
  

    {
      "slug": "en-imprint-html",
      "title": "",
      "content"	 : "Imprint and Privacy PolicyBelow you will find the legally regulated mandatory information on provideridentification and legal information on the internet presence of the OCR-Dproject.ProviderThe provider of this website is the Duke August Library Wolfenb√ºttel in the legal sense.Duke August LibraryLessingplatz 1D-38304 Wolfenb√ºttelPhone: +49 (0) 5331 808-0Fax: +49 (0) 5331 808-302e-mail: auskunft@hab.dewww.hab.deRepresentativesHAB Wolfenb√ºttel is represented by its Director, Prof. Dr. Peter Burschel.Editor-in-ChiefAndrea OpitzDuke August LibraryLessingplatz 1D-38304 Wolfenb√ºttelPhone: +49 (0) 5331 808-303e-mail: opitz@hab.deLegal Notice on CopyrightThe used logo as well as the other contents are protected by copyright. Othergraphics were taken from Pixabay and are licensed underCC0.DisclaimerContent of the Online OfferThe author assumes no liability for the topicality, correctness, completenessor quality of the information provided. Liability claims against the authorrelating to material or immaterial damage caused by the use or non-use of theinformation provided or by the use of incorrect or incomplete information areexcluded, unless there is evidence of willful intent or gross negligence on thepart of the author.All offers are subject to change and non-binding. The author expressly reservesthe right to change, supplement or delete parts of the pages or the entireoffer without prior notice or to discontinue publication temporarily orpermanently.References and LinksThe author is not responsible for any contents linked or referred to from hispages ‚Äì unless he has full knowledge of illegal contents and would technicallybe able to prevent the visitors of his site from viewing those pages.The author hereby expressly declares that at the time the links were created,no illegal content was recognizable on the linked pages. The author has noinfluence whatsoever on the current and future design, content or authorship ofthe linked pages. The author therefore expressly distances himself from allcontents of all linked pages that have been changed since the link was created.This statement applies to all links and references set within the author‚Äôs ownInternet offer as well as to third-party entries in guest books, discussionforums and mailing lists set up by the author. For illegal, incorrect orincomplete contents and in particular for damages arising from the use ornon-use of such information, the provider of the page to which reference ismade is solely liable, not the party who merely refers to the respectivepublication via links.**Copyright and Trademark LawThe author endeavours to observe the copyrights of the graphics, sound documents, video sequences and texts used in all publications, to use graphics, sound documents, video sequences and texts created by himself or to make use of licence-free graphics, sound documents, video sequences and texts.All brands and trademarks mentioned within the Internet offer and possibly protected by third parties are subject without restriction to the provisions of the applicable trademark law and the ownership rights of the respective registered owners. The mere fact that a trademark is mentioned should not lead to the conclusion that it is not protected by the rights of third parties!The copyright for published objects created by the author himself remains solely with the author of the pages. Any duplication or use of objects such as diagrams, sounds or texts in other electronic or printed publications is not permitted without the author‚Äôs agreement.Data ProtectionAs part of its participation in the German Library Statistics, the HerzogAugust Bibliothek uses a procedure for counting visits to its website. Whencalling up the homepage and/or the catalogue page (‚ÄúOPAC‚Äù), an identifier ofthe library and the page concerned, the time of the call and a signature of thecalling computer are stored. The signature is generated using a one-wayfunction consisting of IP address, browser identification and proxyinformation. The collection, storage and evaluation of this data takes place inpseudoanonymised form, i.e. the signature cannot be assigned to specificpersons. The IP address in particular is not stored. The pseudonymousindividual data are summed up after 24 hours at the latest and thus anonymised.The stored data is statistically evaluated in order to design and furtherdevelop our website in line with requirements. The data will only be used forthis purpose and will be deleted after evaluation.The Stuttgart Media University is responsible for the technical andorganisational implementation of the procedure. It has developed the procedurein accordance with the provisions of the Data Protection Act and the TelemediaAct (TMG) and undertakes to comply with them.You have the opportunity to object to the collection, processing and use of theaforementioned data of your visits for the aforementioned purpose. Please clickon the following externalLink.If the opportunity for the input of personal or business data (email addresses,name, addresses) is given, the input of these data takes place voluntarily. Theuse and payment of all offered services are permitted ‚Äì if and so fartechnically possible and reasonable ‚Äì without specification of any personaldata or under specification of anonymized data or an alias.Legal Validity of this DisclaimeThis disclaimer is to be regarded as part of the internet publication which youwere referred from. If sections or individual terms of this statement are notlegal or correct, the content or validity of the other parts remainuninfluenced by this fact.(This disclaimer is based on the text of www.disclaimer.de].Unless otherwise stated, images are taken from Pixabay.",
      "url": " /en/imprint.html"
    },
  

    {
      "slug": "en-gt-guidelines",
      "title": "",
      "content"	 : "                                                         Guidelines for the Ground Truth Transcription            Guidelines for the Ground Truth Transcription                           The Ground-Truth-Guidelines            Conventions for these Guidelines            Transcription                  Level 1                  Level 2                  Level 3                  Fundamentals of the Transcription                        How to Transcribe in Level 1                        How to Transcribe in Level 2                        How to Transcribe in Level 3                                                         Spellings and Symbols                        Distinction between I and J                              Level 1                              Level 2 and 3                                                                           Distinction between u/f, u/v and v/u                              Level 1                              Level 2 and 3                                                                           s-Graphemes                              Level 1                              Level 2 and 3                                                                           r-Graphemes                              Level 1                              Level 2 and 3                                                                           Ligatures                              Level 1                              Level 2                              Level 3                                                                           Umlauts                              Level 1                              Level 2                              Level 3                                                                           Abbreviation Lines                              Level 1                              Level 2                              Level 3                                                                           Diacritics                              Level 1                              Level 2                              Level 3                                                                           Hyphenation                              Level 1                              Level 2 and 3                                                                                                            Numbers                        Fractions                        Roman Numerals                        Superscript Numbers                        Subscript Numbers                                                         Tables                  Handwritten annotations                  Punctuation Basic Rules                        Dash                        Quotation Marks                        Spaces                              Level 1 and Level 2                              Level 3                              Comparison between Level 1, 2 and Level 3                                                                                                            Overviews and Examples                        OCR-D Coordination Project Coding                        Alphabets, Abbreviations and Special Characters                         Ligatures                                                                              Layout and Structure                  General Information                  Print Space                  Page Margin                  ReadingOrder                  Typographical Peculiarities                        Typeface (TextStyle)                        Ligatures (Level 1 and 2)                                                         First Step : The Page Types                  Second Step: Page Regions                        Level 1                        Level 2                        Relations                        TextRegion                               Paragraph                              Heading                              Column header (header)                              Page-number                              Marginalia                              Footnote (footnote / footnote-continued / endnote)                              Initial (drop-capital)                              Signature mark                              Catch-word                              Floating Elements in the Print Space (floating)                              Table of Content (TOC-entry)                                                                           Illustrations, photos (ImageRegion)                        Book decoration, drawings (GraphicRegion)                        Separation Lines, Separators (SeparatorRegion)                              Level 1                              Level 2                                                                           Tables (TableRegion)                        Mathematical characters (MathsRegion)                        Chemical symbols (ChemRegion)                        Notes (MusicRegion)                        Advertisement (AdvertRegion)                        Damage, Dirt, Stains, Noise (NoiseRegion)                        Other (UnknownRegion)                                                                              Documentation of the OCR-D Structure Ground Truth                  Definition                  Background                  Overview and Concordance                  Structure concordance METS and PAGE                                       Documentation of the PAGE XML Format for Page Content                  Main Schema                        Main schema pagecontent.xsd                                                         Element                        Element pc:PcGts                                                         Complex Type                        Complex Type pc:PcGtsType                        Complex Type pc:MetadataType                        Complex Type pc:UserDefinedType                        Complex Type pc:UserAttributeType                        Complex Type pc:MetadataItemType                        Complex Type pc:LabelsType                        Complex Type pc:LabelType                        Complex Type pc:PageType                        Complex Type pc:AlternativeImageType                        Complex Type pc:BorderType                        Complex Type pc:CoordsType                        Complex Type pc:PrintSpaceType                        Complex Type pc:ReadingOrderType                        Complex Type pc:OrderedGroupType                        Complex Type pc:RegionRefIndexedType                        Complex Type pc:OrderedGroupIndexedType                        Complex Type pc:UnorderedGroupIndexedType                        Complex Type pc:RegionRefType                        Complex Type pc:UnorderedGroupType                        Complex Type pc:LayersType                        Complex Type pc:LayerType                        Complex Type pc:RelationsType                        Complex Type pc:RelationType                        Complex Type pc:TextRegionType                        Complex Type pc:RegionType                        Complex Type pc:RolesType                        Complex Type pc:TableCellRoleType                        Complex Type pc:ImageRegionType                        Complex Type pc:LineDrawingRegionType                        Complex Type pc:GraphicRegionType                        Complex Type pc:TableRegionType                        Complex Type pc:ChartRegionType                        Complex Type pc:SeparatorRegionType                        Complex Type pc:MathsRegionType                        Complex Type pc:ChemRegionType                        Complex Type pc:MusicRegionType                        Complex Type pc:AdvertRegionType                        Complex Type pc:NoiseRegionType                        Complex Type pc:UnknownRegionType                        Complex Type pc:CustomRegionType                        Complex Type pc:GridType                        Complex Type pc:GridPointsType                        Complex Type pc:TextLineType                        Complex Type pc:BaselineType                        Complex Type pc:WordType                        Complex Type pc:GlyphType                        Complex Type pc:GraphemesType                        Complex Type pc:GraphemeType                        Complex Type pc:GraphemeBaseType                        Complex Type pc:TextEquivType                        Complex Type pc:NonPrintingCharType                        Complex Type pc:GraphemeGroupType                        Complex Type pc:TextStyleType                        Complex Type pc:MapRegionType                                                         Simple Type                        Simple Type pc:ConfSimpleType                        Simple Type pc:PointsType                        Simple Type pc:GroupTypeSimpleType                        Simple Type pc:ColourSimpleType                        Simple Type pc:ChartTypeSimpleType                        Simple Type pc:GraphicsTypeSimpleType                        Simple Type pc:ColourDepthSimpleType                        Simple Type pc:TextDataTypeSimpleType                        Simple Type pc:ScriptSimpleType                        Simple Type pc:ProductionSimpleType                        Simple Type pc:LanguageSimpleType                        Simple Type pc:ReadingDirectionSimpleType                        Simple Type pc:TextTypeSimpleType                        Simple Type pc:TextLineOrderSimpleType                        Simple Type pc:AlignSimpleType                        Simple Type pc:PageTypeSimpleType                                                                              Page XML Extensions                  Exif-PageXML Konkordanz                                       Imprint                  ",
      "url": " /en/gt-guidelines/"
    },
  

    {
      "slug": "en",
      "title": "",
      "content"	 : "---layout: pagelang: delang-ref: ./site/en/index.html------layout: pagelang: delang-ref: ./site/en/index.html------layout: pagelang: delang-ref: ./site/en/index.html------layout: pagelang: delang-ref: ./site/en/index.html------layout: pagelang: delang-ref: ./site/en/index.html------layout: homelang: enlang-ref: home---                                                                                                  Developers / System Administrators          Start here if you want to want to to contribute to OCR-D development                                                                      System Librarians / Digitization experts          Start here if you are ready to start using OCR-D in your institution                                                                      Decision Makers / Strategists          Start here if you want to find out how OCR-D can help your institution and grow your business                                                    ",
      "url": " /en/"
    },
  

    {
      "slug": "de-gt-guidelines",
      "title": "",
      "content"	 : "                                                         Guidelines for the Ground Truth Transcription            Guidelines for the Ground Truth Transcription                           The Ground-Truth-Guidelines            Conventions for these Guidelines            Transcription                  Level 1                  Level 2                  Level 3                  Fundamentals of the Transcription                        How to Transcribe in Level 1                        How to Transcribe in Level 2                        How to Transcribe in Level 3                                                         Spellings and Symbols                        Distinction between I and J                              Level 1                              Level 2 and 3                                                                           Distinction between u/f, u/v and v/u                              Level 1                              Level 2 and 3                                                                           s-Graphemes                              Level 1                              Level 2 and 3                                                                           r-Graphemes                              Level 1                              Level 2 and 3                                                                           Ligatures                              Level 1                              Level 2                              Level 3                                                                           Umlauts                              Level 1                              Level 2                              Level 3                                                                           Abbreviation Lines                              Level 1                              Level 2                              Level 3                                                                           Diacritics                              Level 1                              Level 2                              Level 3                                                                           Hyphenation                              Level 1                              Level 2 and 3                                                                                                            Numbers                        Fractions                        Roman Numerals                        Superscript Numbers                        Subscript Numbers                                                         Tables                  Handwritten annotations                  Punctuation Basic Rules                        Dash                        Quotation Marks                        Spaces                              Level 1 and Level 2                              Level 3                              Comparison between Level 1, 2 and Level 3                                                                                                            Overviews and Examples                        OCR-D Coordination Project Coding                        Alphabets, Abbreviations and Special Characters                         Ligatures                                                                              Layout and Structure                  General Information                  Print Space                  Page Margin                  ReadingOrder                  Typographical Peculiarities                        Typeface (TextStyle)                        Ligatures (Level 1 and 2)                                                         First Step : The Page Types                  Second Step: Page Regions                        Level 1                        Level 2                        Relations                        TextRegion                               Paragraph                              Heading                              Column header (header)                              Page-number                              Marginalia                              Footnote (footnote / footnote-continued / endnote)                              Initial (drop-capital)                              Signature mark                              Catch-word                              Floating Elements in the Print Space (floating)                              Table of Content (TOC-entry)                                                                           Illustrations, photos (ImageRegion)                        Book decoration, drawings (GraphicRegion)                        Separation Lines, Separators (SeparatorRegion)                              Level 1                              Level 2                                                                           Tables (TableRegion)                        Mathematical characters (MathsRegion)                        Chemical symbols (ChemRegion)                        Notes (MusicRegion)                        Advertisement (AdvertRegion)                        Damage, Dirt, Stains, Noise (NoiseRegion)                        Other (UnknownRegion)                                                                              Documentation of the OCR-D Structure Ground Truth                  Definition                  Background                  Overview and Concordance                  Structure concordance METS and PAGE                                       Documentation of the PAGE XML Format for Page Content                  Main Schema                        Main schema pagecontent.xsd                                                         Element                        Element pc:PcGts                                                         Complex Type                        Complex Type pc:PcGtsType                        Complex Type pc:MetadataType                        Complex Type pc:UserDefinedType                        Complex Type pc:UserAttributeType                        Complex Type pc:MetadataItemType                        Complex Type pc:LabelsType                        Complex Type pc:LabelType                        Complex Type pc:PageType                        Complex Type pc:AlternativeImageType                        Complex Type pc:BorderType                        Complex Type pc:CoordsType                        Complex Type pc:PrintSpaceType                        Complex Type pc:ReadingOrderType                        Complex Type pc:OrderedGroupType                        Complex Type pc:RegionRefIndexedType                        Complex Type pc:OrderedGroupIndexedType                        Complex Type pc:UnorderedGroupIndexedType                        Complex Type pc:RegionRefType                        Complex Type pc:UnorderedGroupType                        Complex Type pc:LayersType                        Complex Type pc:LayerType                        Complex Type pc:RelationsType                        Complex Type pc:RelationType                        Complex Type pc:TextRegionType                        Complex Type pc:RegionType                        Complex Type pc:RolesType                        Complex Type pc:TableCellRoleType                        Complex Type pc:ImageRegionType                        Complex Type pc:LineDrawingRegionType                        Complex Type pc:GraphicRegionType                        Complex Type pc:TableRegionType                        Complex Type pc:ChartRegionType                        Complex Type pc:SeparatorRegionType                        Complex Type pc:MathsRegionType                        Complex Type pc:ChemRegionType                        Complex Type pc:MusicRegionType                        Complex Type pc:AdvertRegionType                        Complex Type pc:NoiseRegionType                        Complex Type pc:UnknownRegionType                        Complex Type pc:CustomRegionType                        Complex Type pc:GridType                        Complex Type pc:GridPointsType                        Complex Type pc:TextLineType                        Complex Type pc:BaselineType                        Complex Type pc:WordType                        Complex Type pc:GlyphType                        Complex Type pc:GraphemesType                        Complex Type pc:GraphemeType                        Complex Type pc:GraphemeBaseType                        Complex Type pc:TextEquivType                        Complex Type pc:NonPrintingCharType                        Complex Type pc:GraphemeGroupType                        Complex Type pc:TextStyleType                        Complex Type pc:MapRegionType                                                         Simple Type                        Simple Type pc:ConfSimpleType                        Simple Type pc:PointsType                        Simple Type pc:GroupTypeSimpleType                        Simple Type pc:ColourSimpleType                        Simple Type pc:ChartTypeSimpleType                        Simple Type pc:GraphicsTypeSimpleType                        Simple Type pc:ColourDepthSimpleType                        Simple Type pc:TextDataTypeSimpleType                        Simple Type pc:ScriptSimpleType                        Simple Type pc:ProductionSimpleType                        Simple Type pc:LanguageSimpleType                        Simple Type pc:ReadingDirectionSimpleType                        Simple Type pc:TextTypeSimpleType                        Simple Type pc:TextLineOrderSimpleType                        Simple Type pc:AlignSimpleType                        Simple Type pc:PageTypeSimpleType                                                                              Page XML Extensions                  Exif-PageXML Konkordanz                                       Imprint                  ",
      "url": " /de/gt-guidelines/"
    },
  

    {
      "slug": "de",
      "title": "",
      "content"	 : "---layout: pagelang: delang-ref: ./site/de/index.html------layout: pagelang: delang-ref: ./site/de/index.html------layout: pagelang: delang-ref: ./site/de/index.html------layout: pagelang: delang-ref: ./site/de/index.html------layout: pagelang: delang-ref: ./site/de/index.html------layout: homelang: delang-ref: home---                                                                                                  Entwickler / Sysadmins          Finden Sie hier alles, um Teil der OCR-D Entwickler-Community zu werden                                                                      System Librarians / Digitization experts          Start here if you are ready to start using OCR-D in your institution                                                                      Decision Makers / Strategists          Start here if you want to find out how OCR-D can help your institution and grow your business                                                    ",
      "url": " /de/"
    },
  

    {
      "slug": "",
      "title": "",
      "content"	 : "                                                      Deutsch                                                                                          English                                                ",
      "url": " /"
    },
  

    {
      "slug": "en-spec",
      "title": "",
      "content"	 : "            Intro      Overview of OCR-D technical documentation                  CLI      Command line tools provided by MP                  METS      OCR-D METS conventions                  OCRD-ZIP      METS workspace serialized as ZIP                  ocrd-tool.json      OCR-D tool description                  PAGE      PAGE conventions                  Dockerfile      OCR-D Dockerfile conventions      ",
      "url": " /en/spec/"
    },
  

    {
      "slug": "de-spec",
      "title": "",
      "content"	 : "            Intro      √úbersicht √ºber die technische Dokumentation zu OCR-D                  CLI      Anforderungen an Kommandozeilentools                  METS      OCR-D METS Konventionen                  OCRD-ZIP      ZIP-Serialisierung METS-basierter Workspaces                  ocrd-tool.json      Die Beschreibungssprache der OCR-D Werkzeuge                  PAGE      PAGE conventions                  Dockerfile      OCR-D Dockerfile conventions      ",
      "url": " /de/spec/"
    },
  

    {
      "slug": "en-installation-html",
      "title": "",
      "content"	 : "Installation OCR-D stackNotationCommands like thissudo apt-get install python3must be executed in a terminal.Basic setupOperating SystemCurrently we only support Ubuntu 18.04 and 18.10.PythonYou need to have Python 3.6 installed:sudo apt-get install python3 python3-virtualenvSetup a virtualenvWe set up a dedicated environment to install Python packages to:python3 -m virtualenv ~/env-ocrdActivate virtualenvsource ~/env-ocrd/bin/activateInstall ocrd core softwarepip install ocrdVerify it has been installed by calling the ocrd command line tool:ocrd --helpOutput should be similar toUsage: ocrd [OPTIONS] COMMAND [ARGS]...  CLI to OCR-DOptions:  --version                       Show the version and exit.  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                  Log level  --help                          Show this message and exit.Commands:  bashlib    Work with bash library  ocrd-tool  Work with ocrd-tool.json JSON_FILE  process    Process a series of tasks  workspace  Working with workspace  zip        Bag/Spill/Validate OCRD-ZIP bagsOCR-D ModulesFunctionality is encapsulated in module projects which can be individually installed and combined.We recommend the following projects to get started:  ocrd_ocropy  ocrd_kraken  ocrd_tesserocrocrd_ocropypip install ocrd_ocropyVerify:ocrd-ocropy-segment --helpocrd_krakenpip install ocrd_ocropyVerify:ocrd-kraken-binarize --helpocrd_tesserocrocrd_tesserocr requires tesseract to be installed in addition to the module project:sudo apt-get install libtesseract-dev tesseract-ocrpip install ocrd_tesseroccrVerify:ocrd-tesserocr-recognize --help",
      "url": " /en/installation.html"
    },
  

    {
      "slug": "en-spec-intro-html",
      "title": "",
      "content"	 : "OCR-D Specs OverviewSince OCR-D focuses on improving access to mass digitization for historicalprints, it is important that its tools are sufficiently uniform in their interfacesand data access patterns to support the widest possible application withinGLAM digitization workflows.This website lays out a set of conventions and interface definitions that mustbe implemented by the OCR-D module projects (MP) to be usable within the OCR-D ecosphere.CLISoftware developed by MP must be executable with acommand line interface (CLI) on a Linux OS. CLI are straightforward to run andtest and can be easily embedded in automated setups. The mechanics of OCR-Dconformant CLI tools are laid out in the CLI specs.METSTo allow processing OCR-related data in a digitization workflow, a uniform dataexchange format is necessary. OCR-D decided to use the widely used METS formatand has developed conventions on how MP must access and manipulateMETS data in order to be interoperable.ocrd-tool.jsonInteroperability needs metadata, both descriptive and technical. OCR-D hasdeveloped a format that allows MP to express general informationabout themselves and detailed information about the tools they develop.RESTOCR-D will offer RESTful access to the MP CLI based on HTTP, usingthe Open API / Swagger set of tools.DockerfileDocker is a widely used system for containerization of software. MPs areencouraged to package the tools they develop as a docker image by providing aDockerfile. OCR-D offers recommendations on how the Dockerfile should bestructured.",
      "url": " /en/spec/intro.html"
    },
  

    {
      "slug": "de-spec-intro-html",
      "title": "",
      "content"	 : "OCR-D Specs OverviewSince OCR-D focuses on improving access to mass digitization for historicalprints, it is important that its tools are sufficiently uniform in their interfacesand data access patterns to support the widest possible application withinGLAM digitization workflows.This website lays out a set of conventions and interface definitions that mustbe implemented by the OCR-D module projects (MP) to be usable within the OCR-D ecosphere.CLISoftware developed by MP must be executable with acommand line interface (CLI) on a Linux OS. CLI are straightforward to run andtest and can be easily embedded in automated setups. The mechanics of OCR-Dconformant CLI tools are laid out in the CLI specs.METSTo allow processing OCR-related data in a digitization workflow, a uniform dataexchange format is necessary. OCR-D decided to use the widely used METS formatand has developed conventions on how MP must access and manipulateMETS data in order to be interoperable.ocrd-tool.jsonInteroperability needs metadata, both descriptive and technical. OCR-D hasdeveloped a format that allows MP to express general informationabout themselves and detailed information about the tools they develop.RESTOCR-D will offer RESTful access to the MP CLI based on HTTP, usingthe Open API / Swagger set of tools.DockerfileDocker is a widely used system for containerization of software. MPs areencouraged to package the tools they develop as a docker image by providing aDockerfile. OCR-D offers recommendations on how the Dockerfile should bestructured.",
      "url": " /de/spec/intro.html"
    },
  

    {
      "slug": "en-kwalitee-html",
      "title": "",
      "content"	 : "           GitHub              Last update              Number of contributors                 cor-asv-ann    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-annn    OCR post-correction with encoder-attention-decoder LSTMsnn[![CircleCI](https://circleci.com/gh/ASVLeipzig/cor-asv-ann.svg?style=svg)](https://circleci.com/gh/ASVLeipzig/cor-asv-ann)nn## IntroductionnnThis is a tool for automatic OCR _post-correction_ (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the _character level_ with a model architecture akin to neural machine translation, i.e. a stacked **encoder-decoder** network with attention mechanism. nnThe **attention model** always applies to full lines (in a _global_ configuration), and uses a linear _additive_ alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a _soft alignment_ between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start &quot;forgetting&quot; the input altogether at some point in the line and behave like unconditional LM generators.)nn...FIXME: mention: n- stacked architecture (with bidirectional bottom and attentional top), configurable depth/widthn- weight tyingn- underspecification and gapn- confidence input and alternative inputn- CPU/GPU optionn- incremental training, LM transfer, shallow transfern- evaluation (CER, PPL)nn### Processing PAGE annotationsnnWhen applied on PAGE-XML (as OCR-D workspace processor), this component also allows processing below the `TextLine` hierarchy level, i.e. on `Word` or `Glyph` level. For that it uses the soft alignment scores to calculate an optimal hard alignment path for characters, and thereby distributes the transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary.nn...nn### Architecturenn...FIXME: show!nn### Input with confidence and/or alternativesnn...FIXME: explain!nn### Multi-OCR inputnnnot yet!nn### ModesnnWhile the _encoder_ can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a **beam-search** _decoder_ requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also **greedily** use the best output only for each position (without beam search). And in doing so, another option is to feed back the softmax output directly into the decoder input instead of its argmax unit vector. This effectively passes the full probability distribution from state to state, which (not very surprisingly) can increase correction accuracy quite a lot ‚Äì it can get as good as a medium-sized beam search results. This latter option also allows to run in parallel again, which is also much faster ‚Äì consuming up to ten times less CPU time.nnThererfore, the backend function `lib.Sequence2Sequence.correct_lines` can operate the encoder-decoder network in either of the following modes:nn#### _fast_nnDecode greedily, but feeding back the full softmax distribution in batch mode.nn#### _greedy_nnDecode greedily, but feeding back the argmax unit vectors for each line separately.nn#### _default_nnDecode beamed, feeding back the argmax unit vectors for the best history/output hypotheses of each line. More specifically:nn&amp;gt; Start decoder with start-of-sequence, then keep decoding untiln&amp;gt; end-of-sequence is found or output length is way off, repeatedly.n&amp;gt; Decode by using the best predicted output characters and several next-bestn&amp;gt; alternatives (up to some degradation threshold) as next input.n&amp;gt; Follow-up on the N best overall candidates (estimated by accumulatedn&amp;gt; score, normalized by length and prospective cost), i.e. do A*-liken&amp;gt; breadth-first search, with N equal `batch_size`.n&amp;gt; Pass decoder initial/final states from character to character,n&amp;gt; for each candidate respectively.n&amp;gt; Reserve 1 candidate per iteration for running through `source_seq`n&amp;gt; (as a rejection fallback) to ensure that path does not fall off then&amp;gt; beam and at least one solution can be found within the search limits.nn### EvaluationnnText lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (`cor-asv-ann-eval`) and PAGE-XML annotations (`ocrd-cor-asv-ann-evaluate`). nnDistances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.nnThere are a number of distance metrics available (all operating on grapheme clusters, not mere codepoints):n- `Levenshtein`:  n  simple unweighted edit distance (fastest, standard; GT level 3)n- `NFC`:  n  like `Levenshtein`, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)n- `NFKC`:  n  like `Levenshtein`, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for `≈ø`, which is already normalized to `s`)n- `historic_latin`:  n  like `Levenshtein`, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter `e` as in `WuÕ§≈øte` instead of  to `W√º≈øte`, `≈ø` vs `s`, or quotation/citation marks; GT level 1)nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-venv`` or ``python3-venv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies:n```shellnmake depsn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtn```nnTo install this module, then do:n```shellnmake installn```nWhich is the equivalent of:n```shellnpip install .n```nn## UsagennThis packages has the following user interfaces:nn### command line interface `cor-asv-ann-train`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-eval`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-repl`nninteractivenn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-process`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). nn...nn```jsonn    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    }n```nn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-evaluate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Inputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.nn...nn```jsonn    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ≈ø-s), Levenshtein for GT level 3&quot;n        }n      }n    }n```nn...nn## Testingnnnot yet!n...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;,n  &quot;version&quot;: &quot;0.1.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    },n    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ≈ø-s), Levenshtein for GT level 3&quot;n        },n        &quot;confusion&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;minimum&quot;: 0,n          &quot;default&quot;: 0,n          &quot;description&quot;: &quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-ann-trainn    - cor-asv-ann-evaln    - cor-asv-ann-repln    - ocrd-cor-asv-ann-processn    - ocrd-cor-asv-ann-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_ann&#39;,n    version=&#39;0.1.2&#39;,n    description=&#39;sequence-to-sequence translator for noisy channel error correction&#39;,n    long_description=README,n    author=&#39;Robert Sachunsky&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-ann&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-ann-train=ocrd_cor_asv_ann.scripts.train:cli&#39;,n            &#39;cor-asv-ann-eval=ocrd_cor_asv_ann.scripts.eval:cli&#39;,n            &#39;cor-asv-ann-repl=ocrd_cor_asv_ann.scripts.repl:cli&#39;,n            &#39;ocrd-cor-asv-ann-process=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_process&#39;,n            &#39;ocrd-cor-asv-ann-evaluate=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 00:58:56 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;49&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-ann&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-ann-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align different textline annotations and compute distance&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-evaluate&quot;, &quot;parameters&quot;=&amp;gt;{&quot;confusion&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;minimum&quot;=&amp;gt;0, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;metric&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;Levenshtein&quot;, &quot;description&quot;=&amp;gt;&quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ≈ø-s), Levenshtein for GT level 3&quot;, &quot;enum&quot;=&amp;gt;[&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/evaluation&quot;]}, &quot;ocrd-cor-asv-ann-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cor-asv-ann-evaluate] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cor-asv-ann-evaluate.parameters.confusion] Additional properties are not allowed (&#39;minimum&#39; was unexpected)n  [tools.ocrd-cor-asv-ann-evaluate.steps.0] &#39;recognition/evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-ann&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_ann&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}         cor-asv-fst    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-fstn    OCR post-correction with error/lexicon Finite State Transducers andn    chararacter-level LSTM language modelsnn## Introductionnnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnIn addition to the requirements listed in `requirements.txt`, the toolnrequires then[pynini](http://www.opengrm.org/twiki/bin/view/GRM/Pynini)nlibrary, which has to be installed from source.nn## UsagennThe package has two user interfaces:nn### Command Line InterfacennThe package contains a suite of CLI tools to work with plaintext data (prefix:n`cor-asv-fst-*`). The minimal working examples and data formats are describednbelow. Additionally, each tool has further optional parameters - for a detailedndescription, call the tool with the `--help` option.nn#### `cor-asv-fst-train`nnTrain FST models. The basic invocation is as follows:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -t TRAINING_FILEn```nnThis will create two transducers, which will be stored in `LEXICON_FILE` andn`ERROR_MODEL_FILE`, respectively. As the training of the lexicon and the errornmodel is done independently, any of them can be skipped by omitting thenrespective parameter.nn`TRAINING_FILE` is a plain text file in tab-separated, two-column formatncontaining a line of OCR-output and the corresponding ground truth line:nn```n¬ª Bergebt mir, da√ü ih niht wei√ü, wiet¬ªVergebt mir, da√ü ich nicht wei√ü, wienaus dem (Gei≈øte aller Nationen Mahrunqtaus dem Gei≈øte aller Nationen NahrungnKann≈øt Du mir die re&amp;lt;h√©e Bahn nich√© zeigen ?tKann≈øt Du mir die rechte Bahn nicht zeigen?nfrag zu bringen. ‚Äîttrag zu bringen. ‚Äîn≈øie ins irdij&amp;lt;he Leben hinein, Mit leichtem,t≈øie ins irdi≈øche Leben hinein. Mit leichtem,n```nnEach line is treated independently. Alternatively to the above, the trainingndata may also be supplied as two files:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -i INPUT_FILE -g GT_FILEn```nnIn this variant, `INPUT_FILE` and `GT_FILE` are both in tab-separated,ntwo-column format, in which the first column is the line ID and the second thenline:nn```n&amp;gt;=== INPUT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ih denke. Aber was die ≈øelige Frau Geheimr√§th1nnalexis_ruhe01_1852_0035_019     ‚ÄûDas fann ich niht, c‚Äôesl absolument impos-nalexis_ruhe01_1852_0087_027     rend. In dem Augenbli&amp;gt; war 1hr niht wohl zunalexis_ruhe01_1852_0099_012     √ºr die fle ≈øich ≈øchlugen.‚Äúnalexis_ruhe01_1852_0147_009     ≈øollte. Nur √úber die Familien, wo man ≈øie einf√ºhrennn&amp;gt;=== GT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ich denke. Aber was die ≈øelige Frau Geheimr√§thinnalexis_ruhe01_1852_0035_019     ‚ÄûDas kann ich nicht, c&#39;est absolument impos‚Äînalexis_ruhe01_1852_0087_027     rend. Jn dem Augenblick war ihr nicht wohl zunalexis_ruhe01_1852_0099_012     f√ºr die ≈øie ≈øich ≈øchlugen.‚Äúnalexis_ruhe01_1852_0147_009     ≈øollte. Nur √ºber die Familien, wo man ≈øie einf√ºhrenn```nn#### `cor-asv-fst-process`nnThis tool applies a trained model to correct plaintext data on a line basis.nThe basic invocation is:nn```shellncor-asv-fst-process -i INPUT_FILE -o OUTPUT_FILE -l LEXICON_FILE -e ERROR_MODEL_FILE (-m LM_FILE)n```nn`INPUT_FILE` is in the same format as for the training procedure. `OUTPUT_FILE`ncontains the post-correction results in the same format.nn`LM_FILE` is a `ocrd_keraslm` language model - if supplied, it is used fornrescoring.nn#### `cor-asv-fst-evaluate`nnThis tool can be used to evaluate the post-correction results. The minimalnworking invocation is:nn```shellncor-asv-fst-evaluate -i INPUT_FILE -o OUTPUT_FILE -g GT_FILEn```nnAdditionally, the parameter `-M` can be used to select the evaluation measuren(`Levenshtein` by default). The files should be in the same two-column formatnas described above.nn### [OCR-D processor](https://ocr-d.github.io/cli) interface `ocrd-cor-asv-fst-process`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML)ndocuments in an [OCR-D](https://ocr-d.github.io) annotation workflow.nInput files need a textual annotation (`TextEquiv`) on the givenn`textequiv_level` (currently _only_ `word`!).nn...nn```jsonn  &quot;tools&quot;: {n    &quot;cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n```nn...nn## Testingnn...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;,n  &quot;version&quot;: &quot;0.1.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-fst-trainn    - cor-asv-fst-processn    - cor-asv-fst-evaluaten    - ocrd-cor-asv-fst-processn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_fst&#39;,n    version=&#39;0.2.0&#39;,n    description=&#39;OCR post-correction with error/lexicon Finite State &#39;n                &#39;Transducers and character-level LSTMs&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Maciej Sumalvico, Robert Sachunsky&#39;,n    author_email=&#39;sumalvico@informatik.uni-leipzig.de, &#39;n                 &#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-fst&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    test_suite=&#39;tests&#39;,n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-fst-train=ocrd_cor_asv_fst.scripts.train:main&#39;,n            &#39;cor-asv-fst-process=ocrd_cor_asv_fst.scripts.process:main&#39;,n            &#39;cor-asv-fst-evaluate=ocrd_cor_asv_fst.scripts.evaluate:main&#39;,n            &#39;ocrd-cor-asv-fst-process=ocrd_cor_asv_fst.wrapper.cli:ocrd_cor_asv_fst&#39;,n        ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 17:54:58 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;178&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-fst&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-fst-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-fst-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;100, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during beam search in language modelling&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;errorfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for error model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;keraslm_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for language model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lexiconfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for lexicon model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the FST output confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pruning_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;transition weight for pruning the hypotheses in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rejection_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;transition weight (per character) for unchanged input in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;, &quot;enum&quot;=&amp;gt;[&quot;word&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-fst&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Maciej Sumalvico, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sumalvico@informatik.uni-leipzig.de, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_fst&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}         ocrd_calamari    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:edgenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /buildnCOPY Makefile .nCOPY setup.py .nCOPY ocrd-tool.json .nCOPY requirements.txt .nCOPY ocrd_calamari ocrd_calamarinnRUN make calamari/buildnRUN pip3 install .nnENTRYPOINT [&quot;/usr/local/bin/ocrd-calamari-recognize&quot;]nn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_calamarinn&amp;gt; Recognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn[![image](https://circleci.com/gh/OCR-D/ocrd_calamari.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_calamari)n[![image](https://img.shields.io/pypi/v/ocrd_calamari.svg)](https://pypi.org/project/ocrd_calamari/)n[![image](https://codecov.io/gh/OCR-D/ocrd_calamari/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_calamari)nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Installationnn### From PyPInn```npip install ocrd_calamarin```nn### From Reponn```shnpip install .n```nn## Install modelsnnDownload models trained on GT4HistOCR data:nn```nmake gt4histocr-calamarinls gt4histocr-calamarin```nn## Example Usagenn~~~nocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn~~~nnWith `test-parameters.json`:n~~~n{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n~~~nn## Development &amp;amp; TestingnFor information regarding development and testing, please seen[README-DEV.md](README-DEV.md).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/kba/ocrd_calamari&quot;,n  &quot;version&quot;: &quot;0.0.3&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-calamari-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-calamari-recognize&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Recognize lines with Calamari&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-CALAMARI&quot;n      ],n      &quot;parameters&quot;: {n        &quot;checkpoint&quot;: {n          &quot;description&quot;: &quot;The calamari model files (*.ckpt.json)&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;file&quot;, &quot;cacheable&quot;: truen        },n        &quot;voter&quot;: {n          &quot;description&quot;: &quot;The voting algorithm to use&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;confidence_voter_default_ctc&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom pathlib import Pathnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_calamari&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;Calamari bindings&#39;,n    long_description=Path(&#39;README.md&#39;).read_text(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Mike Gerber&#39;,n    author_email=&#39;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&#39;,n    url=&#39;https://github.com/kba/ocrd_calamari&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=Path(&#39;requirements.txt&#39;).read_text().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-calamari-recognize=ocrd_calamari.cli:ocrd_calamari_recognize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 16:14:13 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;84&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-calamari-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize lines with Calamari&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-calamari-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-CALAMARI&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;The calamari model files (*.ckpt.json)&quot;, &quot;format&quot;=&amp;gt;&quot;file&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;voter&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;confidence_voter_default_ctc&quot;, &quot;description&quot;=&amp;gt;&quot;The voting algorithm to use&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_calamari&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_calamarinnRecognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Example Usagenn```shnocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn```nnWith `test-parameters.json`:nn```jsonn{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n```nnTODOn----nn* Support Calamari&#39;s &quot;extended prediction data&quot; outputn* Currently, the processor only supports a prediction using confidence voting of multiple models. While this isn  superior, it makes sense to support single model prediction, too.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-calamari&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;numpy&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;calamari-ocr (==0.3.5)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;click&quot;, &quot;ocrd (&amp;gt;=1.0.0b11)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Calamari bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;6229919, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;sha256&quot;=&amp;gt;&quot;cf08ec027390519d465f6be861e5672b48e7b39b3d1f8e13e54cb401034355b6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9320, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:11&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:11.044376Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/30/62/d8efee35233443d444fc49f7f89792979234c1d735285d599f989e63cee1/ocrd_calamari-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;sha256&quot;=&amp;gt;&quot;51a09088d677799258d8c796dbaba8a1b44a318d06c060314499f708fa37bdd4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3884, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:13&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:13.643406Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/46/1a/b5f02d113aa7810cb773f0b586d1202c254d22e4bf3c6b829d937da2c1b0/ocrd_calamari-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;sha256&quot;=&amp;gt;&quot;454164c6b1c063b76c5189ae596115499bffd6e944c896dee3b03f08852f5680&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5247, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:56.460224Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/53/c05186a309284a22d4f1f0399a5fb241d7b11fb0e5b94c33fa8ae229a6fc/ocrd_calamari-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;sha256&quot;=&amp;gt;&quot;39e0f5b334a735fb8fa20e5490dcd07a96a620bc785c8e2b31f64a23fa13a6fe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3952, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:57.972949Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/9d/cc/de53bfd3c2b666cab5ef199c93902c85bb83ee03d923e9ef7abe87377857/ocrd_calamari-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari&quot;}         ocrd_im6convert    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivennENV PREFIX=/usr/localnnWORKDIR /buildnCOPY ocrd-im6convert .nCOPY ocrd-tool.json .nCOPY Makefile .nnRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install apt-utils &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    makennRUN make deps-ubuntu installnnENV DEBIAN_FRONTEND teletypenn# no fixed entrypoint (e.g. also allow `convert` etc)nCMD [&quot;/usr/local/bin/ocrd-im6convert&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_imageconvertnn&amp;gt; Thin wrapper around convert(1)nn## Introductionnn[ImageMagick&#39;s](https://imagemagick.org) `convert` CLI contains a treasure trove of image operations. This wrapper aims to provide much of that as an [OCR-D compliant processor](https://ocr-d.github.io/CLI).nn## InstallationnnThis module requires GNU make (for installation) and the ImageMagick command line tools (at runtime). On Ubuntu 18.04 (or similar), you can install them by running:nn    sudo apt-get install maken    sudo make deps-ubuntu # or: apt-get install imagemagicknnMoreover, an installation of [OCR-D core](https://github.com/OCR-D/core) is needed:nn    make deps # or: pip install ocrdnnThis will install the Python package `ocrd` in your current environment. (Setting up a [venv](https://ocr-d.github.io/docs/guide#python-setup) is strongly recommended.)nnLastly, the provided shell script `ocrd-im6convert` works best when copied into your `PATH`, referencing its ocrd-tool.json under a known path. This can be done by running:nn    make installnnThis will copy the binary and JSON file under `$PREFIX`, which variable you can override to your needs. The default value is to use `PREFIX=$VIRTUAL_ENV` if you have already activated a venv, or `PREFIX=$PWD/.local` (i.e. under the current working directory).nn## UsagennThis package provides `ocrd-im6convert` as a [OCR-D processor](https://ocr-d.github.com/cli) (command line interface). It uses the following parameters:nn```JSONn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }n```nnCf. [IM documentation](https://imagemagick.org/script/command-line-options.php) or man-page `convert(1)` for formats and options.nn### Examplenn    ocrd-im6convert -I OCR-D-IMG -O OCR-D-IMG-SMALL -p &#39;{ &quot;output-format&quot;: &quot;image/png&quot;, &quot;output-options&quot;: &quot;-resize 24%&quot; }&#39;nn(This downscales the images in the input file group `OCR-D-IMG` to 24% and stores them as PNG files under the output file group `OCR-D-IMG-SMALL`.)nn## TestingnnNone yetn&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_im6convert&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {nn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Dec 27 13:38:58 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;27&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_im6convert&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-im6convert&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Convert and transform images&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-im6convert&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;input-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-format&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Desired media type of output&quot;, &quot;enum&quot;=&amp;gt;[&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_im6convert&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;}         ocrd_keraslm    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnn[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_keraslm.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_keraslm)nn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;f√ºr die Wi≈ø≈øen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so sch√§dlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_keraslm&quot;,n  &quot;version&quot;: &quot;0.3.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 10n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the input confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - keraslm-raten    - ocrd-keraslm-raten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_keraslm&#39;,n    version=&#39;0.3.2&#39;,n    description=&#39;character-level language modelling in Keras&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Robert Sachunsky, Konstantin Baierer, Kay-Michael W√ºrzner&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_keraslm&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    extras_require={n        &#39;plotting&#39;: [n            &#39;sklearn&#39;,n            &#39;matplotlib&#39;,n            ]n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;keraslm-rate=ocrd_keraslm.scripts.run:cli&#39;,n            &#39;ocrd-keraslm-rate=ocrd_keraslm.wrapper.cli:ocrd_keraslm_rate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 9 10:13:52 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;0.3.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;91&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-keraslm-rate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-keraslm-rate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;, &quot;OCR-D-COR-CIS&quot;, &quot;OCR-D-COR-ASV&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-LM&quot;], &quot;parameters&quot;=&amp;gt;{&quot;alternative_decoding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the input confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.3.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_keraslm&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael W√ºrzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael W√ºrzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnnn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;f√ºr die Wi≈ø≈øen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so sch√§dlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-keraslm&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/0.3.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0)&quot;, &quot;click&quot;, &quot;keras (&amp;gt;=2.2.4)&quot;, &quot;numpy&quot;, &quot;tensorflow (&amp;lt;2.0)&quot;, &quot;h5py&quot;, &quot;networkx (&amp;gt;=2.0)&quot;, &quot;sklearn; extra == &#39;plotting&#39;&quot;, &quot;matplotlib; extra == &#39;plotting&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;character-level language modelling in Keras&quot;, &quot;version&quot;=&amp;gt;&quot;0.3.2&quot;}, &quot;last_serial&quot;=&amp;gt;6158523, &quot;releases&quot;=&amp;gt;{&quot;0.3.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3ec82a615434e90028722586c6123e4a1887e36b0a57f06566a291892280e88&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2.py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34192, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:09&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:09.567407Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/ba/8f5f0f1801ea99221c772357e2c79d9935a88e89873924e557e24aea6c33/ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;sha256&quot;=&amp;gt;&quot;665a9bf1d7bc46f497d71638b2d33608062edd16ac11b9cff05be56eacda53c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32287, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:12.437293Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/79/0e/744edc5497d706ac558b90d8d85b2e52ad5fb6b794c6f9cb44fc0aaa341a/ocrd_keraslm-0.3.1.tar.gz&quot;}], &quot;0.3.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}         ocrd_kraken    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_kraken ./ocrd_krakennRUN pip3 install --upgrade pipnRUN pip3 install .nnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_kraken&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-kraken-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-binarize&quot;,n      &quot;input_file_grp&quot;: &quot;OCR-D-IMG&quot;,n      &quot;output_file_grp&quot;: &quot;OCR-D-IMG-BIN&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;description&quot;: &quot;Binarize images with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;level-of-operation&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;page&quot;,n          &quot;enum&quot;: [&quot;page&quot;, &quot;block&quot;, &quot;line&quot;]n        }n      }n    },n    &quot;ocrd-kraken-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-segment&quot;,n      &quot;categories&quot;: [n        &quot;Layout analysis&quot;n      ],n      &quot;steps&quot;: [n        &quot;layout/segmentation/region&quot;n      ],n      &quot;description&quot;: &quot;Block segmentation with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;text_direction&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Sets principal text direction&quot;,n          &quot;enum&quot;: [&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;],n          &quot;default&quot;: &quot;horizontal-lr&quot;n        },n        &quot;script_detect&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;Enable script detection on segmenter output&quot;,n          &quot;default&quot;: falsen        },n        &quot;maxcolseps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2},n        &quot;scale&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0},n        &quot;black_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false},n        &quot;white_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false}n      }n    },n    &quot;ocrd-kraken-ocr&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-ocr&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;OCR with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;lines-json&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;url&quot;,n          &quot;required&quot;: &quot;true&quot;,n          &quot;description&quot;: &quot;URL to line segmentation in JSON&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls two binaries:nn    - ocrd-kraken-binarizen    - ocrd-kraken-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_kraken&#39;,n    version=&#39;0.1.1&#39;,n    description=&#39;kraken bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael W√ºrzner&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_kraken&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0a4&#39;,n        &#39;kraken == 0.9.16&#39;,n        &#39;click &amp;gt;= 7&#39;,n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-kraken-binarize=ocrd_kraken.cli:ocrd_kraken_binarize&#39;,n            &#39;ocrd-kraken-segment=ocrd_kraken.cli:ocrd_kraken_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Oct 21 20:52:26 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;85&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-kraken-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG&quot;, &quot;output_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG-BIN&quot;, &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;block&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-kraken-ocr&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;OCR with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-ocr&quot;, &quot;parameters&quot;=&amp;gt;{&quot;lines-json&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;URL to line segmentation in JSON&quot;, &quot;format&quot;=&amp;gt;&quot;url&quot;, &quot;required&quot;=&amp;gt;&quot;true&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-kraken-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Block segmentation with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-segment&quot;, &quot;parameters&quot;=&amp;gt;{&quot;black_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;script_detect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Enable script detection on segmenter output&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;text_direction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;horizontal-lr&quot;, &quot;description&quot;=&amp;gt;&quot;Sets principal text direction&quot;, &quot;enum&quot;=&amp;gt;[&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;white_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-kraken-binarize.input_file_grp] &#39;OCR-D-IMG&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.output_file_grp] &#39;OCR-D-IMG-BIN&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.parameters.level-of-operation] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.maxcolseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.scale] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.black_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.white_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-ocr] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-ocr.parameters.lines-json.required] &#39;true&#39; is not of type &#39;boolean&#39;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_kraken&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-kraken&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/0.1.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0a4)&quot;, &quot;kraken (==0.9.16)&quot;, &quot;click (&amp;gt;=7)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;kraken bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;last_serial&quot;=&amp;gt;6008613, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;sha256&quot;=&amp;gt;&quot;a0de30df5e8b7d9fe1ed3343a8fa3a413620828a2cdf46bcab8d77e864869d53&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10691, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:30&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:30.728403Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b4/52/aea22b8cfab48546e10118e0eb7e70dc108fe633af3e07194dfd04e00fb2/ocrd_kraken-0.0.2-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;sha256&quot;=&amp;gt;&quot;805fb1aa976f9ee1275e347b1fee2413af3ea7cc8972af84464c6f4253ebdd6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9634, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:32.808242Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/00/a9843c2c73a086c1f66e28d6b0d64053ecd66995daddfb5c0f28e566c9f7/ocrd_kraken-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;sha256&quot;=&amp;gt;&quot;a6464f3559acfb36947687d4e2e70cd7cb7e655d70234696e2e7c1b07f99bab8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5003, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:34.101144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/bb/9e4299ec1d5f494e7bf14de447f361455f36ea0255181871ee937aae0528/ocrd_kraken-0.0.2.tar.gz&quot;}], &quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;sha256&quot;=&amp;gt;&quot;4e6b7e9d1930de1f0bd57dfd63f9418c4345842e7cc8fdd9b147e7d378b8fe51&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10442, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:43.225080Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/4b/d7027ac27e1228cf9aa3ecd94e412b371b2a63ab2c93c1b77ad5414380c1/ocrd_kraken-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;sha256&quot;=&amp;gt;&quot;9bec610685e29d29e0614f2dfc300d201fbbff3f728140536031f14e4e65584c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4121, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:44.655031Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cb/35/7be3dd70b97e276ce2300dddf165bfc21c0e469c2626d7d531a07b8bf0fb/ocrd_kraken-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}         ocrd_ocropy    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_ocropy ./ocrd_ocropynRUN pip3 install --upgrade pipnRUN make deps installnnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginen&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_ocropy&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-ocropy-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-ocropy-segment&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;description&quot;: &quot;Segment page&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE&quot;],n      &quot;parameters&quot;: {n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.1},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;,&quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: false},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.2},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 8},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.0},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0}n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls one binary:nn    - ocrd-ocropy-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setupnnsetup(n    name=&#39;ocrd_ocropy&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;ocropy bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_ocropy&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=[&#39;ocrd_ocropy&#39;],n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0b8&#39;,n        &#39;ocrd-fork-ocropy &amp;gt;= 1.4.0a3&#39;,n        &#39;click&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-ocropy-segment=ocrd_ocropy.cli:ocrd_ocropy_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jun 11 14:51:00 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;66&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_ocropy&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginennn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-ocropy&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b8)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;click&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;ocropy bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;4979689, &quot;releases&quot;=&amp;gt;{&quot;0.0.1a1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;sha256&quot;=&amp;gt;&quot;1dc3926e7c28ecb52260c42d0b3b6b3cc3d2964b13ea994601219269c8072d89&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6462, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:48&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:48.327057Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c7/ce/9f578c500afbffba6de78fb1fb0d881c23ddb794256a276e4277d5ad7c25/ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fc72a46a9e3bc7fd601aa6c00992debe566f1838b95bbd61e8c746b3abd0d673&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6105, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:50&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:50.204116Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8f/a1/2030fb1c2c08cac624a7640daa6a12c3d115a52a9d7d66de5c6b427bbbde/ocrd_ocropy-0.0.1a1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;sha256&quot;=&amp;gt;&quot;a1827b7fb49a27e297fb01ceea45c2272d996f498c576637e42d8008d28dfe9b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10625, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:23&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:23.779614Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7f/46/222d127fe28c522ab65448bd552f9b9b66ec6e5582f8cc7e2ee57f5450a5/ocrd_ocropy-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;sha256&quot;=&amp;gt;&quot;d1e4cd90fff395e332814f51de1b46533ac88ea72f99f4502524c0c659572519&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5855, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:25.438144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/18/c634cc95db36cfa523a75f3ae4e5ee3055b8bcf56969bc3231cdddb3d082/ocrd_ocropy-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}         ocrd_olena    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# Patch and build Olena from Git, thenn# Install OCR-D wrapper for binarizationnFROM ocrd/corennMAINTAINER OCR-DnnENV PREFIX=/usr/localnnWORKDIR /build-olenanCOPY .gitmodules .nCOPY Makefile .nCOPY ocrd-tool.json .nCOPY ocrd-olena-binarize .nnENV DEPS=&quot;g++ make automake git&quot;nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends $DEPS &amp;amp;&amp;amp; n    make deps-ubuntu &amp;amp;&amp;amp; n    git init &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/olena.git repo/olena &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/assets.git repo/assets &amp;amp;&amp;amp; n    make build-olena install clean-olena &amp;amp;&amp;amp; n    apt-get -y remove $DEPS &amp;amp;&amp;amp; n    apt-get -y autoremove &amp;amp;&amp;amp; apt-get clean &amp;amp;&amp;amp; n    rm -fr /build-olenannWORKDIR /datanVOLUME /datann#ENTRYPOINT [&quot;/usr/bin/ocrd-olena-binarize&quot;]n#CMD [&quot;--help&quot;]nCMD [&quot;/usr/bin/ocrd-olena-binarize&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_olenann&amp;gt; Binarize with Olena/scribonn[![Build Status](https://travis-ci.org/OCR-D/ocrd_olena.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_olena)n[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_olena.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_olena)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/core.svg)](https://hub.docker.com/r/ocrd/olena/tags/)nn## Requirementsnn```nmake deps-ubuntun```nn...will try to install the required packages on Ubuntu.nn## Installationnn```nmake build-olenan```nn...will download, patch and build Olena/scribo from source, and install locally (in VIRTUAL_ENV or in CWD/local).nn```nmake installn```nn...will do that, but additionally install `ocrd-binarize-olena` (the OCR-D wrapper).nn## Testingnn```nmake testn```nn...will clone the assets repository from Github, make a workspace copy, and run checksum tests for binarization on them.nn## UsagennThis package has the following user interfaces:nn### command line interface `scribo-cli`nnConverts images in any format to netpbm (monochrome portable bitmap).nn```nUsage: scribo-cli [version] [help] COMMAND [ARGS]nnList of available COMMAND argument:nn  Full Toolchainsn  ---------------nnn   * On documentsnn     doc-ppct       Common preprocessing before looking for text.nn     doc-ocr           Find and recognize text. Output: the actual textn     tt       and its location.nn     doc-dia           Analyse the document structure and extract then     tt       text. Output: an XML file with region and textn     tt       information.nnnn   * On picturesnn     pic-loc           Try to localize text if there&#39;s any.nn     pic-ocr           Localize and try to recognize text.nnnn  Toolsn  -----nnn     * xml2doct       Convert the XML results of document toolchainsn       tt       into user documents (HTML, PDF...).nnn  Algorithmsn  ----------nnn   * Binarizationnn     sauvola           Sauvola&#39;s algorithm.nn     sauvola-ms        Multi-scale Sauvola&#39;s algorithm.nn     sauvola-ms-fg     Extract foreground objects and run multi-scalen                       Sauvola&#39;s algorithm.nn     sauvola-ms-split  Run multi-scale Sauvola&#39;s algorithm on each colorn                       component and merge results.nn---------------------------------------------------------------------------nSee &#39;scribo-cli COMMAND --help&#39; for more information on a specific command.n```nnFor example:nn```shnscribo-cli sauvola-ms path/to/input.tif path/to/output.png --enable-negate-outputn```nn### [OCR-D processor](https://ocr-d.github.com/cli) interface `ocrd-olena-binarize`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.github.io) annotation workflow. Input could be any valid workspace with source images available. Currently covers the `Page` hierarchy level only. Uses either (the last) `AlternativeImage`, if any, or `imageFilename`, otherwise. Adds an `AlternativeImage` with the result of binarization for every page.nn```jsonn    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n```nn## LicensennCopyright 2018-2020 Project OCR-DnnLicensed under the Apache License, Version 2.0 (the &quot;License&quot;);nyou may not use this file except in compliance with the License.nYou may obtain a copy of the License atnn   http://www.apache.org/licenses/LICENSE-2.0nnUnless required by applicable law or agreed to in writing, softwarendistributed under the License is distributed on an &quot;AS IS&quot; BASIS,nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.nSee the License for the specific language governing permissions andnlimitations under the License.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;1.1.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_olena&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 18:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v1.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;117&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_olena&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-olena-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-olena-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;impl&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The name of the actual binarization algorithm&quot;, &quot;enum&quot;=&amp;gt;[&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;k&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.34, &quot;description&quot;=&amp;gt;&quot;Sauvola&#39;s formulae parameter&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;win-size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;101, &quot;description&quot;=&amp;gt;&quot;Window size&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;1.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_olena&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;}         ocrd_segment    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_segment&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-segment-repair&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-repair&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Analyse and repair region segmentation&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-EVAL-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;sanitize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;n        },n        &quot;plausibilize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;n        },n        &quot;plausibilize_merge_min_overlap&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 0.90,n          &quot;description&quot;: &quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-regions&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-regions&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract region segmentation as image+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-lines&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-lines&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract line segmentation as image+txt+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-evaluate&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Compare region segmentations&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:nn    - ocrd-segment-repairn    - ocrd-segment-extract-pagesn    - ocrd-segment-extract-regionsn    - ocrd-segment-extract-linesn    - ocrd-segment-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_segment&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Page segmentation and segmentation evaluation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    author=&#39;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_segment&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-segment-repair=ocrd_segment.cli:ocrd_segment_repair&#39;,n            &#39;ocrd-segment-extract-pages=ocrd_segment.cli:ocrd_segment_extract_pages&#39;,n            &#39;ocrd-segment-extract-regions=ocrd_segment.cli:ocrd_segment_extract_regions&#39;,n            &#39;ocrd-segment-extract-lines=ocrd_segment.cli:ocrd_segment_extract_lines&#39;,n            &#39;ocrd-segment-evaluate=ocrd_segment.cli:ocrd_segment_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 10:42:42 2020 +0000&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;60&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-segment-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Compare region segmentations&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-evaluate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-lines&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract line segmentation as image+txt+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-lines&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-regions&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract region segmentation as image+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-regions&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-repair&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analyse and repair region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-repair&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-EVAL-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;plausibilize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;plausibilize_merge_min_overlap&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.9, &quot;description&quot;=&amp;gt;&quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sanitize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_segment&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-segment&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b19)&quot;, &quot;click&quot;, &quot;shapely&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Page segmentation and segmentation evaluation&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6235446, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}         ocrd_tesserocr    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .nCOPY requirements_test.txt .nCOPY ocrd_tesserocr ./ocrd_tesserocrnCOPY Makefile .nRUN make deps-ubuntu &amp;amp;&amp;amp; n    apt-get install -y --no-install-recommends n    g++ n    tesseract-ocr-script-frak n    tesseract-ocr-deu n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; rm -rf /build-ocrd n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove g++ libtesseract-dev maken&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / tables / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, table, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some Tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nnTo run with docker:nn```ndocker run -v path/to/workspaces:/data ocrd/tesserocr ocrd-tesserocrd-crop ...n```nnn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnsudo make deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-table](ocrd_tesserocr/segment_table.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.8.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,n  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-tesserocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-DESKEW-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        },n        &quot;min_orientation_confidence&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 1.5,n          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;description&quot;: &quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-SEG-GLYPH&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;n      ],n      &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;n        },n        &quot;raw_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;n        },n        &quot;char_whitelist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;n        },n        &quot;char_blacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;n        },n        &quot;char_unblacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;n        },n        &quot;model&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-region&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-PAGE&quot;,n        &quot;OCR-D-GT-SEG-PAGE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,n          &quot;default&quot;: 0n        },n        &quot;crop_polygons&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;n        },n        &quot;find_tables&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-table&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the region level&quot;n        }n      }n     },n     &quot;ocrd-tesserocr-segment-line&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextRegion level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-segment-word&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextLine level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,n      &quot;input_file_grp&quot;: [nt&quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [nt&quot;OCR-D-SEG-PAGE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;parameters&quot; : {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,n          &quot;default&quot;: 4n        }n      }n    },n    &quot;ocrd-tesserocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-BIN-BLOCK&quot;,n        &quot;OCR-D-BIN-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls five executables:nn    - ocrd_tesserocr_recognizen    - ocrd_tesserocr_segment_regionn    - ocrd_tesserocr_segment_tablen    - ocrd_tesserocr_segment_linen    - ocrd_tesserocr_segment_wordn    - ocrd_tesserocr_cropn    - ocrd_tesserocr_deskewn    - ocrd_tesserocr_binarizen&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_tesserocr&#39;,n    version=&#39;0.8.0&#39;,n    description=&#39;Tesserocr bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_tesserocr&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-tesserocr-recognize=ocrd_tesserocr.cli:ocrd_tesserocr_recognize&#39;,n            &#39;ocrd-tesserocr-segment-region=ocrd_tesserocr.cli:ocrd_tesserocr_segment_region&#39;,n            &#39;ocrd-tesserocr-segment-table=ocrd_tesserocr.cli:ocrd_tesserocr_segment_table&#39;,n            &#39;ocrd-tesserocr-segment-line=ocrd_tesserocr.cli:ocrd_tesserocr_segment_line&#39;,n            &#39;ocrd-tesserocr-segment-word=ocrd_tesserocr.cli:ocrd_tesserocr_segment_word&#39;,n            &#39;ocrd-tesserocr-crop=ocrd_tesserocr.cli:ocrd_tesserocr_crop&#39;,n            &#39;ocrd-tesserocr-deskew=ocrd_tesserocr.cli:ocrd_tesserocr_deskew&#39;,n            &#39;ocrd-tesserocr-binarize=ocrd_tesserocr.cli:ocrd_tesserocr_binarize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 15:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.8.0&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;334&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;dockerhub&quot;=&amp;gt;&quot;ocrd/tesserocr&quot;, &quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-tesserocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BIN-BLOCK&quot;, &quot;OCR-D-BIN-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-tesserocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Poor man&#39;s cropping via region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-PAGE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;extend detected border by this many (true) pixels on every side&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-tesserocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Detect script, orientation and skew angle for pages or regions&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-DESKEW-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_orientation_confidence&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;Minimum confidence score to apply orientation as detected by OSD&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-tesserocr-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;], &quot;parameters&quot;=&amp;gt;{&quot;char_blacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_unblacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_whitelist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;raw_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-tesserocr-segment-line&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment regions into lines with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-line&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextRegion level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-tesserocr-segment-region&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-region&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-PAGE&quot;, &quot;OCR-D-GT-SEG-PAGE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;crop_polygons&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;annotate polygon coordinates instead of bounding box rectangles&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;find_tables&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;extend detected region rectangles by this many (true) pixels&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-table&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment table regions into cell text regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-table&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the region level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-word&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment lines into words with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-word&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextLine level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/word&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.8.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_tesserocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael W√ºrzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnmake deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## TestingnnTo run with docker:nn```ndocker run ocrd/tesserocr ocrd-tesserocrd-crop ...n```nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nn## DevelopmentnnLatest changes that require pre-release of [ocrd &amp;gt;= 2.0.0](https://github.com/OCR-D/core/tree/edge) are kept in branch [`edge`](https://github.com/OCR-D/ocrd_tesserocr/tree/edge).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-tesserocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/0.7.0/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;tesserocr (&amp;gt;=2.4.1)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Tesserocr bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.7.0&quot;}, &quot;last_serial&quot;=&amp;gt;6506849, &quot;releases&quot;=&amp;gt;{&quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;sha256&quot;=&amp;gt;&quot;64ec4e7a43ddaf199af7da8966996e260454dae4d30f79cb112149cddf5b8fd2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17089, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:24&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:24.592860Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/07/63/e617002f9c2013f8a9ce10baeab48acffc0dff3d21ab160ee67428e08ebd/ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2a7fd61a97bb222f2ac5a6f85b3d2ce43da843509993eef189f09b48f44027f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15424, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:25.913866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4d/48/282d1d793137f1ec30118a9a0bd48534a6a8053bc74a830b6c4eb389653f/ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;sha256&quot;=&amp;gt;&quot;3fecd0a93d9a711552fbd2cf15af1f150f04f503f7b3f09d9c025267601bb42d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9234, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:27.040863Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/a7/66775daafba5937821fd643b6d1069570b262af3a48d701712d2a94350a2/ocrd_tesserocr-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;sha256&quot;=&amp;gt;&quot;d474e372af4266ab4343570c47a448f9f68b3c002f970717663b64acabe1dbe4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:51.905308Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/5c/95/7f29b87ff5be4fdd149400855862840de4681b669d3fda60a2ce8bf24127/ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;sha256&quot;=&amp;gt;&quot;3c0f56fc2c88ec1ea2461eb0610763443b9af279c5260b08a1be079c92bed5c6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:53&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:53.535866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/da/23/fb5e1e125f1fda3b1069960426c5b40a9c5e12fe8f73ac29244888cf110b/ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;sha256&quot;=&amp;gt;&quot;2de460c4d3218ac6e3133b498c01ee7428770edcd60a02f65793ae4006f3db82&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9251, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:54.917641Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/31/73/c2044ae57f402e21947ceb97f574625cf534eccbf432f6916c419cf3d7e7/ocrd_tesserocr-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2409adbb5c529b05eba8be5a9d1c7e11660dc2626bcaf61b407b617d5c7c99e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15453, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:20&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:20.618650Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c1/ca/38355a461d8e29d7039391f5051be291d6a425b078783adb1ebb6ba10e55/ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;sha256&quot;=&amp;gt;&quot;fbde4fc1a5a0340507b6d96bd529a42162e732b7cca31e968b28f6a4fcdccd12&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9242, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:21.805810Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1b/fe/b365c2ffddea53e616408f0213e45614ce3791ead2058df33a795ddc3d21/ocrd_tesserocr-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;sha256&quot;=&amp;gt;&quot;1549fbf8d314dc1f5ea20b45842e971a97b3c276f78d4d167a463432d5b77b18&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17420, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:12.698851Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/18/7f/fd08ca819e6f3980220ac680b5c931080247544c2704963e518db6f7a3d0/ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;sha256&quot;=&amp;gt;&quot;1648df71d28a9b3388f1e701256037eb9023f149a17a22d0a9c2dec4a0510002&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15729, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:14&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:14.276437Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/34/08/ea3ebc9476e1d28672e23b8d1332dbbc95ac9a3246cd7d02be2375995da6/ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;sha256&quot;=&amp;gt;&quot;6ec1b6c5cb4395f6f4e7356219e7019612fdcda685b511de7171dcaf4f39a439&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9442, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:15&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:15.802793Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/10/d1b3c66b891193ccc07200d93391cbcfe9c4c5ea2bb1cac045e7d1cf1fa6/ocrd_tesserocr-0.1.3.tar.gz&quot;}], &quot;0.2.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;sha256&quot;=&amp;gt;&quot;f61661e4cba7b77336dcabc6117d1e4fa90357ec98f263eacfc2c836e3a477f4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;16547, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:21.318896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d1/94/606de830cdba1f81928dc42a71f7e58cc6510d6a8b0f9e945c01f56ee3e7/ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;sha256&quot;=&amp;gt;&quot;05cc4be3ae1404afd45d8b9278d19fcd6a1ea86d376f52f571fefc4af4d96b86&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10356, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:22.854225Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/50/1c/eda34c75846857877176db4f4f0564e8b7c979a872e4c2a521fa8c389fbb/ocrd_tesserocr-0.2.0.tar.gz&quot;}], &quot;0.2.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fd8c18ce5d170e766bccd34c2214e5de22ea13f795bc79642e8be2414c550f2a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15963, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:44.123075Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/af/10f4d710bde5515131fc16ea3408670af8e786998a1e0f6d127e800fbc17/ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;sha256&quot;=&amp;gt;&quot;40f4776bc548be14245de726e744f827742f02e568f6062cc465d6a585624cae&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9534, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:45&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:45.820115Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/df/cc/fd5b999abcae94ff2116a25e31f593b95f0dda4486d89bd4e83d6671b805/ocrd_tesserocr-0.2.1.tar.gz&quot;}], &quot;0.2.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;sha256&quot;=&amp;gt;&quot;7ccdeb2a24f9d93ec6668d02807a4f5fa31d88789a3101ad1fd4ea003128ca65&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;18334, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:06.855632Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4e/5f/37ec32a07681542a1d34fa9764c76ef34d201a82489335d154d34e8b46b2/ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;sha256&quot;=&amp;gt;&quot;ad96c009bcf39b8f9e99f3e58b736ab385e5683935b9146ed9e39e8e8883b4c2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10990, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:08&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:08.563041Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/53/c0186de6ad8429e6b8e0f5e5ac51a8a3d51a2c71bcb597a5879313bf2a2d/ocrd_tesserocr-0.2.2.tar.gz&quot;}], &quot;0.3.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;sha256&quot;=&amp;gt;&quot;09f23770905034ed00f7cb516a907288512a4d21305914b6e2dd7215b9138c6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34706, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:39.261053Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b2/b5/8a890997a3f874498a1f596f3ebdb765daa181858a46cc5a66949945adf8/ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;sha256&quot;=&amp;gt;&quot;11b6742c4c398ea800d0b17276f0efd8a91ccbd6f0c1df05d7046c3e401a33c8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;22743, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:40.918776Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/fa/10af8e05b04c55680b20582c18bed55ffa846bfa65948c6b6138252a8434/ocrd_tesserocr-0.3.0.tar.gz&quot;}], &quot;0.4.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;sha256&quot;=&amp;gt;&quot;4822713547e696dbb327a80f9dd5bad705be4b7dc1f44fdef1d44f9e03c21c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37231, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:05&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:05.083051Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/ee/2b/483b44bf3180e81aa8a5bf7307ae47da4d1656e69dec1a704f9a8d558b88/ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;sha256&quot;=&amp;gt;&quot;616bf420794ef71bcc372fa4c29775c48d6909d01b6849e2d0be83766cd0ed90&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;19943, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:06.605798Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/87/09/b994a5d7310f73b04b7dd840a5fbdd726da42b7980ac0a07595b6c56ef00/ocrd_tesserocr-0.4.0.tar.gz&quot;}], &quot;0.4.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;sha256&quot;=&amp;gt;&quot;d21818eceac8bcdc1fdb38d4a58bfd1620cef8e7a5d0e6276afbd7695c2cac31&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38864, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:27.102775Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1d/78/93c90d9593f62546fea5e2ef9b5edbb5a47121582db724ca41f93830ec87/ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;sha256&quot;=&amp;gt;&quot;bbf3843361c4807c5790790d8a8fc0a0325b2fb9817cd4fa70210659dde8c8cb&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20535, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:28.641792Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/a7/2e/de857738105ed9f1888d3f6724c0c314404b67582652a91b060d25cff808/ocrd_tesserocr-0.4.1.tar.gz&quot;}], &quot;0.5.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3bca0adcb9fce640a010d38d7e1d04b4fc423ec0cc958ff3980afbf74a5711f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;33343, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:17&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:17.958444Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/36/98/a6c6b46903a3b25b1740cde4aedaf62de6441ac887536e36ad24a3c3bf12/ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;sha256&quot;=&amp;gt;&quot;aaf012b2c6adcd9a34b6fa9351dcd16fed3ab848d4d8a563b3825f9b7103be42&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;21170, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:19&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:19.386827Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/85/5b/7c5c21b78ccd00d49f7747ad5b2a381d9860aeed41fe545a24a361544837/ocrd_tesserocr-0.5.0.tar.gz&quot;}], &quot;0.5.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;sha256&quot;=&amp;gt;&quot;18cef805014268db86fd6c32bca83069cdf536298fe8151f59f9197d255a9d14&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38309, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:42&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:42.078476Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/84/b5aca7d06e31dcb91683ab60e154b73a8d0e1cb4d5ae22debf55922573df/ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;sha256&quot;=&amp;gt;&quot;7dd6a5fd556395deb58070d5f6196871a241d89434a26d0a0fc7e106404aa90a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20350, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:43.864345Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/1f/ed95415ee91659222301aa77e4f8c27be33df8e258972059bc031a2c0e3b/ocrd_tesserocr-0.5.1.tar.gz&quot;}], &quot;0.6.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;sha256&quot;=&amp;gt;&quot;41d5309efc4f886569d47dede504cea5e14ffd8e27a33acb69e15c775d34f754&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37693, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:55.328581Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/a9/431c3ad62ac4612b6be3f5cad58b49910a9c00b5f28dd62f8d535ed0c0cf/ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;sha256&quot;=&amp;gt;&quot;3a1aeff23dbf42cc8c003039cc8695cd4e01807245f935c9323e6df2832855a7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20588, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:57.128983Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/48/30/6c8253739ee61d4a42b6512be3fcfe0ce7190ff2835ee1210b1c483da025/ocrd_tesserocr-0.6.0.tar.gz&quot;}], &quot;0.7.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}         ocrd_cis    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:latestnENV VERSION=&quot;Mi 9. Okt 13:26:16 CEST 2019&quot;nENV GITURL=&quot;https://github.com/cisocrgroup&quot;nENV DOWNLOAD_URL=&quot;http://cis.lmu.de/~finkf&quot;nENV DATA=&quot;/apps/ocrd-cis-post-correction&quot;nn# depsnCOPY data/docker/deps.txt ${DATA}/deps.txtnRUN apt-get update nt&amp;amp;&amp;amp; apt-get -y install --no-install-recommends $(cat ${DATA}/deps.txt)nn# localesnRUN sed -i -e &#39;s/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/&#39; /etc/locale.gen n    &amp;amp;&amp;amp; dpkg-reconfigure --frontend=noninteractive locales n    &amp;amp;&amp;amp; update-locale LANG=en_US.UTF-8nn# install the profilernRUNtgit clone ${GITURL}/Profiler --branch devel --single-branch /tmp/profiler nt&amp;amp;&amp;amp; cd /tmp/profiler nt&amp;amp;&amp;amp; mkdir build nt&amp;amp;&amp;amp; cd build nt&amp;amp;&amp;amp; cmake -DCMAKE_BUILD_TYPE=release .. nt&amp;amp;&amp;amp; make compileFBDic trainFrequencyList profiler nt&amp;amp;&amp;amp; cp bin/compileFBDic bin/trainFrequencyList bin/profiler /apps/ nt&amp;amp;&amp;amp; cd / n    &amp;amp;&amp;amp; rm -rf /tmp/profilernn# install the profiler&#39;s language backendnRUNtgit clone ${GITURL}/Resources --branch master --single-branch /tmp/resources nt&amp;amp;&amp;amp; cd /tmp/resources/lexica nt&amp;amp;&amp;amp; make FBDIC=/apps/compileFBDic TRAIN=/apps/trainFrequencyList nt&amp;amp;&amp;amp; mkdir -p /${DATA}/languages nt&amp;amp;&amp;amp; cp -r german latin greek german.ini latin.ini greek.ini /${DATA}/languages nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/resourcesnn# install ocrd_cis (python)nCOPY Manifest.in Makefile setup.py ocrd-tool.json /tmp/build/nCOPY ocrd_cis/ /tmp/build/ocrd_cis/nCOPY bashlib/ /tmp/build/bashlib/n# COPY . /tmp/ocrd_cisnRUN cd /tmp/build nt&amp;amp;&amp;amp; make install nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/buildnn# download ocr models and pre-trainded post-correction modelnRUN mkdir /apps/models nt&amp;amp;&amp;amp; cd /apps/models nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/model.zip &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur1-00085000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur2-00062000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1nnVOLUME [&quot;/data&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{nt&quot;git_url&quot;: &quot;https://github.com/cisocrgroup/ocrd_cis&quot;,nt&quot;version&quot;: &quot;0.0.6&quot;,nt&quot;tools&quot;: {ntt&quot;ocrd-cis-ocropy-binarize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-binarize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/binarization&quot;,ntttt&quot;preprocessing/optimization/grayscale_normalization&quot;,ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-BIN&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;method&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;],nttttt&quot;description&quot;: &quot;binarization method to use (only ocropy will include deskewing)&quot;,nttttt&quot;default&quot;: &quot;ocropy&quot;ntttt},ntttt&quot;grayscale&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;description&quot;: &quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;,nttttt&quot;default&quot;: falsentttt},ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 0.0ntttt},ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-deskew&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-deskew&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 5.0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-denoise&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-denoise&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/despeckling&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-DESPECK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Despeckle pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 3.0ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-clip&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-clip&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Clip text regions / lines at intersections with neighbours&quot;,nttt&quot;parameters&quot;: {ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.7ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-resegment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-resegment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.8ntttt},ntttt&quot;extend_margins&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;,nttttt&quot;default&quot;: 3ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-dewarp&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-dewarp&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/dewarping&quot;nttt],nttt&quot;description&quot;: &quot;Dewarp line images with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;range&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;,nttttt&quot;default&quot;: 4.0ntttt},ntttt&quot;max_neighbour&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;,nttttt&quot;default&quot;: 0.05ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-recognize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-recognize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;,ntttt&quot;OCR-D-SEG-WORD&quot;,ntttt&quot;OCR-D-SEG-GLYPH&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-OCR-OCRO&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;,nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-rec&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-rec&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text snippets&quot;,nttt&quot;parameters&quot;: {ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-segment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-segment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-GT-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Segment pages into regions or regions into lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level to read images from&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;maxcolseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 2,nttttt&quot;description&quot;: &quot;number of white/background column separators to try (when operating on the page level)&quot;ntttt},ntttt&quot;maxseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 5,nttttt&quot;description&quot;: &quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_regions&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextRegion elements (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_lines&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextLine elements (when operating on the region level)&quot;ntttt},ntttt&quot;spread&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;default&quot;: 2.4,nttttt&quot;description&quot;: &quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-train&quot;,nttt&quot;categories&quot;: [ntttt&quot;lstm ocropy model training&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;training&quot;nttt],nttt&quot;description&quot;: &quot;train model with ground truth from mets data&quot;,nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;ntttt},ntttt&quot;ntrain&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;lines to train before stopping&quot;,nttttt&quot;default&quot;: 1000000ntttt},ntttt&quot;outputpath&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;(existing) path for the trained model&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-align&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-align&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Align multiple OCRs and/or GTs&quot;ntt},ntt&quot;ocrd-cis-wer&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-wer&quot;,nttt&quot;categories&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;description&quot;: &quot;calculate the word error rate for aligned page xml files&quot;,nttt&quot;parameters&quot;: {ntttt&quot;testIndex&quot;: {nttttt&quot;description&quot;: &quot;text equiv index for the test/ocr tokens&quot;,nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;gtIndex&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;text equiv index for the gt tokens&quot;,nttttt&quot;default&quot;: -1ntttt}nttt}ntt},ntt&quot;ocrd-cis-jar&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-jar&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Output path to the ocrd-cis.jar file&quot;ntt},ntt&quot;ocrd-cis-profile&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-profile&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Add a correction suggestions and suspicious tokens (profile)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;executable&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;backend&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;language&quot;: {ntttt    &quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;german&quot;ntttt},ntttt&quot;additionalLexicon&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-train.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Train post correction model&quot;,nttt&quot;parameters&quot;: {ntttt&quot;gtArchives&quot;: {nttttt&quot;description&quot;: &quot;List of ground truth archives&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Path (or URL) to a ground truth archive&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;imagePreprocessingSteps&quot;: {nttttt&quot;description&quot;: &quot;List of image preprocessing steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;training&quot;: {nttttt&quot;description&quot;: &quot;Configuration of training command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;trigrams&quot;,ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;leFeatures&quot;,ntttttt&quot;rrFeatures&quot;,ntttttt&quot;dmFeatures&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;trigrams&quot;: {nttttttt&quot;description&quot;: &quot;Path to character trigrams csv file (format: n,trigram)&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;filterClasses&quot;: {nttttttt&quot;description&quot;: &quot;List of filtered feature classes&quot;,nttttttt&quot;required&quot;: false,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Class name of feature class to filter&quot;,ntttttttt&quot;type&quot;: &quot;string&quot;nttttttt}ntttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;leFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the lexicon extension features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;rrFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the reranker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;dmFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the desicion maker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt}nttttt}ntttt}nttt}ntt},ntt&quot;ocrd-cis-post-correct&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-post-correct.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Post correct OCR results&quot;,nttt&quot;parameters&quot;: {ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of additional ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;postCorrection&quot;: {nttttt&quot;description&quot;: &quot;Configuration of post correction command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;model&quot;,ntttttt&quot;runLE&quot;,ntttttt&quot;runDM&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;model&quot;: {nttttttt&quot;description&quot;: &quot;Path to the post correction model file&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;runLE&quot;: {nttttttt&quot;description&quot;: &quot;Do run the lexicon extension step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt},ntttttt&quot;runDM&quot;: {nttttttt&quot;description&quot;: &quot;Do run the ranking and the decision step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt}nttttt}ntttt}nttt}ntt}nt}n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;&quot;&quot;&quot;nInstalls:n    - ocrd-cis-alignn    - ocrd-cis-trainingn    - ocrd-cis-profilen    - ocrd-cis-wern    - ocrd-cis-datan    - ocrd-cis-ocropy-clipn    - ocrd-cis-ocropy-denoisen    - ocrd-cis-ocropy-deskewn    - ocrd-cis-ocropy-binarizen    - ocrd-cis-ocropy-resegmentn    - ocrd-cis-ocropy-segmentn    - ocrd-cis-ocropy-dewarpn    - ocrd-cis-ocropy-recognizen    - ocrd-cis-ocropy-trainn&quot;&quot;&quot;nnimport codecsnfrom setuptools import setupnfrom setuptools import find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cis&#39;,n    version=&#39;0.0.6&#39;,n    description=&#39;CIS OCR-D command line tools&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Florian Fink, Tobias Englmeier, Christoph Weber&#39;,n    author_email=&#39;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&#39;,n    url=&#39;https://github.com/cisocrgroup/ocrd_cis&#39;,n    license=&#39;MIT&#39;,n    packages=find_packages(),n    include_package_data=True,n    install_requires=[n        &#39;ocrd&amp;gt;=2.0.0&#39;,n        &#39;click&#39;,n        &#39;scipy&#39;,n        &#39;numpy&amp;gt;=1.17.0&#39;,n        &#39;pillow&amp;gt;=6.2.0&#39;,n        &#39;shapely&#39;,n        &#39;matplotlib&amp;gt;3.0.0&#39;,n        &#39;python-Levenshtein&#39;,n        &#39;calamari_ocr == 0.3.5&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;, &#39;*.csv.gz&#39;, &#39;*.jar&#39;],n    },n    scripts=[n        &#39;bashlib/ocrd-cis-lib.sh&#39;,n        &#39;bashlib/ocrd-cis-train.sh&#39;,n        &#39;bashlib/ocrd-cis-post-correct.sh&#39;,n    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-cis-align=ocrd_cis.align.cli:ocrd_cis_align&#39;,n            &#39;ocrd-cis-profile=ocrd_cis.profile.cli:ocrd_cis_profile&#39;,n            &#39;ocrd-cis-wer=ocrd_cis.wer.cli:ocrd_cis_wer&#39;,n            &#39;ocrd-cis-data=ocrd_cis.data.__main__:main&#39;,n            &#39;ocrd-cis-ocropy-binarize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_binarize&#39;,n            &#39;ocrd-cis-ocropy-clip=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_clip&#39;,n            &#39;ocrd-cis-ocropy-denoise=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_denoise&#39;,n            &#39;ocrd-cis-ocropy-deskew=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_deskew&#39;,n            &#39;ocrd-cis-ocropy-dewarp=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_dewarp&#39;,n            &#39;ocrd-cis-ocropy-recognize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_recognize&#39;,n            &#39;ocrd-cis-ocropy-rec=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_rec&#39;,n            &#39;ocrd-cis-ocropy-resegment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_resegment&#39;,n            &#39;ocrd-cis-ocropy-segment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_segment&#39;,n            &#39;ocrd-cis-ocropy-train=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_train&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 23 15:42:32 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;436&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cis-align&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align multiple OCRs and/or GTs&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-align&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-jar&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Output path to the ocrd-cis.jar file&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-jar&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-ocropy-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;grayscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;method&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;ocropy&quot;, &quot;description&quot;=&amp;gt;&quot;binarization method to use (only ocropy will include deskewing)&quot;, &quot;enum&quot;=&amp;gt;[&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;, &quot;preprocessing/optimization/grayscale_normalization&quot;, &quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-clip&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Clip text regions / lines at intersections with neighbours&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-clip&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.7, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-denoise&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Despeckle pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-denoise&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESPECK&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/despeckling&quot;]}, &quot;ocrd-cis-ocropy-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Dewarp line images with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;max_neighbour&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4.0, &quot;description&quot;=&amp;gt;&quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-cis-ocropy-rec&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text snippets&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-rec&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-OCRO&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-resegment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-resegment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;extend_margins&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.8, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment pages into regions or regions into lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read images from&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;number of white/background column separators to try (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextLine elements (when operating on the region level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextRegion elements (when operating on the page level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;spread&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2.4, &quot;description&quot;=&amp;gt;&quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;lstm ocropy model training&quot;], &quot;description&quot;=&amp;gt;&quot;train model with ground truth from mets data&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-train&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;ntrain&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1000000, &quot;description&quot;=&amp;gt;&quot;lines to train before stopping&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;outputpath&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;(existing) path for the trained model&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;training&quot;]}, &quot;ocrd-cis-post-correct&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Post correct OCR results&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-post-correct.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of additional ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;postCorrection&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of post correction command&quot;, &quot;properties&quot;=&amp;gt;{&quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the post correction model file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;runDM&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the ranking and the decision step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;runLE&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the lexicon extension step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;required&quot;=&amp;gt;[&quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;model&quot;, &quot;runLE&quot;, &quot;runDM&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-profile&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Add a correction suggestions and suspicious tokens (profile)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-profile&quot;, &quot;parameters&quot;=&amp;gt;{&quot;additionalLexicon&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;backend&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;executable&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;language&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;german&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Train post correction model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-train.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtArchives&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ground truth archives&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path (or URL) to a ground truth archive&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;imagePreprocessingSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of image preprocessing steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;training&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of training command&quot;, &quot;properties&quot;=&amp;gt;{&quot;dmFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the desicion maker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;filterClasses&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of filtered feature classes&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of feature class to filter&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;leFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the lexicon extension features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;rrFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the reranker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;trigrams&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to character trigrams csv file (format: n,trigram)&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;trigrams&quot;, &quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;leFeatures&quot;, &quot;rrFeatures&quot;, &quot;dmFeatures&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-wer&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;evaluation&quot;], &quot;description&quot;=&amp;gt;&quot;calculate the word error rate for aligned page xml files&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-wer&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;text equiv index for the gt tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;testIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;text equiv index for the test/ocr tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;evaluation&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.6&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cis-ocropy-rec] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.textequiv_level] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.ntrain.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-ocropy-train.categories.0] &#39;lstm ocropy model training&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-ocropy-train.steps.0] &#39;training&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-align] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-align.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-wer] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-wer.parameters.testIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.parameters.gtIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.categories.0] &#39;evaluation&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-wer.steps.0] &#39;evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-jar] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-jar.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-profile] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.executable] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.backend] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.language] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.additionalLexicon] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-train.parameters.gtArchives] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.gtArchives.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.training.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training.required] [&#39;trigrams&#39;, &#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;leFeatures&#39;, &#39;rrFeatures&#39;, &#39;dmFeatures&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-train.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-post-correct] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-post-correct.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.postCorrection.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection.required] [&#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;model&#39;, &#39;runLE&#39;, &#39;runDM&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-post-correct.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;cisocrgroup/ocrd_cis&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author-email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author_email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;MIT&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-cis&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/0.0.7/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;scipy&quot;, &quot;numpy (&amp;gt;=1.17.0)&quot;, &quot;pillow (&amp;gt;=6.2.0)&quot;, &quot;matplotlib (&amp;gt;3.0.0)&quot;, &quot;python-Levenshtein&quot;, &quot;calamari-ocr (==0.3.5)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;CIS OCR-D command line tools&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.7&quot;}, &quot;last_serial&quot;=&amp;gt;6235442, &quot;releases&quot;=&amp;gt;{&quot;0.0.6&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;sha256&quot;=&amp;gt;&quot;ac2ada13f48b301831e41cba1e9a86b8e10ac2e8f4036ecdda9eb3524e36461c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044792, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:33.819139Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f7/e0/5e3953c9243d05859e679bb83bef9c6f08e10fe0eef736fce90bc42657bc/ocrd_cis-0.0.6-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;sha256&quot;=&amp;gt;&quot;97aea3f172a5eda7272113eb99d55fddda0a96069a20173ea17563d0532bbd55&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96645, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:38.406783Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8a/a9/1fab502623c41529c13b4ecbedfe224f35843160ddcef4c527a18cfe73b8/ocrd_cis-0.0.6.tar.gz&quot;}], &quot;0.0.7&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}         ocrd_anybaseocr    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-layouterkennungnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nCOPY ocrd_anybaseocr ./ocrd_anybaseocrnRUN pip3 install .n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.n n #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.n n #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)n n #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.n n #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.n n #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.n n #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.n n #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.n n #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/mjenckel/LAYoutERkennung/&quot;,n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-anybaseocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-binarize&quot;,n      &quot;description&quot;: &quot;Binarize images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;parameters&quot;: {n        &quot;nocheck&quot;:         {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;disable error checking on inputs&quot;},n        &quot;show&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;display final results&quot;},n        &quot;raw_copy&quot;:        {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;also copy the raw image&quot;},n        &quot;gray&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;force grayscale processing even if image seems binary&quot;},n        &quot;bignore&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1,   &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;debug&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,     &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;escale&quot;:          {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0,   &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;hi&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 90,    &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;lo&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 5,     &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;perc&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 80,    &quot;description&quot;: &quot;percentage for filters&quot;},n        &quot;range&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 20,    &quot;description&quot;: &quot;range for filters&quot;},n        &quot;threshold&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;zoom&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;zoom for page background estimation, smaller=faster&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-deskew&quot;,n      &quot;description&quot;: &quot;Deskew images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;parameters&quot;: {n        &quot;escale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;bignore&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1, &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;threshold&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5, &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;maxskew&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;skew angle estimation parameters (degrees)&quot;},n        &quot;skewsteps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8,   &quot;description&quot;: &quot;steps for skew angle estimation (per degree)&quot;},n        &quot;debug&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;parallel&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;???&quot;},n        &quot;lo&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 5,   &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;hi&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 90,   &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-crop&quot;,n      &quot;description&quot;: &quot;Image crop using non-linear processing&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;parameters&quot;: {n        &quot;colSeparator&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.04, &quot;description&quot;: &quot;consider space between column. 25% of width&quot;},n        &quot;maxRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Consider maximum rular area&quot;},n        &quot;minArea&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.05, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;minRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Consider minimum rular area&quot;},n        &quot;positionBelow&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.75, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;positionLeft&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.4, &quot;description&quot;: &quot;rular position in left&quot;},n        &quot;positionRight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.6, &quot;description&quot;: &quot;rular position in right&quot;},n        &quot;rularRatioMax&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 10.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularRatioMin&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 3.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularWidth&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.95, &quot;description&quot;: &quot;maximum rular width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-dewarp&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-dewarp&quot;,n      &quot;description&quot;: &quot;dewarp image with anyBaseOCR&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/dewarping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DEWARP&quot;],n      &quot;parameters&quot;: {n        &quot;imgresize&quot;:    { &quot;type&quot;: &quot;string&quot;,                      &quot;default&quot;: &quot;resize_and_crop&quot;, &quot;description&quot;: &quot;run on original size image&quot;},n        &quot;pix2pixHD&quot;:    { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;: &quot;Path to pix2pixHD library&quot;},n        &quot;model_name&quot;:t{ &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models&quot;, &quot;description&quot;: &quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;},n        &quot;checkpoint_dir&quot;:   { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;./&quot;, &quot;description&quot;: &quot;Path to where to look for dir with model name&quot;},n        &quot;gpu_id&quot;:       { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,    &quot;description&quot;: &quot;gpu id&quot;},n        &quot;resizeHeight&quot;: { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image height&quot;},n        &quot;resizeWidth&quot;:  { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-tiseg&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-tiseg&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;separate text and non-text part with anyBaseOCR&quot;,n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-textline&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-textline&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE-ANY&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;description&quot;: &quot;separate each text line&quot;,n      &quot;parameters&quot;: {n        &quot;minscale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 12.0, &quot;description&quot;: &quot;minimum scale permitted&quot;},n        &quot;maxlines&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 300, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.0, &quot;description&quot;: &quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.0, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.7, &quot;description&quot;: &quot;non-standard scaling of vertical parameters&quot;},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.2, &quot;description&quot;: &quot;baseline threshold&quot;},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8, &quot;description&quot;: &quot;noise threshold for removing small components from lines&quot;},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;use gaussian instead of uniform&quot;},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum black column separators&quot;},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 10, &quot;description&quot;: &quot;widen black separators (to account for warping)&quot;},n        &quot;blackseps&quot;:   {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;also check for black column separators&quot;},n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum # whitespace column separators&quot;},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.1, &quot;description&quot;: &quot;minimum aspect ratio for column separators&quot;},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 6.5, &quot;description&quot;: &quot;minimum column height (units=scale)&quot;},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;padding for extracted lines&quot;},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;expand mask for grayscale extraction&quot;},n        &quot;parallel&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0, &quot;description&quot;: &quot;number of CPUs to use&quot;},n        &quot;libpath&quot;:     {&quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;.&quot;, &quot;description&quot;: &quot;Library Path for C Executables&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-layout-analysis&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-layout-analysis&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LAYOUT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {n        &quot;batch_size&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 4, &quot;description&quot;: &quot;Batch size for generating test images&quot;},n        &quot;model_path&quot;:         { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/structure_analysis.h5&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classification Model&quot;},n        &quot;class_mapping_path&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/mapping_DenseNet.pickle&quot;,&quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classes&quot;}n      }n    },n    &quot;ocrd-anybaseocr-block-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-block-segmentation&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-BLOCK-SEGMENT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {        n        &quot;block_segmentation_model&quot;:   { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to block segmentation Model&quot;},n        &quot;block_segmentation_weights&quot;: { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/block_segmentation_weights.h5&quot;,  &quot;required&quot;: false, &quot;description&quot;: &quot;Path to model weights&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }       n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd-anybaseocr&#39;,n    version=&#39;0.0.1&#39;,n    author=&quot;DFKI&quot;,n    author_email=&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;,n    url=&quot;https://github.com/mjenckel/LAYoutERkennung&quot;,n    license=&#39;Apache License 2.0&#39;,n    long_description=open(&#39;README.md&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    packages=find_packages(exclude=[&quot;work_dir&quot;, &quot;src&quot;]),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;]n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-anybaseocr-binarize           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_binarize&#39;,n            &#39;ocrd-anybaseocr-deskew             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_deskew&#39;,n            &#39;ocrd-anybaseocr-crop               = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_cropping&#39;,        n            &#39;ocrd-anybaseocr-dewarp             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_dewarp&#39;,n            &#39;ocrd-anybaseocr-tiseg              = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_tiseg&#39;,n            &#39;ocrd-anybaseocr-textline           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_textline&#39;,n            &#39;ocrd-anybaseocr-layout-analysis    = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_layout_analysis&#39;,n            &#39;ocrd-anybaseocr-block-segmentation = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_block_segmentation&#39;n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Dec 17 13:28:07 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;111&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_anybaseocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung/&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-anybaseocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;gray&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;force grayscale processing even if image seems binary&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;nocheck&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;disable error checking on inputs&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;perc&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;80, &quot;description&quot;=&amp;gt;&quot;percentage for filters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;20, &quot;description&quot;=&amp;gt;&quot;range for filters&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;raw_copy&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also copy the raw image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;show&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;display final results&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;zoom&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;zoom for page background estimation, smaller=faster&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-anybaseocr-block-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-block-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BLOCK-SEGMENT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;block_segmentation_model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/&quot;, &quot;description&quot;=&amp;gt;&quot;Path to block segmentation Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;block_segmentation_weights&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/block_segmentation_weights.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to model weights&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Image crop using non-linear processing&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;colSeparator&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.04, &quot;description&quot;=&amp;gt;&quot;consider space between column. 25% of width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.3, &quot;description&quot;=&amp;gt;&quot;Consider maximum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.01, &quot;description&quot;=&amp;gt;&quot;Consider minimum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;positionBelow&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.75, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionLeft&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.4, &quot;description&quot;=&amp;gt;&quot;rular position in left&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionRight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.6, &quot;description&quot;=&amp;gt;&quot;rular position in right&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMax&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMin&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.95, &quot;description&quot;=&amp;gt;&quot;maximum rular width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-anybaseocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;skew angle estimation parameters (degrees)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;???&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;skewsteps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;steps for skew angle estimation (per degree)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-anybaseocr-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;dewarp image with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DEWARP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint_dir&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;./&quot;, &quot;description&quot;=&amp;gt;&quot;Path to where to look for dir with model name&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;gpu_id&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;gpu id&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;imgresize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;resize_and_crop&quot;, &quot;description&quot;=&amp;gt;&quot;run on original size image&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_name&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models&quot;, &quot;description&quot;=&amp;gt;&quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pix2pixHD&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;=&amp;gt;&quot;Path to pix2pixHD library&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;resizeHeight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image height&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;resizeWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image width&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-anybaseocr-layout-analysis&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-layout-analysis&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LAYOUT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;batch_size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;Batch size for generating test images&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;class_mapping_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/mapping_DenseNet.pickle&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classes&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/structure_analysis.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classification Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-textline&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate each text line&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-textline&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE-ANY&quot;], &quot;parameters&quot;=&amp;gt;{&quot;blackseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also check for black column separators&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;minimum aspect ratio for column separators&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;6.5, &quot;description&quot;=&amp;gt;&quot;minimum column height (units=scale)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;expand mask for grayscale extraction&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;libpath&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;.&quot;, &quot;description&quot;=&amp;gt;&quot;Library Path for C Executables&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum # whitespace column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxlines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum black column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;12.0, &quot;description&quot;=&amp;gt;&quot;minimum scale permitted&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;noise threshold for removing small components from lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;padding for extracted lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;number of CPUs to use&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;widen black separators (to account for warping)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;baseline threshold&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;use gaussian instead of uniform&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.7, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of vertical parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-anybaseocr-tiseg&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate text and non-text part with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-tiseg&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-anybaseocr-tiseg.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-layout-analysis.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-block-segmentation.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_anybaseocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author-email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author_email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.nn #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.nn #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)nn #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.nn #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.nn #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.nn #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.nn #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.nn #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/0.0.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;opencv-python-headless (&amp;gt;=3.4)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;ocrd-fork-pylsd (&amp;gt;=0.0.3)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;torch (&amp;gt;=1.1.0)&quot;, &quot;torchvision&quot;, &quot;pandas&quot;, &quot;keras&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;scikit-image&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;last_serial&quot;=&amp;gt;6317222, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr&quot;}         ocrd_pc_segmentation    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# WORK IN PROGRESS - NOT READYnFROM ocrd/corenVOLUME [&quot;/data&quot;]nMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .n#COPY requirements_test.txt .nCOPY ocrd_pc_segmentation ./ocrd_pc_segmentationnCOPY Makefile .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n        build-essential n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove build-essentialn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depsn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;,n  &quot;version&quot;: &quot;0.1.0&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-pixelclassifier-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-pc-segmentation&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG-BIN&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;xheight&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;height of character x in pixels used during training&quot;,n          &quot;default&quot;: 8n        },n        &quot;model&quot;:  {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;trained model for pixel classifier&quot;,n          &quot;default&quot;: &quot;__DEFAULT__&quot;n        },n        &quot;gpu_allow_growth&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;nn        },n        &quot;resize_height&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;default&quot;: 300,n          &quot;description&quot;: &quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;n        }nn      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_pc_segmentation&#39;,n    version=&#39;0.1.3&#39;,n    description=&#39;pixel-classifier based page segmentation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Alexander Gehrke, Christian Reul, Christoph Wick&#39;,n    author_email=&#39;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&#39;,n    url=&#39;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&quot;requirements.txt&quot;).read().split(),n    extras_require={n        &#39;tf_cpu&#39;: [&#39;ocr4all_pixel_classifier[tf_cpu]&amp;gt;=0.0.1&#39;],n        &#39;tf_gpu&#39;: [&#39;ocr4all_pixel_classifier[tf_gpu]&amp;gt;=0.0.1&#39;],n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    classifiers=[n        &quot;Programming Language :: Python :: 3&quot;,n        &quot;License :: OSI Approved :: Apache Software License&quot;,n        &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;nn    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-pc-segmentation=ocrd_pc_segmentation.cli:ocrd_pc_segmentation&#39;,n        ]n    },n    data_files=[(&#39;&#39;, [&quot;requirements.txt&quot;])],n    include_package_data=True,n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Jan 20 10:00:24 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;29&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd-pixelclassifier-segmentation.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-pixelclassifier-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;gpu_allow_growth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;__DEFAULT__&quot;, &quot;description&quot;=&amp;gt;&quot;trained model for pixel classifier&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;resize_height&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;xheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;height of character x in pixels used during training&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-pixelclassifier-segmentation.parameters.xheight.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-pixelclassifier-segmentation.parameters.resize_height.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author-email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author_email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[&quot;License :: OSI Approved :: Apache Software License&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;], &quot;description&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/0.1.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0a1)&quot;, &quot;click&quot;, &quot;ocr4all-pixel-classifier (&amp;gt;=0.1.3)&quot;, &quot;numpy&quot;, &quot;ocr4all-pixel-classifier[tf_cpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_cpu&#39;&quot;, &quot;ocr4all-pixel-classifier[tf_gpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_gpu&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;pixel-classifier based page segmentation&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.3&quot;}, &quot;last_serial&quot;=&amp;gt;6169845, &quot;releases&quot;=&amp;gt;{&quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;sha256&quot;=&amp;gt;&quot;c22e9fad55a01f29bea78943c8ac93bc1a0780cbc6b606cbf81bac5f888d2294&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559195, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:31&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:31.585016Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/72/f6/5936ad2bdc878920ae26b448bd68eb580f04632b373d5fba62c79a8c8148/ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;sha256&quot;=&amp;gt;&quot;9f908f54f86d85a10b5d1d339e9f964f1b2ade3b4032ee8dadeeaa474dc299b7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547673, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:39.690671Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d9/82/c3fee56b73554529fe319dd596df56758e5429b1d5ee4b8603d404f7c94e/ocrd_pc_segmentation-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;sha256&quot;=&amp;gt;&quot;026be378afb3104e0f2367254da1da0f3ba212f5d4d5c8f6a7880b4eddc5b9a5&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559196, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:53:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:53:55.306178Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cc/d6/396ad6297c509445f03fddedc5efcd6f882ce5bb223c050157d675574858/ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;sha256&quot;=&amp;gt;&quot;e2dcd0b641accb8c6594d6dd24dcf1899c3cefbc033c5860b4ff72c20f1ad4ca&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547671, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:54:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:54:12.122137Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0c/47/46c39455cc4c5739e4599f7715c4b618193b561885aa302777fd7b11c1b5/ocrd_pc_segmentation-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;}         dinglehopper    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;dinglehoppern============nndinglehopper is an OCR evaluation tool and reads [ALTO](https://github.com/altoxml), [PAGE](https://github.com/PRImA-Research-Lab/PAGE-XML) and text files.nn[![Build Status](https://travis-ci.org/qurator-spk/dinglehopper.svg?branch=master)](https://travis-ci.org/qurator-spk/dinglehopper)nnGoalsn-----n* Usefuln  * As a UI tooln  * For an automated evaluationn  * As a libraryn* Unicode supportnnInstallationn------------nIt&#39;s best to use pip, e.g.:n~~~nsudo pip install .n~~~nnUsagen-----n~~~ndinglehopper some-document.gt.page.xml some-document.ocr.alto.xmln~~~nThis generates `report.html` and `report.json`.nnnAs a OCR-D processor:n~~~nocrd-dinglehopper -m mets.xml -I OCR-D-GT-PAGE,OCR-D-OCR-TESS -O OCR-D-OCR-TESS-EVALn~~~nThis generates HTML and JSON reports in the `OCR-D-OCR-TESS-EVAL` filegroup.nnn![dinglehopper displaying metrics and character differences](.screenshots/dinglehopper.png?raw=true)nnTestingn-------nUse `pytest` to run the tests in [the tests directory](qurator/dinglehopper/tests):n~~~nvirtualenv -p /usr/bin/python3 venvn. venv/bin/activatenpip install -r requirements.txtnpip install pytestnpytestn~~~n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/qurator-spk/dinglehopper&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-dinglehopper&quot;: {n      &quot;executable&quot;: &quot;ocrd-dinglehopper&quot;,n      &quot;description&quot;: &quot;Evaluate OCR text against ground truth with dinglehopper&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-PAGE&quot;,n        &quot;OCR-D-OCR&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-EVAL&quot;n      ],n      &quot;categories&quot;: [n        &quot;Quality assurance&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ]n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;from io import opennfrom setuptools import find_packages, setupnnwith open(&#39;requirements.txt&#39;) as fp:n    install_requires = fp.read()nnsetup(n    name=&#39;dinglehopper&#39;,n    author=&#39;Mike Gerber, The QURATOR SPK Team&#39;,n    author_email=&#39;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&#39;,n    description=&#39;The OCR evaluation tool&#39;,n    long_description=open(&#39;README.md&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    keywords=&#39;qurator ocr&#39;,n    license=&#39;Apache&#39;,n    namespace_packages=[&#39;qurator&#39;],n    packages=find_packages(exclude=[&#39;*.tests&#39;, &#39;*.tests.*&#39;, &#39;tests.*&#39;, &#39;tests&#39;]),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;templates/*&#39;],n    },n    entry_points={n      &#39;console_scripts&#39;: [n        &#39;dinglehopper=qurator.dinglehopper.cli:main&#39;,n        &#39;ocrd-dinglehopper=qurator.dinglehopper.ocrd_cli:ocrd_dinglehopper&#39;,n      ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jan 14 13:22:42 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;56&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper.git&quot;}, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-dinglehopper&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Quality assurance&quot;], &quot;description&quot;=&amp;gt;&quot;Evaluate OCR text against ground truth with dinglehopper&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-dinglehopper&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-PAGE&quot;, &quot;OCR-D-OCR&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-EVAL&quot;], &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [] &#39;version&#39; is a required propertyn&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;qurator-spk/dinglehopper&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Mike Gerber, The QURATOR SPK Team&quot;, &quot;author-email&quot;=&amp;gt;&quot;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;UNKNOWN&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;}         ocrd_typegroups_classifier    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebr√§isch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;git_url&quot;: &quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-typegroups-classifier&quot;: {n      &quot;executable&quot;: &quot;ocrd-typegroups-classifier&quot;,n      &quot;description&quot;: &quot;Classification of 15th century type groups&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/font-identification&quot;n      ],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;parameters&quot;: {n        &quot;network&quot;: {n          &quot;description&quot;: &quot;The file name of the neural network to use, including sufficient path information&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: truen        },n        &quot;stride&quot;: {n          &quot;description&quot;: &quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 112n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_typegroups_classifier&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Typegroups classifier for OCR&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Matthias Seuret, Konstantin Baierer&#39;,n    author_email=&#39;seuretm@users.noreply.github.com&#39;,n    url=&#39;https://github.com/seuretm/ocrd_typegroups_classifier&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    include_package_data=True,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.tgc&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;typegroups-classifier=ocrd_typegroups_classifier.cli.simple:cli&#39;,n            &#39;ocrd-typegroups-classifier=ocrd_typegroups_classifier.cli.ocrd_cli:cli&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 11:38:59 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;77&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-typegroups-classifier&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Classification of 15th century type groups&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;network&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The file name of the neural network to use, including sufficient path information&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;stride&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;112, &quot;description&quot;=&amp;gt;&quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/font-identification&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_typegroups_classifier&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebr√§isch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.1)&quot;, &quot;pandas&quot;, &quot;scikit-image&quot;, &quot;torch (&amp;gt;=1.4.0)&quot;, &quot;torchvision (&amp;gt;=0.5.0)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Typegroups classifier for OCR&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6465300, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;sha256&quot;=&amp;gt;&quot;d469964e37069a2dab403bbf7400eec4ddabcf4ee83c86d6e88bda1bd96e9c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26290742, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:55.449239Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/e6/1b/5d0e6967985a7e23d01f558677bd7de4385dacc0186e4896ad23cb4e2f0d/ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;sha256&quot;=&amp;gt;&quot;6b339f6b52cb62acc93f64d11637aa895a2cfbe7958df3391e4d6480d8c87d28&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15969, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:59&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:59.723574Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6a/d0/620fd50f319ef68ec959b67d0c048bb0f1d602ca5cc0baa0ff46fd235382/ocrd_typegroups_classifier-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier&quot;}         ",
      "url": " /en/kwalitee.html"
    },
  

    {
      "slug": "en-spec-logging-html",
      "title": "",
      "content"	 : "THIS IS JUST A FIRST DRAFT!Conventions for LOGGINGThis section specifies how the output of the digitization workflow is logged.Target AudienceUsers and developers of digitization workflows in libraries and/or digitization centers.IntroductionLogging is essential for developers and users to debug applications.You always have to choose between two contradictory goals:  Runtime  InformationMany issues make troubleshooting easier but have a negative effect on the runtime.Therefore, log levels have been introduced to customize the output to suit your needs.When a workflow is executed, the output of the applications MAY be stored in files.All log files were stored in a subfolder ‚Äòmetadata/log‚Äô.The file name SHOULD contain the ID of the activity in the provenance and the name of the stream.FILENAME := ACTIVITY_ID + ‚Äú_‚Äù + OUTPUT_STREAM + ‚Äú.log‚Äù‚Äò&#39;‚ÄôExample:‚Äô‚Äô‚Äô ocrd-kraken-bin_0001_stdout.logLog LevelsA more detailed description will be found hereTRACE / ALLThis is the most verbose logging level to trace the path of the algorithm.E.g.: Start/End/Duration of a method, even loops may be logged. Note that logging within loops can dramatically affect performance.DEBUGThis level is used if parameters and or the status of an algorithm/method should be logged.INFOLog information about used settings on application level.WARNThis level is used to log information about missing/wrong configurations which may lead to errors.ERRORLog all events that produce no or a wrong result. It is useful to output all the information that helps to determine the cause without the need for further investigation.FormatThe format of the logging output has to be formatted like this:TIMESTAMP LEVEL LOGGERNAME - MESSAGEExample:08:03:40.017 WARN edu.kit.ocrd.MyTestClass - A warn messageMETSThe log files are not referenced inside METS.If they are listed in the provenance, their content must be included in the provenance.Ingest Workspace to OCR-D RepositoriumNo log files will be stored in repositoryUse CasesLog during the WorkflowAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are also stored in the provenance, only information that is important for later analysis should be provided.STDERR only contains error messages that cause the program to terminate (see Loglevel ERROR).STDOUT should only contain outputs that are maximum of the log level INFO (see Loglevel INFO).For automated workflows it is recommended to save STDERR only.Analyze applicationsAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are used to analyze the program flow and possible errors and the performance is not important, all information should be output here.",
      "url": " /en/spec/logging.html"
    },
  

    {
      "slug": "de-spec-logging-html",
      "title": "",
      "content"	 : "THIS IS JUST A FIRST DRAFT!Conventions for LOGGINGThis section specifies how the output of the digitization workflow is logged.Target AudienceUsers and developers of digitization workflows in libraries and/or digitization centers.IntroductionLogging is essential for developers and users to debug applications.You always have to choose between two contradictory goals:  Runtime  InformationMany issues make troubleshooting easier but have a negative effect on the runtime.Therefore, log levels have been introduced to customize the output to suit your needs.When a workflow is executed, the output of the applications MAY be stored in files.All log files were stored in a subfolder ‚Äòmetadata/log‚Äô.The file name SHOULD contain the ID of the activity in the provenance and the name of the stream.FILENAME := ACTIVITY_ID + ‚Äú_‚Äù + OUTPUT_STREAM + ‚Äú.log‚Äù‚Äò&#39;‚ÄôExample:‚Äô‚Äô‚Äô ocrd-kraken-bin_0001_stdout.logLog LevelsA more detailed description will be found hereTRACE / ALLThis is the most verbose logging level to trace the path of the algorithm.E.g.: Start/End/Duration of a method, even loops may be logged. Note that logging within loops can dramatically affect performance.DEBUGThis level is used if parameters and or the status of an algorithm/method should be logged.INFOLog information about used settings on application level.WARNThis level is used to log information about missing/wrong configurations which may lead to errors.ERRORLog all events that produce no or a wrong result. It is useful to output all the information that helps to determine the cause without the need for further investigation.FormatThe format of the logging output has to be formatted like this:TIMESTAMP LEVEL LOGGERNAME - MESSAGEExample:08:03:40.017 WARN edu.kit.ocrd.MyTestClass - A warn messageMETSThe log files are not referenced inside METS.If they are listed in the provenance, their content must be included in the provenance.Ingest Workspace to OCR-D RepositoriumNo log files will be stored in repositoryUse CasesLog during the WorkflowAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are also stored in the provenance, only information that is important for later analysis should be provided.STDERR only contains error messages that cause the program to terminate (see Loglevel ERROR).STDOUT should only contain outputs that are maximum of the log level INFO (see Loglevel INFO).For automated workflows it is recommended to save STDERR only.Analyze applicationsAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are used to analyze the program flow and possible errors and the performance is not important, all information should be output here.",
      "url": " /de/spec/logging.html"
    },
  

    {
      "slug": "en-spec-mets-html",
      "title": "",
      "content"	 : "Requirements on handling METS/PAGEOCR-D has decided to base its data exchange format on top of METS.For layout and text recognition results, the primary exchange format is PAGEThis document defines a set of conventions and mechanism for using METS.Conventions for PAGE are outlined in a separate documentPixel density of images must be explicit and high enoughThe pixel density is the ratio of the number of pixels that represent a a unit of measure of the scanned object. It is typically measured in pixels per inch (PPI, a.k.a. DPI).The original input images MUST have &amp;gt;= 150 ppi.Every processing step that generates new images and changes their dimensions MUST make sure to adapt the density explicitly when serialising the image.$&amp;gt; exiftool input.tif |grep &#39;X Resolution&#39;&quot;300&quot;# WRONG (ppi unchanged)$&amp;gt; convert input.tif -resize 50% output.tif# RIGHT:$&amp;gt; convert input.tif -resize 50% -density 150 -unit inches output.tif$&amp;gt; exiftool output.tif |grep &#39;X Resolution&#39;&quot;150&quot;However, since technical metadata about pixel density is so often lost inconversion or inaccurate, processors should assume 300 ppi for images withmissing or suspiciously low pixel density metadata.No multi-page imagesImage formats like TIFF support encoding multiple images in a single file.Data providers MUST provide single-image TIFF files.OCR-D processors MUST raise an exception if they encounter multi-image TIFF files.Unique ID for the document processedMETS provided to the MP must be uniquely addressable within the global library community.For this purpose, the METS file MUST contain a mods:identifier that must contain a globally unique identifier for the document and have a type attribute with a value of, in order of preference:  purl  urn  handle  urlFile GroupAll mets:file inside a mets:fileGrp MUST have the same MIMETYPE.File Group USE syntaxAll mets:fileGrp MUST have a unique USE attribute that hints at the provenance of the files.It SHOULD have the structureID := &quot;OCR-D-&quot; + PREFIX? + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?PREFIX := (&quot;&quot; | &quot;GT-&quot;)WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}PREFIX can be GT- to indicate that these files are ground truth.WORKFLOW_STEP can be one of:  IMG: Image(s)  SEG: Segmented blocks / lines / words  OCR: OCR produced from image  COR: Post-correctionPROCESSOR should be a mnemonic of the processor or result type in a terse,all-caps form, such as the name of the tool (KRAKEN) or the organisationCIS or the type of manipulation (CROP) or a combination of both startingwith the type of manipulation (BIN-KRAKEN).Examples            &amp;lt;mets:fileGrp USE&amp;gt;      Type of use for OCR-D                  &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      The unmanipulated source images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN&quot;&amp;gt;      Black-and-White images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-CROP&quot;&amp;gt;      Cropped images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DESKEW&quot;&amp;gt;      Deskewed images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DESPECK&quot;&amp;gt;      Despeckled images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DEWARP&quot;&amp;gt;      Dewarped images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-BLOCK&quot;&amp;gt;      Block segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-LINE&quot;&amp;gt;      Line segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-WORD&quot;&amp;gt;      Word segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-TESS&quot;&amp;gt;      Tesseract OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-ANY&quot;&amp;gt;      AnyOCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-CIS&quot;&amp;gt;      CIS post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-ASV&quot;&amp;gt;      ASV post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-BIN&quot;&amp;gt;      Black-and-White images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-CROP&quot;&amp;gt;      Cropped images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DESKEW&quot;&amp;gt;      Deskewed images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DESPECK&quot;&amp;gt;      Despeckled images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DEWARP&quot;&amp;gt;      Dewarped images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-BLOCK&quot;&amp;gt;      Block segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-LINE&quot;&amp;gt;      Line segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-WORD&quot;&amp;gt;      Word segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation ground truth      File ID syntaxEach mets:file must have an ID attribute. The ID attribute of a mets:file SHOULD be the USE of the containing mets:fileGrp combined with a 4-zero-padded number.The ID MUST be unique inside the METS file.FILEID := ID + &quot;_&quot; + [0-9]{4}ID := &quot;OCR-D-&quot; + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}Examples            &amp;lt;mets:file ID&amp;gt;      ID of the file for OCR-D                  &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;      The unmanipulated source image              &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN_0001&quot;&amp;gt;      Black-and-White image      Grouping files by pageEvery METS file MUST have exactly one physical map that contains a singlemets:div[@TYPE=&quot;physSequence&quot;] which in turn must contain amets:div[@TYPE=&quot;page&quot;] for every page in the work.These mets:div[@TYPE=&quot;page&quot;] can contain an arbitrary number of mets:fptrpointers to mets:file elements to signify that all the files within a div areencodings of the same page.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;Images and coordinatesCoordinates are always absolute, i.e. relative to extent defined in the imageWidth/imageHeight attribute of the nearest &amp;lt;pc:Page&amp;gt;.When a processor wants to access the image of a layout element like a TextRegion or TextLine, the algorithm should be:  If the element in question has an attribute imageFilename, resolve this value  If the element has a &amp;lt;pc:Coords&amp;gt; subelement, resolve by passing the attribute imageFilename of the nearest &amp;lt;pc:Page&amp;gt; and the points attribute of the &amp;lt;pc:Coords&amp;gt; elementMedia Type for PAGE XMLEvery &amp;lt;mets:file&amp;gt; representing a PAGE document MUST have its MIMETYPE attribute set to application/vnd.prima.page+xml.Always use URL or relative filenamesAlways use URL, except for files located in the directory or any subdirectories of the METS file.Example/tmp/foo/ws1‚îú‚îÄ‚îÄ mets.xml‚îú‚îÄ‚îÄ foo.tif‚îî‚îÄ‚îÄ foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  /tmp/foo/ws1/foo.xml (absolute path)  file:///tmp/foo/ws1/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)If in PAGE then in METSEvery image URL referenced via imageFileName or the filename attribute of any pc:AlternativeImage MUST be represented in the METS file as a mets:file with corresponding mets:FLocat@xlink:href.Recording processing information in METSProcessors should add information to the METS metadata header to indicate thatthey changed the METS. This information is mainly for human consumption to getan overview of the software agents involved in the METS file‚Äôs creation. Moredetailed or machine-actionable provenance information is outside the scope ofthe processor.To add agent information, a processor must:1) locate the first mets:metsHdr M.2) Add to M a new mets:agent A with these attributes  TYPE must be the string OTHER  OTHERTYPE must be the string SOFTWARE  ROLE must be the string OTHER  OTHERROLE must be the processing step this processor provided, from the list in the ocrd-tool.json spec3) Add to A a mets:name N that should include, in free-text form, these data points  Name of the processor, e.g. the name of the executable from ocrd-tool.json  Version of the processor, e.g. from ocrd-tool.jsonExample:&amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;OTHER&quot; OTHERROLE=&quot;preprocessing/optimization/binarization&quot;&amp;gt;  &amp;lt;mets:name&amp;gt;ocrd_tesserocr v0.1.2&amp;lt;/mets:name&amp;gt;&amp;lt;/mets:agent&amp;gt;",
      "url": " /en/spec/mets.html"
    },
  

    {
      "slug": "de-spec-mets-html",
      "title": "",
      "content"	 : "Requirements on handling METS/PAGEOCR-D has decided to base its data exchange format on top of METS.For layout and text recognition results, the primary exchange format is PAGEThis document defines a set of conventions and mechanism for using METS.Conventions for PAGE are outlined in a separate documentPixel density of images must be explicit and high enoughThe pixel density is the ratio of the number of pixels that represent a a unit of measure of the scanned object. It is typically measured in pixels per inch (PPI, a.k.a. DPI).The original input images MUST have &amp;gt;= 150 ppi.Every processing step that generates new images and changes their dimensions MUST make sure to adapt the density explicitly when serialising the image.$&amp;gt; exiftool input.tif |grep &#39;X Resolution&#39;&quot;300&quot;# WRONG (ppi unchanged)$&amp;gt; convert input.tif -resize 50% output.tif# RIGHT:$&amp;gt; convert input.tif -resize 50% -density 150 -unit inches output.tif$&amp;gt; exiftool output.tif |grep &#39;X Resolution&#39;&quot;150&quot;However, since technical metadata about pixel density is so often lost inconversion or inaccurate, processors should assume 300 ppi for images withmissing or suspiciously low pixel density metadata.No multi-page imagesImage formats like TIFF support encoding multiple images in a single file.Data providers MUST provide single-image TIFF files.OCR-D processors MUST raise an exception if they encounter multi-image TIFF files.Unique ID for the document processedMETS provided to the MP must be uniquely addressable within the global library community.For this purpose, the METS file MUST contain a mods:identifier that must contain a globally unique identifier for the document and have a type attribute with a value of, in order of preference:  purl  urn  handle  urlFile GroupAll mets:file inside a mets:fileGrp MUST have the same MIMETYPE.File Group USE syntaxAll mets:fileGrp MUST have a unique USE attribute that hints at the provenance of the files.It SHOULD have the structureID := &quot;OCR-D-&quot; + PREFIX? + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?PREFIX := (&quot;&quot; | &quot;GT-&quot;)WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}PREFIX can be GT- to indicate that these files are ground truth.WORKFLOW_STEP can be one of:  IMG: Image(s)  SEG: Segmented blocks / lines / words  OCR: OCR produced from image  COR: Post-correctionPROCESSOR should be a mnemonic of the processor or result type in a terse,all-caps form, such as the name of the tool (KRAKEN) or the organisationCIS or the type of manipulation (CROP) or a combination of both startingwith the type of manipulation (BIN-KRAKEN).Examples            &amp;lt;mets:fileGrp USE&amp;gt;      Type of use for OCR-D                  &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      The unmanipulated source images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN&quot;&amp;gt;      Black-and-White images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-CROP&quot;&amp;gt;      Cropped images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DESKEW&quot;&amp;gt;      Deskewed images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DESPECK&quot;&amp;gt;      Despeckled images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-DEWARP&quot;&amp;gt;      Dewarped images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-BLOCK&quot;&amp;gt;      Block segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-LINE&quot;&amp;gt;      Line segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-WORD&quot;&amp;gt;      Word segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-TESS&quot;&amp;gt;      Tesseract OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-ANY&quot;&amp;gt;      AnyOCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-CIS&quot;&amp;gt;      CIS post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-ASV&quot;&amp;gt;      ASV post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-BIN&quot;&amp;gt;      Black-and-White images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-CROP&quot;&amp;gt;      Cropped images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DESKEW&quot;&amp;gt;      Deskewed images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DESPECK&quot;&amp;gt;      Despeckled images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-IMG-DEWARP&quot;&amp;gt;      Dewarped images ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-BLOCK&quot;&amp;gt;      Block segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-LINE&quot;&amp;gt;      Line segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-WORD&quot;&amp;gt;      Word segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation ground truth      File ID syntaxEach mets:file must have an ID attribute. The ID attribute of a mets:file SHOULD be the USE of the containing mets:fileGrp combined with a 4-zero-padded number.The ID MUST be unique inside the METS file.FILEID := ID + &quot;_&quot; + [0-9]{4}ID := &quot;OCR-D-&quot; + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}Examples            &amp;lt;mets:file ID&amp;gt;      ID of the file for OCR-D                  &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;      The unmanipulated source image              &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN_0001&quot;&amp;gt;      Black-and-White image      Grouping files by pageEvery METS file MUST have exactly one physical map that contains a singlemets:div[@TYPE=&quot;physSequence&quot;] which in turn must contain amets:div[@TYPE=&quot;page&quot;] for every page in the work.These mets:div[@TYPE=&quot;page&quot;] can contain an arbitrary number of mets:fptrpointers to mets:file elements to signify that all the files within a div areencodings of the same page.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;Images and coordinatesCoordinates are always absolute, i.e. relative to extent defined in the imageWidth/imageHeight attribute of the nearest &amp;lt;pc:Page&amp;gt;.When a processor wants to access the image of a layout element like a TextRegion or TextLine, the algorithm should be:  If the element in question has an attribute imageFilename, resolve this value  If the element has a &amp;lt;pc:Coords&amp;gt; subelement, resolve by passing the attribute imageFilename of the nearest &amp;lt;pc:Page&amp;gt; and the points attribute of the &amp;lt;pc:Coords&amp;gt; elementMedia Type for PAGE XMLEvery &amp;lt;mets:file&amp;gt; representing a PAGE document MUST have its MIMETYPE attribute set to application/vnd.prima.page+xml.Always use URL or relative filenamesAlways use URL, except for files located in the directory or any subdirectories of the METS file.Example/tmp/foo/ws1‚îú‚îÄ‚îÄ mets.xml‚îú‚îÄ‚îÄ foo.tif‚îî‚îÄ‚îÄ foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  /tmp/foo/ws1/foo.xml (absolute path)  file:///tmp/foo/ws1/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)If in PAGE then in METSEvery image URL referenced via imageFileName or the filename attribute of any pc:AlternativeImage MUST be represented in the METS file as a mets:file with corresponding mets:FLocat@xlink:href.Recording processing information in METSProcessors should add information to the METS metadata header to indicate thatthey changed the METS. This information is mainly for human consumption to getan overview of the software agents involved in the METS file‚Äôs creation. Moredetailed or machine-actionable provenance information is outside the scope ofthe processor.To add agent information, a processor must:1) locate the first mets:metsHdr M.2) Add to M a new mets:agent A with these attributes  TYPE must be the string OTHER  OTHERTYPE must be the string SOFTWARE  ROLE must be the string OTHER  OTHERROLE must be the processing step this processor provided, from the list in the ocrd-tool.json spec3) Add to A a mets:name N that should include, in free-text form, these data points  Name of the processor, e.g. the name of the executable from ocrd-tool.json  Version of the processor, e.g. from ocrd-tool.jsonExample:&amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;OTHER&quot; OTHERROLE=&quot;preprocessing/optimization/binarization&quot;&amp;gt;  &amp;lt;mets:name&amp;gt;ocrd_tesserocr v0.1.2&amp;lt;/mets:name&amp;gt;&amp;lt;/mets:agent&amp;gt;",
      "url": " /de/spec/mets.html"
    },
  

    {
      "slug": "en-module-processors-html",
      "title": "",
      "content"	 : "cor-asv-ann          cor-asv-fst          ocrd_calamari          ocrd_im6convert          ocrd_keraslm          ocrd_kraken        ocrd_ocropy          ocrd_olena          ocrd_segment          ocrd_tesserocr          ocrd_cis        ocrd_anybaseocr        ocrd_pc_segmentation          dinglehopper        ocrd_typegroups_classifier          ",
      "url": " /en/module-processors.html"
    },
  

    {
      "slug": "en-module-processors-html",
      "title": "",
      "content"	 : "            cor-asv-ann                                                        cor-asv-fst                                                        ocrd_calamari                                                        ocrd_im6convert                                                        ocrd_keraslm                                                        ocrd_kraken                                                        ocrd_ocropy                                                        ocrd_olena                                                        ocrd_segment                                                        ocrd_tesserocr                                                        ocrd_cis                                                        ocrd_anybaseocr                                                        ocrd_pc_segmentation                                                        dinglehopper                                                        ocrd_typegroups_classifier                                            ",
      "url": " /en/module-processors.html"
    },
  

    {
      "slug": "en-module-projects-html",
      "title": "",
      "content"	 : "Module ProjectsIn the first project phase, a functional model for the OCR-D workflow wasdeveloped. Full text recognition is seen as a complex process that includesseveral upstream and downstream steps in addition to the actual textrecognition. First, a digital image is preprocessed for text recognition bycropping, deskewing, dewarping, despeckling and binarizing it into a black andwhite image. This is followed by layout recognition, which identifies the textareas of a page down to line level. The recognition of the lines or thebaseline is particularly important for the subsequent text recognition, whichis based on neural networks in all modern approaches. The individual structuresor elements of the fully text-recognized document are then classified accordingto their typographic function before the OCR result is improved in thepost-correction, if necessary. Finally the end result is transferred torepositories for long-term archiving.From the project proposals for the DFG‚Äôs module project call in March 2017,eight projects were approved:Scalable methods of text and structure recognition for full text digitization of historical prints: Image OptimisationGerman Research Center for Artificial Intelligence (DFKI)Project participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungDFKI was involved in the OCR-D project with two modules: Image optimization andlayout recognition. In both modules several processors were developed andintegrated into the OCR-D software system.The first module project image optimization focused on the pre-processing ofthe digitized material with the aim of improving the image quality and thus theperformance of the subsequent OCR modules. For this purpose, tools forbinarization, deskewing, cropping and dewarping were implemented.The cropping tool based on computer vision is particularly noteworthy for itsperformance. It predominantly achieves very good results on the entire projectdata. The dewarping tool is also interesting due to its novel architecture.Generative neural networks are used to generate equalized variants of imagesinstead of determining explicit transformations for the equalization.Scalable text and structure recognition methods for the full text digitization of historical prints: Layout RecognitionDFKIProject participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungIn the second DFKI module project layout recognition, the aim was to extractthe document structure, both of individual document pages and the entiredocument. On the one hand, the metadata obtained in this way helps to digitizethe document as a whole, on the other hand, the extraction of certain documentstructures is necessary. For example, most OCR methods can only processindividual lines of text. The tools developed are used for text-non-textsegmentation, block segmentation and classification, text line detection andstructure analysis.One focus of development was the combined block segmentation and classificationbased on the MaskRCNN architecture known from video and image segmentation.This tool works with the unprocessed raw data, so that on the one hand nopre-processing is necessary and on the other hand the full information spectrumcan be used.Further development of a semi-automated open source tool for layout analysis and region extraction and classification (LAREX) of early printingJulius-Maximilians-University of W√ºrzburg   Institute of Computer Science: Chair of Artificial Intelligence and Applied Computer ScienceProject participants: Frank Puppe, Alexander GehrkeGitHub: ocr-d-modul-2-segmentierungAt the Department of Computer Science VI at the University of W√ºrzburg, LAREXwas developed in the preliminary work. LAREX is a comfortable editor forannotating regions and layout elements on book pages. In the furtherdevelopment of the OCR-D module project, the focus was not only on improvingefficient operability but also on expanding automatic procedures.For this purpose a Convolutional-Neural-Net (CNN) was implemented and trained,which assigns each pixel of a page scan a classification in different classesin order to separate image and text. By considering the pixels of only oneclass each, a segmentation of the page is then carried out with classicalmethods. Another tested approach first used classical segmentation methods andthen classified the segments.The segmentation method based on CNN output was adapted to the OCR-Dinterfaces. Good results were achieved on pure text pages or pages with clearlyseparated images. There is potential for improvement especially in therecognition of decorative initials of older prints and other images close tothe text as well as multi-column layouts.NN/FST ‚Äì Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state TransducersUniversity of Leipzig  Institut f√ºr Informatik: Department of Automatic Language ProcessingProject participants: Gerhard Heyer, Robert SachunskyGitHub: ASVLeipzig/cor-asv-fstA fully automatic post-correction separately from the actual OCR only makessense if statistical knowledge about ‚Äúcorrect text‚Äù and about typical OCRerrors is added a priori. Neural networks (NN) as well as weighted finitetransducers (WFST), which can be trained on corresponding additional data, aresuitable for this purpose.For the implementation of a combined architecture of NN and FST it was decided to implement three modules:  a pure NN solution with continuously (end-to-end) trained model on thecharacter level alone - as a deep (multi-layer), bidirectional recurrentnetwork according to the encoder-decoder scheme (for different input andoutput lengths) with an attention mechanism and A*-Beamsearch withadjustable rejection threshold (against overcorrection), i.e.post-correction of text lines is treated like machine translation,  a NN language model (LM) at the character level - as a deep (multi-layer),bidirectional recurrent network with interface for graph input andincremental decoding  a WFST component with an error model to be trained explicitly on glyph leveland word model/lexicon, as well as connection to 2. - via WFST compositionof input graph with error and word model according to the sliding windowprinciple, conversion of the single windows to one hypothesis graph per textline, and combination of the respective output weights with LM-evaluationsin an efficient search for the best path.The combination of 3. with 2. thus represents a hybrid solution. But also 1.can benefit from 2. (if the same network topology is used) by initializing theweights from a language model trained on larger amounts of pure text (transferlearning).Both approaches benefit from a close connection to the OCR search space, i.e. atransfer of alternative character hypotheses and their confidence (as so faronly possible with Tesseract and realized in cooperation with the moduleproject of the Mannheim University Library). However, they also deliver goodresults on pure full text (with CER reduction of up to 5%), provided thatsufficient suitable training data is available and the OCR itself deliversuseful results (below 10% CER).Command line interfaces for training and evaluation as well as full OCR-Dinterfaces for processing and evaluation are available for all modules.Optimized use of OCR processes ‚Äì Tesseract as a component in the OCR-D workflowUniversity of Mannheim  &amp;lt;/br&amp;gt;University Library MannheimProject participants: Stefan Weil, Noah MetzgerGitHub: tesseract-ocr/tesseract/The module project focused on the OCR software Tesseract, which has beendeveloped by Ray Smith since 1985, since 2005 as open source under a freelicense.The project had two main goals: The integration of Tesseract into the OCR-Dworkflow including support of the other module projects by providinginterfaces, and the general improvement of stability, code quality andperformance of Tesseract.The integration into the OCR-D workflow required much less effort thanoriginally planned; mainly because most of the work had already been doneoutside the module project and the existing Python interface tesserocr could beused.For the OCR-D module project of the University of Leipzig, Tesseract wasextended to generate alternative OCR results for the single characters. Asinput data for an OCR post-correction model, text recognition can thus befurther improved. A valuable side-effect of the new code are more accuratecharacter and word coordinates.With several hundred corrections, the code quality was significantly improvedand a much more stable program flow was achieved. Tesseract is now moremaintainable, requires less memory and is faster than before.A significant improvement in recognition accuracy for most of the printingunits relevant for OCR-D was achieved by new generic models for Tesseract.These were trained from September 2019 until January 2020 on the basis of thedata collection GT4HistOCR.Automatic post-correction of historical OCR captured prints with integrated optional interactive correctionLudwig-Maximilians-University of MunichCentre for Information and Language Processing (CIS)Project participants: Klaus Schulz, Floran Fink, Tobias EnglmeierGitHub: https://github.com/cisocrgroup/ocrd-postcorrection, https://github.com/cisocrgroup/cis-ocrd-pyThe result of the project is a A-I-PoCoTo system integrated into the OCR-D workflow for fully automatic post-correction of full text recognized historical prints. The system also includes an optional interactive post-correction (I-PoCoTo), which is integrated into the interactive post-correction system PoCoWeb. The system can thus be used alternatively as a stand-alone tool for collaborative web-based post-correction of OCR documents.The basis of the fully automatic post-correction is a flexible, feature-based Machine Learning (ML) procedure for fully automatic OCR post-correction with a special focus on avoiding the problem of disimprovement. The system uses the document-dependent profiling technology developed at CIS to detect errors and to generate correction candidates. In addition to various confidence values, the features of the system also use information from additional auxiliary OCRs.The system logs all correction decisions. Via this protocol mechanism the automatic post correction in PoCoWeb can be checked interactively. You can manually undo individual correction decisions that have been made, and also subsequently execute correction decisions that have not been made.The entire system is integrated into the OCR-D workflow and follows the conventions valid there.Development of a model repository and an automatic font recognition for OCR-D_University Leipzig  Institute of Computer Science: Chair of Digital Humanities_  _Friedrich Alexander University Erlangen-Nuremberg  Department of Computer Science: Chair of Computer Science 5: Pattern Recognition_  _Johannes Gutenberg University Mainz  Gutenberg Institute for World Literature and Writing-Oriented Media: Department of Book Science_Project participants: Gregory Crane, Nikolaus Weichselbaumer, Saskia Limbach, Andreas Meier, Vincent Christlein, Mathias Seuret, Rui DongGitHub: OCR-D/okralact, https://github.com/seuretm/ocrd_typegroups_classifierThe recognition rates of OCR for prints produced before 1800 vary greatly, asthe diversity of historical fonts is either not taken into account at all oronly insufficiently in the training data. Therefore this module project,consisting of computer scientists and book historians, has set itself threegoals:On the one hand, we have developed a tool for the automatic recognition offonts in digitised images. Here, we have concentrated especially on brokenfonts besides fracture, which have received little attention so far, but werewidely used in the 15th and 16th centuries: Bastarda, Rotunda, Textura andSchwabacher. The tool has been trained with 35,000 images and achieves anaccuracy of 98% in determining fonts. Overall, it can not only differentiatebetween the above mentioned fonts, but also distinguish between Hebrew, Greek,Fraktur, Antiqua and Italic.In a second step, an online training infrastructure was created (Okralact). Itsimplifies the use of different OCR engines (Tesseract, Ocropus, Octopus,Calamari) and at the same time makes it possible to train specific models forcertain fonts.Finally, a model repository has been set up that contains already developedfont-specific OCR models. To lay a foundation here, we have transcribed a totalof about 2,500 lines for Bastarda, Textura and Schwabacher from a variety ofdifferent books.The high accuracy of the font recognition tool opens up the possibility ofhaving the tool even distinguish between the fonts of individual printers inthe future through further training data, which would address severaldesiderata of historical research.OLA-HD ‚Äì An OCR-D long-term archive for historical booksGeorg-August-University of G√∂ttingenState and University Library of Lower SaxonySociety for Scientific Data Processing mbH G√∂ttingenProject participants: Mustafa Dogan, Kristine Schima-Voigt, Philip Wieder, Triet Doan, J√∂rg-Holger PanzerGitHub: subugoe/OLA-HD-IMPLIn September 2018 the Digital Library Department of the State and University Library of Lower Saxony and the Gesellschaft f√ºr wissenschaftliche Datenverarbeitung G√∂ttingen started the DFG project OLA-HD - Ein OCR-D Langzeitarchiv f√ºr historische Drucke.The aim of OLA-HD is to develop an integrated concept for long-term archivingand persistent identification of OCR objects, as well as a prototypicalimplementation.In regular exchange with the project partners, the basic requirements forlong-term archiving and persistent identification were determined and recordedin the form of a specification for technical and economic-organizationalimplementation.With the prototype the user can upload OCR results of a work as OCRD-ZIP intothe system. The system validates the zip file, assigns a PID and sends the fileto the archive manager (CDSTAR - GWDG Common Data StorageArchitecture). This writes the Zip file to thearchive (tape storage). Depending on the configuration (file type, file size,etc.), files are also written to an online storage (hard disk) for fast access.The user has access to all OCR versions and can download versions as BagIt-Zipfiles. All works and versions have their own PIDs. The PIDs are generated bythe European Persistent Identifier Consortium(ePIC) service. The different OCR versions ofa work are linked via the PID, so that the system can map the versioning in atree structure.Users who are not logged in can browse the inventory and preview text and - ifavailable - images in the file structure or navigate through the differentversions. Users can register and log in via the GWDG portal and manage theirfiles via a dashboard.By March 2020, minor optimizations will be made to the user interface and theconcept will be finalized. The concept will describe further expansion stagesthat may be useful for transferring the prototype software into a product.",
      "url": " /en/module-projects.html"
    },
  

    {
      "slug": "de-module-projects-html",
      "title": "",
      "content"	 : "ModulprojekteAus den Projektantr√§gen f√ºr die Modulprojektausschreibung der DFG im M√§rz 2017 wurden acht Projekte bewilligt:  Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: Bildoptimierung  Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: Layouterkennung  Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von fr√ºhen Buchdrucken  NN/FST ‚Äì Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers  Optimierter Einsatz von OCR-Verfahren ‚Äì Tesseract als Komponente im OCR-D-Workflow  Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur  Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung f√ºr OCR-D  OLA-HD ‚Äì Ein OCR-D-Langzeitarchiv f√ºr historische Drucke**Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: BildoptimierungDeutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz (DFKI)Das DFKI war als Projektpartner im OCR-D Projekt mit zwei Modulen vertreten:Bildoptimierung und Layouterkennung. In beiden Modulen wurden mehrereProzessoren entwickelt und in das OCR-D-Softwaresystem integriert.Das erste Modul-Projekt Bildoptimierung fokussierte sich auf dieVorverarbeitung der Digitalisate mit dem Ziel, die Bildqualit√§t und somit auchdie Performanz der nachfolgenden OCR-Module zu verbessern. Daf√ºr wurdenWerkzeuge f√ºr die Binarisierung, das Deskewing, das Cropping und das Dewarpingimplementiert.Das auf Computer Vision basierte Cropping-Werkzeug ist als besonders performanthervorzuheben. Es erzielt auf den gesamten Projektdaten vorwiegend sehr guteErgebnisse. Auch das Dewarping-Werkzeug ist aufgrund seiner neuartigenArchitektur interessant. Mit Hilfe generativer neuronaler Netze werdenentzerrte Varianten von Bildern generiert, anstatt explizite Transformationenf√ºr die Entzerrung zu bestimmen.Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: Layouterkennung**&amp;lt;/a&amp;gt;DFKIGitHub: mjenckel/OCR-D-LAYoutERkennung/tree/masterIm zweiten Modul-Projekt des DFKI Layouterkennung galt es, dieDokumentstruktur, sowohl einzelner Dokumentseiten als auch im Gesamtdokument,zu extrahieren. Die dabei gewonnenen Metadaten helfen zum einen, das Dokumentals Ganzes zu digitalisieren, zum anderen ist das Extrahieren bestimmterDokumentstrukturen notwendig. Die meisten OCR-Methoden k√∂nnen z.B. nur einzelneTextzeilen verarbeiten. Die entwickelten Werkzeuge dienen derText-Nicht-Text-Segmentierung, der Blocksegmentierung und -klassifizierung, derTextzeilenerkennung sowie der Strukturanalyse.Ein Entwicklungsschwerpunkt war die kombinierte Blocksegmentierung und-klassifizierung, welche auf der, aus der Video- und Bildsegmentierungbekannten, MaskRCNN-Architektur basiert. Dieses Werkzeug arbeitet mit denunbearbeiteten Rohdaten, sodass einerseits keine Vorverarbeitung notwendig istund andererseits das volle Informationsspektrum ausgenutzt werden kann.Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von fr√ºhen BuchdruckenJulius-Maximilians-Universit√§t W√ºrzburg  ¬† Institut f√ºr Informatik: Lehrstuhl f√ºr K√ºnstliche Intelligenz und angewandte InformatikGitHub: ocr-d-modul-2-segmentierungAm Lehrstuhl f√ºr Informatik VI der Uni W√ºrzburg wurde in den Vorarbeiten LAREXentwickelt, ein komfortabler Editor zur Annotation von Regionen undLayout-Elementen auf Buchseiten. Bei der Weiterentwicklung imOCR-D-Modulprojekt lag der Schwerpunkt neben der Verbesserung der effizientenBedienbarkeit vor allem auch in dem Ausbau der automatischen Verfahren.Hierzu wurde ein Convolutional-Neural-Net (CNN) implementiert und trainiert,welches jedem Pixel eines Seitenscans eine Einordnung in verschiedene Klassenzuweist, um so Bild und Text zu trennen. Unter Betrachtung der Pixel je nureiner Klasse wird anschlie√üend mit klassischen Verfahren eine Segmentierung derSeite durchgef√ºhrt. Ein weiterer getesteter Ansatz nutzte zuerst klassischeSegmentierungsverfahren und ordnete die Segmente anschlie√üend ein.Das auf der CNN-Ausgabe basierende Segmentierungsverfahren wurde an dieOCR-D-Schnittstellen angepasst. Auf reinen Textseiten oder Seiten mit deutlichabgetrennten Bildern wurden gute Ergebnisse erzielt. Verbesserungspotentialbesteht vor allem bei der Erkennung von Zierinitialen √§lterer Drucke undweiteren nah am Text liegenden Bildern sowie mehrspaltigen Layouts.NN/FST ‚Äì Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers**Universit√§t Leipzig  ¬† Institut f√ºr Informatik: Abteilung Automatische SprachverarbeitungGitHub: ASVLeipzig/cor-asv-fstEine vollautomatische Nachkorrektur separat von der eigentlichen OCR ist immernur dann sinnvoll, wenn dabei statistisches Wissen √ºber ‚Äúrichtigen Text‚Äù und√ºber typische OCR-Fehler a priori hinzukommt. Daf√ºr eignen sich neuronaleNetze (NN) ebenso wie gewichtete endliche Transduktoren (WFST), die aufentsprechenden zus√§tzlichen Daten trainiert werden k√∂nnen.F√ºr die Umsetzung einer kombinierten Architektur aus NN und FST wurdeentschieden, drei Module zu implementieren:  eine reine NN-L√∂sung mit durchgehend (end-to-end) trainiertem Modellallein auf Zeichenebene ‚Äì als tiefes (mehrschichtiges), bidirektionalesrekurrentes Netzwerk nach dem Encoder-Decoder-Schema (f√ºr verschiedeneEingabe- und Ausgabel√§nge) mit Attention-Mechanismus und A*-Beamsearch miteinstellbarer R√ºckweisungsschwelle (gegen √úberkorrektur), d.h. dieNachkorrektur von Textzeilen wird wie maschinelle √úbersetzung behandelt,  ein NN-Sprachmodell (LM) auf Zeichenebene ‚Äì als tiefes (mehrschichtiges),bidirektionales rekurrentes Netzwerk mit Schnittstelle f√ºr Graph-Eingabeund inkrementeller Dekodierung,  eine WFST-Komponente mit explizit zu trainierendem Fehlermodell aufZeichenebene und Wortmodell/Lexikon, sowie Anbindung an 2. ‚Äì perWFST-Komposition von Eingabegraph mit Fehler- und Wortmodell nachSliding-Window-Prinzip, Konversion der Einzelfenster zu einemHypothesengraph pro Textzeile, und Kombination der jeweiligenAusgabegewichte mit LM-Bewertungen in einer effizienten Suche nach dembesten Pfad.Die Kombination von 3. mit 2. stellt also eine hybride L√∂sung dar. Aber auch 1.kann von 2. profitieren (sofern die gleiche Netzwerk-Topologie benutzt wird),indem die Gewichte aus einem auf gr√∂√üeren Mengen reinem Text trainiertenSprachmodell initialisiert werden (Transfer-Learning).Beide Ans√§tze profitieren von einer engen Anbindung an den OCR-Suchraum, d.h.eine √úbergabe alternativer Zeichen-Hypothesen und ihrer Konfidenz (wie bishernur mit Tesseract m√∂glich und in Zusammenarbeit mit dem Modulprojekt der UBMannheim realisiert). Sie liefern aber auch auf reinem Volltext bereits guteErgebnisse (mit CER-Reduktion von bis zu 5%), sofern gen√ºgend passendeTrainingsdaten zur Verf√ºgung stehen und die OCR ihrerseits brauchbareErgebnisse (unterhalb 10% CER) liefert.F√ºr alle Module stehen Kommandozeilen-Schnittstellen f√ºr Training undEvaluierung, sowie volle OCR-D-Schnittstellen f√ºr Prozessierung und Evaluierungzur Verf√ºgung.Optimierter Einsatz von OCR-Verfahren ‚Äì Tesseract als Komponente im OCR-D-WorkflowUniversit√§t Mannheim  ¬† Universit√§tsbibliothek MannheimGitHub: tesseract-ocr/tesseract/Im Fokus des Modulprojekts stand die OCR-Software Tesseract, die seit 1985 vonRay Smith entwickelt wurde, seit 2005 als Open Source unter einer freienLizenz.Das Projekt umfasste zwei Hauptziele: Die Einbindung von Tesseract in denOCR-D-Workflow inklusive Unterst√ºtzung der anderen Modulprojekte durch dieBereitstellung von Schnittstellen, sowie die allgemeine Verbesserung derStabilit√§t, Codequalit√§t und Performance von Tesseract.Die Einbindung in den OCR-D-Workflow erforderte wesentlich weniger Aufwand alsurspr√ºnglich geplant; haupts√§chlich, weil die meiste Arbeit bereits au√üerhalbdes Modulprojekts geleistet war und dabei die schon vorhandenePython-Schnittstelle tesserocr genutzt werden konnte.F√ºr das OCR-D-Modulprojekt der Universit√§t Leipzig wurde Tesseract um dieGenerierung von alternativen OCR-Ergebnissen f√ºr die Einzelzeichen erweitert.Als Eingabedaten f√ºr ein OCR-Postkorrektur-Modell l√§sst sich damit dieTexterkennung weiter verbessern. Ein wertvoller Nebeneffekt des neuen Codessind genauere Zeichen- und Wortkoordinaten.Mit mehreren hundert Korrekturen konnte die Codequalit√§t signifikant gesteigertund ein deutlich stabilerer Programmfluss erreicht werden. Tesseract ist jetztwartbarer, braucht weniger Speicher und ist schneller als zuvor.Eine wesentliche Verbesserung der Erkennungsgenauigkeit f√ºr die meisten der f√ºrOCR-D relevanten Druckwerke konnte durch neue generische Modelle f√ºr Tesseracterreicht werden. Diese wurden ab Septem-ber 2019 bis Januar 2020 auf Basis derDatensammlung GT4HistOCR trainiert.Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur**&amp;lt;/a&amp;gt;Ludwig-Maximilians-Universit√§t M√ºnchen¬† Centrum f√ºr Informations- und Sprachverarbeitung (CIS)GitHub: cisocrgroup/ocrd-postcorrection](https://github.com/cisocrgroup/ocrd-postcorrection), cisocrgroup/cis-ocrd-pyDas Ergebnis des Projekts ist ein in den OCR-D-Workflow integriertes SystemA-I-PoCoTo zur vollautomati-schen Nachkorrektur OCR-erfasster historischerDrucke. Das System beinhaltet zudem eine optional nachge-schaltete interaktiveNachkorrektur (I-PoCoTo), die in das interaktive NachkorrektursystemPoCoWeb einge-bunden ist. Das System kann damit auch alternativ alsStand-Alone-Tool zur gemeinschaftlichen webbasierten Nachkorrektur vonOCR-Dokumenten eingesetzt werden.Die Grundlage der vollautomatischen Nachkorrektur ist ein flexibles,featurebasiertes Machine-Learning (ML) Verfahren zur vollautomatischenOCR-Nachkorrektur mit einem besonderen Fokus auf die Vermeidung derVerschlimmbesserungsproblematik. Zur Erkennung von Fehlern und f√ºr dieErzeugung von Korrekturkandida-ten verwendet das System die am CIS entwickeltedokumentenabh√§ngige Profilierungstechnologie. Die Fea-tures des Systemsverwenden neben verschiedenen Konfidenzwerten insbesondere auch Informationenaus zus√§tzlichen Hilfs-OCRs.Das System protokolliert s√§mtliche Korrekturentscheidungen. √úber diesenProtokollmechanismus kann die automatische Postkorrektur in PoCoWebinteraktiv √ºberpr√ºft werden. Dabei k√∂nnen sowohl einzelne get√§tigteKorrekturentscheidungen manuell r√ºckg√§ngig gemacht werden, als auch nichtget√§tigte Korrekturentschei-dungen nachtr√§glich ausgef√ºhrt werden.Das gesamte System ist in den OCR-D-Workflow eingebunden und folgt den dortg√ºltigen Konventionen.&amp;lt;a id=‚ÄùSchriftarterkennung/&amp;gt;Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung f√ºr OCR-DUniversit√§t Leipzig   ¬† Institut f√ºr Informatik: Lehrstuhl f√ºr Digital Humanities   Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg    ¬† Department Informatik: Lehrstuhl f√ºr Informatik 5: MustererkennungJohannes Gutenberg-Universit√§t Mainz   &amp;lt;/br/&amp;gt;¬† Gutenberg-Institut f√ºr Weltliteratur und schriftorientierte Medien: Abteilung BuchwissenschaftGitHub: OCR-D/okralact, seuretm/ocrd_typegroups_classifierDie Erkennungsquoten von OCR f√ºr Drucke, die vor 1800 produziert wurden,variieren sehr stark, da die Diversit√§t historischer Schriftarten in denTrainingsdaten entweder gar nicht oder nur unzureichend ber√ºcksichtigt wird.Daher hat sich dieses Modulprojekt, bestehend aus Informatikerinnen undBuchhistorikerinnen, drei Ziele gesteckt:Zum einen haben wir ein Tool zur automatischen Erkennung von Schriftarten inBilddigitalisaten entwickelt. Hier haben wir uns besonders auf gebrocheneSchriften neben der Fraktur konzentriert, die bisher wenig Beachtung gefundenhaben, jedoch im 15. und 16. Jahrhundert weit verbreitet waren: Bastarda,Rotunda, Textura und Schwabacher. Das Tool wurde mit 35.000 Bildern trainiertund erreicht eine Genauigkeit von 98% bei der Bestimmung von Schriftarten.Insgesamt kann es nicht nur zwischen den o.g. Schriftarten differenzieren,sondern auch Hebr√§isch, Griechisch, Fraktur, Antiqua und Kursiv unterscheiden.In einem zweiten Schritt wurde eine Online-Trainingsinfrastruktur geschaffen(Okralact). Sie vereinfacht die Benutzung verschiedener OCR-engines (Tesseract,Ocropus, Kraken, Calamari) und erm√∂glicht es zugleich, spezifische Modelle f√ºrbestimmte Schriftarten zu trainieren.Zum Abschluss wurde ein Modellrepositorium eingerichtet, das bereitserarbeitete schriftartspezifische OCR-Modelle enth√§lt. Um hier einen Grundstockzu legen, haben wir insgesamt ca. 2.500 Zeilen f√ºr Bastarda, Textura undSchwabacher aus einer Vielzahl verschiedener B√ºcher transkribiert.Die hohe Genauigkeit des Tools zur Erkennung der Schriftarten er√∂ffnet dieM√∂glichkeit, in Zukunft durch weitere Trainingsdaten das Tool sogar zwischenden Schriften einzelner Drucker unterscheiden zu lassen, was mehrere Desiderateder historischen Forschung adressieren w√ºrde.OLA-HD ‚Äì Ein OCR-D-Langzeitarchiv f√ºr historische DruckeGeorg-August-Universit√§t G√∂ttingen  bb¬† Nieders√§chsische Staats- und Universit√§tsbibliothek   bbGesellschaft f√ºr Wissenschaftliche Datenverarbeitung mbH G√∂ttingen bbGitHub: subugoe/OLA-HD-IMPLIm September 2018 starteten die Abteilung Digitale Bibliothek derNieders√§chsischen Staats- und Universi-t√§tsbibliothek und die Gesellschaft f√ºrwissenschaftliche Datenverarbeitung G√∂ttingen das DFG-Projekt OLA-HD ‚Äì EinOCR-D Langzeitarchiv f√ºr historischeDrucke.Ziel von OLA-HD ist die Entwicklung eines integrierten Konzepts f√ºr dieLangzeitarchivierung und persistente Identifizierung von OCR-Objekten, sowieeine prototypische Implementierung.Im regelm√§√üigen Austausch mit den Projektpartnern wurden dieBasis-Anforderungen f√ºr die Langzeitarchivierung und persistente Identifikationermittelt und in Form einer Spezifikation zur technischen undwirtschaftlich-organisatorischen Umsetzung festgehalten.Mit dem Prototypen kann der Anwender OCR-Ergebnisse eines Werkes als OCRD-ZIPin das System laden. Das System validiert die Zip-Datei, vergibt eine PID undschickt die Datei an den Archiv-Manager (CDSTAR ‚Äì GWDG Common Data StorageArchitecture). Dieser schreibt die Zip-Datei in dasArchiv (Bandspeicher). Abh√§ngig von der Konfiguration (Datei-Typ, Datei-Gr√∂√üeetc.) werden Dateien zus√§tzlich in ein Online Storage geschrieben (Festplatte),um einen schnellen Zugriff zu erm√∂glichen. Der Nutzer hat Zugriff auf alleOCR-Versionen und kann Versionen als BagIt-Zip Dateien herunterladen. AlleWerke und Versionen haben eigene PIDs. Die PIDs werden vom European PersistentIdentifier Consortium (ePIC) Servicegeneriert. Die verschiedenen OCR-Versionen eines Werkes sind √ºber die PIDverkn√ºpft, sodass das System die Versionierung in einer Baumstruktur abbildenkann.Nicht angemeldete Anwender k√∂nnen den Bestand durchsuchen und in derDateistruktur eine Vorschau von Text und ‚Äì sofern vorhanden ‚Äì Bild erhaltenbzw. √ºber die verschiedenen Versionen navigieren. Die Anwender k√∂nnen sich √ºberdas GWDG-Portal registrieren und anmelden und k√∂nnen √ºber ein Dashboard ihreDateien verwalten.Bis M√§rz 2020 werden kleinere Optimierungen am User-Interface vorgenommen unddas Konzept finalisiert. Im Konzept werden weitere Ausbaustufen beschrieben,die sinnvoll sein k√∂nnen, um die prototypische Soft-ware in ein Produkt zu√ºberf√ºhren.",
      "url": " /de/module-projects.html"
    },
  

    {
      "slug": "en-ocrd-gt-repo-html",
      "title": "",
      "content"	 : "Working with OCR-D-(Ground-Truth)-RepositoryUpload bagit container from scratch to OCR-D(-GT)-RepositoryExample to upload a scanned page to OCR-D-Repo.Preparation: Create WorkspaceRequirements: ocrd (Version 1.0.0) See Setup OCR-D StackActivate virtualenvuser@hostname:~$source ~/env-ocrd/bin/activate(env-ocrd) user@hostname:~$Initialize Workspace(env-ocrd) user@hostname:~$ ocrd workspace init communist_manifesto(env-ocrd) user@hostname:~$ cd communist_manifestoCreate Folder for Scanned Page(env-ocrd) user@hostname:~/communist_manifesto$ mkdir OCR-D-IMGDownload Image (Google)(env-ocrd) user@hostname:~/communist_manifesto$ wget https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Manifesto_of_the_Communist_Party.djvu/page15-2745px-Manifesto_of_the_Communist_Party.djvu.jpg -O OCR-D-IMG/OCR-D-IMG_0015.jpgAdd Image to Workspace(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace add -g P0015 -G OCR-D-IMG -i OCR-D-IMG_0015 -m image/jpg OCR-D-IMG/OCR-D-IMG_0015.jpgSet Unique ID for Workspace(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace set-id &#39;communist_manifesto&#39;Validate WorkspaceFor some images, the resolution of the image is not set. To avoid validation errors, the resolution check is skipped.For further details see ‚Äòocrd workspace validate ‚Äìhelp‚Äô.(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace validate --skip pixel_density mets.xmlCreate BagIt Container(env-ocrd) user@hostname:~/communist_manifesto$ cd ..(env-ocrd) user@hostname:~/$ ocrd zip bag -i communist_manifesto -d communist_manifesto/Validate BagIt Container(env-ocrd) user@hostname:~/$ ocrd zip validate communist_manifesto.ocrd.zip[...]OKUpload BagIt Containeruser@hostname:~/$ curl -u ingest:GENERATED_PASSWORD -v -F &quot;file=@communist_manifesto.ocrd.zip&quot; http://localhost:8080/api/v1/metastore/bagit [...]OKDownload all BagIt Containersuser@hostname:~/Download$ wget -O listOfContainers.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagituser@hostname:~/Download$ ocrdzips=$(cat listOfContainers.json | tr &quot;,[]&quot;&quot; &quot;n&quot;)user@hostname:~/Download$ for addr in $ocrdzipsdo  wget $addr  filename=$(basename -- &quot;$addr&quot;)  directory=&quot;${filename%.*}&quot;  mkdir $directory  cd $directory  unzip ../$filename  cd ..doneList all Documents (in Browser)https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagitThe list shows all ingested documents with its  ‚ÄòUpload Date‚Äô  ‚ÄòVersion‚Äô  ‚ÄòOCR-D Identifier‚Äô  ‚ÄòLink for Download‚Äô  ‚ÄòReferenced Files‚Äô  ‚ÄòMetadata‚Äô  and ‚ÄòSemantic Labeling‚Äô(Upload is only available for authorized users)Download Documenthttps://ocr-d-repo.scc.kit.edu/api/v1/dataresources/71e19490-343a-4d68-a5a7-7cf4c725c843/data/arent_dichtercharaktere_1885.zipDownload of the complete document as bagit container.List all Files inside Documenthttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/filesAll files of given resourceID referenced inside the mets.xml are listed here.Download Single Filehttps://ocr-d-repo.scc.kit.edu/api/v1/dataresources/71e19490-343a-4d68-a5a7-7cf4c725c843/data/bagit/data/DEFAULT/DEFAULT_0002Download/view single file (Tiff) of given resourceID, file group and fileID.List Metadatahttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/metadataList metadata of the document (e.g.: title, author, year, identifier, languages, classifications) of given resourceID.List Ground Truth Metadatahttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/groundtruthList all semantic labels of given resourceID.Search Inside RepositoryAll searches will return a list of fitting resourceIDs. In order to further investigate the found resources, the listings above can be used.Search via browserhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/searchSearch on command lineSearch for Semantic Labelhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/labeling?label=condition/acquisition/method-flaws/imaging/uneven-illuminationSearch for documents with e.g. uneven illumination.Search for Documents Containing Multiple Semantic Labels at Oncehttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/labeling?label=condition/acquisition/method-flaws/imaging/uneven-illumination,condition/acquisition/content-or-background/included-objects/preceeding-or-proceedingSearch for Documents with Classification ‚ÄòFachtext‚Äôhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/classification?class=FachtextSearch for Documents with Language ‚Äòdeu‚Äôhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/language?lang=deuSearch for Documents with Identifier ‚Äò16488‚Äôhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/identifier?identifier=16529Search for Documents with Specific Identifier and Typehttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/identifier?identifier=urn:nbn:de:kobv:b4-200905196929&amp;amp;type=urnSearch for document with specific identifier of a specific type.Possible types are:  purl  urn  handle  url  dtaid  ‚Ä¶    Download selected BagIt Containers    E.g.: All with Classification ‚ÄòBelletristik‚Äô```bash=bash    Get all containers    user@hostname:~/Download$ wget -O listOfAllContainers.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit              user@hostname:~/Download$ allocrdzips=$(cat listOfAllContainers.json      tr ‚Äú,[]&quot;‚Äù ‚Äún‚Äù)      Get IDs of fitting containersuser@hostname:~/Download$ wget -O filteredList.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/classification?class=Belletristik            user@hostname:~/Download$ filteredIds=$(cat filteredList.json      tr ‚Äú,[]&quot;‚Äù ‚Äún‚Äù)      user@hostname:~/Download$ for bagitid in $filteredIdsdo  for addr in $allocrdzips  do    if echo ‚Äú$addr‚Äù | grep -q ‚Äú$bagitid‚Äù; then      wget $addr      filename=$(basename ‚Äì ‚Äú$addr‚Äù)      directory=‚Äù${filename%.*}‚Äù  mkdir $directory  cd $directory  unzip ../$filename  cd ..fi   done done ```",
      "url": " /en/ocrd-gt-repo.html"
    },
  

    {
      "slug": "en-spec-ocrd-tool-html",
      "title": "",
      "content"	 : "ocrd-tool.jsonTools MUST be described in a file ocrd-tool.json in the root of the repository.It must contain a JSON object adhering to the ocrd-tool JSON Schema.In particular, every tool provided must be described in an array item under thetools key. These definitions drive the CLI and the webservices.To validate a ocrd-tool.json file, use ocrd ocrd-tool /path/to/ocrd-tool.json validate.File parametersTo mark a parameter as expecting the address of a file, it must declare thecontent-type property as a valid mediatype.Optionally, workflow processors can be notified that this file is potentiallylarge and static (e.g. a fixed dataset or a precomputed model) and should be cachedindefinitely after download by setting the cacheable property to true.Input / Output file groupsTools should define the names of both expected input and produced output filegroups as a list of USE attributes of mets:fileGrp elements. If more thanone file group is expected or produced, this should be explained in thedescription of the tool.NOTE: Both input and output file groups can be overridden atruntime. Tools must therefore ensure not tohardcode file group names. When multiple groups are expected, the order of theoverride reflects the order in which they are defined in the ocrd-tool.json.Definitiontype: objectdescription: Schema for tools by OCR-D MPrequired:  - version  - git_url  - toolsadditionalProperties: falseproperties:  version:    description: &quot;Version of the tool, expressed as MAJOR.MINOR.PATCH.&quot;    type: string    pattern: &#39;^[0-9]+.[0-9]+.[0-9]+$&#39;  git_url:    description: Github/Gitlab URL    type: string    format: url  dockerhub:    description: DockerHub image    type: string  tools:    type: object    additionalProperties: false    patternProperties:      &#39;ocrd-.*&#39;:        type: object        additionalProperties: false        required:          - description          - steps          - executable          - categories          - input_file_grp          # Not required because not all processors produce output files          # - output_file_grp        properties:          executable:            description: The name of the CLI executable in $PATH            type: string          input_file_grp:            description: Input fileGrp@USE this tool expects by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          output_file_grp:            description: Output fileGrp@USE this tool produces by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          parameters:            description: Object describing the parameters of a tool. Keys are parameter names, values sub-schemas.            type: object            patternProperties:              &quot;.*&quot;:                type: object                additionalProperties: false                required:                  - description                  - type                  # also either &#39;default&#39; or &#39;required&#39;                properties:                  type:                    type: string                    description: Data type of this parameter                    enum:                      - string                      - number                      - boolean                  format:                    description: Subtype, such as `float` for type `number` or `uri` for type `string`.                  description:                    description: Concise description of syntax and semantics of this parameter                  required:                    type: boolean                    description: Whether this parameter is required                  default:                    description: Default value when not provided by the user                  enum:                    type: array                    description: List the allowed values if a fixed list.                  content-type:                    type: string                    description: &quot;If parameter is reference to file: Media type of the file&quot;                  cacheable:                    type: boolean                    description: &quot;If parameter is reference to file: Whether the file should be cached, e.g. because it is large and won&#39;t change.&quot;                    default: false          description:            description: Concise description what the tool does          categories:            description: Tools belong to this categories, representing modules within the OCR-D project structure            type: array            items:              type: string              enum:                - Image preprocessing                - Layout analysis                - Text recognition and optimization                - Model training                - Long-term preservation                - Quality assurance          steps:            description: This tool can be used at these steps in the OCR-D functional model            type: array            items:              type: string              enum:                - preprocessing/characterization                - preprocessing/optimization                - preprocessing/optimization/cropping                - preprocessing/optimization/deskewing                - preprocessing/optimization/despeckling                - preprocessing/optimization/dewarping                - preprocessing/optimization/binarization                - preprocessing/optimization/grayscale_normalization                - recognition/text-recognition                - recognition/font-identification                - recognition/post-correction                - layout/segmentation                - layout/segmentation/text-nontext                - layout/segmentation/region                - layout/segmentation/line                - layout/segmentation/word                - layout/segmentation/classification                - layout/analysisExampleThis is from the ocrd_kraken sample project:{  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_kraken&quot;,  &quot;version&quot;: &quot;0.0.2&quot;,  &quot;tools&quot;: {    &quot;ocrd-kraken-binarize&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-binarize&quot;,      &quot;input_file_grp&quot;: &quot;OCR-D-IMG&quot;,      &quot;output_file_grp&quot;: &quot;OCR-D-IMG-BIN&quot;,      &quot;categories&quot;: [        &quot;Image preprocessing&quot;      ],      &quot;steps&quot;: [        &quot;preprocessing/optimization/binarization&quot;      ],      &quot;description&quot;: &quot;Binarize images with kraken&quot;,      &quot;parameters&quot;: {        &quot;level-of-operation&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;page&quot;,          &quot;enum&quot;: [&quot;page&quot;, &quot;block&quot;, &quot;line&quot;]        }      }    },    &quot;ocrd-kraken-segment&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-segment&quot;,      &quot;categories&quot;: [        &quot;Layout analysis&quot;      ],      &quot;steps&quot;: [        &quot;layout/segmentation/region&quot;      ],      &quot;description&quot;: &quot;Block segmentation with kraken&quot;,      &quot;parameters&quot;: {        &quot;text_direction&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;description&quot;: &quot;Sets principal text direction&quot;,          &quot;enum&quot;: [&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;],          &quot;default&quot;: &quot;horizontal-lr&quot;        },        &quot;script_detect&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;description&quot;: &quot;Enable script detection on segmenter output&quot;,          &quot;default&quot;: false        },        &quot;maxcolseps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2},        &quot;scale&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0},        &quot;black_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false},        &quot;white_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false}      }    },    &quot;ocrd-kraken-ocr&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-ocr&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;steps&quot;: [        &quot;recognition/text-recognition&quot;      ],      &quot;description&quot;: &quot;OCR with kraken&quot;,      &quot;parameters&quot;: {        &quot;lines-json&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;format&quot;: &quot;url&quot;,          &quot;required&quot;: &quot;true&quot;,          &quot;description&quot;: &quot;URL to line segmentation in JSON&quot;        }      }    }  }}",
      "url": " /en/spec/ocrd_tool.html"
    },
  

    {
      "slug": "de-spec-ocrd-tool-html",
      "title": "",
      "content"	 : "ocrd-tool.jsonTools MUST be described in a file ocrd-tool.json in the root of the repository.It must contain a JSON object adhering to the ocrd-tool JSON Schema.In particular, every tool provided must be described in an array item under thetools key. These definitions drive the CLI and the webservices.To validate a ocrd-tool.json file, use ocrd ocrd-tool /path/to/ocrd-tool.json validate.File parametersTo mark a parameter as expecting the address of a file, it must declare thecontent-type property as a valid mediatype.Optionally, workflow processors can be notified that this file is potentiallylarge and static (e.g. a fixed dataset or a precomputed model) and should be cachedindefinitely after download by setting the cacheable property to true.Input / Output file groupsTools should define the names of both expected input and produced output filegroups as a list of USE attributes of mets:fileGrp elements. If more thanone file group is expected or produced, this should be explained in thedescription of the tool.NOTE: Both input and output file groups can be overridden atruntime. Tools must therefore ensure not tohardcode file group names. When multiple groups are expected, the order of theoverride reflects the order in which they are defined in the ocrd-tool.json.Definitiontype: objectdescription: Schema for tools by OCR-D MPrequired:  - version  - git_url  - toolsadditionalProperties: falseproperties:  version:    description: &quot;Version of the tool, expressed as MAJOR.MINOR.PATCH.&quot;    type: string    pattern: &#39;^[0-9]+.[0-9]+.[0-9]+$&#39;  git_url:    description: Github/Gitlab URL    type: string    format: url  dockerhub:    description: DockerHub image    type: string  tools:    type: object    additionalProperties: false    patternProperties:      &#39;ocrd-.*&#39;:        type: object        additionalProperties: false        required:          - description          - steps          - executable          - categories          - input_file_grp          # Not required because not all processors produce output files          # - output_file_grp        properties:          executable:            description: The name of the CLI executable in $PATH            type: string          input_file_grp:            description: Input fileGrp@USE this tool expects by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          output_file_grp:            description: Output fileGrp@USE this tool produces by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          parameters:            description: Object describing the parameters of a tool. Keys are parameter names, values sub-schemas.            type: object            patternProperties:              &quot;.*&quot;:                type: object                additionalProperties: false                required:                  - description                  - type                  # also either &#39;default&#39; or &#39;required&#39;                properties:                  type:                    type: string                    description: Data type of this parameter                    enum:                      - string                      - number                      - boolean                  format:                    description: Subtype, such as `float` for type `number` or `uri` for type `string`.                  description:                    description: Concise description of syntax and semantics of this parameter                  required:                    type: boolean                    description: Whether this parameter is required                  default:                    description: Default value when not provided by the user                  enum:                    type: array                    description: List the allowed values if a fixed list.                  content-type:                    type: string                    description: &quot;If parameter is reference to file: Media type of the file&quot;                  cacheable:                    type: boolean                    description: &quot;If parameter is reference to file: Whether the file should be cached, e.g. because it is large and won&#39;t change.&quot;                    default: false          description:            description: Concise description what the tool does          categories:            description: Tools belong to this categories, representing modules within the OCR-D project structure            type: array            items:              type: string              enum:                - Image preprocessing                - Layout analysis                - Text recognition and optimization                - Model training                - Long-term preservation                - Quality assurance          steps:            description: This tool can be used at these steps in the OCR-D functional model            type: array            items:              type: string              enum:                - preprocessing/characterization                - preprocessing/optimization                - preprocessing/optimization/cropping                - preprocessing/optimization/deskewing                - preprocessing/optimization/despeckling                - preprocessing/optimization/dewarping                - preprocessing/optimization/binarization                - preprocessing/optimization/grayscale_normalization                - recognition/text-recognition                - recognition/font-identification                - recognition/post-correction                - layout/segmentation                - layout/segmentation/text-nontext                - layout/segmentation/region                - layout/segmentation/line                - layout/segmentation/word                - layout/segmentation/classification                - layout/analysisExampleThis is from the ocrd_kraken sample project:{  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_kraken&quot;,  &quot;version&quot;: &quot;0.0.2&quot;,  &quot;tools&quot;: {    &quot;ocrd-kraken-binarize&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-binarize&quot;,      &quot;input_file_grp&quot;: &quot;OCR-D-IMG&quot;,      &quot;output_file_grp&quot;: &quot;OCR-D-IMG-BIN&quot;,      &quot;categories&quot;: [        &quot;Image preprocessing&quot;      ],      &quot;steps&quot;: [        &quot;preprocessing/optimization/binarization&quot;      ],      &quot;description&quot;: &quot;Binarize images with kraken&quot;,      &quot;parameters&quot;: {        &quot;level-of-operation&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;page&quot;,          &quot;enum&quot;: [&quot;page&quot;, &quot;block&quot;, &quot;line&quot;]        }      }    },    &quot;ocrd-kraken-segment&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-segment&quot;,      &quot;categories&quot;: [        &quot;Layout analysis&quot;      ],      &quot;steps&quot;: [        &quot;layout/segmentation/region&quot;      ],      &quot;description&quot;: &quot;Block segmentation with kraken&quot;,      &quot;parameters&quot;: {        &quot;text_direction&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;description&quot;: &quot;Sets principal text direction&quot;,          &quot;enum&quot;: [&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;],          &quot;default&quot;: &quot;horizontal-lr&quot;        },        &quot;script_detect&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;description&quot;: &quot;Enable script detection on segmenter output&quot;,          &quot;default&quot;: false        },        &quot;maxcolseps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2},        &quot;scale&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0},        &quot;black_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false},        &quot;white_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false}      }    },    &quot;ocrd-kraken-ocr&quot;: {      &quot;executable&quot;: &quot;ocrd-kraken-ocr&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;steps&quot;: [        &quot;recognition/text-recognition&quot;      ],      &quot;description&quot;: &quot;OCR with kraken&quot;,      &quot;parameters&quot;: {        &quot;lines-json&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;format&quot;: &quot;url&quot;,          &quot;required&quot;: &quot;true&quot;,          &quot;description&quot;: &quot;URL to line segmentation in JSON&quot;        }      }    }  }}",
      "url": " /de/spec/ocrd_tool.html"
    },
  

    {
      "slug": "en-spec-ocrd-zip-html",
      "title": "",
      "content"	 : "OCRD-ZIPThis document describes an exchange format to bundle a workspace described by aMETS file following OCR-D‚Äôs conventions.RationaleMETS is the exchange format of choice by OCR-D for describing relations offiles such as images and metadata about those images such as PAGE or ALTOfiles. METS is a textual format, not suitable for embedding arbitrary,potentially binary, data. For various use cases (such as transfer via network,long-term preservation, reproducible tests etc.) it is desirable to have aself-contained representation of a workspace.With such a representation, data producers are not forced to providedereferenceable HTTP-URL for the files they produce and data consumers are notforced to dereference all HTTP-URL.While METS does have mechanisms for embedding XML data and even base64-encodedbinary data, the tradeoffs in file size, parsing speed and readability are toogreat to make this a viable solution for a mass digitization scenario.Instead, we propose an exchange format (‚ÄúOCRD-ZIP‚Äù) based on the BagIt specused for data ingestion adopted in the web archiving community.BagIt profileAs a baseline, an OCRD-ZIP must adhere to v0.97+ of the BagItspecs, i.e.  all files in data/  a file bagit.txt  a file bag-info.txtIn accordance with the BagIt standard, bagit.txt MUST consist of exactlythese two lines:BagIt-Version: 1.0Tag-File-Character-Encoding: UTF-8In addition, OCRD-ZIP adhere to a BagItprofile (see Appendix A forthe full definition):  bag-info.txt MUST additionally contain these tags:          BagIt-Profile-Identifier: URL of the OCR-D BagIt profile      Ocrd-Identifier: A globally unique identifier for this bag      Ocrd-Base-Version-Checksum: Checksum of the version this bag is based on        bag-info.txt MAY additionally contain these tags:                Ocrd-Manifestation-Depth: Whether all URL are dereferenced as files or only some      BagIt-Profile-IdentifierThe BagIt-Profile-Identifier must be the string https://ocr-d.github.io/bagit-profile.json.Ocrd-MetsOcrd-Mets can be provided to declare that the METS file will not be thestandard mets.xml but another path relative to /data/.Implementations MUST check for the Ocrd-Mets tag: If it has a value, look for theMETS file at that location, relative to /data. Otherwise, assume the defaultmets.xml.Ocrd-Manifestation-DepthSpecify whether the bag contains the full manifestation of the data referenced in the METS (full)or only those files that were file:// URLs before (partial). Default: partial.Ocrd-IdentifierA globally unique identifier identifying the work/works/parts of works thisbundle of file represents.This is to be used for repositories to identify new ingestions of existing works.To ensure global uniqueness, the identifier should be prefixed with anidentifier of the organization, e.g. an ISIL or domain name.Ocrd-Base-Version-ChecksumThe SHA512 checksum of the manifest-sha512.txt file of the version this bagwas based on, if any.InvariantsZIPAn OCRD-ZIP MUST be a serialized as a ZIP file.manifest-sha512.txtChecksums for the files in /data must be calculated with the SHA512algorithm only and provided as manifest-sha512.txt.Since the checksum of this manifest file can be relevant (seeOcrd-Base-Version-Checksum), in addition to the requirementsof the BagIt spec, the entries MUST be sorted.NOTE: These checksums can be generated with find data -type f | sort -sf |xargs sha512sum &amp;gt; manifest-sha512.txt.File names must be relative to METSWithin an OCRD-ZIP, all local file resources referenced in the METS (andconsequently all those referenced in other files within the workspace ‚Äì seerule ‚ÄúIf in PAGE then in METS‚Äù must berelative to the location of the METS file.Example/tmp/foo/ws1/data‚îú‚îÄ‚îÄ mets.xml‚îú‚îÄ‚îÄ foo.tif‚îî‚îÄ‚îÄ foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  /tmp/foo/ws1/data/foo.xml (absolute path)  file:///tmp/foo/ws1/data/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)When in data then in METSAll files except mets.xml itself that are contained in data directory mustbe referenced in a mets:file/mets:Flocat in the mets.xml.When in METS and not in dataDue to partial OCRD-ZIP not all files may be part of the payload. If so they have to be mentioned in fetch.txt and in all payload manifest files.Optional metadata about the payloadIn addition to the actual data files in /data, the following metadata filesare allowed to be present in the root of the bag:  README.md: An extended, human-readable description of the dataset in the Markdown syntax  Makefile: A GNU make build file to reproduce the data in /data.  build.sh: A bash script to reproduce the data in /data.  sources.csv: A comma-separated values list to be used in the scripts. For straightforward HTTP downloads, prefer fetch.txt.These files are purely for documentation and should not be used by processors in any way.AlgorithmsPacking a workspace as OCRD-ZIPTo pack a workspace to OCRD-ZIP:  Create a temporary folder TMP  Foreach mets:file f in the source METS:          Strip file:// from the beginning of the xlink:href of f      If it is not a file path (begins with http:// or https://):                  If Ocrd-Manifestation-Depth is partial,continue                    Download/Copy the file to a location within TMP/data. The structure SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt; where                  &amp;lt;USE&amp;gt; is the USE attribute of the parent mets:fileGrp          &amp;lt;ID&amp;gt; is the ID attribute of the mets:file                    Replace the URL of f with the path relative to /data (SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt;) in                  all mets:FLocat of the METS          all other files in the workspace, esp. PAGE-XML                      Write out the changed METS to TMP/data/mets.xml  Package TMP as a BagIt bagUnpacking OCRD-ZIP to a workspace  Unzip OCRD-ZIP z to a folder TMP  If the value M of Ocrd-Mets is different from mets.xml:          Rename TMP/data/mets.xml to TMP/data/ + M        Move TMP/data to an appropriate location to use as a workspaceAppendix A - BagIt profile definitionBagIt-Profile-Info:  BagIt-Profile-Identifier: https://ocr-d.github.io/bagit-profile.json  BagIt-Profile-Version: &#39;1.2.0&#39;  Source-Organization: OCR-D  External-Description: BagIt profile for OCR data  Contact-Name: Konstantin Baierer  Contact-Email: konstantin.baierer@sbb.spk-berlin.de  Version: 0.1Bag-Info:  Bagging-Date:    required: false  Source-Organization:    required: false  Ocrd-Mets:    required: false    default: &#39;mets.xml&#39;  Ocrd-Manifestation-Depth:    required: false    default: partial    values: [&quot;partial&quot;, &quot;full&quot;]  Ocrd-Identifier:    required: true  Ocrd-Checksum:    required: false    # echo -n | sha512sum    default: &#39;cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e&#39;Manifests-Required: [&#39;sha512&#39;]Tag-Manifests-Required: []Tag-Files-Required: []Tag-Files-Allowed:  - README.md  - Makefile  - build.sh  - sources.csv  - metadata/*.xml  - metadata/*.txtAllow-Fetch.txt: trueSerialization: requiredAccept-Serialization: application/zipAccept-BagIt-Version:  - &#39;1.0&#39;Appendix B - IANA considerationsProposed media type of OCRD-ZIP: application/vnd.ocrd+zipProposed extension: .ocrd.zip",
      "url": " /en/spec/ocrd_zip.html"
    },
  

    {
      "slug": "de-spec-ocrd-zip-html",
      "title": "",
      "content"	 : "OCRD-ZIPThis document describes an exchange format to bundle a workspace described by aMETS file following OCR-D‚Äôs conventions.RationaleMETS is the exchange format of choice by OCR-D for describing relations offiles such as images and metadata about those images such as PAGE or ALTOfiles. METS is a textual format, not suitable for embedding arbitrary,potentially binary, data. For various use cases (such as transfer via network,long-term preservation, reproducible tests etc.) it is desirable to have aself-contained representation of a workspace.With such a representation, data producers are not forced to providedereferenceable HTTP-URL for the files they produce and data consumers are notforced to dereference all HTTP-URL.While METS does have mechanisms for embedding XML data and even base64-encodedbinary data, the tradeoffs in file size, parsing speed and readability are toogreat to make this a viable solution for a mass digitization scenario.Instead, we propose an exchange format (‚ÄúOCRD-ZIP‚Äù) based on the BagIt specused for data ingestion adopted in the web archiving community.BagIt profileAs a baseline, an OCRD-ZIP must adhere to v0.97+ of the BagItspecs, i.e.  all files in data/  a file bagit.txt  a file bag-info.txtIn accordance with the BagIt standard, bagit.txt MUST consist of exactlythese two lines:BagIt-Version: 1.0Tag-File-Character-Encoding: UTF-8In addition, OCRD-ZIP adhere to a BagItprofile (see Appendix A forthe full definition):  bag-info.txt MUST additionally contain these tags:          BagIt-Profile-Identifier: URL of the OCR-D BagIt profile      Ocrd-Identifier: A globally unique identifier for this bag      Ocrd-Base-Version-Checksum: Checksum of the version this bag is based on        bag-info.txt MAY additionally contain these tags:                Ocrd-Manifestation-Depth: Whether all URL are dereferenced as files or only some      BagIt-Profile-IdentifierThe BagIt-Profile-Identifier must be the string https://ocr-d.github.io/bagit-profile.json.Ocrd-MetsOcrd-Mets can be provided to declare that the METS file will not be thestandard mets.xml but another path relative to /data/.Implementations MUST check for the Ocrd-Mets tag: If it has a value, look for theMETS file at that location, relative to /data. Otherwise, assume the defaultmets.xml.Ocrd-Manifestation-DepthSpecify whether the bag contains the full manifestation of the data referenced in the METS (full)or only those files that were file:// URLs before (partial). Default: partial.Ocrd-IdentifierA globally unique identifier identifying the work/works/parts of works thisbundle of file represents.This is to be used for repositories to identify new ingestions of existing works.To ensure global uniqueness, the identifier should be prefixed with anidentifier of the organization, e.g. an ISIL or domain name.Ocrd-Base-Version-ChecksumThe SHA512 checksum of the manifest-sha512.txt file of the version this bagwas based on, if any.InvariantsZIPAn OCRD-ZIP MUST be a serialized as a ZIP file.manifest-sha512.txtChecksums for the files in /data must be calculated with the SHA512algorithm only and provided as manifest-sha512.txt.Since the checksum of this manifest file can be relevant (seeOcrd-Base-Version-Checksum), in addition to the requirementsof the BagIt spec, the entries MUST be sorted.NOTE: These checksums can be generated with find data -type f | sort -sf |xargs sha512sum &amp;gt; manifest-sha512.txt.File names must be relative to METSWithin an OCRD-ZIP, all local file resources referenced in the METS (andconsequently all those referenced in other files within the workspace ‚Äì seerule ‚ÄúIf in PAGE then in METS‚Äù must berelative to the location of the METS file.Example/tmp/foo/ws1/data‚îú‚îÄ‚îÄ mets.xml‚îú‚îÄ‚îÄ foo.tif‚îî‚îÄ‚îÄ foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  /tmp/foo/ws1/data/foo.xml (absolute path)  file:///tmp/foo/ws1/data/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)When in data then in METSAll files except mets.xml itself that are contained in data directory mustbe referenced in a mets:file/mets:Flocat in the mets.xml.When in METS and not in dataDue to partial OCRD-ZIP not all files may be part of the payload. If so they have to be mentioned in fetch.txt and in all payload manifest files.Optional metadata about the payloadIn addition to the actual data files in /data, the following metadata filesare allowed to be present in the root of the bag:  README.md: An extended, human-readable description of the dataset in the Markdown syntax  Makefile: A GNU make build file to reproduce the data in /data.  build.sh: A bash script to reproduce the data in /data.  sources.csv: A comma-separated values list to be used in the scripts. For straightforward HTTP downloads, prefer fetch.txt.These files are purely for documentation and should not be used by processors in any way.AlgorithmsPacking a workspace as OCRD-ZIPTo pack a workspace to OCRD-ZIP:  Create a temporary folder TMP  Foreach mets:file f in the source METS:          Strip file:// from the beginning of the xlink:href of f      If it is not a file path (begins with http:// or https://):                  If Ocrd-Manifestation-Depth is partial,continue                    Download/Copy the file to a location within TMP/data. The structure SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt; where                  &amp;lt;USE&amp;gt; is the USE attribute of the parent mets:fileGrp          &amp;lt;ID&amp;gt; is the ID attribute of the mets:file                    Replace the URL of f with the path relative to /data (SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt;) in                  all mets:FLocat of the METS          all other files in the workspace, esp. PAGE-XML                      Write out the changed METS to TMP/data/mets.xml  Package TMP as a BagIt bagUnpacking OCRD-ZIP to a workspace  Unzip OCRD-ZIP z to a folder TMP  If the value M of Ocrd-Mets is different from mets.xml:          Rename TMP/data/mets.xml to TMP/data/ + M        Move TMP/data to an appropriate location to use as a workspaceAppendix A - BagIt profile definitionBagIt-Profile-Info:  BagIt-Profile-Identifier: https://ocr-d.github.io/bagit-profile.json  BagIt-Profile-Version: &#39;1.2.0&#39;  Source-Organization: OCR-D  External-Description: BagIt profile for OCR data  Contact-Name: Konstantin Baierer  Contact-Email: konstantin.baierer@sbb.spk-berlin.de  Version: 0.1Bag-Info:  Bagging-Date:    required: false  Source-Organization:    required: false  Ocrd-Mets:    required: false    default: &#39;mets.xml&#39;  Ocrd-Manifestation-Depth:    required: false    default: partial    values: [&quot;partial&quot;, &quot;full&quot;]  Ocrd-Identifier:    required: true  Ocrd-Checksum:    required: false    # echo -n | sha512sum    default: &#39;cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e&#39;Manifests-Required: [&#39;sha512&#39;]Tag-Manifests-Required: []Tag-Files-Required: []Tag-Files-Allowed:  - README.md  - Makefile  - build.sh  - sources.csv  - metadata/*.xml  - metadata/*.txtAllow-Fetch.txt: trueSerialization: requiredAccept-Serialization: application/zipAccept-BagIt-Version:  - &#39;1.0&#39;Appendix B - IANA considerationsProposed media type of OCRD-ZIP: application/vnd.ocrd+zipProposed extension: .ocrd.zip",
      "url": " /de/spec/ocrd_zip.html"
    },
  

    {
      "slug": "en-spec-page-html",
      "title": "",
      "content"	 : "Conventions for PAGEIn addition to these conventions, refer to the PAGE APIdocs for extensivedocumentation on the PAGE XML format itself.Media TypeThe preliminary media type of a PAGEdocument is application/vnd.prima.page+xml, which MUST be used as the MIMETYPE of a &amp;lt;mets:file&amp;gt;representing a PAGE document.One page in one PAGEA single PAGE XML file represents one page in the original document.Every &amp;lt;pc:Page&amp;gt; element MUST have an attribute image which MUST always be the source image.The PAGE XML root element &amp;lt;pc:PcGts&amp;gt; MUST have exactly one &amp;lt;pc:Page&amp;gt;.ImagesURL for imageFilename / filenameThe imageFilename of the &amp;lt;pg:Page&amp;gt; and filename of the&amp;lt;pg:AlternativeImage&amp;gt; element MUST be a URL. A local filename should be afile:// URL.All URL used in imageFilename and filename MUST be referenced in a fileGrpin METS.Original image as imageFilenameThe imageFilename attribute of the &amp;lt;pg:Page&amp;gt; MUST reference the originalimage and MUST NOT change between processing steps.AlternativeImage for derived imagesTo encode images derived from the original image, the &amp;lt;pc:AlternativeImage&amp;gt;should be used. Its filename attribute should reference the URL of thederived image.The comments attribute should be one or more (separated by comma) terms ofthe following list:AlternativeImage: classificationThe comments attribute of the &amp;lt;pg:AlternativeImage&amp;gt; attribute should be used  binarized  grayscale_normalized  deskewed  despeckled  cropped  rotated-90 / rotated-180 / rotated-270  dewarpedAlternativeImage on sub-page level elementsFor the results of image processing that changes the positions of pixels (e.g.cropping, rotation, dewarping), AlternativeImage on page level and polygon ofrecognized zones is not sufficient for accessing the section of the image that a region is based onsince coordinates are always relative to the original image.For such use cases, &amp;lt;pg:AlternativeImage&amp;gt; may be used as a child of&amp;lt;pg:TextRegion&amp;gt;, &amp;lt;pg:TextLine&amp;gt;, &amp;lt;pg:Word&amp;gt; or &amp;lt;pg:Glyph&amp;gt;.Attaching text recognition results to elementsA PAGE document can attach recognized text to typographical units ofa page at different levels, such as block (&amp;lt;pg:TextRegion&amp;gt;), line(&amp;lt;pg:TextLine&amp;gt;), word (&amp;lt;pg:Word&amp;gt;) or glyph (&amp;lt;pg:Glyph&amp;gt;).To attach recognized text to an element E, it must be encoded asUTF-8 in a single &amp;lt;pg:Unicode&amp;gt; element U within a &amp;lt;pg:TextEquiv&amp;gt;element T of E.T must be the last element of E.Leading and trailing whitespace (U+0020, U+000A) in the content of a&amp;lt;pg:Unicode&amp;gt; is not significant and must be removed from the string byprocessors.To encode an actual space character at the start or end of the content&amp;lt;pg:Unicode&amp;gt;, use a non-breaking space U+00A0.Text recognition confidenceThe confidence score describing the assumed correctness of the text recognition results in a&amp;lt;pg:TextEquiv&amp;gt; can be expressed in an attribute @conf as a float valuebetween 0 and 1, where 0 means ‚Äúcertainly wrong‚Äù and 1 means ‚Äúcertainlycorrect‚Äù.Attaching multiple text recognition results to elementsAlternative text recognition results can be expressed by using multiple&amp;lt;pg:TextEquiv&amp;gt; wherever a single &amp;lt;pg:TextEquiv&amp;gt; would be allowed. Whenusing multiple &amp;lt;pg:TextEquiv&amp;gt;, they each must have an attribute @index withan integer number unique per set of &amp;lt;pg:TextEquiv&amp;gt; that allows ranking themin order of preference. @index of the first (preferred) &amp;lt;pg:TextEquiv&amp;gt; must bethe value 1.Consistency of text results on different levelsSince text results can be defined on different levels and those levels can benested, text results information can be redundant. To avoid inconsistencies,the following assertions must be true:  text of &amp;lt;pg:Word&amp;gt; must be equal to the text of all &amp;lt;pg:Glyph&amp;gt;    contained within, concatenated with empty string  text of &amp;lt;pg:TextLine&amp;gt; must be equal to the text of all    &amp;lt;pg:Word&amp;gt; contained  within, concatenated with a single space (U+0020).  text of &amp;lt;pg:TextRegion&amp;gt; must be equal to the text of all    &amp;lt;pg:TextLine&amp;gt; contained within, concatenated with a newline (U+000A).NOTE: ‚ÄúConcatenation‚Äù means joining a list of strings with a separator, noseparator is added to the start or end of the resulting string.These assertions are only to be enforced for the first &amp;lt;pg:TextEquiv&amp;gt; of bothcontaining and contained elements, i.e. the only &amp;lt;pg:TextEquiv&amp;gt; of an elementor the &amp;lt;pg:TextEquiv&amp;gt; with @index = 1 if multiple textresults are attached.Consistency strictnessA consistency checker must support four levels of strictness:strictIf any of the assertions fail for a PAGE document, an exceptionshould be raised and the document no further processedlaxIf any of the assertions fail for a PAGE document, another comparisondisregarding all whitespace shall be made. If this still fails, an exceptionshould be raised and the document no further processedfixIf any of the assertions fail for a specific element in PAGE document, the textresults of this element must be recreated, by concatenating the text results ofits children elements. This algorithm needs to be recursive, i.e. if any of thechildren elements is itself inconsistent, its text results must be recreated inthe same way before concatenation.offThese consistency checks are so restrictive to spot data that cannot beunambigiously processed. However, there are valid use cases where the‚Äúindex-1-consistency‚Äù is too narrow, esp. in post-correction with languagemodels. For such use cases, it must be possible to disable the consistencyvalidation altogether in the workflow.Example&amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;f&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;    &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;foof&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;toot&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;&amp;lt;/Word&amp;gt;In this example, the &amp;lt;pg:Word&amp;gt; has text foof butthe concatenation of the first text results of the contained &amp;lt;pg:Glyphs&amp;gt;spells foot. As a result:  Validation should raise an exception for inconsistency.  Data consumers should assume the text result to be foot.TextStyleTypographical information (type, cut etc.) must be documented in PAGE XML using the&amp;lt;TextStyle&amp;gt; element.See the PAGE documentation on TextStyle for all possible values.The &amp;lt;TextStyle&amp;gt; element can be used in all relevant elements:  &amp;lt;TextRegion&amp;gt;  &amp;lt;TextLine&amp;gt;  &amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;Example:&amp;lt;Word&amp;gt;  &amp;lt;TextStyle fontFamily=&quot;Arial&quot; fontSize=&quot;17.0&quot; bold=&quot;true&quot;/&amp;gt;  &amp;lt;!-- [...] --&amp;gt;&amp;lt;/Word&amp;gt;Font familiesThe pg:TextStyle/@fontFamily attribute can list one or more fontfamilies, separated by comma (,).font-families    := font-family (&quot;,&quot; font-family)*font-family      := font-family-name (&quot;:&quot; confidence)?font-family-name := [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot;]+ | &#39;&quot;&#39; [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot; | &quot; &quot;]+ &#39;&quot;&#39;confidence       := (&quot;0&quot; | &quot;1&quot;)? &quot;.&quot; [&quot;0&quot; - &quot;9&quot;]+Font family names that contain a space must be quoted with double quotes (&quot;).Clusters of typesetsSometimes it is necessary to not express that an element is typeset in aspecific font family but in font family from a cluster of related font groups.For such typeset clusters, the pg:TextStyle/@fontFamily attribute should be re-used.This specification doesn‚Äôt restrict the naming of font families.However, we recommend to choose one of the following list of type groups names ifapplicable:  textura  rotunda  bastarda  antiqua  greek  hebrew  italic  frakturFont families and confidenceProviding multiple font families means that the element inquestion is set in one of the font families listed.It is not possible to declare that multiple font families are used in anelement. Instead, data producers are advised to increase output granularityuntil every element is set in a single font family.The degree of confidence in the font family can be expressed by concatenatingfont family names with colon (:) followed by a float between 0 (informationis certainly wrong) and 1 (information is certainly correct).If a font family is not suffixed with a confidence value, the confidence isconsidered to be 1.Examples&amp;lt;TextStyle fontFamily=&quot;Arial:0.8, Times:0.7, Courier:0.4&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:.8, Times:0.5&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:1&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial&quot;/&amp;gt;ColumnsTo model columns, use constructs in the &amp;lt;pg:ReadingOrder&amp;gt; of the PAGEdocument.A grid layout must be wrapped in a &amp;lt;pg:OrderedGroup&amp;gt; with a@caption that has the form column_&amp;lt;horizontal&amp;gt;_&amp;lt;vertical&amp;gt; where&amp;lt;vertical&amp;gt; is the number of columns and &amp;lt;horizontal&amp;gt; is the number of rows.&amp;lt;OrderedGroup caption=&quot;column_1_1&quot;&amp;gt; &amp;lt;!-- the default: single column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_2&quot;&amp;gt; &amp;lt;!-- two-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_3&quot;&amp;gt; &amp;lt;!-- three-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_2_3&quot;&amp;gt; &amp;lt;!-- three-column layout split in top and bottom --&amp;gt;Regions that belong to the same column must be grouped within&amp;lt;pg:OrderedGroupIndexed&amp;gt; with a caption that begins with column_&amp;lt;y&amp;gt;_&amp;lt;x&amp;gt;where &amp;lt;y&amp;gt; is the row position and &amp;lt;x&amp;gt; is the column position (counting starts at 1):&amp;lt;OrderedGroup caption=&quot;column_2_2&quot;&amp;gt; &amp;lt;!-- two-column two-row layout --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-right column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-right column --&amp;gt;&amp;lt;/OrderedGroup&amp;gt;",
      "url": " /en/spec/page.html"
    },
  

    {
      "slug": "de-spec-page-html",
      "title": "",
      "content"	 : "Conventions for PAGEIn addition to these conventions, refer to the PAGE APIdocs for extensivedocumentation on the PAGE XML format itself.Media TypeThe preliminary media type of a PAGEdocument is application/vnd.prima.page+xml, which MUST be used as the MIMETYPE of a &amp;lt;mets:file&amp;gt;representing a PAGE document.One page in one PAGEA single PAGE XML file represents one page in the original document.Every &amp;lt;pc:Page&amp;gt; element MUST have an attribute image which MUST always be the source image.The PAGE XML root element &amp;lt;pc:PcGts&amp;gt; MUST have exactly one &amp;lt;pc:Page&amp;gt;.ImagesURL for imageFilename / filenameThe imageFilename of the &amp;lt;pg:Page&amp;gt; and filename of the&amp;lt;pg:AlternativeImage&amp;gt; element MUST be a URL. A local filename should be afile:// URL.All URL used in imageFilename and filename MUST be referenced in a fileGrpin METS.Original image as imageFilenameThe imageFilename attribute of the &amp;lt;pg:Page&amp;gt; MUST reference the originalimage and MUST NOT change between processing steps.AlternativeImage for derived imagesTo encode images derived from the original image, the &amp;lt;pc:AlternativeImage&amp;gt;should be used. Its filename attribute should reference the URL of thederived image.The comments attribute should be one or more (separated by comma) terms ofthe following list:AlternativeImage: classificationThe comments attribute of the &amp;lt;pg:AlternativeImage&amp;gt; attribute should be used  binarized  grayscale_normalized  deskewed  despeckled  cropped  rotated-90 / rotated-180 / rotated-270  dewarpedAlternativeImage on sub-page level elementsFor the results of image processing that changes the positions of pixels (e.g.cropping, rotation, dewarping), AlternativeImage on page level and polygon ofrecognized zones is not sufficient for accessing the section of the image that a region is based onsince coordinates are always relative to the original image.For such use cases, &amp;lt;pg:AlternativeImage&amp;gt; may be used as a child of&amp;lt;pg:TextRegion&amp;gt;, &amp;lt;pg:TextLine&amp;gt;, &amp;lt;pg:Word&amp;gt; or &amp;lt;pg:Glyph&amp;gt;.Attaching text recognition results to elementsA PAGE document can attach recognized text to typographical units ofa page at different levels, such as block (&amp;lt;pg:TextRegion&amp;gt;), line(&amp;lt;pg:TextLine&amp;gt;), word (&amp;lt;pg:Word&amp;gt;) or glyph (&amp;lt;pg:Glyph&amp;gt;).To attach recognized text to an element E, it must be encoded asUTF-8 in a single &amp;lt;pg:Unicode&amp;gt; element U within a &amp;lt;pg:TextEquiv&amp;gt;element T of E.T must be the last element of E.Leading and trailing whitespace (U+0020, U+000A) in the content of a&amp;lt;pg:Unicode&amp;gt; is not significant and must be removed from the string byprocessors.To encode an actual space character at the start or end of the content&amp;lt;pg:Unicode&amp;gt;, use a non-breaking space U+00A0.Text recognition confidenceThe confidence score describing the assumed correctness of the text recognition results in a&amp;lt;pg:TextEquiv&amp;gt; can be expressed in an attribute @conf as a float valuebetween 0 and 1, where 0 means ‚Äúcertainly wrong‚Äù and 1 means ‚Äúcertainlycorrect‚Äù.Attaching multiple text recognition results to elementsAlternative text recognition results can be expressed by using multiple&amp;lt;pg:TextEquiv&amp;gt; wherever a single &amp;lt;pg:TextEquiv&amp;gt; would be allowed. Whenusing multiple &amp;lt;pg:TextEquiv&amp;gt;, they each must have an attribute @index withan integer number unique per set of &amp;lt;pg:TextEquiv&amp;gt; that allows ranking themin order of preference. @index of the first (preferred) &amp;lt;pg:TextEquiv&amp;gt; must bethe value 1.Consistency of text results on different levelsSince text results can be defined on different levels and those levels can benested, text results information can be redundant. To avoid inconsistencies,the following assertions must be true:  text of &amp;lt;pg:Word&amp;gt; must be equal to the text of all &amp;lt;pg:Glyph&amp;gt;    contained within, concatenated with empty string  text of &amp;lt;pg:TextLine&amp;gt; must be equal to the text of all    &amp;lt;pg:Word&amp;gt; contained  within, concatenated with a single space (U+0020).  text of &amp;lt;pg:TextRegion&amp;gt; must be equal to the text of all    &amp;lt;pg:TextLine&amp;gt; contained within, concatenated with a newline (U+000A).NOTE: ‚ÄúConcatenation‚Äù means joining a list of strings with a separator, noseparator is added to the start or end of the resulting string.These assertions are only to be enforced for the first &amp;lt;pg:TextEquiv&amp;gt; of bothcontaining and contained elements, i.e. the only &amp;lt;pg:TextEquiv&amp;gt; of an elementor the &amp;lt;pg:TextEquiv&amp;gt; with @index = 1 if multiple textresults are attached.Consistency strictnessA consistency checker must support four levels of strictness:strictIf any of the assertions fail for a PAGE document, an exceptionshould be raised and the document no further processedlaxIf any of the assertions fail for a PAGE document, another comparisondisregarding all whitespace shall be made. If this still fails, an exceptionshould be raised and the document no further processedfixIf any of the assertions fail for a specific element in PAGE document, the textresults of this element must be recreated, by concatenating the text results ofits children elements. This algorithm needs to be recursive, i.e. if any of thechildren elements is itself inconsistent, its text results must be recreated inthe same way before concatenation.offThese consistency checks are so restrictive to spot data that cannot beunambigiously processed. However, there are valid use cases where the‚Äúindex-1-consistency‚Äù is too narrow, esp. in post-correction with languagemodels. For such use cases, it must be possible to disable the consistencyvalidation altogether in the workflow.Example&amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;f&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;    &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;foof&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;toot&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;&amp;lt;/Word&amp;gt;In this example, the &amp;lt;pg:Word&amp;gt; has text foof butthe concatenation of the first text results of the contained &amp;lt;pg:Glyphs&amp;gt;spells foot. As a result:  Validation should raise an exception for inconsistency.  Data consumers should assume the text result to be foot.TextStyleTypographical information (type, cut etc.) must be documented in PAGE XML using the&amp;lt;TextStyle&amp;gt; element.See the PAGE documentation on TextStyle for all possible values.The &amp;lt;TextStyle&amp;gt; element can be used in all relevant elements:  &amp;lt;TextRegion&amp;gt;  &amp;lt;TextLine&amp;gt;  &amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;Example:&amp;lt;Word&amp;gt;  &amp;lt;TextStyle fontFamily=&quot;Arial&quot; fontSize=&quot;17.0&quot; bold=&quot;true&quot;/&amp;gt;  &amp;lt;!-- [...] --&amp;gt;&amp;lt;/Word&amp;gt;Font familiesThe pg:TextStyle/@fontFamily attribute can list one or more fontfamilies, separated by comma (,).font-families    := font-family (&quot;,&quot; font-family)*font-family      := font-family-name (&quot;:&quot; confidence)?font-family-name := [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot;]+ | &#39;&quot;&#39; [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot; | &quot; &quot;]+ &#39;&quot;&#39;confidence       := (&quot;0&quot; | &quot;1&quot;)? &quot;.&quot; [&quot;0&quot; - &quot;9&quot;]+Font family names that contain a space must be quoted with double quotes (&quot;).Clusters of typesetsSometimes it is necessary to not express that an element is typeset in aspecific font family but in font family from a cluster of related font groups.For such typeset clusters, the pg:TextStyle/@fontFamily attribute should be re-used.This specification doesn‚Äôt restrict the naming of font families.However, we recommend to choose one of the following list of type groups names ifapplicable:  textura  rotunda  bastarda  antiqua  greek  hebrew  italic  frakturFont families and confidenceProviding multiple font families means that the element inquestion is set in one of the font families listed.It is not possible to declare that multiple font families are used in anelement. Instead, data producers are advised to increase output granularityuntil every element is set in a single font family.The degree of confidence in the font family can be expressed by concatenatingfont family names with colon (:) followed by a float between 0 (informationis certainly wrong) and 1 (information is certainly correct).If a font family is not suffixed with a confidence value, the confidence isconsidered to be 1.Examples&amp;lt;TextStyle fontFamily=&quot;Arial:0.8, Times:0.7, Courier:0.4&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:.8, Times:0.5&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:1&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial&quot;/&amp;gt;ColumnsTo model columns, use constructs in the &amp;lt;pg:ReadingOrder&amp;gt; of the PAGEdocument.A grid layout must be wrapped in a &amp;lt;pg:OrderedGroup&amp;gt; with a@caption that has the form column_&amp;lt;horizontal&amp;gt;_&amp;lt;vertical&amp;gt; where&amp;lt;vertical&amp;gt; is the number of columns and &amp;lt;horizontal&amp;gt; is the number of rows.&amp;lt;OrderedGroup caption=&quot;column_1_1&quot;&amp;gt; &amp;lt;!-- the default: single column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_2&quot;&amp;gt; &amp;lt;!-- two-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_3&quot;&amp;gt; &amp;lt;!-- three-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_2_3&quot;&amp;gt; &amp;lt;!-- three-column layout split in top and bottom --&amp;gt;Regions that belong to the same column must be grouped within&amp;lt;pg:OrderedGroupIndexed&amp;gt; with a caption that begins with column_&amp;lt;y&amp;gt;_&amp;lt;x&amp;gt;where &amp;lt;y&amp;gt; is the row position and &amp;lt;x&amp;gt; is the column position (counting starts at 1):&amp;lt;OrderedGroup caption=&quot;column_2_2&quot;&amp;gt; &amp;lt;!-- two-column two-row layout --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-right column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-right column --&amp;gt;&amp;lt;/OrderedGroup&amp;gt;",
      "url": " /de/spec/page.html"
    },
  

    {
      "slug": "en-people-html",
      "title": "",
      "content"	 : "People of OCR-DProject ManagersDr. Johannes MangeiDuke August Library Wolfenb√ºttelmangei[at]hab.deDr. Alexander GeykenBerlin-Brandenburg Academy of Sciences and Humanities in Berlingeyken[at]bbaw.deReinhard Altenh√∂nerState Library of Berlin Prussian Cultural Heritagereinhard.altenhoener[at]sbb.spk-berlin.deDr. Rainer StotzkaKarlsruhe Institute of Technologyrainer.stotzka[at]kit.eduProject ParticipantsElisabeth EnglProject CoordinatorDuke August Library Wolfenb√ºttelengl[at]hab.deAndrea OpitzProject Coordinator (interim)Duke August Library Wolfenb√ºttelopitz[at]hab.deMatthias BoenigBerlin-Brandenburg Academy of Sciences and Humanities in Berlinocrd[at]bbaw.deKonstantin BaiererState Library of Berlin Prussian Cultural Heritagekonstantin.baierer@sbb.spk-berlin.deVolker HartmannKarlsruhe Institute of Technology /Steinbuch Centre for Computingvolker.hartmann[at]kit.eduFormer ManagersDr. Thomas St√§ckerDuke August Library Wolfenb√ºttelstaecker[at]hab.deDr. Markus BrantlBavarian State Library Munichmarkus.brantl[at]bsb-muenchen.deFormer participantsElisa HerrmannDuke August Library Wolfenb√ºttelelisa.herrmann[at]hab.deKay-Michael W√ºrznerBerlin-Brandenburg Academy of Sciences and Humanities in Berlinocrd[at]bbaw.deAjinkya PrabhuneKarlsruhe Institute of Technologyajinkya.prabhune[at]kit.edu**Sebastian Mangold.Bavarian State Library Munichsebastian.mangold[at]bsb-muenchen.de",
      "url": " /en/people.html"
    },
  

    {
      "slug": "en-platforms-html",
      "title": "",
      "content"	 : "OCR-D PlatformsIn OCR-D, we strive for maximum transparency and communication of our workinternally as well as externally. The project is active on various platforms,which are mostly open to the professional public to read and participate.GitHub ‚Äì try it out yourself: On GitHub you can find the latest status of our developmental work on the OCR-D framework and the module projects.https://github.com/OCR-DGitter ‚Äì follow up on our discussions: We use the chat platform Gitter for short-term and low-threshold communication with each other.https://gitter.im/OCR-D/LobbyGround Truth Repository ‚Äì make use of our data. We collect our test and reference data in the Ground Truth Repository. You may use the data for your own training.http://www.ocr-d.de/daten**Technology Watch: In the Zotero group of OCR-D we collect relevant literature about OCR together with interested parties.Zotero list: https://www.zotero.org/groups/ocr-d",
      "url": " /en/platforms.html"
    },
  

    {
      "slug": "en-project-summary-html",
      "title": "",
      "content"	 : "Brief Description &amp;amp; Project GoalsCoordinated Funding Initiative for the Further Development of Methods for Optical Character Recognition (OCR) ‚ÄúShort name: OCR-DProject PartnersDuke August Library Wolfenb√ºttel](http://www.hab.de/), Berlin-Brandenburg Academy of Sciences and Humanities, State Library of Berlin Prussian Cultural Heritage, Karlsruhe Institute of Technology(Project managers and contact persons see Contact)Project Duration2015‚Äì2020Funded byGerman Research Foundation Scientific Literature and Information Systems (LIS)Project ProgressThe ‚ÄúCoordinated Funding Initiative for the Further Development of Optical Character Recognition Methods‚Äù (OCR-D) began its first project phase in the third quarter of 2015. Requirements for the further development of automatic text recognition were collected and analyzed in six work packages. The work finally led to the DFG‚Äôs call for proposals ‚ÄúScalable methods of text and structure recognition for the full text digitisation of historical prints‚Äù in March 2017. The approval of eight (module) projects by the DFG at the end of December 2017 marks the end of the first and the beginning of the second project phase, in which the module projects are coordinated and supported and their project results are tested and integrated. In order to be able to fulfill the tasks of the coordination project over the entire duration of all module projects, the DFG approved an extension of the project until July 2020.GoalsAn essential main goal of OCR-D is the conceptual preparation of the transformation of VD-prints (16th‚Äì19th century) into machine-readable form and the provision of the necessary tools.In order to achieve this, the coordination project and the module projects aim to meet the following objectives:  the creation of reference corpora for training and testing  the development of standards in the areas of metadata, documentation and ground truth  the further development of individual processing steps, with a particular focus on Optical Layout Recognition (OLR)  the analysis of existing tools and their further development  the creation of an OCR-D framework  the establishment of quality assurance procedures**At the end of the overall project, a software package for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries will be made available. Furthermore an accompanying concept will provide answers to technical, information scientifical and organisational questions regarding the possible mass processing of these data.",
      "url": " /en/project-summary.html"
    },
  

    {
      "slug": "en-project-html",
      "title": "",
      "content"	 : "The OCR-D Project**OCR-D is a coordination project aimed at the further development of Optical Character Recognition (OCR) techniques for historical prints.Workflows and methods of automatic text recognition are examined, described and, if necessary, optimized. An essential goal is the conceptual preparation of the transformation of German prints from the 16th to the 19th century into electronic full texts.This project involves the Duke August Library Wolfenb√ºttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the State Library of Berlin Prussian Cultural Heritage and the Karlsruhe Institute of Technology. The Bavarian State Library was also involved until 31/08/2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have image digitised extensive holdings. With the help of OCR procedures, searchable full texts can be automatically generated from these image data. The use of digital full texts is indispensable today in many scientific disciplines, especially in the field of (digital) humanities.So far, however, access to the electronic full text has often been impossible, or only inadequately possible. Many historical holdings are available in digitised form through the ‚Äú Union Catalogues of Books Printed in German Speaking Countries ‚Äú (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially gothic types, are difficult to identify.There is a need for development, which we have uncovered in OCR-D. On the basis of existing tools and investigations, the OCR process is to be optimized for VD prints. In addition, answers will be found to the associated technical, information scientific and organizational problems. In contrast to other OCR projects, the focus is not on developing a new, powerful OCR engine. Instead, full text digitization is seen as a process that is implemented in modular open source software. The processes and parameters can be traced and, if required, tailor-made workflows can be defined that deliver optimal results for specific titles.The project is funded by the German Research Foundation](http://www.dfg.de/) and runs until July 2020. In the first phase, needs were identified and concepts for the further course were developed. The cooperation structure was consolidated and continued in the second phase. In this phase, the identified needs are addressed by eight module projects, which partly develop existing tools for the automated processing of early modern printing, partly set up new tools. In all steps, we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries is to be developed.",
      "url": " /en/project.html"
    },
  

    {
      "slug": "en-spec-provenance-html",
      "title": "",
      "content"	 : "ProvenanceProvenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. (Source: The PROV Data Model (PROV-DM))Data ModelThe PROV Data Model (PROV-DM) is used to store all provenance metadata.All provenance have to be stored in files. There‚Äôs provenance at the page level, but there‚Äôs also provenance at the document level.All files regarding provenance are stored in a subfolder metadata.FormatThe workflow provenance is stored in PROV-XML.TypesAll Activities, Entities belonging to the OCR-D workflow have the same namespace.Namespace  Prefix  ocrd  Namespace  http://www.ocr-d.de            Type      Data Type      Description                  Entity      ocrd:mets      Filename  of METS file              Entity      ocrd:mets_referencedFile      ID of the file referenced inside METS.              Entity      ocrd:parameter_file      Content of the parameter file.              Activity      ocrd:processor      Processor that was executed              Activity      ocrd:workflow      Workflow that was executed      ContentOnly the following information is stored for provenance:(a) General data  Workflow engine          Label including version      Start date      End date      (b) Processor data  Processor          Label including version, conforming to OCR-D mets:agent/mets:name (e.g.: ocrd-kraken-binarize_Version 0.1.0, ocrd/core 1.0.0)      Start date      End date        Content of METS file before executing the processor  Content of METS file after executing processor  ID of the input file(s)  ID of output file(s)  Content of parameter.json (optional)Input/OutputAll files referenced in METS must also be referenced in provenance by their mets:file/@ID.A file may be linked to its location (URL). The location may be replaced due to different uses:  local files  external filesAll files not referenced in METS must be linked to their content in provenance. (e.g.: parameter.json)Ingest Workspace to OCR-D RepositoriumAt least before ingesting into repository/LTA, the entire provenance must be stored in one file (metadata/ocrd_provenance.xml) to make the provenance searchable.Therfore all the provenance files are merged into one big file.This file replaces all provenance files stored in subfolder ‚Äòmetadata‚ÄôExampleThe file structure could look like this after a workflow with 4 steps has been executed.metadata/   |   +-- mets.xml.&#39;workflowid&#39;_0000   |   +-- mets.xml.&#39;workflowid&#39;_0001   |   +-- mets.xml.&#39;workflowid&#39;_0002   |   +-- mets.xml.&#39;workflowid&#39;_0003   |   +-- mets.xml.&#39;workflowid&#39;_0004   |   +-- ocrd_provenance.xml   |   +-- provenance_&#39;workflowid&#39;.xml (optional)Provenance and BagItThe provenance MAY be stored as tag directory in the bagIt container.E.g.:&amp;lt;base directory&amp;gt;/         |         +-- bagit.txt         |         +-- manifest-&amp;lt;algorithm&amp;gt;.txt         |         +-- [additional tag files]         |         +-- data/         |     |         |     +-- mets.xml         |     |         |     +-- ...         |         +-- metadata                |                +-- mets.xml.&#39;workflowid&#39;_0000                |                +-- ...                |                +-- mets.xml.&#39;workflowid&#39;_XXXX                |                +-- ocrd_provenance.xml",
      "url": " /en/spec/provenance.html"
    },
  

    {
      "slug": "de-spec-provenance-html",
      "title": "",
      "content"	 : "ProvenanceProvenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. (Source: The PROV Data Model (PROV-DM))Data ModelThe PROV Data Model (PROV-DM) is used to store all provenance metadata.All provenance have to be stored in files. There‚Äôs provenance at the page level, but there‚Äôs also provenance at the document level.All files regarding provenance are stored in a subfolder metadata.FormatThe workflow provenance is stored in PROV-XML.TypesAll Activities, Entities belonging to the OCR-D workflow have the same namespace.Namespace  Prefix  ocrd  Namespace  http://www.ocr-d.de            Type      Data Type      Description                  Entity      ocrd:mets      Filename  of METS file              Entity      ocrd:mets_referencedFile      ID of the file referenced inside METS.              Entity      ocrd:parameter_file      Content of the parameter file.              Activity      ocrd:processor      Processor that was executed              Activity      ocrd:workflow      Workflow that was executed      ContentOnly the following information is stored for provenance:(a) General data  Workflow engine          Label including version      Start date      End date      (b) Processor data  Processor          Label including version, conforming to OCR-D mets:agent/mets:name (e.g.: ocrd-kraken-binarize_Version 0.1.0, ocrd/core 1.0.0)      Start date      End date        Content of METS file before executing the processor  Content of METS file after executing processor  ID of the input file(s)  ID of output file(s)  Content of parameter.json (optional)Input/OutputAll files referenced in METS must also be referenced in provenance by their mets:file/@ID.A file may be linked to its location (URL). The location may be replaced due to different uses:  local files  external filesAll files not referenced in METS must be linked to their content in provenance. (e.g.: parameter.json)Ingest Workspace to OCR-D RepositoriumAt least before ingesting into repository/LTA, the entire provenance must be stored in one file (metadata/ocrd_provenance.xml) to make the provenance searchable.Therfore all the provenance files are merged into one big file.This file replaces all provenance files stored in subfolder ‚Äòmetadata‚ÄôExampleThe file structure could look like this after a workflow with 4 steps has been executed.metadata/   |   +-- mets.xml.&#39;workflowid&#39;_0000   |   +-- mets.xml.&#39;workflowid&#39;_0001   |   +-- mets.xml.&#39;workflowid&#39;_0002   |   +-- mets.xml.&#39;workflowid&#39;_0003   |   +-- mets.xml.&#39;workflowid&#39;_0004   |   +-- ocrd_provenance.xml   |   +-- provenance_&#39;workflowid&#39;.xml (optional)Provenance and BagItThe provenance MAY be stored as tag directory in the bagIt container.E.g.:&amp;lt;base directory&amp;gt;/         |         +-- bagit.txt         |         +-- manifest-&amp;lt;algorithm&amp;gt;.txt         |         +-- [additional tag files]         |         +-- data/         |     |         |     +-- mets.xml         |     |         |     +-- ...         |         +-- metadata                |                +-- mets.xml.&#39;workflowid&#39;_0000                |                +-- ...                |                +-- mets.xml.&#39;workflowid&#39;_XXXX                |                +-- ocrd_provenance.xml",
      "url": " /de/spec/provenance.html"
    },
  

    {
      "slug": "en-publications-html",
      "title": "",
      "content"	 : "Publications2016Thomas St√§cker, Elisa Herrmann: ‚ÄûKooperationsprojekt zur Weiterentwicklung von OCR-Verfahren‚Äú. Lecture at the 12th workshop ‚ÄûTexterkennung in historischen Dokumenten‚Äú, 09/02/2016 in Rostock.Elisa Herrmann: ‚ÄûOCR-D: Koordinierungsprojekt zur Weiterentwicklung von OCR-Verfahren‚Äú. Lecture at the Philtag 13, 25‚Äì26/02/2016 in W√ºrzburg.Matthias Boenig, Kay-Michael W√ºrzner, Arne Binder, Uwe Springmann: ‚Äû√úber den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts‚Äú. Lecture at the DHd 2016, 07‚Äì12/03/2016 in Leipzig.2017Elisa Herrmann: ‚ÄûAktuelle OCR-Entwicklungen und ihr Einsatz in der Praxis‚Äú. Lecture at the ‚ÄûBerliner Bibliothekswissenschaftlichem Kolloquium‚Äú, 17/01/2017 in Berlin.Thomas St√§cker, Elisa Herrmann: ‚ÄûOCR-D: Koordinierte F√∂rderinitiative zur Weiterentwicklung von OCR f√ºr historische Dokumente‚Äú. Lecture at the 106th Bibliothekartag, 30/05‚Äì02/06/2017 in Frankfurt a. Main. urn:nbn:de:0290-opus4-30040.Kay-Michael W√ºrzner: ‚Äû(Open-Source-)OCR-Workflows‚Äú Lecture at the DH-colloquium at the BBAW, 04/08/2017 in Berlin. https://edoc.bbaw.de/frontdoor/index/index/docId/2786.            Kay-Michael W√ºrzner, Matthias Boenig: ‚ÄûPerspektiven der automatischen Texterfassung als Grundlage wissenschaftlicher Editionen am Beispiel der Brief- und Schriftenausgabe der Bernd Alois Zimmermann-Gesamtausgabe‚Äú. Workshop of the ‚ÄûAG eHumanities      Mainz. Geisteswissenschaftliche Forschungsdaten. Methoden zur digitalen Erfassung‚Äú. Preparation and presentation, 18‚Äì20/10/2017 in Mainz.      Kay-Michael W√ºrzner, Matthias Boenig: ‚ÄûCompilation of a Large Ground-Truth Data Set: Using Transkribus‚Äú. Presentation at the Transkribus User Conference 2017, 02‚Äì03/11/2017 in Wien.Thomas St√§cker, Elisa Herrmann: ‚ÄûOCR-D ‚Äì Koordinierte F√∂rderinitiative zur Weiterentwicklung von OCR-Verfahren‚Äú. Bibliotheksdienst 05/12/2017, Vol. 52 (1). M√ºnchen: De Gruyter Saur.2018Matthias Boenig, Maria Federbusch, Elisa Herrmann, Clemens Neudecker, Kay-Michael W√ºrzner: ‚ÄûGround Truth: Grundwahrheit oder Ad-Hoc-L√∂sung? Wo stehen die Digital Humanities?‚Äú. Lecture at the DHd 2018, 28/02/2018 in K√∂ln.Elisa Herrmann: ‚ÄûWie gut sind 85%?‚Äú. Lecture at the MWW / DARIAH-DE expert workshop ‚ÄûSuchtechnologien‚Äú, 24/05/2018 in Weimar. https://docs.google.com/presentation/d/1zoa7z2oj2KY5cIM88-1QzEh53YAj5tX5P9iaF46oqhU/edit#slide=id.p1.Konstantin Baierer, Kay-Michael W√ºrzner: ‚ÄûAn open-source framework for integrating multi-source layout and text recognition tools into scalable OCR workflows‚Äú. Lecture at the Bibliotheca Baltica Symposium, 04‚Äì05/10/2018 in Rostock, https://ocr-d.github.io/2018-10-05-baltica/index.html#/.Stefan Weil: ‚Äû126 Jahre Zeitung online ‚Äì Fundgrube f√ºr historisch Interessierte und Motor f√ºr die Bibliotheks-IT‚Äú. Lecture at the 107th Bibliothekartag, 15/06/2018 in Berlin. https://madoc.bib.uni-mannheim.de/46507/.Klaus Schulz, Florian Fink: ‚ÄúNovel software fro cleansing digitised historical texts‚Äù. Scientia 28/11/2018. [https://doi.org/10.26320/SCIENTIA278] (https://doi.org/10.26320/SCIENTIA278)2019Elisa Herrmann: ‚ÄûVon der Vision zur Umsetzung: Der aktuelle Entwicklungsstand von OCR-D‚Äú. Lecture at the 7th Bibliothekskongress, 18/03/2019 in Leipzig. https://www.researchgate.net/publication/332173701_Von_der_Vision_zur_Umsetzung_Der_aktuelle_Entwicklungsstand_von_OCR-D.Matthias Boenig: ‚ÄûOCR-D in der Praxis: Ein gemeinsamer Ausblick mit Dienstleistern und Anwendern‚Äú. Public working session at the 7th Bibliothekskongress, 18/03/2019 in Leipzig. https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/17097/docId/16357/start/0/rows/20.Stefan Weil: ‚ÄûHands-On Lab digital / Vom Bild zum Text. Automatisierte Texterkennung in historischen Drucken mit der freien Software Tesseract‚Äú. Lecture at the 108th Bibliothekartag and the 7th Bibliothekskongress, 18/03/2019 in Leipzighttps://nbn-resolving.org/urn:nbn:de:0290-opus4-163511.Konstantin Baierer, Matthias Boenig, Volker Hartmann, Elisa Herrmann: ‚ÄûVom gedruckten Werk zu elektronischem Volltext‚Äú. Workshop at the DHd 2019, 25/03/2019 in Mainz. http://kba.cloud/2019-03-25-dhd/.Jan Kamlah, Stefan Weil: ‚ÄûForschungsdaten aus Digitalisaten‚Äú. Presentation at the ‚ÄûE-Science-Tage‚Äú, 28/03/2019 in Heidelberg. https://heibox.uni-heidelberg.de/d/31bb269467/files/?p=%2FVortr%C3%A4ge%2FC3_2019-03-28-Kamlah-Weil.pdf.Nikolaus Weichselbaumer, Mathias Seuret, Saskia Limbach, Vincent Christlein, Andreas Maier: ‚ÄûAutomatic Font Group Recognition in Early Printed Books‚Äú. Lecture at the DHd 2019, 25‚Äì29/03/19 in Mainz und Frankfurt a. Main.Matthias Boenig, Konstantin Baierer, Volker Hartmann, Maria Federbusch and Clemens Neudecker: ‚ÄûLabelling OCR Ground Truth for Usage in Repositories‚Äú. Lecture at the DATeCH 2019. 3rd International Conference on Digital Access to Textual Cultural Heritage 2019, 09/05/2019 in Br√ºssel.Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-Michael W√ºrzner, Matthias Boenig, Elisa Hermann, Volker Hartmann: ‚ÄûOCR-D: An end-to-end open-source OCR framework for historical documents‚Äú. Lecture at the DATeCH 2019. 3rd International Conference on Digital Access to Textual Cultural Heritage 2019, 09/05/2019 in Br√ºssel. https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents.Tobias Englmeier, Florian Fink, Klaus Schulz: ‚ÄúA-I-PoCoTo - Combining automated and interactive OCR postcorrection‚Äù. Lecture at the DATeCH 2019. 3rd International Conference on Digital Access to Textual Cultural Heritage 2019, 09/05/2019 in Br√ºssel.Stefan Weil: ‚ÄûTesseract OCR ‚Äì News‚Äú. Lecture at the ELAG 2019, 09/05/2019 in Berlin. https://www.elag2019.de/talks/2019-05-09-tesseract-elag.pdf.Noah Metzger, Stefan Weil: ‚ÄûOptimierter Einsatz von OCR-Verfahren ‚Äì Tesseract als Komponente im OCR-D-Workflow‚Äú. Workshop at the MAD HD, 30/07/2019 in Heidelberg.Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-Michael W√ºrzner, Matthias Boenig, Elisa Herrmann, Volker Hartmann: ‚ÄûOCR-D: An end-to-end open source OCR framework for historical documents‚Äú. EuropeanaTech Insight 31/07/2019, Issue 13. https://pro.europeana.eu/page/issue-13-ocr#ocr-d-an-end-to-end-open-source-ocr-framework-for-historical-documents.Noah Metzger: ‚ÄûProjektabschlusspr√§sentation‚Äú. University library Mannheim, 19/09/2019 in Mannheim. https://madoc.bib.uni-mannheim.de/52213/.Konstantin Baierer, Rui Dong, Clemens Neudecker. ‚Äûokralact ‚Äì a multi-engine Open Source OCR training system‚Äù. Lecture at the 5th International Workshop on Historical Document Imaging and Processing HIP 2019 as part of the ICDAR 2019, 20/9/2019 in Sydney https://hackmd.io/@kba/SyiQKUCUH#/.Mathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier and Vincent Christlein. ‚ÄûDataset of Pages from Early Printed Books with Multiple Font Groups‚Äù. Lecture at the 5th International Workshop on Historical Document Imaging and Processing HIP 2019 as part of the ICDAR 2019, 20/9/2019 in Sydney.Konstantin Baierer, Elisabeth Engl, Michael Luetgen. ‚ÄúOCR(-D) und Kitodo‚Äù. Presentation at the Kitodo user meeting 2019, 19/11/2019 in Hamburg https://hackmd.io/@kba/S1peIVxhH#/.",
      "url": " /en/publications.html"
    },
  

    {
      "slug": "search-index-json",
      "title": "",
      "content"	 : "[  {% for post in site.pages %}    {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    },  {% endfor %}  {% for post in site.posts %}  {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    }    {% unless forloop.last %},{% endunless %}  {% endfor %}]",
      "url": " /search-index.json"
    },
  

    {
      "slug": "search-html",
      "title": "Search",
      "content"	 : "Search",
      "url": " /search.html"
    },
  

    {
      "slug": "en-setup-html",
      "title": "",
      "content"	 : "# Setup guide for pilot librariesOCR-D&#39;s software is a modular collection of many projects (called _modules_)with many tools per module (called _processors_) that you can combine freelyto achieve the workflow best suited for OCRing your content.All [OCR-D modules](https://github.com/topics/ocr-d) follow the same[interface](https://ocr-d.github.io/cli) and common design patterns. So onceyou understand how to install and use one project, you know how to install anduse all of them.## InstallationThere are three ways to install OCR-D modules:  1. Using the [`ocrd/all` Docker module collection](https://hub.docker.com/r/ocrd/all) (**recommended**)  2. Using `ocrd/all` to install OCR-D modules locally  3. Installing modules indivudally via Docker or natively (not recommended)We recommend using the Docker image since this does not require any changes tothe host system besides [installing Docker](https://hub.docker.com/r/ocrd/all).We do not recommend installing modules individually because it can be difficult to keep thesoftware up-to-date and ensure that they are at working and interoperable versions.## ocrd_allThe [`ocrd_all`](https://github.com/OCR-D/ocrd_all) project is an effort by theOCR-D community, now maintained by the OCR-D coordination team. It streamlinesthe native installation of OCR-D modules with a versatile Makefile approach.Besides allowing local installation of the full OCR-D stack, it is also thebase for the [`ocrd/all`](https://hub.docker.com/r/ocrd/all)Docker images available from DockerHub that contain the full stack of OCR-Dmodules ready for deployment.Technically, `ocrd_all` is a Git repository that keeps all the necessary softwareas Git submodules at specific revisions. This way, the software tools are knownto be at a stable version and guaranteed to be interoperable with one another.## ocrd_all via Docker### mini medi maxiThere are three versions of the[`ocrd/all`](https://hub.docker.com/r/ocrd/all) image:`minimum`, `medium` and `maximum`. They differ in which modules are includedand hence the size of the image. Only use the `minimum` or `medium` images ifyou are certain that you do not need the full OCR-D stack for your workflows, otherwisewe encourage you to use the large but complete `maximum` image.Check this table to see which modules are included in which version:| Module                      | `minimum` | `medium` | `maximum` || -----                       | ----      | ----     | ----      || core                        | ‚òë         | ‚òë        | ‚òë         || ocrd_cis                    | ‚òë         | ‚òë        | ‚òë         || ocrd_im6convert             | ‚òë         | ‚òë        | ‚òë         || ocrd_repair_inconsistencies | ‚òë         | ‚òë        | ‚òë         || ocrd_tesserocr              | ‚òë         | ‚òë        | ‚òë         || tesserocr                   | ‚òë         | ‚òë        | ‚òë         || workflow-configuration      | ‚òë         | ‚òë        | ‚òë         || cor-asv-ann                 | -         | ‚òë        | ‚òë         || dinglehopper                | -         | ‚òë        | ‚òë         || format-converters           | -         | ‚òë        | ‚òë         || ocrd_calamari               | -         | ‚òë        | ‚òë         || ocrd_keraslm                | -         | ‚òë        | ‚òë         || ocrd_olena                  | -         | ‚òë        | ‚òë         || ocrd_segment                | -         | ‚òë        | ‚òë         || tesseract                   | -         | ‚òë        | ‚òë         || ocrd_anybaseocr             | -         | -        | ‚òë         || ocrd_kraken                 | -         | -        | ‚òë         || ocrd_ocropy                 | -         | -        | ‚òë         || ocrd_pc_segmentation        | -         | -        | ‚òë         || ocrd_typegroups_classifier  | -         | -        | ‚òë         || sbb_textline_detector       | -         | -        | ‚òë         || cor-asv-fst                 | -         | -        | ‚òë         |### Fetch docker imageTo fetch the `maximum` version of the `ocrd/all` Docker image:```shdocker pull ocrd/all:maximum```Replace `maximum` accordingly if you want the `minimum` or `medium` variant.If no specific version is chosen, `latest` is selected by default, which is equivalent to `medium`.### Updating docker imageTo update the docker images to their latest version, just run the `docker pull` command again:```shdocker pull ocrd/all:```This can even be set up as a cron-job to ensure the image is always up-to-date.### Translating native commands to docker callsIn the documentation, both of the [OCR-D coordinationproject](https://ocr-d.github.io) as well as the documentation of the[individual OCR-D modules](https://github.com/topics/ocr-d), you will find*native commands*, i.e. command line calls that expect the software to beinstalled natively. These are simple to translate to commands based on thedocker images by prepending the boilerplate telling Docker which image to use,which user to run as, which files to bind to a container path etc.For example a call to[`ocrd-tesserocr-binarize`](https://github.com/OCR-D/tesserocr) might nativelylook like this:```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK```To run it with the [`ocrd/all:maximum`] Docker container:```shdocker run -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK           _________/ ___________/ ______/ _________________/ ___________________________________________________________/              (1)          (2)         (3)          (4)                            (5)```* (1) tells Docker to run the container as the calling user instead of root* (2) tells Docker to bind the current working directory as the `/data` folder in the container* (3) tells Docker to change the container&#39;s working directory to `/data`* (4) tells docker which image to run* (5) is the unchanged call to `ocrd-tesserocr-segment-region`It can also be useful to delete the container after creation with the `--rm`parameter.## ocrd_all nativelyThe `ocrd_all` project contains a sophisticated Makefile to install or compileprerequisites as necessary, set up a virtualenv, install the core software,install OCR-D modules and more. Detailed documentation [can be found in itsREADME](https://github.com/OCR-D/ocrd_all).### PrerequisitesThere are some [system requirements](https://github.com/OCR-D/ocrd_all#system-packages) for ocrd_all.You need to have `git` and `make` installed to make use of `ocrd_all`:```shsudo apt install git make```It is easiest to install all the possible system requirements by calling `make deps-ubuntu` as root:```shsudo make deps-ubuntu```### Cloning the repositoryClone the repository and all its submodules:```shgit clone --recursive https://github.com/OCR-D/ocrd_allcd ocrd_all```### Updating the repositoryAs `ocrd_all` is in [activedevelopment](https://github.com/OCR-D/ocrd_all/commits/master), it is wise toregularly update the repository and its submodules:```shgit pullmake modules```### Installing with ocrd_allYou can either install  1. all the software at once with the `all` target (equivalent to the [`maximum` Docker version](#mini-medi-maxi))  2. modules individually by using an executable from that module as the target or :  3. modules invidually by using the project name for the `OCRD_MODULES` variable:```shmake all                       # Installs all the software (recommended)make ocrd-tesserocr-binarize   # Install ocrd_tesserocr which contains ocrd-tesserocr-binarizemake ocrd-cis-ocropy-binarize  # Install ocrd_cis  which contains ocrd-cis-ocropy-binarizemake all OCRD_MODULES=&quot;ocrd_tesserocr ocrd_cis&quot;  # Will install both ocrd_tesserocr and ocrd_cis```## Individual installationWith all variants of individual module installation, it will be up to you tokeep the repositories up-to-date and installed. We therefore discourageindividual installation of modules and recommend using ocrd_all as outlined above..### Individual Docker containerThis is the best option if you want full control over which modules youactually intend to use while still profiting from the simple installation ofDocker containers.You need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)All OCR-D modules are also [published as Docker containers to DockerHub](https://hub.docker.com/u/ocrd). To find the dockerimage for a module, replace the `ocrd_` prefix with `ocrd/`:```shdocker pull ocrd/tesserocr  # Installs ocrd_tesserocrdocker pull ocrd/olena  # Installs ocrd_olena```To run the containers, please see [the notes on translating native command linecalls to docker calls above](#translating-native-commands-to-docker-calls). Make sure the imagename matches the executable. For example to run the same example in the dedicated `ocrd_tesserocr` container:```shdocker run -u $(id -u) -w /data -v $PWD:/data -- ocrd/tesserocr ocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK-DOCKER```### Native installation&gt; **NOTE**&gt; &gt; ocrd_tesserocr requires **tesseract-ocr &gt;= 4.1.0**. But the Tesseract packages&gt; bundled with **Ubuntu  please enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr),&gt; which has up-to-date builds of tesseract and its dependecies:&gt; &gt; ```sh&gt; sudo add-apt-repository ppa:alex-p/tesseract-ocr&gt; sudo apt-get update&gt; ```#### virtualenv* **Always install python modules into a virtualenv*** **Never run `pip`/`pip3` as root**First install Python 3 and `venv`:```shsudo apt install python3 python3-venv``````sh# If you haven&#39;t created the venv yet:python3 -m venv ~/venv# Activate the venvsource ~/venv/bin/activate```Once you have activated the virtualenv, you should see `(venv)` prepended toyour shell prompt.#### From PyPIThis is the best option if you want to use the stable, released version of individual modules.However, many modules require a number of non-Python (system) packages. For theexact list of packages you need to look at the README of the module inquestion. (If you are not on Ubuntu &gt;= 18.04, then your requirements maydeviate from that.)For example to install `ocrd_tesserocr` from PyPI:```shsudo apt-get install git python3 python3-pip python3-venv libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetpip3 install ocrd_tesserocr```Many ocrd modules conventionally contain a Makefile with a `deps-ubuntu` target that can handle calls to `apt-get` for you:```shsudo make deps-ubuntu```#### From git This is the best option if you want to change the source code or install the latest, unpublished changes.```shgit clone https://github.com/OCR-D/ocrd_tesserocrcd ocrd_tesserocrsudo make deps-ubuntu # or manually with apt-getmake deps             # or pip3 install -r requirementsmake install          # or pip3 install .```If you intend to develop a module, it is best to install the module editable:```shpip install -e .```This way, you won&#39;t have to reinstall after making changes.## Testing the installationFor example, let&#39;s fetch a document from the [OCR-D GT Repo](https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/):```shwget &#39;https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/736a2f9a-92c6-4fe3-a457-edfa3eab1fe3/data/wundt_grundriss_1896.ocrd.zip&#39;unzip wundt_grundriss_1896.ocrd.zipcd data```### Test native installationThis section applies if you installed the software natively, either [via`ocrd_all`](#ocrd_all-natively) or [on a per-module basis](#native-installation).First, activate your venv:```sh# Activate the venvsource ~/venv/bin/activate```Let&#39;s segment the images in file group `OCR-D-IMG` into regions (creating afirst [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) file group`OCR-D-SEG-BLOCK`):```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK```### Test Docker installationThis section applies if you installed the software as docker container(s), either [via`ocrd_all`](#ocrd_all-via-docker) or [on a per-module basis](#individual-docker-container).You can spin up a docker container, mounting the current working directory like this:```shdocker run -u $(id -u) -w /data -v $PWD:/data -- ocrd/all:maximum ocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK-DOCKER```Note that the CLI is exactly the same, the only difference is the prefix to instruct Docker, as [explained above](#mini-medi-maxi)## Running a small workflow### With PyPI and workflow engine from coreThe [core package](https://github.com/OCR-D/core) will be installed with everyOCR-D module package, but you can also install it manually:```shpip3 install ocrd```Its CLI `ocrd` contains a simple workflow engine, available with the `ocrd process` command, which allows you to chain multiple OCR-D processor calls into simple sequential workflows.For example, let&#39;s combine the ocropy-based binarization of the[ocrd_cis](https://github.com/cisocrgroup/ocrd_cis) module project with the segmentation and recognitionin [ocrd_tesserocr](https://github.com/OCR-D/ocrd_tesserocr).First, install ocrd_cis, too:```sh# Install ocrd_cispip3 install ocrd_cis # with pip```Next, install a suitable OCR model for Tesseract:```sh# Install OCR model into Tesseract datapathsudo apt-get install tesseract-ocr-script-frak```Now we can define the workflow (as a list of processor calls in abbreviatedform, and a number of parameter files where defaults are insufficient):```sh# Create parameter filesecho &#39;{ &quot;model&quot;: &quot;Fraktur&quot; }&#39; &gt; param-tess-fraktur.json# Run workflowocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -p param-tess-fraktur.json&#39; ```",
      "url": " /en/setup.html"
    },
  

    {
      "slug": "en-slides-html",
      "title": "",
      "content"	 : "# OCR-D slides## 2017* [Akademienunion_2017/slides/ocr-perspektiven.pdf](/slides/Akademienunion_2017)* [DH-Kolloquium-2017/dh-kolloquium-2017.pdf](slides/DH-Kolloquium-2017)* [OCR-Workshop-2017](slides/OCR-Workshop-2017)## 2018* [PhilTag-2018](/slides/PhilTag-2018)* [Tutorial 2018-07-11](https://kba.github.io/ocrd-2018-07-11)* [Slides-2018-06-26](/slides/Slides-2018-06-26)* [Transkribus-WS-2017](/slides/Transkribus-WS-2017)* [2018-09-18](/slides/2018-09-18)* [2018-10-05-baltica](https://ocr-d.github.io/2018-10-05-baltica/index.html)## 2019* [2019-02-27-ocrd-dev-ws](https://kba.github.io/2019-02-27-ocrd-dev-ws)* [2019-03-18-bid2019-talk](https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16356)* [2019-03-18-bid2019-workshop](https://docs.google.com/presentation/d/10w6SAZ76Nm1p1Q0Q6FtDIwvrKAHrTLShSVgt4vBdozs/edit#slide=id.g52306a8c08_2_109)* [2019-03-25-dhd](http://kba.cloud/2019-03-25-dhd)* [2019-05-09-gt](https://kba.cloud/2019-05-09)* [2019-05-09-ocrd](https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents)* [2019-06-14](https://docs.google.com/presentation/d/1d2FYGDWH65iahXobnWibDix0FlXtufA6xAyN5Z73qi0)* [2019-icdar](https://hackmd.io/@kba/SyiQKUCUH)* [Kurzpraesentation_OCR-D_2019-11/Kurzvorstellung OCR-D_Nov19.pptx](Kurzpr√§sentation OCR-D)* [2019-11-19](https://hackmd.io/@kba/S1peIVxhH)",
      "url": " /en/slides.html"
    },
  

    {
      "slug": "de-slides-html",
      "title": "",
      "content"	 : "# OCR-D slides## 2017* [Akademienunion_2017](/slides/Akademienunion_2017)* [DH-Kolloquium-2017](/slides/DH-Kolloquium-2017)* [OCR-Workshop-2017](/slides/OCR-Workshop-2017)## 2018* [PhilTag-2018](/slides/PhilTag-2018)* [Tutorial 2018-07-11](https://kba.github.io/ocrd-2018-07-11)* [Slides-2018-06-26](/slides/Slides-2018-06-26)* [Transkribus-WS-2017](/slides/Transkribus-WS-2017)* [2018-09-18](/slides/2018-09-18)* [2018-10-05-baltica](https://ocr-d.github.io/2018-10-05-baltica/index.html)## 2019* [2019-02-27-ocrd-dev-ws](https://kba.github.io/2019-02-27-ocrd-dev-ws)* [2019-03-18-bid2019-talk](https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16356)* [2019-03-18-bid2019-workshop](https://docs.google.com/presentation/d/10w6SAZ76Nm1p1Q0Q6FtDIwvrKAHrTLShSVgt4vBdozs/edit#slide=id.g52306a8c08_2_109)* [2019-03-25-dhd](http://kba.cloud/2019-03-25-dhd)* [2019-05-09-gt](https://kba.cloud/2019-05-09)* [2019-05-09-ocrd](https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents)* [2019-06-14](https://docs.google.com/presentation/d/1d2FYGDWH65iahXobnWibDix0FlXtufA6xAyN5Z73qi0)* [2019-icdar](https://hackmd.io/@kba/SyiQKUCUH)* [Kurzpraesentation_OCR-D_2019-11](/slides/Kurzpraesentation_OCR-D_2019-11)* [2019-11-19](https://hackmd.io/@kba/S1peIVxhH)",
      "url": " /de/slides.html"
    },
  

    {
      "slug": "en-test-html",
      "title": "",
      "content"	 : "foo: {{ page.foo }}",
      "url": " /en/test.html"
    },
  

    {
      "slug": "assets-main-css",
      "title": "",
      "content"	 : "@import &quot;minima&quot;;",
      "url": " /assets/main.css"
    },
  

    {
      "slug": "feed-xml",
      "title": "",
      "content"	 : "{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != &quot;posts&quot; %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: &quot; | &quot; | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: &quot; | &quot; | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% assign posts = site[page.collection] | where_exp: &quot;post&quot;, &quot;post.draft != true&quot; | sort: &quot;date&quot; | reverse %}{% if page.category %}{% assign posts = posts | where: &quot;category&quot;,page.category %}{% endif %}{% for post in posts limit: 10 %}{{ post.title | smartify | strip_html | normalize_whitespace | xml_escape }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{{ post.content | strip | xml_escape }}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: &quot;&quot; | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% endif %}{% for tag in post.tags %}{% endfor %}{% if post.excerpt and post.excerpt != empty %}{{ post.excerpt | strip_html | normalize_whitespace | xml_escape }}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains &quot;://&quot; %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}",
      "url": " /feed.xml"
    },
  
  
  {
      "slug": "en-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Full texts - the future of old prints: OCR-D-Workshop in Bonn",
      "content"	 : "The OCR-D workshop ‚ÄúFull texts - the future of old prints‚Äù will take place inBonn on 12 February. The findings and desiderata of the DFG project will bepresented and discussed there with a broad audience of developers, OCR experts,users and funding agencies.The first morning lectures will focus on the OCR-D software itself. It will bedemonstrated with its range of functions and technical possibilities,specifications and documentation will be described as a basis for its creationand use. The experiences and insights that the software developers have gainedon the creation of OCR in libraries are of special interest with regard tofuture OCR projects. The morning section will be concluded in a paneldiscussion with the developers of the OCR-D module projects.The afternoon section is reserved for the discussion about the future of theOCR-D software. The workshop participants will discuss various ways andpossibilities for implementing the software at libraries and transferring it topractical use. Subsequently, possible further funding measures of the DFGproject will be discussed, with the help of which the last steps can be takentowards the mass processing of the VD titles, which is ultimately targeted.",
      "url": " /en/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "de-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Volltexte ‚Äì die Zukunft alter Drucke: OCR-D-Workshop in Bonn",
      "content"	 : "Am 12. Februar findet in Bonn der OCR-D-Workshop ‚ÄúVolltexte ‚Äì die Zukunft alterDrucke‚Äù statt. Die Erkenntnisse und Desiderate des DFG-Projekts werden dorteinem bereiten Publikum aus Entwicklern, OCR-Experten, Anwendern undF√∂rdergebern vorgestellt und diskutiert.Die ersten Vortr√§ge am Vormittag stellen die OCR-D-Software selbst in denMittelpunkt. Diese wird mit ihrem Funktionsumfang und ihren technischenM√∂glichkeiten vorgef√ºhrt sowie Spezifikationen und Dokumentationen als Basisf√ºr deren Erstellung und Verwendung beschrieben. Die Erfahrungen undErkenntnisse, die die Software- Entwickler zur Praxis der OCR-Erstellung inbestandshaltenden Einrichtungen gewonnen haben, sind mit Blick auf k√ºnftigeOCR-Projekte von √ºbergeordnetem Interesse. Beschlossen wird der Vormittagsteilin einer Podiumsdiskussion mit den Entwicklern aus den einzelnenOCR-D-Modulprojekten.Die Nachmittagssektion ist der Diskussion √ºber die Zukunft der OCR-D-Softwarevorbehalten. Mit den Workshop-Teilnehmern sollen verschiedene Wege undM√∂glichkeiten zur Implementierung der Software an bestandshaltendenEinrichtungen und deren √úberf√ºhrung in den Praxiseinsatz er√∂rtert werden. Darananschlie√üend werden m√∂gliche weitere F√∂rderma√ünahmen des DFG-Projektsbesprochen, mit deren Hilfe die letzten Schritte zur letztlich anvisiertenmassenhaften Prozessierung der VD-Titel gegangen werden k√∂nnen.",
      "url": " /de/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "en-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Cooperation with Kitodo signed",
      "content"	 : "Kitodo and OCR-D have signed a Letter of Intent to cooperate on the coordinated and sustainable development and provision of OCR software solutions for mass full text digitization. Kitodo is an open source software suite for the digitisation of cultural property and widely used in the library sector.The basic aim of the cooperation is to make use of overlaps between Kitodo and OCR-D to achieve synergies in technical and organizational aspects. It will be perspectively aspired, to integrate OCR-D as a separate functional area in Kitodo.Production.. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and upcoming developments. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on Interfaces to Kitodo.Production and current and upcoming developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. willcoordinating institutions of the OCR-D project and the In the coming years, OCR-D intends to expand the resulting software solution into a stable, widely used and extensively documented software solution. In mutual agreement and against the background of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and future developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. will support the coordination project and the implementation partners with its extensive experience.",
      "url": " /en/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "de-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Kooperation mit Kitodo vereinbart",
      "content"	 : "Kitodo und OCR-D haben einen Letter of Intent zur Zusammenarbeit bei derkoordinierten und nachhaltigen Entwicklung und Bereitstellung vonOCR-Softwarel√∂sungen zur Massenvolltextdigitalisierung unterzeichnet. Kitodoist eine quelloffene Softwaresuite f√ºr die Digitalisierung von Kulturgut inbestandshaltenden Einrichtungen und im Bibliotheksbereich weit verbreitet.Grundlegendes Ziel der Kooperation ist es, inhaltliche und konzeptionelle√úberschneidungen zwischen Kitodo und OCR-D zum Erzielen von Synergien intechnischer und organisatorischer Hinsicht zu nutzen. Es wird perspektivischangestrebt, OCR-D als eigenen Funktionsbereich in Kitodo.Productionaufzunehmen. OCR-D beabsichtigt in den kommenden Jahren, die entstandeneSoftwarel√∂sung zu einer stabil lauff√§higen, breit eingesetzten sowieausf√ºhrlich dokumentierten Softwarel√∂sung auszubauen. Im gegenseitigenEinverst√§ndnis und vor dem Hintergrund der anstehenden Implementierungsphasevon OCR-D tauschen sich die Kooperationspartner dazu vor allem √ºberSchnittstellen zu Kitodo.Production und aktuelle sowie kommende Entwicklungender OCR-Massenvolltextdigitalisierung aus. Mit Blick auf die technischeIntegration von OCR-D in Kitodo.Production, wird Kitodo e.V. diekoordinierenden Einrichtungen des OCR-D-Projektes und dieImplementierungspartner mit seinen Erfahrungen umfassend unterst√ºtzen.",
      "url": " /de/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "en-2019-08-23-icdar-html",
      "title": "OCR-D at the ICDAR 2019 in Sydney",
      "content"	 : "The OCR-D paper ‚Äúokralact - a multi-engine Open Source OCR training system (Konstantin Baierer, Rui Dong, Clemens Neudecker) was accepted for the 5th International Workshop on Historical Document Imaging and Processing HIP 2019 (https://www.primaresearch.org/hip2019/) in the context of the ICDAR Conference 2019 in Sydney (https://icdar2019.org/).The source code for the prototype for a multi-engine OCR Training infrastructure presented there is available at https://github.com/Doreenruirui/okralact. Training infrastructure is available at https://github.com/Doreenruirui/okralact for find. With ‚ÄúDataset of Pages from Early Printed Books with Multiple Font Groups‚Äù the module project for the development of a model repository and an automatic font recognition for OCR-D is also at the same workshop with a paper of Mathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier and Vincent Christlein.",
      "url": " /en/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "de-2019-08-23-icdar-html",
      "title": "OCR-D bei der ICDAR 2019 in Sydney",
      "content"	 : "Das OCR-D Paper ‚Äûokralact ‚Äì a multi-engine Open Source OCR training system‚Äú(Konstantin Baierer, Rui Dong, Clemens Neudecker) wurde f√ºr den 5thInternational Workshop on Historical Document Imaging and Processing HIP 2019(https://www.primaresearch.org/hip2019/) im Rahmen der ICDAR Konferenz 2019 inSydney (https://icdar2019.org/) angenommen.Der Quellcode zum dort vorgestellten Prototypen f√ºr eine multi-Enginge OCRTrainingsinfrastruktur ist unter https://github.com/Doreenruirui/okralact zufinden. Mit ‚ÄûDataset of Pages from Early Printed Books with Multiple FontGroups‚Äù ist zudem das Modulprojekt zur Entwicklung eines Modellrepositoriumsund einer automatischen Schriftarterkennung f√ºr OCR-D mit einem paper vonMathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier andVincent Christlein auf dem gleichen Workshop vertreten.",
      "url": " /de/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "en-2019-08-09-europeanatech-html",
      "title": "OCR-D auf EuropeanaTech Insights",
      "content"	 : "????",
      "url": " /en/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "de-2019-08-09-europeanatech-html",
      "title": "OCR-D auf EuropeanaTech Insights",
      "content"	 : "????",
      "url": " /de/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "en-2019-05-13-datech-best-paper-html",
      "title": "Datech Best Paper",
      "content"	 : "At the international conference ‚ÄúDigital Access to Textual Cultural Heritage 2019‚Äù (DATeCH2019) held in Brussels, the paper on OCR-D entitled ‚ÄúOCR-D: An end-to-end open-source OCR framework for historical documents‚Äù was awarded the Best Paper Award. The authors are Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-Michael W√ºrzner, Matthias Boenig, Elisa Herrmann and Volker Hartmann.",
      "url": " /en/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "de-2019-05-13-datech-best-paper-html",
      "title": "Datech Best Paper",
      "content"	 : "Bei der in Br√ºssel veranstalteten internationalen Konferenz ‚ÄúDigital Access toTextual Cultural Heritage 2019‚Äù (DATeCH2019) wurde der Beitrag √ºber OCR-D mitdem Titel ‚ÄúOCR-D: An end-to-end open-source OCR framework for historicaldocuments‚Äù mit dem Best Paper Award ausgezeichnet. Als Autorinnen und Autorensind Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-MichaelW√ºrzner, Matthias Boenig, Elisa Herrmann und Volker Hartmann verantwortlich.",
      "url": " /de/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "en-2018-08-31-ocrd-verlaengert-html",
      "title": "Ocrd Verlaengert",
      "content"	 : "The German Research Foundation (DFG) has approved an extension of the OCR-D project for another 18 months. The new funding phase will start in October 2018 and will therefore end in March 2020. This good news allows us to continue supporting the module projects and to consolidate the results. On the other hand, it will also allow the coordination committee‚Äôs own work packages to be continued and deepened.The current funding phase would end in the 3rd quarter of 2018 and project-immanent tasks such as combining the individual module project results into a software package or updating the DFG‚Äôs ‚ÄúDigitisation‚Äù rules of practice based on the findings of this project would not have been possible. During the extension period, the Coordinating Committee will work particularly on the OCR-D framework, the continuous development of which you can follow via the GitHub site (http://www.github.de/ocr-d).In addition, we are working on the later distribution of the software, be it by evaluating the modules on the basis of application examples, by contacts to OCR service providers or a pilot program in spring 2019. Within the pilot programme, the requirements already identified should be even better tailored to the needs of the future users.The third focus is on the extension of the ground truth data and the format maintenance and, of course, the coordination and management of the Communication with the module projects.  Besides the internal workshops the coordinating body will hold a final workshop in summer 2019, for which we will invite the professional public.",
      "url": " /en/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "de-2018-08-31-ocrd-verlaengert-html",
      "title": "Ocrd Verlaengert",
      "content"	 : "Die Deutsche Forschungsgemeinschaft (DFG) hat eine Verl√§ngerung des ProjektsOCR-D f√ºr weitere 18 Monate bewilligt. Die neue F√∂rderphase beginnt im Oktober2018 und endet demnach im M√§rz 2020. Diese erfreuliche Nachricht erm√∂glicht unszum einen die weitere Unterst√ºtzung der Modulprojekte sowie die Zusammenf√ºhrungder Ergebnisse. Zum anderen k√∂nnen dadurch auch die eigenen Arbeitspakete desKoordinierungsgremiums weitergef√ºhrt und vertieft werden.Die aktuelle F√∂rderphase w√§re im 3. Quartal 2018 geendet und projektimmanenteAufgaben, wie die Zusammenf√ºhrung der einzelnen Modulprojektergebnisse zu einemSoftwarepaket oder die Aktualisierung der DFG-Praxisregeln ‚ÄúDigitalisierung‚Äùauf Grundlage der Erkenntnisse aus diesem Vorhaben, w√§ren nicht m√∂glichgewesen. Das Koordinierungsgremium wird in der Verl√§ngerung besonders amOCR-D-Framework arbeiten, dessen stetige Entwicklung Sie √ºber die GitHub-Seite(http://www.github.de/ocr-d) verfolgen k√∂nnen.Daneben arbeiten wir an der sp√§teren Verbreitung der Software, sei es durch dieEvaluierung der Module anhand von Anwendungsbeispielen, durch Kontakte zuOCR-Dienstleistern oder ein Pilotprogramm im Fr√ºhjahr 2019. Im Rahmen diesesPilotprogramms sollen die schon ermittelten Anforderungen noch besser auf dieBed√ºrfnisse der sp√§teren Anwender abgestimmt werden.Der dritte Schwerpunkt liegt auf der Erweiterung der Ground-Truth-Daten und derdazugeh√∂rigen Formatpflege sowie naturgem√§√ü auf der Koordinierung undKommunikation mit den Modulprojekten.  Neben den internen Workshops organisiertdas Koordinierungsgremium einen Abschlussworkshop im Sommer 2019, zu dem wirdie Fach√∂ffentlichkeit einladen werden.",
      "url": " /de/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "en-2018-03-28-start-der-modulprojekte-html",
      "title": "Start Der Modulprojekte",
      "content"	 : "From March 5th to 6th the big kick-off meeting of the module projects took place in the Herzog August Bibliothek in Wolfenb√ºttel, which officially heralds the second phase of OCR-D.The coordination committee of OCR-D met for the first time with the ten module project participants to get to know each other and to present the work done and planned on both sides.In short presentations, the project application contents were presented and connections between the module projects and the coordination committee were sought. During the personal exchange in the World Caf√© as well as during the joint dinner, first project-strategic discussions could be initiated, which will accompany us in the near future.Of the more than 20 project proposals received in response to the DFG call for proposals in March 2017, eight module projects were approved at the end of the year:Scalable methods of text and structure recognition for the full text digitisation of historical prints: Image optimisationGerman Research Center for Artificial Intelligence (DFKI)Scalable methods of text and structure recognition for the full text digitisation of historical prints: layout recognitionDFKIFurther development of a semi-automatic open-source tool for layout analysis and region extraction and classification (LAREX) of early printed booksUniversity of W√ºrzburgNN/FST - Unsupervised OCR Postcorrection based on Neural Networks and Finite-state TransducersLeipzig UniversityOptimized use of OCR processes - Tesseract as a component in the OCR-D workflowUniversity of MannheimAutomatic and semi-automatic post-correction of OCR-recorded historical printsMunich UniversityDevelopment of a model repository and automatic font recognition for OCR-DLeipzig University, Erlangen University, Mainz UniversityOLA-HD - An OCR-D long-term archive for historical printsSUB G√∂ttingen, GWDG G√∂ttingenThe module projects usually have a duration of 18 months, a public final workshop is planned for June 2019. Until then, the coordination committee will organise two project-internal developer meetings to discuss the previous versions of the work. An insight into the development can also be found on the GitHub page of OCR-D: https://github.com//ocr-dIn addition, the coordinating committee and the module projects are in regular exchange via video conferences during this intensive work phase.Further details on the individual module projects and later on the developer meetings will follow on the OCR-D website.We are looking forward to working together!For questions and suggestions: contact",
      "url": " /en/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "de-2018-03-28-start-der-modulprojekte-html",
      "title": "Start Der Modulprojekte",
      "content"	 : "Vom 05.-06. M√§rz fand in der Herzog August Bibliothek in Wolfenb√ºttel das gro√üe Kick-Off-Treffen der Modulprojekte statt, welches offiziell die zweite Phase von OCR-D einl√§utet.Das Koordinierunsggremium von OCR-D traf sich zum ersten Mal mit den zehn Modulprojektnehmern zum gegenseitigen Kennenlernen und zum Vorstellen der geleisteten und geplanten Arbeiten auf beiden Seiten.In kurzen Pr√§sentationen wurden die Projektantragsinhalte dargestellt und nach Ankn√ºpfungspunkten zwischen den Modulprojekten sowie zum Koordinierungsgremium gesucht. Beim pers√∂nlichen Austausch im World Caf√© sowie beim gemeinsamen Abendessen konnten zudem erste projektstrategische Diskussionen angesto√üen werden, die uns in naher Zukunft noch begleiten werden.Von den √ºber 20 eingegangenen Projektantr√§gen auf die DFG-Ausschreibung im M√§rz 2017 wurden Ende des Jahres acht Modulprojekte bewilligt:Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: BildoptimierungDeutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz (DFKI)Skalierbare Verfahren der Text- und Strukturerkennung f√ºr die Volltextdigitalisierung historischer Drucke: LayouterkennungDFKIWeiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von fr√ºhen Buchdrucken,Universit√§t W√ºrzburgNN/FST ‚Äì Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state TransducersUniversit√§t LeipzigOptimierter Einsatz von OCR-Verfahren ‚Äì Tesseract als Komponente im OCR-D-WorkflowUniversit√§t MannheimAutomatische und semi-automatische Nachkorrektur OCR-erfasster historischer DruckeUniversit√§t M√ºnchenEntwicklung eines Modellrepositoriums und einer automatischen Schriftarterkennung f√ºr OCR-DUniversit√§t Leipzig, Universit√§t Erlangen, Universit√§t MainzOLA-HD ‚Äì Ein OCR-D-Langzeitarchiv f√ºr historische DruckeSUB G√∂ttingen, GWDG G√∂ttingenDie Modulprojekte haben in der Regel eine Laufzeit von 18 Monaten, ein √∂ffentlicher Abschlussworkshop ist f√ºr Juni 2019 geplant. Bis dahin organisiert das Koordinierungsgremium zwei projektinterne Entwicklertreffen, auf denen die bisherigen Arbeitsversionen besprochen werden. Einen Einblick in die Entwicklung bietet auch die GitHub-Seite von OCR-D: https://github.com//ocr-dZudem stehen das Koordinierungsgremium und die Modulprojekte in dieser intensiven Arbeitsphase per Videokonferenzen im regelm√§√üigen Austausch.Weitere Details zu den einzelnen Modulprojekten und sp√§ter zu den Entwicklertreffen folgen auf der Seite von OCR-D.Wir freuen uns auf die gemeinsame Zusammenarbeit!F√ºr Fragen und Anregungen: Kontakt",
      "url": " /de/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "en-2017-03-06-modulprojektausschreibung-html",
      "title": "Modulprojektausschreibung",
      "content"	 : "The call for module projects within the framework of OCR-D can now be found online on the website of the German Research Foundation (DFG) (link to the call)The aim of the OCR-D coordination project, which was launched in autumn 2015, is to describe procedures and develop guidelines in order to achieve an optimal workflow and the greatest possible standardisation of OCR-related processes and metadata. Furthermore, the complete transformation of the written German cultural heritage into a machine-readable form (structured full text) is to be prepared conceptually. Primarily, works from the Union Catalogue of Books Printed in German Speaking Countries in the 16th-18th century (VD) as well as books published in the 19th century in the German language area will be considered. The VD projects comprise about 1 million titles that are currently being digitized and are to be processed by means of OCR in the future.In the first project phase of OCR-D, development needs for automatic text recognition processes were identified. Based on this, the DFG is now issuing calls for proposals for six module project topics, which will be managed in terms of content and technology by the OCR-D coordination project. The following topics are announced:Image presortingLayout recognitionText optimizationModel trainingLong-term archiving and persistenceQuality assuranceIn order to get an impression of the material to be treated, we provide Ground-Truth data (link to the data).",
      "url": " /en/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "de-2017-03-06-modulprojektausschreibung-html",
      "title": "Modulprojektausschreibung",
      "content"	 : "Die Ausschreibung f√ºr Modulprojekte im Rahmen von OCR-D ist ab sofort online auf der Seite der Deutschen Forschungsgemeinschaft (DFG) zu finden (Link zur Ausschreibung)Das im Herbst 2015 gestartete Koordinierungsprojekt OCR-D hat zum Ziel, zum einen Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine m√∂glichst weitreichende Standardisierung von OCR bezogenen Prozessen und Metadaten zu erzielen. Zum anderen soll die vollst√§ndige Transformation des schriftlichen deutschen Kulturerbes in eine maschinenlesbare Form (strukturierter Volltext) konzeptionell vorbereitet werden. Vornehmlich betrachtet werden Werke aus den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD) sowie des 19. Jhs. Die VD-Projekte umfassen ca. 1 Mio. Titel die derzeit digitalisiert und zuk√ºnftig mittels einer OCR prozessiert werden sollen.In der ersten Projektphase von OCR-D wurden Entwicklungsbedarfe f√ºr Verfahren der automatischen Texterkennung ermittelt. Darauf aufbauend erfolgt nun die Ausschreibungen der DFG zu sechs Modulprojektthemen, die inhaltlich und technisch durch das Koordinierungsgremium von OCR-D betreut. Folgende Themen sind ausgeschrieben:BildvorsortierungLayouterkennungTextoptimierungModelltrainingLangzeitarchivierung und PersistenzQualit√§tssicherungUm einen Eindruck des zu behandelnden Materials zu bekommen stellen wir Ground-Truth-Daten zur Verf√ºgung (Link zu den Daten).",
      "url": " /de/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "en-2016-12-06-staatsbibliothek-html",
      "title": "State Library in Berlin is New Project Partner of OCR-D",
      "content"	 : "The coordinating body of OCR-D consists of the Herzog August Bibliothek Wolfenb√ºttel, the Berlin-Brandenburg Academy of Sciences and Humanities Berlin, in particular the German Text Archive, and now the Staatsbibliothek zu Berlin. In future, the SBB will take over the work of the Bayerische Staatsbibliothek, which withdrew from the project on 31 August 2016. The work packages include long-term archiving and persistence, the Conception of workflows and use cases as well as the composition of training corpora. OCR-D investigates further development possibilities for Optical Character Recognition (OCR) process. The project sees itself as a coordinating body and network at the same time, bringing together developers, researchers and users to combine current research findings with practical requirements in a practicable solution. During the first project phase, development needs were identified on the basis of which module project calls will follow. In these, solutions for the development needs will be worked out and thus the current state of research on OCR will be brought together with the requirements from practice. The calls for proposals are planned for the first half of 2017. The results from OCR-D will have far-reaching changes for digitisation projects. In addition to the goal of preparing the transformation of the titles from the VD projects into machine-readable form, proposals for the DFG‚Äôs ‚ÄúDigitisation‚Äù practice rules will also be drawn up in response to the new findings in order to complete the media conversion of the entire written cultural heritage published in the German-speaking world in the medium to long term in the spirit of European and national agendas.",
      "url": " /en/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "de-2016-12-06-staatsbibliothek-html",
      "title": "Staatsbibliothek zu Berlin ist neuer Partner von OCR-D",
      "content"	 : "Das Koordinierungsgremium von OCR-D setzt sich aus der Herzog August BibliothekWolfenb√ºttel, der Berlin-Brandenburgischen Akademie der Wissenschaften Berlin,dort insbesondere dem Deutschen Textarchiv, sowie nun der Staatsbibliothek zuBerlin zusammen. Die SBB √ºbernimmt dabei zuk√ºnftig die Arbeiten der BayerischenStaatsbibliothek, die zum 31.08.2016 aus dem Projekt ausgeschieden ist. DieArbeitspakete umfassen u.a. die Langzeitarchivierung und Persistenz, dieKonzeption von Workflows und Use Cases sowie die Zusammenstellung vonTrainingskorpora. OCR-D untersucht Weiterentwicklungsm√∂glichkeiten f√ºrVerfahren der Optical Character Recognition (OCR). Das Projekt versteht sichdabei als Koordinierungsgremium und Netzwerk zugleich, bringt Entwickler,Forscher und Anwender zusammen um aktuelle Erkenntnisse aus der Forschung mitden Anforderungen aus der Praxis in einer praktikablen L√∂sung zu vereinen. Inder ersten Projektphase wurden Entwicklungsbedarfe aufgedeckt auf Basis dererModulprojektausschreibungen folgen. In diesen werden f√ºr dieEntwicklungsbedarfe L√∂sungen erarbeitet und so der aktuelle Forschungsstand zurOCR mit den Anforderungen aus der Praxis zusammen gebracht. Die Ausschreibungensind f√ºr das erste Halbjahr 2017 geplant. Die Ergebnisse aus OCR-D werdenweitreichende Ver√§nderungen f√ºr Digitalisierungsprojekte haben. Neben dem Ziel,die Transformation der Titel aus den VD-Projekten in maschinenlesbare Formvorzubereiten, werden auch Vorschl√§ge f√ºr die DFG-Praxisregeln‚ÄûDigitalisierung‚Äú an die neuen Erkenntnisse erarbeitet, um im Geisteeurop√§ischer und nationaler Agenden die Medienkonversion des gesamten imdeutschen Sprachraum erschienenen schriftlichen kulturellen Erbes mittel- bislangfristig zu vollenden.",
      "url": " /de/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "en-2016-06-01-ocrd-html",
      "title": "Ocrd",
      "content"	 : "The Project OCR-DOCR-D is a coordination project that is aimed at the further development of Optical Character Recognition (OCR) processes for historical prints.Workflow and methods of automatic text recognition are investigated, described and, if necessary, optimized. A major goal is to conceptually prepare the transformation of prints of the German-speaking countries from the 16th to 19th century into electronic full text.The Herzog August Bibliothek Wolfenb√ºttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the Staatsbibliothek zu Berlin Preu√üischer Kulturbesitz and the Karlsruhe Institute of Technology are participating in this project. The Bayerische Staatsbibliothek was also involved until 31 August 2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have digitised extensive collections of images. Searchable full texts can be automatically generated from these image data using OCR procedures. The added value provided by the use of digital full texts is indispensable in many scientific disciplines today, especially in the field of humanities research.So far, however, access to the electronic full text is often not possible or only possible in an insufficient form. Many historical holdings are available in digitalised form through the ‚ÄúVerzeichnisse der im deutschen Sprachbereich erschienenen Drucke‚Äù (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially fracture, are hardly recognized.There is a need for development here, which we are uncovering in OCR-D. We build on the already existing tools and investigations. By a new combination, in rare cases also by new development, the OCR process for VD prints shall be specialized. Thereby we are looking for answers to current technical, information scientific and organisational problems.The project is funded by the German Research Foundation (DFG) and will run for three years until September 2018. In the first phase, needs will be identified and concepts for the further development will be developed. The cooperation structure will be consolidated and continued in the second phase.  In this phase, calls for proposals for pilot projects will be issued, which will enable other institutions to participate. In all steps we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digitised material from the printed German cultural heritage of the 16th to 19th centuries will be developed.",
      "url": " /en/2016/06/01/ocrd.html"
    }
    ,
  
  {
      "slug": "de-2016-06-01-ocrd-html",
      "title": "Ocrd",
      "content"	 : "Das Projekt OCR-DOCR-D ist ein Koordinierungsprojekt, welches auf die Weiterentwicklung vonVerfahren der Optical Character Recognition (OCR) f√ºr historische Druckeausgerichtet ist.Dabei werden Workflow und Verfahren der automatischen Texterkennung untersucht,beschrieben und ggf. optimiert. Ein wesentliches Ziel ist es, dieTransformation von Drucke des deutschsprachigen Raums aus dem 16.-19.Jahrhundert in elektronischen Volltext konzeptuell vorzubereiten.An diesem Vorhaben beteiligen sich die Herzog August Bibliothek Wolfenb√ºttel,die Berlin-Brandenburgische Akademie der Wissenschaften in Berlin sowie dieStaatsbibliothek zu Berlin Preu√üischer Kulturbesitz und dem Karlsruher Institutf√ºr Technologie. Ebenfalls beteiligt war bis zum 31.08.2016 die BayerischeStaatsbibliothek. Unterst√ºtz wird das Projekt durch Experten, Wissenschaftlerund Bibliotheken.In den letzten Jahren haben vor allem wissenschaftliche Bibliothekenumfangreiche Best√§nde bilddigitalisiert. Mit Hilfe von OCR-Verfahren k√∂nnen ausdiesen Bilddaten durchsuchbare Volltexte automatisch generiert werden. DerMehrwert durch die Nutzung von digitalen Volltexten ist in vielenWissenschaftsdisziplinen, insbesondere im Bereich der geisteswissenschaftlichenForschung heute unverzichtbar.Bislang ist der  Zugriff auf den elektronischen Volltext jedoch oft nicht odernur in unzureichender Form m√∂glich. Viele historische Best√§nde liegen in digitalisierter Form durch die ‚ÄûVerzeichnisse der im deutschen Sprachbereich erschienenen Drucke‚Äú (kurz VD) vor. Resultate aus g√§ngigen OCR-Verfahren waren bislang ungen√ºgend. Insbesondere werden alte Drucktypen, vor allem Fraktur, nur schwerlich erkannt.Hier besteht Entwicklungsbedarf, den wir in OCR-D  aufdecken. Wir bauen dabei auf die bereits bestehende Tools und Untersuchungen auf. Durch eine Neu-Kombination, in seltenen F√§llen auch durch Neuentwicklung, soll der OCR-Prozess f√ºr die VD-Drucke spezialisiert werden. Dabei wird nach Antworten auf aktuelle technische, informationswissenschaftliche und organisatorische Probleme gesucht.Das Projekt wird durch die Deutsche Forschungsgemeinschaft (DFG) gef√∂rdert und hat eine Laufzeit von drei Jahren bis September 2018. In der ersten Phase  werden Bedarfe aufgedeckt und Konzepte f√ºr den weiteren Verlauf erarbeitet. Die Kooperationsstruktur wird gefestigt und in der zweiten Phase fortgef√ºhrt.  In dieser werden Ausschreibungen f√ºr Pilotprojekte erfolgen, die eine Beteiligung weiterer Einrichtungen erm√∂glicht. In allen Schritten begr√º√üen wir einen regen Austausch mit Kolleginnen und Kollegen aus  artverwandten Projekten und Einrichtungen sowie Dienstleistern.Am Ende des Gesamtvorhabens soll ein konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des gedruckten deutschen Kulturerbes des 16. bis 19. Jh. erarbeitet sein.",
      "url": " /de/2016/06/01/ocrd.html"
    }
    
  
]
