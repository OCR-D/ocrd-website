[
  

    {
      "slug": "404-html",
      "title": "",
      "content"	 : "  404 - Page not found",
      "url": " /404.html"
    },
  

    {
      "slug": "en-spec-changelog-html",
      "title": "Change Log",
      "content"	 : "Change LogAll notable changes to the specs will be documented in this file.Versioned according to Semantic Versioning.UnreleasedChanged:  CLI: Processors being called without any arguments -&amp;gt; show help, #1563.12.0 - 2021-01-26Changed:  Resource lookup: Remove XDG_CONFIG_HOME and XDG_CACHE_HOME  Resource lookup: Add /usr/local/share/ocrd-resources3.11.0 - 2021-01-20Changed:  Resource lookup in an intermediary ocrd-resources directory  Drop python-specific resource locations  Drop /usr/local/share resource location3.10.0 - 2020-12-02Changed:  Revise glossary, mostly by @bertsky3.9.1 - 2020-10-12Changed:  processor parameter values can be arrays, #1743.9.0 - 2020-07-21Changed:  CLI: Processors being called without valid METS file -&amp;gt; show help, #1563.8.0 - 2020-07-13Added:  Parameter JSON files may contain #-prefixed comments, #161  Processor resources, encompassing bundled/user-provided parameter JSON files and file parameter values like models, #158, #162  Mechanism for resolving file parameter values to actual filenames, #163  CLI: -P/--parameter-override to override single key-value pairs of parameter JSON, #166Changed:  mets:file representing page:AlternativeImage should not be added to separate mets:fileGrp but rather to the PAGE-XML whence they originate, #164  Recommendation how file IDs should be derived from existing mets:file, #164  CLI: -p/--parameter option repeatable, results are merged right to left, #161  METS: Simplify the convention for mets:file/@ID for derived images, #164  mets:fileGrp for prerprocessing steps should use the qualifier PRE instead of IMG, #164Removed:  Recommendations on fileGrp/@USE for images, #1643.7.0 - 2020-06-07Added:  ocrd-tool.json: Parameter values may be objects, #143  glossary: definitions of “print space” and “border”, #1143.6.0 - 2020-04-30Added:  CLI: --overwrite flag to delete existing output files before processing, #1513.5.0 - 2020-04-20Changed:  CLI: clarify requirements on processors, ht @bertsky, #148  Use region instead of block for areas on the page, #135  PAGE: imageFilename must NOT be a URL but a relative filename, #140  Updated URLs to point to https://ocr-d.de instead of https://ocr-d.github.io, #149Added:  docker: instructions on naming and labelling images, #139  CLI tools must implement -h/--help, #1153.4.2 - 2020-01-08Changed:  bagit-profile accepts metadata as non-payload dir, #133  Relaxed the requirement for the mets:fileGrp/@USE syntax, #1383.4.1 - 2020-01-03Added:  No multi-page TIFF, #1323.4.0 - 2019-11-05Fixed  Various typos, #128Changed:  Dockerfile: no CMD, no ENTRYPOINT, #130  Processors should assume 300 dpi if image metadata cannot be trusted, #129Added:  Spec for provenance, #1263.3.0 - 2019-10-23Added:  Draft spec for logging  Draft spec for provenanceChanged:  ocrd-tool: Additional additional category layout/segmentation/text-image  ocrd-tool: Remove syntactical restriction for content-type  ocrd-tool: output_file_grp no longer required  CLI: --mets and --working-dir are optional not required  CLI: --output-file-grp is optional, OCR-D/core#2963.2.1 - 2019-06-25Added:  glossary: “MP”, #112  glossary: “font family”, #100 #109  cli: allow JSON strings for -p, OCR-D/core#239 #110Fixed:  bagit: path of OcrdMets must be relative to /data, fix #107, #1133.2.0 - 2019-02-27Added:Convention for columnsFixed:PAGE: link to the page xml docs3.1.0 - 2018-12-20Added:  Consistency check level ‘lax’Fixed:  Example in ocrd_tool.md is from ocrd_kraken, not ocrd_tesserocr3.0.0 - 2018-12-13Added:  PAGE text result and consistency checks, #82, OCR-D/assets#16Changed:  :fire: Drop recommendation on reusing source file ID for page grouping  :fire: Drop GROUPID and replace with mets:structMap[@TYPE=”PHYSICAL”] throughout  :fire: CLI: Replace -g/-group-id with -g/--page-id  CLI: Mark possible comma-separated multi-value parameters as such  CLI: Update ocrd process example  OCRD-ZIP: Set BagIt-Profile-Version to 1.22.7.0 - 2018-12-04Added:  Font information, #76, #962.6.3 - 2018-11-23Changed:  OCRD-ZIP: Ocrd-Mets and mets:FLocat URI/paths must be relative to /data, #99  OCRD-ZIP: Ocrd-Mets only relevant for extraction  OCRD-ZIP: Filenames MUST be relative to mets.xml  METS: Filenames MAY/SHOULD be relative to mets.xml  OCRD-ZIP: Allow a limited set of files in the bag basedir (readme, build files), #972.6.2 - 2018-11-22Changed:  OCRD-ZIP bagit profile: Add empty list requirement for Tag-Manifest-Required, Tag-Files-Required  OCRD-ZIP bagit profile: Contact info  OCRD-ZIP allow fetch.txt, #982.6.1 - 2018-11-09Fixed:  OCRD-ZIP: typo in bagit-profile: Bagit- –&amp;gt; BagIt-  OCRD-ZIP: Require BagIt-Profile-Identifier  OCRD-ZIP: Version number must be a string, bagit-profile/bagit-profile#132.6.0 - 2018-11-06Changed:  Base workspace and workspace serialization mechanics on bagit, #702.5.0 - 2018-10-30Added:  Recording processing information in METS, #89  Input and output file groups can be provided in ocrd-tool.json, #91Changed  :fire: METS: grouping pages by physical structMap not GROUPID, #812.4.0 - 2018-10-19Added:  File parameters, #69  Step for post-correction, #642.3.1 - 2018-10-10Fixed  CLI: Example used repeated options2.3.0 - 2018-09-26Changed:  CLI: filtering by log level required, OCR-D/core#173, #74  CLI: log messages must adhere to uniform pattern, #78Added:  CLI: Convention to prefer comma-separated values over repeated flags, #682.2.2 - 2018-08-14Fixed:  Missed description for parameters2.2.1 - 2018-07-25Changed  spell out parameter properties in ocrd-tool.json schem2.2.0 - 2018-07-23Added:  CLI: Conventions for handling URL on the command line2.1.2 - 2018-07-19Added:  Reference PAGE media type in PAGE conventions, #652.1.1 - 2018-06-18Fixed:  ocrd-tool: regex for version had a YAML error2.1.0 - 2018-06-18Added:  ocrd-tool: Must define version  METS: mets:file must have ID  METS: mets:fileGrp must have consistent MIMETYPE  METS: mets:file GROUPID must be unique with a mets:fileGrp2.0.0 - 2018-06-18Removed:  –output-mets CLI option1.3.0 - 2018-06-15Added:  Glossary, #56Removed:  drop OCR-D-GT-PAGE, #61Fixed:  explain GT- prefix for fileGrp@USE of ground truth files, #58  various typos1.2.0 - 2018-05-25Fixed:  Fix example for ocrd_tool  Fix TIFF media typeAdded:  -J/–dump-json, #30Changed  ocrd-tool: tags -&amp;gt; category, #44  ocrd-tool: step -&amp;gt; steps (now an array), #44  ocrd-tool: parameterSchema -&amp;gt; parameters, #48  ocrd-tool: ‘tools’ is an object now, not an array, #431.1.5 - 2018-05-15Added:  ocrd-tool: Steps: preprocessing/optimization/grayscale_normalization and layout/segmentation/word  PAGE conventions1.1.4 - 2018-05-02Added:  PAGE/XML media type, #33  mets:file@GROUPID == pg:pcGtsId, #311.1.3 - 2018-04-28Added:  Add OCR-D-SEG-WORD and OCR-D-SEG-GLYPH as USE attributes1.1.2 - 2018-04-23Changed:  rename repo OCR-D/pyocrd -&amp;gt; OCR-D/core  rename repo OCR-D/ocrd-assets -&amp;gt; OCR-D/assets  renamed docker base image ocrd/pyocrd -&amp;gt; ocrd/coreFixed:  In ocrd_tool example: renamed parameter structure-level -&amp;gt; level-of-operation1.1.1 - 2018-04-19Fixed:  typo: exceutable -&amp;gt; executable  disallow custom properties1.1.0 - 2018-04-19Added  Spec for OCRD-ZIPChanged  Use executable instead of binary to reduce confusionFixed  typos (@stweil)Removed1.0.0 - 2018-04-16Initial Release",
      "url": " /en/spec/CHANGELOG.html"
    },
  

    {
      "slug": "de-spec-changelog-html",
      "title": "",
      "content"	 : "Change LogAll notable changes to the specs will be documented in this file.Versioned according to Semantic Versioning.UnreleasedChanged:  CLI: Processors being called without any arguments -&amp;gt; show help, #1563.12.0 - 2021-01-26Changed:  Resource lookup: Remove XDG_CONFIG_HOME and XDG_CACHE_HOME  Resource lookup: Add /usr/local/share/ocrd-resources3.11.0 - 2021-01-20Changed:  Resource lookup in an intermediary ocrd-resources directory  Drop python-specific resource locations  Drop /usr/local/share resource location3.10.0 - 2020-12-02Changed:  Revise glossary, mostly by @bertsky3.9.1 - 2020-10-12Changed:  processor parameter values can be arrays, #1743.9.0 - 2020-07-21Changed:  CLI: Processors being called without valid METS file -&amp;gt; show help, #1563.8.0 - 2020-07-13Added:  Parameter JSON files may contain #-prefixed comments, #161  Processor resources, encompassing bundled/user-provided parameter JSON files and file parameter values like models, #158, #162  Mechanism for resolving file parameter values to actual filenames, #163  CLI: -P/--parameter-override to override single key-value pairs of parameter JSON, #166Changed:  mets:file representing page:AlternativeImage should not be added to separate mets:fileGrp but rather to the PAGE-XML whence they originate, #164  Recommendation how file IDs should be derived from existing mets:file, #164  CLI: -p/--parameter option repeatable, results are merged right to left, #161  METS: Simplify the convention for mets:file/@ID for derived images, #164  mets:fileGrp for prerprocessing steps should use the qualifier PRE instead of IMG, #164Removed:  Recommendations on fileGrp/@USE for images, #1643.7.0 - 2020-06-07Added:  ocrd-tool.json: Parameter values may be objects, #143  glossary: definitions of “print space” and “border”, #1143.6.0 - 2020-04-30Added:  CLI: --overwrite flag to delete existing output files before processing, #1513.5.0 - 2020-04-20Changed:  CLI: clarify requirements on processors, ht @bertsky, #148  Use region instead of block for areas on the page, #135  PAGE: imageFilename must NOT be a URL but a relative filename, #140  Updated URLs to point to https://ocr-d.de instead of https://ocr-d.github.io, #149Added:  docker: instructions on naming and labelling images, #139  CLI tools must implement -h/--help, #1153.4.2 - 2020-01-08Changed:  bagit-profile accepts metadata as non-payload dir, #133  Relaxed the requirement for the mets:fileGrp/@USE syntax, #1383.4.1 - 2020-01-03Added:  No multi-page TIFF, #1323.4.0 - 2019-11-05Fixed  Various typos, #128Changed:  Dockerfile: no CMD, no ENTRYPOINT, #130  Processors should assume 300 dpi if image metadata cannot be trusted, #129Added:  Spec for provenance, #1263.3.0 - 2019-10-23Added:  Draft spec for logging  Draft spec for provenanceChanged:  ocrd-tool: Additional additional category layout/segmentation/text-image  ocrd-tool: Remove syntactical restriction for content-type  ocrd-tool: output_file_grp no longer required  CLI: --mets and --working-dir are optional not required  CLI: --output-file-grp is optional, OCR-D/core#2963.2.1 - 2019-06-25Added:  glossary: “MP”, #112  glossary: “font family”, #100 #109  cli: allow JSON strings for -p, OCR-D/core#239 #110Fixed:  bagit: path of OcrdMets must be relative to /data, fix #107, #1133.2.0 - 2019-02-27Added:Convention for columnsFixed:PAGE: link to the page xml docs3.1.0 - 2018-12-20Added:  Consistency check level ‘lax’Fixed:  Example in ocrd_tool.md is from ocrd_kraken, not ocrd_tesserocr3.0.0 - 2018-12-13Added:  PAGE text result and consistency checks, #82, OCR-D/assets#16Changed:  :fire: Drop recommendation on reusing source file ID for page grouping  :fire: Drop GROUPID and replace with mets:structMap[@TYPE=”PHYSICAL”] throughout  :fire: CLI: Replace -g/-group-id with -g/--page-id  CLI: Mark possible comma-separated multi-value parameters as such  CLI: Update ocrd process example  OCRD-ZIP: Set BagIt-Profile-Version to 1.22.7.0 - 2018-12-04Added:  Font information, #76, #962.6.3 - 2018-11-23Changed:  OCRD-ZIP: Ocrd-Mets and mets:FLocat URI/paths must be relative to /data, #99  OCRD-ZIP: Ocrd-Mets only relevant for extraction  OCRD-ZIP: Filenames MUST be relative to mets.xml  METS: Filenames MAY/SHOULD be relative to mets.xml  OCRD-ZIP: Allow a limited set of files in the bag basedir (readme, build files), #972.6.2 - 2018-11-22Changed:  OCRD-ZIP bagit profile: Add empty list requirement for Tag-Manifest-Required, Tag-Files-Required  OCRD-ZIP bagit profile: Contact info  OCRD-ZIP allow fetch.txt, #982.6.1 - 2018-11-09Fixed:  OCRD-ZIP: typo in bagit-profile: Bagit- –&amp;gt; BagIt-  OCRD-ZIP: Require BagIt-Profile-Identifier  OCRD-ZIP: Version number must be a string, bagit-profile/bagit-profile#132.6.0 - 2018-11-06Changed:  Base workspace and workspace serialization mechanics on bagit, #702.5.0 - 2018-10-30Added:  Recording processing information in METS, #89  Input and output file groups can be provided in ocrd-tool.json, #91Changed  :fire: METS: grouping pages by physical structMap not GROUPID, #812.4.0 - 2018-10-19Added:  File parameters, #69  Step for post-correction, #642.3.1 - 2018-10-10Fixed  CLI: Example used repeated options2.3.0 - 2018-09-26Changed:  CLI: filtering by log level required, OCR-D/core#173, #74  CLI: log messages must adhere to uniform pattern, #78Added:  CLI: Convention to prefer comma-separated values over repeated flags, #682.2.2 - 2018-08-14Fixed:  Missed description for parameters2.2.1 - 2018-07-25Changed  spell out parameter properties in ocrd-tool.json schem2.2.0 - 2018-07-23Added:  CLI: Conventions for handling URL on the command line2.1.2 - 2018-07-19Added:  Reference PAGE media type in PAGE conventions, #652.1.1 - 2018-06-18Fixed:  ocrd-tool: regex for version had a YAML error2.1.0 - 2018-06-18Added:  ocrd-tool: Must define version  METS: mets:file must have ID  METS: mets:fileGrp must have consistent MIMETYPE  METS: mets:file GROUPID must be unique with a mets:fileGrp2.0.0 - 2018-06-18Removed:  –output-mets CLI option1.3.0 - 2018-06-15Added:  Glossary, #56Removed:  drop OCR-D-GT-PAGE, #61Fixed:  explain GT- prefix for fileGrp@USE of ground truth files, #58  various typos1.2.0 - 2018-05-25Fixed:  Fix example for ocrd_tool  Fix TIFF media typeAdded:  -J/–dump-json, #30Changed  ocrd-tool: tags -&amp;gt; category, #44  ocrd-tool: step -&amp;gt; steps (now an array), #44  ocrd-tool: parameterSchema -&amp;gt; parameters, #48  ocrd-tool: ‘tools’ is an object now, not an array, #431.1.5 - 2018-05-15Added:  ocrd-tool: Steps: preprocessing/optimization/grayscale_normalization and layout/segmentation/word  PAGE conventions1.1.4 - 2018-05-02Added:  PAGE/XML media type, #33  mets:file@GROUPID == pg:pcGtsId, #311.1.3 - 2018-04-28Added:  Add OCR-D-SEG-WORD and OCR-D-SEG-GLYPH as USE attributes1.1.2 - 2018-04-23Changed:  rename repo OCR-D/pyocrd -&amp;gt; OCR-D/core  rename repo OCR-D/ocrd-assets -&amp;gt; OCR-D/assets  renamed docker base image ocrd/pyocrd -&amp;gt; ocrd/coreFixed:  In ocrd_tool example: renamed parameter structure-level -&amp;gt; level-of-operation1.1.1 - 2018-04-19Fixed:  typo: exceutable -&amp;gt; executable  disallow custom properties1.1.0 - 2018-04-19Added  Spec for OCRD-ZIPChanged  Use executable instead of binary to reduce confusionFixed  typos (@stweil)Removed1.0.0 - 2018-04-16Initial Release",
      "url": " /de/spec/CHANGELOG.html"
    },
  

    {
      "slug": "en-spec-readme-html",
      "title": "",
      "content"	 : "Specification of the technical architecture, interface definitions and data exchange format(s)See https://ocr-d.de/en/spec/.",
      "url": " /en/spec/README.html"
    },
  

    {
      "slug": "de-spec-readme-html",
      "title": "",
      "content"	 : "Specification of the technical architecture, interface definitions and data exchange format(s)See https://ocr-d.de/en/spec/.",
      "url": " /de/spec/README.html"
    },
  

    {
      "slug": "en-about-html",
      "title": "The OCR-D project",
      "content"	 : "The OCR-D projectBackgroundWith the Union Catalogue of Books of the 16th–18th century (VD16, VD17, VD18) published in the German-speaking countries, a retrospective national bibliography of early modern writings from the German-speaking countries is being compiled. In order to facilitate research access to these texts, great concerted efforts have been and are being undertaken to make fully digitised copies or key pages for the recorded titles available in digital form.In view of the developments and new possibilities in the field of Optical Character Recognition (OCR), experts at a DFG workshop in March 2014 assessed the full-text transformation of VD as an ambitious but achievable goal. Making full texts available for the purpose of full-text search and further processing, for example with tools of the Digital Humanities, is a major desideratum of research, which is to be addressed by a coordinated funding initiative.OCR is a comprehensive process that typically involves a sequence of several steps in the workflow: Besides the pure recognition of letters and words, techniques such as pre-processing (image optimization and binarization), layout analysis (recognition and classification of structural features such as headings, paragraphs, etc.) and post-processing (error correction) are applied. While most of these steps can also benefit from the use of Deep Neural Networks, so far hardly any free and open standard tools and related best practices have emerged. The full text recognition of historical documents is particularly complicated due to their great variability in font, layout, language and orthography.Goals and structure of the OCR-D projectThis is where the DFG-funded project OCR-D comes in. Its main goal is the conceptual and technical preparation of the full text transformation of the VD. The task of automatic full-text recognition is broken down into its individual process steps, which can be retraced in the open source OCR-D software. This allows to create optimal workflows for the old prints to be processed and thus to generate scientifically usable full texts.For this purpose, a coordination project was formed by the Berlin-Brandenburg Academy of Sciences and Humanities, the Herzog-August Library Wolfenbüttel, the Berlin State Library and the Karlsruhe Institute of Technology. In the first project phase, the project identified development needs, which are currently being addressed by a total of eight OCR-D module projects in the second project phase.Full-text recognition is understood as a complex process that includes several preprocessing and postprocessing steps in additionto the actual text recognition (see Fig. 1). First, a digital image is prepared for text recognition in preprocessing by binarization, cropping,deskewing, dewarping and despeckling. This is followed by layout recognition, which identifies the text areas of a page down to line level. Especially the recognition of the lines respectively the baseline is important for the following actual text recognition, which in all modern approaches is based on neural networks. The individual structures or elements of the full-text recognized document are then classified according to their typographic function and the OCR result is improved in the post-correction process if necessary, before it is transferred to repositories for long-term archiving.For the individual process steps, tools of eight module-projects are developed. Furthermore, already existing Open Source tools or tools developed in other projects can be integrated into the OCR-D framework through the modular structure of OCR-D and thus synergies can be used.In addition to the envisaged full text transformation of VD titles (16th-19th century), which is technically and conceptually prepared within the OCR-D project, OCR-D pursues the following further objectives:  the creation of reference corpora for training and testing  the development of standards in the fields of metadata, documentation and ground truth  the further development of individual processing steps, with a particular focus on Optical Layout Recognition (OLR)  the analysis of existing tools and their further development  the creation of reusable software packages  the establishment of quality assurance proceduresIn all steps we welcome a lively exchange with colleagues from other projects and institutions as well as service providers, in order to finally be able to realize a consolidated procedure for the OCR processing of digitized material of the printed German cultural heritage of the 16th–19th century.",
      "url": " /en/about.html"
    },
  

    {
      "slug": "de-about-html",
      "title": "Das OCR-D-Projekt",
      "content"	 : "Das OCR-D-ProjektHintergrundMit den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.–18. Jahrhunderts (VD16, VD17, VD18) wird eine retrospektive Nationalbibliografie des frühneuzeitlichen Schriftguts aus dem deutschsprachigen Raum erstellt. Um der Forschung die Zugänglichkeit zu diesen Texten zu erleichtern, wurden und werden große, konzertierte Anstrengungen unternommen, Volldigitalisate oder Schlüsselseiten zu den einzelnen verzeichneten Titeln digital bereitzustellen.Mit Blick auf die Entwicklungen und neuen Möglichkeiten im Bereich der Optical Character Recognition (OCR) haben Experten im März 2014 im Rahmen eines DFG-Workshops die Volltexttransformation der VD als ambitioniertes, aber erreichbares Ziel eingeschätzt. Die Verfügbarmachung von Volltexten zum Zweck der Volltextsuche und Weiterbearbeitung, bspw. mit Werkzeugen der Digital Humanities, ist ein großes Desiderat der Forschung, das durch eine koordinierte Förderinitiative zu bearbeiten ist.OCR ist ein umfassender Prozess, der typischerweise eine Abfolge von mehreren Schritten im Workflow beinhaltet: Neben der reinen Erkennung von Buchstaben und Wörtern werden Techniken wie die Vorverarbeitung (Bildoptimierung und Binarisierung), die Layoutanalyse (Erkennung und Klassifizierung von Strukturmerkmalen wie Überschriften, Absätzen usw.) und die Nachbearbeitung (Fehlerkorrektur) angewendet. Während die meisten dieser Schritte auch von der Nutzung von Tiefen Neuronalen Netzen profitieren können, sind bisher kaum freie und offene Standardwerkzeuge und damit verbundene Best Practices entstanden. Die Volltexterkennung historischer Dokumente wird insbesondere durch deren große Variabilität in Schriftart, Layout, Sprache und Orthographie erschwert.Ziele und Aufbau des OCR-D-ProjektsHier setzt das DFG-geförderte Projekt OCR-D an, dessen Hauptziel die konzeptionelle und technische Vorbereitung der Volltexttransformation der VD ist. Die Aufgabe der automatischen Volltexterkennung wird in ihre einzelnen Prozessschritte zerlegt, die in der Open Source OCR-D-Software nachvollzogen werden können. Dies ermöglicht es, optimale Workflows für die zu prozessierenden alten Drucke zu erstellen und damit wissenschaftlich verwertbare Volltexte zu generieren.Dazu wurde ein Koordinationsprojekt aus der Berlin-Brandenburgischen Akademie der Wissenschaften, der Herzog-August Bibliothek Wolfenbüttel, der Staatsbibliothek zu Berlin sowie dem Karlsruher Institut für Technologie gebildet. Dieses identifizierte in der ersten Projektphase Entwicklungsbedarfe, die in der derzeitigen zweiten Projektphase von insgesamt acht OCR-D-Modulprojekten bearbeitet werden.Volltexterkennung wird dabei als ein komplexer Prozess aufgefasst, der neben der eigentlichen Texterkennungmehrere vor- und nachgelagerte Schritte mit einschließt (vgl. Abb. 1). Zunächst wird ein Bilddigitalisat im Preprocessingfür die Texterkennung aufbereitet, indem es nach Bedarf in ein Schwarz-Weiß-Bild umgewandelt (Binarization), zugeschnitten (Cropping),begradigt (Deskewing), entzerrt (Dewarping) und von Flecken bereinigt (Despeckling) wird. Im Anschluss erfolgt dieLayouterkennung, die die Textbereiche einer Seite bis auf Zeilenebene identifiziert. Besonders die Erkennung der Zeilen bzw. der Grundlinie ist wichtig für die anschließende eigentliche Texterkennung, die in allen modernen Ansätzen auf Neuronalen Netzen beruht. Danach werden die einzelnen Strukturen bzw. Elemente des volltexterkannten Dokuments ihrer typografischen Funktion nach klassifiziert und das OCR-Ergebnis ggf. in der Nachkorrektur verbessert, bevor es in Repositorien zur Langzeitarchivierung überführt wird.Für die einzelnen Prozessschritte werden zum einen Werkzeuge von acht Modulprojekten entwickelt. Zum anderen können bereits vorhandene bzw. in anderen Projekten entwickelte Open Source Werkzeuge durch den modularen Aufbau von OCR-D in das OCR-D-Framework integriert und so Synergien genutzt werden.Neben der anvisierten Volltexttransformation von VD-Titeln (16.–19. Jahrhundert), die im Rahmen des OCR-D-Projekts technisch und konzeptionell vorbereitet wird, verfolgt OCR-D die folgenden weiteren Ziele:  die Erstellung von Referenzkorpora zum Trainieren und Testen  die Erarbeitung von Standards in den Bereichen Metadaten, Dokumentation und Ground Truth  die Weiterentwicklung einzelner Verarbeitungsschritte, wobei der Fokus insbesondere auf der Optical Layout Recognition (OLR) liegt  die Analyse vorhandener Tools und deren Weiterentwicklung  die Erstellung nachnutzbarer Softwarepakete  die Erstellung von Verfahren der QualitätssicherungIn allen Schritten begrüßen wir einen regen Austausch mit Kolleginnen und Kollegen aus anderen Projekten und Einrichtungen sowie Dienstleistern, um schließlich ein konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des gedruckten deutschen Kulturerbes des 16.–19. Jahrhunderts realisieren zu können.",
      "url": " /de/about.html"
    },
  

    {
      "slug": "en-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /en/blog.html"
    },
  

    {
      "slug": "de-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /de/blog.html"
    },
  

    {
      "slug": "en-spec-cli-html",
      "title": "Command Line Interface (CLI)",
      "content"	 : "Command Line Interface (CLI)All tools provided by MP must be standalone executables, installable into $PATH.Those tools intended for run-time data processing (but not necessarily tools for training or deployment) are called processors.Processors must adhere to the following uniform interface, including mandatory and optional parameters (i.e. no more or fewer are permissible).NOTE: Command line options cannot be repeated, except those explicitlymarked as REPEATABLE (e.g. -p params.json -p &#39;{&quot;val&quot;: 1}&#39; is allowedbecause -p is repeatable.NOTE: Parameters marked MULTI-VALUE cannot be repeated but can specifymultiple values, formatted as a single string with comma-separated items (e.g.-I group1,group2,group3 instead of -I group1 -I group2 -I group3).CLI executable nameEvery CLI executable’s name must begin with ocrd-.Examples:  ocrd-kraken-binarize  ocrd-tesserocr-recognizeNo parametersIf no arguments are passed to a processor, it must show the --helpmessage message and exit with return code 1.Mandatory parameters-I, --input-file-grp GRPMULTI-VALUEMETS file group(s) used as input.Input file groups must not be modified.Optional parameters-O, --output-file-grp GRPMULTI-VALUEMETS file group(s) used as output.Omit to resort to default output file groups of the processor, or for processors that inherently do not produce output files.-g, --page-id IDMULTI-VALUEThe mets:div[@TYPE=&#39;page&#39;]/@ID that contains the mets:fptr/@FILEID pointersto files representing a page. Effectively, only those files in the input filegroup that are referenced in thesemets:div[@TYPE=&quot;page&quot;] will be processed.Omit to process all pages.--overwriteDelete files in the output file group(s) before processing.If --overwrite is set, but --page-id is not set, deleteall output file groups set with--output-file-grp, including all files that belong tothose file groups.If --overwrite is set and --page-id is set, delete all files that representany of the page IDs given with --page-id from all outputfile groups set with--output-file-grp“File deletion” in the context of --overwrite means deletion of matchingmets:file elements from the METS document and all local files thesemets:file represent.“Group deletion” in the context of --overwrite means deletion of themets:fileGrp element from METS, and deletion of all files that belong to thismets:fileGrp element.-p, --parameter PARAM_JSONREPEATABLEURL of parameter file in JSON format corresponding to theparameters section of the processor’s ocrd-tool metadata. Ifthat file is not readable and PARAM_JSON begins with { (opening brace), tryto parse PARAM_JSON as JSON. If that also fails, throw an exception.When parsing JSON, all lines matching the regular expression ^s*#.*, i.e.lines whose first non-whitespace character is #, are to be disregarded ascomments.When -p is repeated, the parsed values should be shallowly merged from rightto left.-p can be omitted to use default parameters only, or for processors withoutany parameters.-P, --parameter-override KEY VALREPEATABLECompanion to -p, --parameter PARAM_JSON that allows overriding KEY in the parameter JSON object with VAL. All P key-value-pairs are applied to the parameter JSON in the order they are given on the command line.Syntactically, VAL SHOULD be a valid JSON datatype:  &quot;a string&quot; - a string should be enclosed with double quotes, contained double quotes backslash-escaped  123 - an int  123.45 - a float  true, false - a boolean  [1, &quot;two&quot;, 3.33] - an array  {&quot;foo&quot;: 42} - an objectAs a convenience, if VAL fails to parse as a valid JSON type, it isinterpreted as a string (a string is equivalent to &quot;a string&quot;, but truewill be parsed as the boolean value true, not the string &quot;true&quot;).-m, --mets METS_INInput METS URL. Default: mets.xml-w, --working-dir DIRWorking Directory. Default: current working directory.-l, --log-level LOGLEVELSet the global maximum verbosity level. More verbose log entries will beignored. (One of OFF, ERROR, WARN, INFO (default), DEBUG, TRACE).NOTE: Setting the log level via --log-level parameter should override anyother implementation-specific means of logging configuration. For example, with--log-level TRACE no log messages should be filtered globally, whereas--log-level ERROR, only errors should be output globally.-J, --dump-jsonInstead of processing METS, output the ocrd-tool description forthis executable, in particular its parameters.-C, --show-resource FILENAMEPrint the contents of processor resource FILENAME. Look up the resp. absolute filename according to the file parameter lookup rules.-L, --list-resourcesList the names of processor resources in all of the paths defined by.the file parameter lookup rules, one name per line.-h, --helpPrint a concise description of the tool, the command line options andparameters it accepts as well as the input/output groups. This information shouldbe generated from ocrd-tool.json as much as possible.Return valueSuccessful execution should signal 0. Any non-zero return value is considered a failure.LoggingData printed to STDERR and STDOUT is captured linewise and stored as log data.Processors must adjust logging verbosity according to the --log-level parameter.Errors, especially those leading to exceptions, must be printed to STDERR.The log messages must have the format TIME LEVEL LOGGERNAME - MESSAGEn, where  TIME is the current time in the format HH:MM:ss.mmm, e.g. 07:05:31.007  LEVEL is the log level of the message, in uppercase, e.g. INFO  LOGGERNAME is the name of the logging component, such as the class name. Segments of LOGGERNAME should be separated by dot ., e.g. ocrd.fancy_tool.analyze  MESSAGE is the message to log, should not contain new lines.  n is ASCII char 0x0a (newline)Processor resourcesParameters that reference files can be resolved from relative to absolutefilename by following the conventions laid out in the ocrd_toolspec. These files, either bundled by the processordeveloper or put in place by the user, are called processor resources. Theprocessor resources of a processor can be listed with the -L/--list-resourcesoption and individual processor resources can be retrieved with the-C/--show-resource option. Since processor resources use the same mechanismas file parameters, they can be used  as the argument to the -p/--parameter option (i.e. a preset file), and  as the value of a file-type parameter (e.g. a model file)and the processor must resolve them to absolute paths according to the rulesfor file parameters.URL/file conventionWhenever a URL is expected, it should be possible to use a local file pathinstead and have the implementation interpret as a file:// URL on the fly.Implementations should adhere to this algorithm when resolving a URL u:  If u contains the string ://: Do not modify.  If u is an absolute path according to the mechanics of the underlying file system: Prepend file:// to u.  Otherwise: Resolve u as a path relative to the current working directory, prepend file:// to u.NOTE: This convention is limited to the CLI for convenience of users anddevelopers. In METS and PAGE documents, URLs must be strictly valid andresolvable by common software agents as-is.ExampleThis is how the CLI provided by the MP should work:$&amp;gt; ocrd-kraken-binarize     --mets &quot;file:///path/to/file/mets.xml&quot;     --working-dir &quot;file:///path/to/workingDir/&quot;     --parameters &quot;file:///path/to/file/parameters.json&quot;     --page-id PHYS_0001,PHYS_0002,PHYS_0003     --input-file-grp OCR-D-IMG    --output-file-grp OCR-D-IMG-BIN-KRAKENAnd this is how it will be called with the ocrd process CLI:$&amp;gt; ocrd process     &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN-KRAKEN -p /path/to/file/parameters.json&#39;    -m &quot;file:///path/to/file/mets.xml&quot;     -g PHYS_0001,PHYS_0002,PHYS_0003    preprocessing/binarization/kraken-binarizeMETS input&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;Input JSON parameter file{    &quot;threshold&quot;: 0.05,    &quot;zoom&quot;: 2,    &quot;range&quot;: [5, 10],}METS outputThis is the METS file after being run through the MP CLI:&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div DMDID=&quot;DMDPHYS_0000&quot; ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN-KRAKEN&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0001.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0002.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0003.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /en/spec/cli.html"
    },
  

    {
      "slug": "de-spec-cli-html",
      "title": "",
      "content"	 : "Command Line Interface (CLI)All tools provided by MP must be standalone executables, installable into $PATH.Those tools intended for run-time data processing (but not necessarily tools for training or deployment) are called processors.Processors must adhere to the following uniform interface, including mandatory and optional parameters (i.e. no more or fewer are permissible).NOTE: Command line options cannot be repeated, except those explicitlymarked as REPEATABLE (e.g. -p params.json -p &#39;{&quot;val&quot;: 1}&#39; is allowedbecause -p is repeatable.NOTE: Parameters marked MULTI-VALUE cannot be repeated but can specifymultiple values, formatted as a single string with comma-separated items (e.g.-I group1,group2,group3 instead of -I group1 -I group2 -I group3).CLI executable nameEvery CLI executable’s name must begin with ocrd-.Examples:  ocrd-kraken-binarize  ocrd-tesserocr-recognizeNo parametersIf no arguments are passed to a processor, it must show the --helpmessage message and exit with return code 1.Mandatory parameters-I, --input-file-grp GRPMULTI-VALUEMETS file group(s) used as input.Input file groups must not be modified.Optional parameters-O, --output-file-grp GRPMULTI-VALUEMETS file group(s) used as output.Omit to resort to default output file groups of the processor, or for processors that inherently do not produce output files.-g, --page-id IDMULTI-VALUEThe mets:div[@TYPE=&#39;page&#39;]/@ID that contains the mets:fptr/@FILEID pointersto files representing a page. Effectively, only those files in the input filegroup that are referenced in thesemets:div[@TYPE=&quot;page&quot;] will be processed.Omit to process all pages.--overwriteDelete files in the output file group(s) before processing.If --overwrite is set, but --page-id is not set, deleteall output file groups set with--output-file-grp, including all files that belong tothose file groups.If --overwrite is set and --page-id is set, delete all files that representany of the page IDs given with --page-id from all outputfile groups set with--output-file-grp“File deletion” in the context of --overwrite means deletion of matchingmets:file elements from the METS document and all local files thesemets:file represent.“Group deletion” in the context of --overwrite means deletion of themets:fileGrp element from METS, and deletion of all files that belong to thismets:fileGrp element.-p, --parameter PARAM_JSONREPEATABLEURL of parameter file in JSON format corresponding to theparameters section of the processor’s ocrd-tool metadata. Ifthat file is not readable and PARAM_JSON begins with { (opening brace), tryto parse PARAM_JSON as JSON. If that also fails, throw an exception.When parsing JSON, all lines matching the regular expression ^s*#.*, i.e.lines whose first non-whitespace character is #, are to be disregarded ascomments.When -p is repeated, the parsed values should be shallowly merged from rightto left.-p can be omitted to use default parameters only, or for processors withoutany parameters.-P, --parameter-override KEY VALREPEATABLECompanion to -p, --parameter PARAM_JSON that allows overriding KEY in the parameter JSON object with VAL. All P key-value-pairs are applied to the parameter JSON in the order they are given on the command line.Syntactically, VAL SHOULD be a valid JSON datatype:  &quot;a string&quot; - a string should be enclosed with double quotes, contained double quotes backslash-escaped  123 - an int  123.45 - a float  true, false - a boolean  [1, &quot;two&quot;, 3.33] - an array  {&quot;foo&quot;: 42} - an objectAs a convenience, if VAL fails to parse as a valid JSON type, it isinterpreted as a string (a string is equivalent to &quot;a string&quot;, but truewill be parsed as the boolean value true, not the string &quot;true&quot;).-m, --mets METS_INInput METS URL. Default: mets.xml-w, --working-dir DIRWorking Directory. Default: current working directory.-l, --log-level LOGLEVELSet the global maximum verbosity level. More verbose log entries will beignored. (One of OFF, ERROR, WARN, INFO (default), DEBUG, TRACE).NOTE: Setting the log level via --log-level parameter should override anyother implementation-specific means of logging configuration. For example, with--log-level TRACE no log messages should be filtered globally, whereas--log-level ERROR, only errors should be output globally.-J, --dump-jsonInstead of processing METS, output the ocrd-tool description forthis executable, in particular its parameters.-C, --show-resource FILENAMEPrint the contents of processor resource FILENAME. Look up the resp. absolute filename according to the file parameter lookup rules.-L, --list-resourcesList the names of processor resources in all of the paths defined by.the file parameter lookup rules, one name per line.-h, --helpPrint a concise description of the tool, the command line options andparameters it accepts as well as the input/output groups. This information shouldbe generated from ocrd-tool.json as much as possible.Return valueSuccessful execution should signal 0. Any non-zero return value is considered a failure.LoggingData printed to STDERR and STDOUT is captured linewise and stored as log data.Processors must adjust logging verbosity according to the --log-level parameter.Errors, especially those leading to exceptions, must be printed to STDERR.The log messages must have the format TIME LEVEL LOGGERNAME - MESSAGEn, where  TIME is the current time in the format HH:MM:ss.mmm, e.g. 07:05:31.007  LEVEL is the log level of the message, in uppercase, e.g. INFO  LOGGERNAME is the name of the logging component, such as the class name. Segments of LOGGERNAME should be separated by dot ., e.g. ocrd.fancy_tool.analyze  MESSAGE is the message to log, should not contain new lines.  n is ASCII char 0x0a (newline)Processor resourcesParameters that reference files can be resolved from relative to absolutefilename by following the conventions laid out in the ocrd_toolspec. These files, either bundled by the processordeveloper or put in place by the user, are called processor resources. Theprocessor resources of a processor can be listed with the -L/--list-resourcesoption and individual processor resources can be retrieved with the-C/--show-resource option. Since processor resources use the same mechanismas file parameters, they can be used  as the argument to the -p/--parameter option (i.e. a preset file), and  as the value of a file-type parameter (e.g. a model file)and the processor must resolve them to absolute paths according to the rulesfor file parameters.URL/file conventionWhenever a URL is expected, it should be possible to use a local file pathinstead and have the implementation interpret as a file:// URL on the fly.Implementations should adhere to this algorithm when resolving a URL u:  If u contains the string ://: Do not modify.  If u is an absolute path according to the mechanics of the underlying file system: Prepend file:// to u.  Otherwise: Resolve u as a path relative to the current working directory, prepend file:// to u.NOTE: This convention is limited to the CLI for convenience of users anddevelopers. In METS and PAGE documents, URLs must be strictly valid andresolvable by common software agents as-is.ExampleThis is how the CLI provided by the MP should work:$&amp;gt; ocrd-kraken-binarize     --mets &quot;file:///path/to/file/mets.xml&quot;     --working-dir &quot;file:///path/to/workingDir/&quot;     --parameters &quot;file:///path/to/file/parameters.json&quot;     --page-id PHYS_0001,PHYS_0002,PHYS_0003     --input-file-grp OCR-D-IMG    --output-file-grp OCR-D-IMG-BIN-KRAKENAnd this is how it will be called with the ocrd process CLI:$&amp;gt; ocrd process     &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN-KRAKEN -p /path/to/file/parameters.json&#39;    -m &quot;file:///path/to/file/mets.xml&quot;     -g PHYS_0001,PHYS_0002,PHYS_0003    preprocessing/binarization/kraken-binarizeMETS input&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;Input JSON parameter file{    &quot;threshold&quot;: 0.05,    &quot;zoom&quot;: 2,    &quot;range&quot;: [5, 10],}METS outputThis is the METS file after being run through the MP CLI:&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div DMDID=&quot;DMDPHYS_0000&quot; ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN-KRAKEN&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0001.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0002.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0003.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /de/spec/cli.html"
    },
  

    {
      "slug": "en-contact-html",
      "title": "OCR-D is you and me",
      "content"	 : "OCR-D is you and meContactContact person:Lena HinrichsenProject coordination OCR-DDuke August Library Wolfenbüttelhinrichsen[at]hab.deSubstitution:Andrea OpitzDeputy head of departmentNew Media, Digital LibraryDuke August Library Wolfenbüttelopitz[at]hab.deJohannes MangeiDeputy Director of the Herzog August Library WolfenbüttelHead of Department New Media, Digital LibraryDuke August Library Wolfenbüttelmangei[at]hab.deManagersJohannes MangeiDuke August Library Wolfenbüttelmangei[at]hab.deAlexander GeykenBerlin-Brandenburg Academy of Sciences and Humanities in Berlingeyken[at]bbaw.deReinhard AltenhönerState Library of Berlin Prussian Cultural Heritagereinhard.altenhoener[at]sbb.spk-berlin.deMustafa DoganGöttingen State and University Librarydogan[at]sub.uni-goettingen.dePhilipp WiederGesellschaft für wissenschaftliche Datenverarbeitung Göttingenphilipp.wieder[at]gwdg.deStaffLena HinrichsenProject CoordinatorDuke August Library Wolfenbüttelhinrichsen[at]hab.deMatthias BoenigBerlin-Brandenburg Academy of Sciences and Humanities in Berlinocrd[at]bbaw.deKonstantin BaiererBerlin State Library - Prussian Cultural Heritagekonstantin.baierer[at]sbb.spk-berlin.deClemens NeudeckerBerlin State Library - Prussian Cultural Heritage clemens.neudecker[at]sbb.spk-berlin.deMareen GeestmannGöttingen State and University Librarygeestmann[at]sub.uni-goettingen.deMichelle WeidlingGöttingen State and University Libraryweidling[at]sub.uni-goettingen.dePaul PestovGöttingen State and University Librarypestov[at]sub.uni-goettingen.deFormer ManagersRainer StotzkaKarlsruhe Institute of TechnologyThomas StäckerDuke August Library WolfenbüttelMarkus BrantlBavarian State Library MunichFormer staffElisabeth EnglDuke August Library WolfenbüttelVolker HartmannKarlsruhe Institute of Technology/Steinbuch Centre for ComputingElisa HerrmannDuke August Library WolfenbüttelKay-Michael WürznerBerlin-Brandenburg Academy of Sciences and Humanities in BerlinAjinkya PrabhuneKarlsruhe Institute of TechnologySebastian MangoldBavarian State Library MunichScientific Advisory BoardThe project is counseled by a scientific advisory board, which is constituted by the following persons at the moment:Gregory CraneTufts UniversityMax KaiserAustrian National LibraryJoachim KöhlerFraunhofer IAISSebastian MeyerSLUB DresdenGünter MühlbergerUniversity of InnsbruckKlaus SchulzLMU Munich – CISRobert StrötgenTU BraunschweigCooperation PartnersThe project has signed Letters of Intent with several companies, non-profit organizations and related projects,mainly in order to exchange ideas and information on interfaces for and current developments in mass digitization.Furthermore, those cooperations are part of OCR-D’s dissemination strategy, which will gain more and more prominenceas the project continues and the OCR-D-software is developing into an operable, stabilized product.  Content Conversion Specialists (CCS)  Kitodo. Key to digital objects e.V.  OCR4all, GitHub  semantics Kommunikationsmanagement GmbH  Zeutschel GmbH",
      "url": " /en/contact.html"
    },
  

    {
      "slug": "de-contact-html",
      "title": "Das OCR-D-Team",
      "content"	 : "Das OCR-D-TeamKontaktAnsprechpartnerin:Lena HinrichsenProjektkoordinatorin OCR-DHerzog August Bibliothek Wolfenbüttelhinrichsen[at]hab.deStellvertreter*innen:Andrea OpitzStellvertreterin der Abteilungsleitung Neuere Medien, Digitale BibliothekHerzog August Bibliothek Wolfenbüttelopitz[at]hab.deJohannes MangeiStellvertretender Direktor der Herzog August BibliothekAbteilungsleitung Neuere Medien, Digitale BibliothekHerzog August Bibliothek Wolfenbüttelmangei[at]hab.deProjektverantwortlicheJohannes MangeiHerzog August Bibliothek Wolfenbüttelmangei[at]hab.deAlexander GeykenBerlin-Brandenburgische Akademie der Wissenschaften in Berlingeyken[at]bbaw.deReinhard AltenhönerStaatsbibliothek zu Berlin Preußischer Kulturbesitzreinhard.altenhoener[at]sbb.spk-berlin.deMustafa DoganNiedersächsische Staats- und Universitätsbibliothek Göttingendogan[at]sub.uni-goettingen.dePhilipp WiederGesellschaft für wissenschaftliche Datenverarbeitung Göttingenphilipp.wieder[at]gwdg.deProjektmitarbeitendeLena HinrichsenProjektkoordinatorinHerzog August Bibliothek Wolfenbüttelhinrichsen[at]hab.deMatthias BoenigBerlin-Brandenburgische Akademie der Wissenschaften in Berlinocrd[at]bbaw.deKonstantin BaiererStaatsbibliothek zu Berlin - Preußischer Kulturbesitzkonstantin.baierer[at]sbb.spk-berlin.deClemens NeudeckerStaatsbibliothek zu Berlin - Preußischer Kulturbesitzclemens.neudecker[at]sbb.spk-berlin.deMareen GeestmannNiedersächsische Staats- und Universitätsbibliothek Göttingengeestmann[at]sub.uni-goettingen.deMichelle WeidlingNiedersächsische Staats- und Universitätsbibliothek Göttingenweidling[at]sub.uni-goettingen.dePaul PestovNiedersächsische Staats- und Universitätsbibliothek Göttingenpestov[at]sub.uni-goettingen.deEhemalige ProjektbeteiligteRainer StotzkaKarlsruhe Institute of TechnologyThomas StäckerHerzog August Bibliothek WolfenbüttelMarkus BrantlBayerische Staatsbibliothek MünchenEhemalige ProjektmitarbeitendeElisabeth EnglHerzog August Bibliothek WolfenbüttelVolker HartmannKarlsruher Institut für Technologie/Steinbuch Centre for ComputingElisa HerrmannHerzog August Bibliothek WolfenbüttelKay-Michael WürznerBerlin-Brandenburgische Akademie der Wissenschaften in BerlinAjinkya PrabhuneKarlsruher Institut für TechnologieSebastian MangoldBayerische Staatsbibliothek MünchenWissenschaftlicher BeiratDas Projekt wird von einem Wissenschaftlichen Beirat beraten, der sich derzeit aus den folgenden Personen zusammensetzt:Gregory CraneTufts UniversityMax KaiserÖsterreichische NationalbibliothekJoachim KöhlerFraunhofer IAISSebastian MeyerSLUB DresdenGünter MühlbergerUniversität InnsbruckKlaus SchulzLMU München – CISRobert StrötgenTU BraunschweigKooperationspartnerDas Projekt hat mit mehreren Unternehmen, Non-Profit-Organisationen und verwandten Projekten Absichtserklärungen (Letters of Intent)unterzeichnet, die hauptsächlich darauf abzielen, sich über Schnittstellen für und aktuelle Entwicklungen in der Massenvolltextdigitalisierungauszutauschen. Darüber hinaus sind diese Kooperationen Teil der Disseminationsstrategie von OCR-D, die im weiteren Verlauf des Projektsund mit der Entwicklung der OCR-D-Software zu einem einsatzfähigen, stabilisierten Produkt zunehmend an Bedeutung gewinnen wird.  Content Conversion Specialists (CCS)  Kitodo. Key to digital objects e.V.  OCR4all, GitHub  semantics Kommunikationsmanagement GmbH  Zeutschel GmbH",
      "url": " /de/contact.html"
    },
  

    {
      "slug": "en-data-html",
      "title": "OCR-D data",
      "content"	 : "DataReference DataThe reference data includes a Ground Truth corpus and other special corpora.The Ground Truth corpus contains pages from publications printed between 1500and 1900. The content of the corpus is based on a particular selection from theholdings of the DFG project “German TextArchive”, the Digitized Collections of theStaatsbibliothek zu Berlin and theWolfenbüttel DigitalLibrary ofthe Duke August library. The holdings of projects and digital collections ofother libraries as well as additional Ground Truth data, which are compiledtogether with module projects, can be included in the corpus as specialextensions in concertation with the OCR-D coordination project. If additionalannotations or texts are necessary, these can be created in consultation withthe OCR-D coordination project.Depth of Annotation, Text Accuracy and ArtifactsThe Ground Truth corpus offers three annotation depths:  Structural regions, text lines, word coordinates  Structure regions, text lines  Text linesOverview (The list will be extended continuously.)The special corpora contain:  a corpus of data with lower text accuracy (dirty OCR), which can be used for individual comparisons and evaluations  a corpus of artifacts with objects that show disturbancesOverviewCreation of the Ground TruthThe image data were first subjected to a layout analysis (text region and line recognition) using Transkribus and then segmented automatically. The automatically recognized text as well as the lines and text regions were corrected manually. Finally the data in form of PAGE files, digital images and METS files were zipped as a BagIt file.If you are interested in further Ground Truth data (e.g. for binarization) please contact us: hinrichsen[at]hab.deThe data are subject to a CC-BY-SA license, for the use of the image datadifferent licenses may exist. Please contact the project and/or the owning library.OCR-D Research Data RepositoryThe OCR-D research data repository collects all versions of documents and (intermediate) results created during the document analysis. It contains at least the end results of every processed document. During the ingest much metadata about the document will be extracted and made available for search/filter (e.g. identifier(s), title, classification(s), genre(s), semantic label(s), used processor(s), text). In future, there may also be export options for specific tools.OCR-D Ground Truth RepositorySimilarly, there is a publicly available OCR-D repository, which contains all ground truth data provided by OCR-D.  For further information about the metadata format, see https://github.com/OCR-D/gt-labelling  The repository itself is available at https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/search",
      "url": " /en/data.html"
    },
  

    {
      "slug": "de-data-html",
      "title": "Daten",
      "content"	 : "DatenDie ReferenzdatenDie Referenzdaten umfassen ein Ground-Truth-Korpus und weitere Spezialkorpora.Das Ground-Truth-Korpus umfasst Seiten aus Publikationen aus dem Zeitraum 1500–1900. Der Inhalt des Korpus basiert auf einer gezielten Auswahl aus demBestand des DFG-Projektes „Deutsches Textarchiv“, der DigitalisiertenSammlungen der Staatsbibliothek zu Berlin und der Wolfenbütteler DigitalenBibliothek der Herzog August Bibliothek. Bestände von Projekten und digitalenSammlungen anderer Bibliotheken sowie zusätzliche Ground-Truth-Daten, diezusammen mit Modulprojekten erarbeitet werden, können in Abstimmung mit demOCR-D-Koordinierungsgremium in das Korpus als spezielle Erweiterungenaufgenommen werden. Sollten zusätzliche Annotationen oder Texte notwendig sein,können diese in Abstimmung erstellt werden.Annotationstiefe, Textgenauigkeit und ArtefakteDas Ground-Truth-Korpus bietet drei Annotationstiefen an:  Strukturregionen, Textzeilen, Wortkoordinaten  Strukturregionen, Textzeilen  TextzeilenZur ÜbersichtDie Spezialkorpora umfassen:  Spezialkorpus von Daten geringerer Textgenauigkeit (schmutzige OCR), kann für einzelne Vergleiche und Evaluationen herangezogen werden.  Spezialkorpus Artefakte: Dieses Korpus beinhaltet ausschließlich Objekte die Störungen aufweisen.Zur ÜbersichtErstellung des Ground TruthDie Image-Daten wurden mittels Transkribus und Aletheia zunächst einer Layout-Analyseunterzogen und anschließend automatisiert segmentiert. Der so automatisch segmentierte Text (Wörter) sowie die Zeilen und Textregionen wurden manuell bearbeitet. Abschließend wurde ein Datenpaket aus den Daten im PAGE-Format, den digitalen Bildern und einem METS-Metadatensatz als Bagit-Datei gepackt.Wenn Sie Interesse an weiteren Ground-Truth-Daten haben (bspw. zurBinarisierung) schreiben Sie uns bitte: hinrichsen[at]hab.deDie Daten unterliegen einer CC-BY-SA-Lizenz, für die Verwendung der Bilddatenkönnen abweichende Lizenzen vorliegen. Bitte kontaktieren Sie diesbezüglich dasProjekt und/oder die besitzende Bibliothek.OCR-D ForschungsdatenrepositoriumDer OCR-D-Forschungsdatenspeicher sammelt alle Versionen von Dokumenten und (Zwischen-)Ergebnissen, die während der Dokumentenanalyse erstellt wurden. Es enthält mindestens die Endergebnisse jedes verarbeiteten Dokuments. Während der Aufnahme werden viele Metadaten über das Dokument extrahiert und für die Suche/Filterung zur Verfügung gestellt (z.B. Identifizierer, Titel, Klassifikation(en), Genre(s), semantische Bezeichnung(en), verwendete Prozessor(en), Text). In Zukunft wird es möglicherweise auch Exportoptionen für bestimmte Werkzeuge geben.OCR-D Ground Truth RepositoriumEbenso gibt es ein öffentlich zugängliches OCR-D Repositorium, das alle von OCR-D bereitgestellten Ground Truth Daten enthält.  Weitere Informationen über das Metadatenformat finden Sie unter https://github.com/OCR-D/gt-labelling  Das Repository selbst ist unter https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/search verfügbar.",
      "url": " /de/data.html"
    },
  

    {
      "slug": "en-dev-best-practice-html",
      "title": "How we want to develop software in OCR-D",
      "content"	 : "Best Practices for Software Development in OCR-DThis guide is informed by the experience of coordinating distributed development of the OCR-D software and specifications. We strive for a broad consensus on the practicalities and logistics of software development, particular in preparation of the upcoming Phase III of OCR-D.You can find a concise summary in the slides to our call on Best Practices, 2020-07-07Communications  Be boldSoftware Development is as much a social endeavor as it is technical. To effectively develop in a distributed setting, transparency and sharing early in the process is vital. We therefore strongly recommend using the following channels over E-Mail.GitHub AccountSince all development shall be openly and transparently carried out on GitHub, it is essential that you create a GitHub account.ChatThe OCR-D gitter “Lobby” is the general channel for all things OCR-D. We strive to keep the threshold for participation as low as possible. Every question or comment is welcome. It is also the channel we use to announce changes to OCR-D core, the specifications or updates on processors.Gitter accounts can also be used for private/direct conversations, for which the contacts in the Lobby are a good starting point.GitHub IssuesAs with the chat, we aim for low barriers of entry for participation on issues in GitHub. If something is not working within OCR-D projects, you encounter error messages or unexpected behavior: Feel free to open an issue. If you cannot identify which component or project is the cause of the problem, open an issue in OCR-D/core, the moderators can transfer it to the right place later.This openness must go both ways: If you maintain or contribute to a project, it is essential that we treat each other with respect. Many users and even developers are still learning about OCR, software development and distributed version control. A clueless “newbie” can become a helpful contributor if you support them.GitHub ForksA fork is a clone of a GitHub repository hosted on the same site. If you fork a repository, a clone of it will be created under your username. The advantage of a fork over just the original repository is that you have full control of it and can make changes to it to fit your use case, while others also can access it. If you deem your changes a useful contribution to the original project, you can create a pull request to allow discussing and refining them with others, and ultimately merge.Pull RequestsTODO Explain how suggestions workA pull request (“PR”, in Gitlab they are called merge requests) is essentially an issue with an attached Git branch of either the repository or one of the forks. You should create a dedicated branch for your Pull Request. Describe the changes you are proposing in a concise manner and try to find a good title. Try to keep PR as small as possible, ideally not more than a handful of changed files or a few dozen changed lines at most. If a PR is too large to review and integrate effectively, consider splitting it into smaller PRs.If you are maintaining a project and somebody sends a PR: great, because that means your users are not only interested in your software but also willing to spend time on improving it. Check the “Files” tab of the PR to see a visualization of the changes. You can comment directly on code lines, which helps to focus the discussion on specific changes. Once such a discussion is complete, click on “Resolve conversation” which will hide these finished conversations. You can review a PR and either approve, request changes or reject a PR. Nobody likes to feel rejected, so try to help improve a suboptimal PR rather than outright rejecting it.WikiThe wiki of the ocrd-website is the central repository for user-generated documentation, and complements the official OCR-D website. If you have any experiences, configurations, scenarios or documentation in general which are related to your use of OCR-D software, feel free to contribute them to the Wiki. To add a new page, click on “New Page”, decide on a concise title for the page and add your information.It is easy to reorganize and edit pages later, so do not feel pressured to overcome a relevancy threshold. Everybody profits from sharing experiences in the long run. Additionally, the wiki serves as a source for official documentation and the coordination project is very much interested in letting the official documentation be strongly informed by the experience of OCR-D users.WebsiteThe official website of OCR-D is https://ocr-d.de. You can find documentation, tailored towards users and developers, as well as a blog and much more.The website itself is available as repository OCR-D/ocrd-website on GitHub. If you have any comments or questions, don’t understand a section of the documentation or have found typos or broken links: Please open an issue in ocrd-website! We are also happy to merge PR for the website, but issues are just as helpful.Development WorkflowGit basicsBefore doing anything else, configure the user name and e-mail you want to assign to your commits:git config --global user.name &quot;Erika Mustermann&quot;git config --global user.email &quot;erika@mustermann.de&quot;Also make sure that the e-mail address you use is linked to your GitHub account so your commits will be attributed to you, even if you use another e-mail address than the one you registered with.It is worth investing some time in studying quality-of-life features of git, such as aliases or shell integration as these will make working with git substantially more pleasant. Also consider mining $HOME/.gitconfig files on GitHub for your own setup.Branches and Pull RequestsAs mentioned in the section on Pull Requests, you should organize your changes in feature branches that contain a narrow set of commits. When new features depend on others, don’t hesitate to branch from branches.Unit testsUnit tests are code that makes assertions over the behavior of the components of your main code, which can be used for automatic regression testing.Code CoverageThe coverage of your tests is the percentage of lines of code that were executed running all your tests. You should strive for 100% coverage, so that everything your code does is encountered by the tests at least once. Even though a high code coverage is no panacea against bugs, it substantially reduces the amount of regressions, i.e. code changes that unexpectedly break existing behavior.Test dataThe OCR-D coordination project maintains a central repository OCR-D/assets that contains test data for unit tests. The test data consists of unpacked OCRD-ZIP of workspaces, i.e. one or more METS XML files, as well as PAGE-XML, ALTO, images. While they use the same mechanism, these folders are not Ground Truth. Instead, they serve as examples of a wide range of “real-life” data, warts and all. When testing your software, consider using the data from OCR-D/assets. If none of the provided OCRD-ZIP satisfy your needs, please open an issue in OCR-D/assets and describe your test data needs, so we can cooperate on creating test data.Making uniform and well-described test data available to all serves everybody and the coordination project will continue to maintain and update the test data as requirements shift or specifications change. Such a test-specific corpus also improves comparability of processor performance and frees test engineers to focus on testing the actual software, not wrangling with their own test data.Continuous IntegrationContinuous Integration (CI) platforms offer computing resources to software developers that allow building and testing software. These platforms, such as Travis CI, Circle CI, Scrutinizer or GitHub Actions can be integrated into GitHub repositories to automatically build/test when certain actions happen, such as “a PR was opened” or “new commit to master branch”. The big advantage of CI is that you can automate testing of changes to your software in a variety of environments, so you can be confident that a PR did not introduce regressions, or make the module break for an operating system you didn’t anticipate.Semantic VersioningThere is a wide variety of versioning schemes, such as date-based or plain incremental integers. At OCR-D we favor Semantic Versioning. In essence, a version should have the syntax MAJOR.MINOR.PATCH with this convention:  Increase MAJOR by one if a changeset introduces breaking changes.  Increase MINOR by one if a changeset introduces new features.  Increase PATCH by one if a changeset fixes problems with the minor version.You should start versioning at 0.0.1. While MAJOR is 0, the software is to be considered alpha but you should still stick to the scheme for minor and patch revisions. Once you consider the software mature enough to merit a 1.0.0 release, please take extra care to adhere to the convention as much as possible.The advantage of semantic versioning is that it conveys the consequences of an upgrade to the user. There is never a reason not to upgrade to the latest patch release whereas a upgrading to a new major version should be undertaken with care.LicensingOCR-D is firmly committed to the values of F(L)OSS and permissive licensing, as these not only encourage community use and uptake, but also create trust between all stakeholders and transparency of the development process.OCR-D favours the Apache Software License 2.0 (ASL) for it is a permissive license that supports the waiving of liabilities while still granting sufficient freedom for e.g. commercial parties to use and redistribute products that include source code from OCR-D.Second to the ASL, other permissive open source licenses that can be used include (in order of preference): MIT, BSD. If there are strong reasons that prevent using a permissive license for your project, please get in touch with the coordination project to discuss other licensing options. Python project setupSince most processors are implemented in Python, we have been developing certain conventions that have proven effective for interoperability, deployment and robustness. Following these conventions makes it easier to contribute and frees developers from the grunt work of managing a project, to spend more time on development, documentation and support.Have a look at existing projects that follow these cvonventions, e.g. ocrd_tesserocr. Much can be copied, pasted and adapted.Project layoutWhen starting a new project ocrd_foo, create this folder layout:ocrd_foo/ocrd_foo/__init__.pyocrd_foo/ocrd-tool.jsontests/test_foo.pyLICENSEMakefileREADME.mdsetup.pysetup.pyWe use setuptools for python packaging in OCR-D.tests/LICENSEMakefileDeploymentDockerfileocrd_all",
      "url": " /en/dev-best-practice.html"
    },
  

    {
      "slug": "en-dev-html",
      "title": "OCR-D for the technically inclined",
      "content"	 : "Welcome to the OCR-D Developer Section!This section contains all information relevant for the further development of the OCR-D-software, i.e. especially specifications, documentation and information on our GT. You are very welcome to support our development efforts on Github!  OCR-D Specifications          OCR-D Specifications for CLI, METS, PAGE etc.        OCR-D/core API documentation          Python API documentation of core implementation        OCR-D development best practices          Practical information on distributed development of software and specifications in OCR-D        Ground Truth Guidelines          Guidelines for Ground Truth        PAGE-XML format documentation          Documentation for the PAGE-XML format      ",
      "url": " /en/dev.html"
    },
  

    {
      "slug": "de-dev-html",
      "title": "Willkommen im Entwicklerbereich von OCR-D!",
      "content"	 : "Willkommen im Entwicklerbereich von OCR-D!Dieser Bereich enthält alle Informationen, die für die Entwicklung der OCR-D-Software relevant sind, d.h. Spezifikationen, Dokumentation und Informationen zu Ground Truth. Gerne wollen wir sie auch dazu einladen unsere Entwicklungsarbeiten auf GitHub zu verfolgen und zu unterstützen.  OCR-D Spezifikationen          Spezifikationen für CLI, METS, PAGE, etc.        OCR-D/core API Dokumentation          Python API Dokumentation für die core Referenzimplementierung        Best practices für Software Entwicklung in OCR-D          Praktische Informationen zur verteilten Entwicklung von Software und Spezifikationen in OCR-D        Ground Truth Richtlinien          Transkriptionsrichtlinien für Ground Truth        PAGE-XML Formatdokumentation          Dokumentation zum PAGE-XML Format      ",
      "url": " /de/dev.html"
    },
  

    {
      "slug": "en-developer-guide-html",
      "title": "OCR-D Developer Guide",
      "content"	 : "OCR-D Developer Guide  A practical guide to the OCR-D frameworkIntroductionThe “OCR-D guide” helps developers writing software and usingtools within the OCR-D ecosystem.Scope and purpose of the OCR-D guideThe OCR-D guide is a collection of concise recipes that provide pragmatic advise on how to  bootstrap a development environment,  work with the ocrd command line tool,  manipulate METS and PAGE documents,  create spec-compliant softwareNotationLines in code examples  starting with #  are comments;  starting with $  are typed shell input (everything after $  is);  are output otherwise.Words in ALL CAPS with a preprended $ are variable names:  $METS_URL: URL or file path to a mets.xml file, e.g.          https://github.com/OCR-D/assets/raw/master/data/kant_aufklaerung_1784/mets.xml        $WORKSPACE_DIR: File path of the workspace created, e.g.          $WORKSPACE_DIR      /data/ocrd-workspaces/kant-aufklaerung-2018-07-11      When referring to a “something command”, it is actually ocrd something onthe command line.Other OCR-D documentation  Specification: Formal specifications  Glossary: A glossary of terms in the OCRdomain as used throughout our documentationBootstrappingUbuntu LinuxOCR-D development is targeted towards Ubuntu Linux &amp;gt;= 18.04 since it is free,widely used and well-documented.Most of the setup will be the same for other Debian-based Linuxes and olderUbuntu versions. You might run into problems with outdated system packagesthough.In particular, it can be tricky at times to install tesseract at the rightversion. Try alex-p’s PPA or buildtesseract from source.Essential system packagessudo apt install   git   build-essential   python python-pip   python3 python3-pip  git: Version control, OCR-D uses git extensively  build-essential: Installs make and C/C++ compiler  python: Python 2.7 for legacy applications like ocropy  python3: Current version of Python on which the OCR-D software core stack is built  pip/pip3: Python package managementPython API and CLIThe OCR-D toolkit is based on a Python API thatyou can reuse if you are developing software in Python.This API is exposed via a command line tool ocrd. This CLI offers much of thesame functionality of the API without the need to write Python code and can be readilyintegrated into shell scripts and external command callouts in your code.So, If you do not intend to code in Python or want to wrapexisting/legacy tools, a major part of the functionality of the API isavailable as a command line tool ocrd.Python setupCreate virtualenvWe strongly recommend using virtualenv (or similar tools if they are morefamiliar to you) over system-wide installation of python packages. It reducesthe amount of pain supporting multiple Python versions and allows you to testyour software in various configurations while you develop it, spinning up andtearing down environments as necessary.sudo apt install   python3-virtualenv   python-virtualenv # If you require Python2 compatCreate a virtualenv in an easy to remember or easy-to-search-shell-history-for location:$ virtualenv -p python3.6 $HOME/ocrd-venv3$ virtualenv -p python2.7 $HOME/ocrd-venv2 # If you require Python2 compatActivate virtualenvYou need to activate this virtual environment whenever you open a new terminal:$ source $HOME/ocrd-venv3/bin/activateIf you tend to forget sourcing the script before working on your code, addsource $HOME/ocrd-venv3 to the end of your .bashrc/.zshrc file and logout and back in.Install ocrd in virtualenv from pypiMake sure, the virtualenv is activated and install ocrd with pip:$ pip install ocrdGeneric setupIn this variant, you still need to install the ocrd Python package. But sinceit’s only used for its CLI (and as a dependency for Python-based OCR-Dsoftware), you can install it system-wide:$ sudo pip install ocrdSetup from sourceIf you want to build the ocrd package fromsource to stay up-to-date on unreleased changesor to contribute code, you can clone the repository and build from source:$ git clone https://github.com/OCR-D/core$ cd coreIf you are using the python setup:$ pip install -r requirements.txt$ pip install -e .If you are using the generic setup:$ sudo pip install -r requirements.txt$ sudo pip install .Verify setupAfter setting up, check that these commands do not throw errors and have theminimum version:$ git --version# Version 1.7 or higher?$ make --version# Version 9.0.1 or higher?$ ocrd --version# ocrd, version 0.4.0Anatomy of an OCR-D module project (MP)MP are git repositories with at least a description of the MP andits provided tools (ocrd-tool.json and aMakefile for installing the MP into a suitable OS.ocrd-tool.jsonThis is a JSON file that describes the software of a particular MP. It servesmainly three purposes:  providing a machine-actionable description of MP and the bundled tools andtheir parameters  concise human-targeted descriptions as the foundation for the applicationdocumentation  ensuring compatible definitions and interfaces, which is essential forsustainable, scalable workflowsThis document is mainly focusing on the first point.The structure and syntax of the ocrd-tool.json is defined by a JSONSchema and expects JSON Schemafor the parameter definitions. In addition to the schema, the ocrd commandline tool can help you validate the ocrd-tool.jsonMechanics of the ocrd-tool.json:fire: TODO :fire:  [kba] Wir brauchen einen besseren Namen, ich kann das schon nicht mehrschreiben dauernd, ocrd-tool.json. Vielleicht einfach manifest.json oderpackage.json oder tool-desc odr irgendwas.:fire: TODO :fire:The ocrd-tool.json has two conceptual levels:  Information about the MP as a whole and thepeople and processes involved  Technical metadata on the level of the individual toolsBeyond the ocrd-tool.json file, it is part of the requirements that the toolscan provide the section of the ocrd-tool.json about ‘themselves’ at runtimewith the -J/--dump-json flags.The reason for this redundancy is to make the tools inspectable at runtime andto prevent “feature drift” where the  software evolves to the point where thedescription/documentation is out-of-date with the actual implementation.From a developer’s perspective, the easiest way to handle this is by bundlingthe ocrd-tool.json into your software, e.g. by the following pattern:  Store the ocrd-tool.json at a location where it is easy to deploy andaccess after installation  Symlink it to the root of the repository: ln -sr src/ocrd-tool.json .  Handle --dump-json by parsing the ocrd-tool.json and sending out therelevant section  Validate input and provide defaults basedon the JSON schema mechanicsMetadata about the module projectRequired properties are bold.  version: Version of the tool, adhering to SemanticVersioning  git_url: URL of the Github  tool: See next section  dockerhub: The project’s DockerHub URL  creators: :rotating_light: TODO  :rotating_light::  institution: :rotating_light: TODO  :rotating_light::  synopsis: :rotating_light: TODO  :rotating_light::Example:{  &quot;version&quot;: &quot;0.0.1&quot;,  &quot;name&quot;: &quot;ocrd-blockissifier&quot;,  &quot;synopsis&quot;: &quot;Tools for reasoning about how these blocks fit on this here page&quot;,  &quot;git_url&quot;: &quot;https://githbub.com/johndoe/ocrd_blocksifier&quot;,  &quot;dockerhub&quot;: &quot;https://hub.docker.com/r/johndoe/ocrd_blocksifier&quot;,  &quot;authors&quot;: [{    &quot;name&quot;: &quot;John Doe&quot;,    &quot;email&quot;: &quot;johndoe@ocr-corp.com&quot;,    &quot;url&quot;: &quot;johndoe.github.io&quot;  }],  &quot;bugs&quot;: {    &quot;url&quot;: &quot;https://github.com/sindresorhus/temp-dir/issues&quot;  },  &quot;tools&quot;: {      /* see next section */    }}Metadata about the toolsThe tools section is an object with the key being the name of the executable described and the value being an object with the following properties (bold means required):  executable: Name of the executable. Must match the key and start with ocrd-  parameters: Description of the parameters this tool accepts  description: Concise description what the tool does  categories: Tools belong to these categories, representing modules within the OCR-D project structure, list is part of the specs  steps: This tool can be used at these steps in the OCR-D functional model, list of values in the specsMetadata about parametersRequired properties are bold.  type: What kind of parameter this is, either a string, a number or a boolean  format: Subtype defining the syntax of the value such as float/integer for numbers or uri for string  required: If true, this parameter must be provided by the user  default: Default value if not required  enum: List of possible values if a fixed listrequired: true and setting default are mutually exclusive.MakefileAll MP should provide a Makefile with at least two targets: deps and install.make deps should install any dependencies, such as required python modules.make install should install the executable(s) into $(PREFIX)/bin.make test should start the unit/regression test suite if provided.Makefile for Python MPmake deps should install dependencies with pip.make install should call python setup.py install.See the makefile of the ocrd_kraken project for an example.Makefile for generic MPmake deps should install dependencies either by compiling from source or using apt-get.make install should  Copy the executables to $(PREFIX)/bin, creating $(PREFIX)/bin if necessary.  Copy any required files to $(PREFIX)/share/&amp;lt;name-of-the-package&amp;gt;, creating the latter if necessarySee the makefile of the ocrd_olena project for an example.ocrd workspace - Working with METSMETS is the container format of choice for OCR-D because it is widely used indigitzation workflows in cultural heritage institutions.A METS file references files in file groups and can contain a variety ofmetadata, the details can be found in the specs.From METS to WorkspaceWithin the OCR-D toolkit, we use the term “workspace”, a folder containing afile mets.xml and any number of the files referenced by the METS.One can think of the mets.xml as the MANIFEST of a JAR or the .git folderof a git repository.The workspace command of the ocrd tool allows various manipulations ofworkspaces and therefore METS files.Git similarity intendedThe workspace command’s syntax and mechanics are strongly inspired bygit so if you know git, this should be familiar.            git      ocrd workspace                  init      init              clone      clone              add      add              ls-files      find              fetch      find --download              archive      pack      Set the workspace to work onFor most commands, workspace assumes the workspace is the current workingdirectory. If you want to use a different directory, use the -d / --directory option# Listing files in the workspace at $PWD$ ocrd workspace find# Listing files in the workspace at $WORKSPACE_DIR$ ocrd workspace -d $WORKSPACE_DIR findUse another name than mets.xmlAccording to convention, the METS of a workspace is named mets.xml.To select a different basename for that file, use the -M / --mets-basename option:# Assume this workspace structure$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets3000.xml# This will fail in a loud and unpleasant manner$ ocrd workspace -d $WORKSPACE_DIR find# This will not$ ocrd workspace -d $WORKSPACE_DIR -m mets3000.xml findCreating an empty workspaceTo create an empty workspace to which you can add files, use the workspace init command$ ocrd workspace -d ws1 init/home/ocr/ws1Load an existing METS as a workspaceTo create a workspace and save a METS file, use the workspace clone command:$ ocrd workspace -d new-workspace clone $METS_URL/home/ocr/new-workspace$ find new-workspacenew-workspacenew-workspace/mets.xmlLoad an existing METS and referenced files as a workspaceTo not only clone the METS but alsodownload the contained files, use workspace clone with the --download flag:$ ocrd workspace -d $WORKSPACE_DIR clone --download $METS_URL$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-GT-ALTO$WORKSPACE_DIR/OCR-D-GT-ALTO/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-GT-PAGE$WORKSPACE_DIR/OCR-D-GT-PAGE/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifNOTE: This will download all files, which can mean hundreds ofhigh-resolution images. If you want more fine-grained control,clone the bare workspaceand then use the workspace find command with the download flagSearching the files in a METSYou can search the files in a METS file with the workspace find command.  All files: ocrd workspace find  All TIFF files: ocrd workspace find --mimetype image/tiff  All TIFF files in the OCR-D-IMG-BIN group: ocrd workspace find --mimetype image/tiff --file-grp OCR-D-IMG-BINSee ocrd workspace --find for the full range of selection optionsDownloading/Copying files to the workspaceTo download remote or copy local files referenced in the mets.xml to theworkspace, append the --download flag to the workspace findcommand:# Clone Bare workspace:$ ocrd workspace clone $METS_URL$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml# Download all files in the `OCR-D-IMG` file group$ ocrd workspace -d $WORKSPACE_DIR find --file-grp OCR-D-IMG --download[...]$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifThe convention is that files will be downloaded to $WORKSPACE_DIR/$FILE_GROUP/$BASENAME where  $FILE_GROUP is the @USE attribute of the mets:fileGrp  $BASENAME is the last URL segment of the @xlink:href attribute of the mets:FLocatNOTE Downloading a file not only copies the file to the $WORKSPACE_DIRbut also changes the URL of the file from its original to the absolute filepath of the downloaded file.Adding files to the workspaceWhen running a module project, new files are created (PAGE XML, images …). Toregister these new files, they need to be added to the mets.xml as amets:file with a mets:FLocat within a mets:fileGrp, each with the rightattributes. The workspace add command makes this possible:$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$ ocrd workspace -d $WORKSPACE_DIR add   --file-grp OCR-D-IMG-BIN   --file-id PAGE-0013-BIN   --mimetype image/png   --group-id PAGE-0013   page0013binarized.png$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$WORKSPACE_DIR/OCR-D-IMG-BIN/page0013binarized.tifValidating OCR-D compliant METSTo ensure a METS file and the workspace it describes adheres to the OCR-Dspecs, use the workspace validate command:# Create a bare workspaceocrd workspace -d  $WORKSPACE_DIR init# Validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;METS has no unique identifier&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Oops, let&#39;s set the identifier ...$ ocrd workspace -d $WORKSPACE_DIR set-id &#39;scheme://my/identifier/syntax/kant_aufklaerung_1784&#39;# ... and add a file$ ocrd workspace -d $WORKSPACE_DIR add -G OCR-D-IMG-BIN -i PAGE-0013-BIN -m image/png -g PAGE-0013 page0013binarized.png# Validate again&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;ocrd tool – Working with ocrd-tool.jsonThis command helps you to explore and validate the information in any ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json SUBCOMMANDocrd-tool validateValidate that an ocrd-tool.json is syntactically valid and adheres to the schema.This is useful while developing to make sure there are no typos and all required properties are set.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;steps&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;categories&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[] &#39;version&#39; is a required property&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;This example shows that the ocrd-wip-xyzzy executable is missing the required steps andcategories properties and the root level object is missing the versionproperty.Adding them should result in$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;Introspect an ocrd-tool.jsonThese commands are used for enumerating the executables contained in anocrd-tool.json and get root level metadata, such as the version.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json version0.0.1# Lists all the tools (executables) one per-line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json list-toolsocrd-wip-xyzzyocrd-wip-frobozzIntrospect individual toolsThis set of commands allows introspection of the metadata on individualtools within an ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json tool EXECUTABLE SUBCOMMAND$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy dump{  &quot;description&quot;: &quot;Nothing happens&quot;,  &quot;categories&quot;: [&quot;Text recognition and optimization&quot;, &quot;Arcane Magic&quot;],  &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],  &quot;executable&quot;: &quot;ocrd-wip-xyzzy&quot;}# Description$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy descriptionNothing happens# List categories one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy categoriesText recognition and optimizationArcane Magic# List steps one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy stepsrecognition/text-recognitionParse parametersThe details of how a tool is configured at run-time are determined byparameters. When a parameter file is passed to atool, it should:  ensure it is valid JSON  validate according to the parameter schema  add default values when no explicit values were providedThe ocrd ocrd-tool tool parse-params command does just that and can outputthe resulting default-enriched parameter as either JSON or as shell scriptassignments to evaluate:# Get JSON$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params --json -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;){  &quot;val1&quot;: 42,  &quot;val2&quot;: false,  &quot;val-with-default&quot;: 23}# Get back shell assignments to an associative array &quot;params&quot;$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;)params[&quot;val1&quot;]=&quot;42&quot;params[&quot;val2&quot;]=&quot;true&quot;params[&quot;val-with-default&quot;]=&quot;23&quot;ocrd process - Run a multi-step workflowOCR requires multiple steps, such as binarization, layout recognition, textrecognition etc. These steps are implemented with command line tools thatadhere to the same command line interface whichmakes it straightforward to chain these calls.For example, to run kraken binarization and tesseract block segmentation, one could execute:ocrd-kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BINocrd-tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.jsonThe disadvantage of individual calls is that it requires the user to check whether runs wereactually successful. To remedy this, users can use the ocrd process CLI which  simplifies the CLI syntax for multiple calls  checks for required and expected-to-be-produced file groups  checks for return value  sets logging levels uniformly across toolsThe same calls mentioned before can be passed to ocrd process as follows:ocrd process -l DEBUG   &quot;kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BIN&quot;   &quot;tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.json&quot;Wrapping a CLI using bashThis section describes how you can make an existing tool OCR-Dcompliant, i.e. provide a CLI which implements all the specs and callsout to another executable.For this purpose, the ocrd offers a bash library that handles:  command line option parsing  on-line help  parsing and providing defaults for parametersThe shell library is bundled with the ocrd command line tool and can be accessed with theocrd bashlib command.ocrd bashlibTo get the filename of the shell lib, use ocrd bashlib filename, which youcan employ to source the shell code in a wrapper script. After sourcing this scriptyou will have access to a number of shell functions that begin with ocrd__.The only function you definitely need is ocrd__wrap which parses anocrd-tool.json and scaffolds a spec-compliant CLI, parses command linearguments and parameters and lets the developer then react to the inputs.In combination with the ocrd workspace command this allowsyou to write CLI applications without touching any METS or PAGE/XML files by hand.ocrd__wrapocrd__wrap has this signature:ocrd__wrap OCRD_TOOL_JSON EXECUTABLE_NAME ...ARGSwhere  OCRD_TOOL_JSON is the path to the ocrd-tool.json  EXECUTABLE_NAME is the name of an executable within OCRD_TOOL_JSON  ...ARGS are 0..n command line arguments passed on from the userExample:   ocrd__wrap /usr/share/ocrd-wip/ocrd-tool.json ocrd-wip-xyzzy &quot;$@&quot;",
      "url": " /en/developer-guide.html"
    },
  

    {
      "slug": "en-dita-html",
      "title": "",
      "content"	 : "Only available in german",
      "url": " /en/dita.html"
    },
  

    {
      "slug": "de-dita-html",
      "title": "OCR-D: Anforderungsprofil für die Dokumentation der Modulprojekte",
      "content"	 : "OCR-D: Anforderungsprofil für die Dokumentation der Modulprojekte  Allgemein:  Die Dokumentation der Tools und Schnittstellen betrifft sowohl die Anwendung selbst (Anwendungsdokumentation) als auch deren Anwendung von Nutzern (Benutzerdokumentation).Das OCR-D: Anforderungsprofil für die Dokumentation der Modul Projekte stellt nicht eine DITA-Einführung oder DITA-Dokumentation dar, das gleiche betrifft die Markdown Auszeichnungssprache. In dieser Dokumentation werden ergänzende Informationen sowie unmittelbare Anforderungen für eine OCR-D konforme Dokumentation dargelegt.Adressaten der Dokumentation:Es sollen sowohl Techniker, die vor allem Informationen zur Anwendung benötigen (Installation, Wartung sowie Integration im Umfeld der eigenen Werkzeuge), als auch Benutzer, die das Werkzeug nutzen möchten bzw. mit der Anwendung ein bestimmtes Ergebnis erzielen möchten im Rahmen der Anwendungsdokumentation und Benutzerdokumentation informiert werden.Stil: Der Stil der Dokumentation sollte verständlich und in kurzen Sätzen abgefasst sein. Die Dokumentation muss alle Aspekte der Anwendung und Benutzung umfassend beinhalten.Format: Die Dokumentation ist entweder im xmlbasierten DITA-Format oder im Auszeichnungsformat Markdown (Markdown-DITA-Syntax) abzufassen.Software: Zur Erstellung der Dokumentation wird ein Editor  (empfohlen wird ein Editor, mit XML-Unterstützung) sowie das DITA-OT (Open Toolkit) benötigt. Nähere Information finden sich unter: https://www.dita-ot.org/Die Erstellung der DokumentationDie Erstellung der Dokumentation erfolgt stufenweise.Die erste Stufe bildet die Dokumentation des Werkzeuges in Form der ocrd_tool.json (siehe https://ocr-d.github.io/ocrd_tool).In der zweiten Stufe werden manuell u. a. Funktionen, Parameter, Fehlerbehandlungen der Anwendung in Form einzelner Dateien entsprechend den folgenden Formatvorgaben abgefasst.StrukturvorgabenDie Dokumentation sollte wie folgt strukturell aufgebaut sein. Auf Grund der Adressaten der Dokumentation können Schwerpunkte unterschiedlich gesetzt werden.Zum Beispiel wird der Schwerpunkt auf die Benutzerdokumentation gelegt, sollte auf folgende Punkte geachtet werden:  Was kann man mit dem Tool machen? Welches Ergebnis ist von der Anwendung zu erwarten.  Wie wird die Anwendung bedient?  Welche Probleme und Fehlermeldungen können auftreten.Bei der Abfassung ist folgendem allgemeinem Aufbau zu folgen.StrukturvorgabenErstellungVorlagen1. Tool nameInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert2. Release notesmanuell erstellenreleaseNote.md3. Installationmanuell erstelleninstallation.md4. Simple tool descriptionInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert5. Descriptionmanuell erstellenDescription.md6. Optionmanuell erstellenOption.md7. Input format descriptionInhalt der ocrd_tool.jsonInputFormatDescription.md8. ParametersInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert9. Output format descriptionmanuell erstellenOutputFormatDescription.md10. Troubleshootingmanuell erstellenTroubleshooting.dita11. Resourcesmanuell erstellenResources.md12. Glossarmanuell erstellenGlossar.dita13. AuthorsInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert14. ReportingInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert15. CopyrightInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiertDie Dokumentation schreibenDas unmittelbare Schreiben stellt die zweite Stufe der Dokumentation dar. Anhand der Strukturvorgaben ist zu sehen, dass die Dokumentation nicht aus einer homogenen in sich geschlossenen Beschreibung besteht. Sondern einzelne Aspekte u. a. der Name des Werkzeuges, der Installations- und Bedienungsteil, Fehlerbetrachtungen und eventuell weiterführende Hinweise Bestandteile oder Themenbereiche (Topics) der Dokumentation sind. Sowohl zur Schreibunterstützung als auch zum Lesen, der Veröffentlichung sowie der späteren Pflege werden vom OCR-D Projekt diese spezifischen Formatvorgaben vorgenommen.DITA“Die Darwin Information Typing Architecture (DITA) ist ein topic- und xmlbasiertes Dateiformat.”1 DITA ist ein OASIS-Standard (Organization for the Advancement of Structured Information Standards).2 DITA ist ein Format, das die Dokumentation bei der Erstellung, Verbreitung und (Wieder)verwendung unterstützen soll.TopicsMit Hilfe von Topics werden in sich inhaltlich geschlossene spezifische Bestandteile der Dokumentation gegliedert und typisiert. Allgemein beinhaltet ein Topic immer die Angabe eines Titels (&amp;lt;title&amp;gt;), den sogenannten Textkörper (u. a. &amp;lt;body&amp;gt;) sowie beispielsweise einzelne Absätze (&amp;lt;p&amp;gt;) oder Listen (&amp;lt;ul&amp;gt;,  &amp;lt;ol&amp;gt;). Das Topic wird in der Regel in einer Datei gespeichert.Folgende Topic-Typen stehen für die OCR-D Dokumentation zur Verfügung. Die einzelnen Topic-Typen basieren auf eigenen formalen Dokumentspezifikationen. Die kurzen Beschreibungen in der Tabelle basieren auf der DITA-Spezifikation 1.3.[^3][^3]: siehe http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/dita-v1.3-errata01-os-part3-all-inclusive-complete.html#dita_ref_topic            Topic-Typ      Beschreibung      Konkordanz zur OCR-D Strukturvorgaben      Verweis                  General task      Das general task-Topic beinhaltet allgemein abgefasste Handlungsanweisungen. Diese können in einzelnen Abschnitten  angeordnet werden. Im Unterschied zum *strict task-Topic* können in diesem  verwendet werden. Die  beschreiben in einem umfangreichen Absatz den einzelnen Schritt mit dem jeweiligen Kontext.      3. Installation (alternative Möglichkeit)      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-generic-task-topic.html#dita_generic_task_topic              Task topic (strict task)      Das task topic (strict task) beinhaltet die Handlungsanweisungen die notwendig sind zur Bedienung des jeweiligen Werkzeuges. Dabei werden die einzelnen notwendigen Schritte klar in einzelnen  dokumentiert. Ein Schritt-Element  beinhaltet immer ein Komanndozeilen-Element .      3. Installation      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-task-topic.html#dita_task_topic              Concept      Das concept-Topic beinhaltet maßgebende Informationen, die zur Bedienung des Werkzeuges notwendig sind. Das Topic kann dabei notwendiges Hintergrundwissen für die Bedienung und den Umgang mit dem Werkzeug bieten sowie Definitionen oder Erklärungen enthalten.      4. Simple tool description      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-concept-topic.html#dita_concept_topic              Reference      Das reference-Topic konzentriert sich auf die unmittelbaren Informationen des Werkzeuges oder einer spezifischen Schnittstelle. Mit dem Reference-Topic soll der Nutzer schnell und präzise informiert werden.      5. Input format description, 6. Input Parameters, 7. Output format description, 8. Setting Parameters      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-reference-topic.html#dita_ref_topic              Troubleshooting      Das troubleshooting-Topic beinhalt Anweisungen zur Fehlerbehandlung. Dabei wird zuerst der Fehler oder die Symtome  beschrieben und im darauf folgenden Lösungsteil der Grund  für den Fehler benannt und abschließend die Lösung  des Fehlers dokumentiert.      8. Troubleshooting      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-troubleshooting-topic.html#dita-troubleshooting-topic              Glossary entry      Im glossary entry-Topic wird die Bedeutung eines Begriffes oder Vorgehens definiert. Im  kann der zu definierende Term näher beschrieben werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossary-topic.html#glossaryArch              Glossary group      Im glossary group-Topic können die einzelnen Glossary entry-Topic zusammengefasst werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossarygroup-topic.html      Markdown DITA syntaxAlternativ zum DITA-XML-Markup kann Markdown zur Abfassung folgender Topic-Typen für das Schreiben der Dokumentation genutzt werden. Die einfache Auszeichnungssprache Markdown im Besonderen Markdown-DITA-Syntax ist entsprechend der Dokumentation des DITA-Open Toolkit http://www.dita-ot.org/3.0/topics/markdown-dita-syntax-reference.html zu verwenden.Folgende Topics werden in Markdown unterstützt:  concept  task (im Besonderen das Task topic: strict task)  referenceDie Topic-Typen:  troubleshooting  glossary group  glossary entrysind ausschließlich in DITA zu schreiben.Beispiele für Topics in der jeweiligen spezifischen Syntax in Markdown-DITA-Syntax oder DITABeispiel: für ein Topic concept in Markdown-DITA-Syntax# Simple tool description {#toolDescription .concept}&quot;A command-line interface or command languageinterpreter (CLI), also known as command-line user interface,console user interface and character user interface (CUI), isa means of interacting with a computer program where the user(or client) issues commands to the program in the form ofsuccessive lines of text (command lines).&quot; Source: Wikipediacontributors. (2018, June 5). Command-line interface. InWikipedia, The Free Encyclopedia. Retrieved 12:45, June 6,2018, from [Wikipeadia](https://en.wikipedia.org/w/index.php?title=Command-line_interface&amp;amp;oldid=844566807)Beispiel: für ein Topic task in Markdown-DITA-Syntax# Installation {#installation .task}1.    erster Schritt2.    zweiter SchrittBeispiel: für ein Topic reference in Markdown-DITA-Syntax# Release Note {#releaseNote .reference}The Command Line Interface (CLI) is a maintenancerelease that fixes issues reported in OCR-D.## RequirementsThe CL can be used with all operating systems.Beispiel: für ein Topic troubleshooting in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE troubleshooting PUBLIC &quot;-//OASIS//DTD DITA 1.3 Troubleshooting//EN&quot; &quot;troubleshooting.dtd&quot;&amp;gt;&amp;lt;troubleshooting id=&quot;Troubleshooting&quot;&amp;gt;    &amp;lt;title&amp;gt;Troubleshooting&amp;lt;/title&amp;gt;    &amp;lt;troublebody&amp;gt;        &amp;lt;condition&amp;gt;            &amp;lt;title&amp;gt;Condition&amp;lt;/title&amp;gt;            &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;        &amp;lt;/condition&amp;gt;        &amp;lt;troubleSolution&amp;gt;            &amp;lt;cause&amp;gt;                &amp;lt;title&amp;gt;Cause&amp;lt;/title&amp;gt;                &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;            &amp;lt;/cause&amp;gt;            &amp;lt;remedy&amp;gt;                &amp;lt;title&amp;gt;Remedy&amp;lt;/title&amp;gt;                &amp;lt;responsibleParty&amp;gt;&amp;lt;/responsibleParty&amp;gt;                &amp;lt;steps&amp;gt;                    &amp;lt;step&amp;gt;                        &amp;lt;cmd&amp;gt;&amp;lt;/cmd&amp;gt;                    &amp;lt;/step&amp;gt;                &amp;lt;/steps&amp;gt;            &amp;lt;/remedy&amp;gt;        &amp;lt;/troubleSolution&amp;gt;    &amp;lt;/troublebody&amp;gt;&amp;lt;/troubleshooting&amp;gt;Beispiel: für ein Topic glossary group in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossgroup PUBLIC &quot;-//OASIS//DTD DITA Glossary Group//EN&quot; &quot;glossgroup.dtd&quot;&amp;gt;&amp;lt;glossgroup id=&quot;Glossar&quot;&amp;gt;    &amp;lt;title&amp;gt;Glossar&amp;lt;/title&amp;gt;    &amp;lt;glossentry id=&quot;txtline&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Textline&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;A TextLine is a block of text without line break.        &amp;lt;/glossdef&amp;gt;    &amp;lt;/glossentry&amp;gt;    &amp;lt;glossentry id=&quot;gt&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are        transcriptions, specific structure descriptions and word lists.        These are essentially available in PAGE XML format in        combination with the original image. Essential parts of         the GT were created manually.        &amp;lt;/glossdef&amp;gt;&amp;lt;/glossgroup&amp;gt;Beispiel: für ein Topic glossary entry in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossentry PUBLIC &quot;-//OASIS//DTD DITA Glossary//EN&quot; &quot;glossary.dtd&quot;&amp;gt;&amp;lt;glossentry id=&quot;gt&quot;&amp;gt;    &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;    &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are    transcriptions, specific structure descriptions and word lists.    These are essentially available in PAGE XML format in combination    with the original image. Essential parts of the GT were created    manually.    &amp;lt;/glossdef&amp;gt;&amp;lt;/glossentry&amp;gt;Technische Organisation der DokumenationTechnisch organisiert und zusammengefasst wird die DITA-Dokumentation mit einer sogenannten DITA-Map. Die DITA-Map ähnelt einem Inhaltsverzeichnis, die die Topics auflistet. Die Topics sind in einzelnen Dateien gespeichert.Beispiel DITA-Map ocr-d.ditamap&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE map PUBLIC &quot;-//OASIS//DTD DITA Map//EN&quot; &quot;map.dtd&quot;&amp;gt;&amp;lt;map&amp;gt;&amp;lt;title&amp;gt;Titel der Dokumentation&amp;lt;/title&amp;gt;    &amp;lt;topicref href=&quot;releaseNote.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;installation.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;simpletoolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;toolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Option.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;InputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Parameters.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;OutputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Troubleshooting.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Glossar.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Authors.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Reporting.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Copyright.md&quot;/&amp;gt;&amp;lt;/map&amp;gt;Die Verwendung des DITA-Open Toolkits zur Publikation der DokumentationFür die Generierung der Dokumentation ist die Kommandozeilen-Anwendung des DITA-Open Toolkits (http://www.dita-ot.org/3.0/topics/build-using-dita-command.html) zu verwenden.Mit dieser Anwendung können verschiedene Formate der Dokumentation erstellt werden. Für die finale Dokumentation (Publikation) des OCR-D Moduls ist nur das Format DITA gefordert. Wird die Dokumentation in DITA geschrieben ist die Nutzung der Kommandozeilen-Anwendung nicht notwendig. Bei der Verwendung mit Markdown ist eine Konvertierung mit der Kommandozeilen-Anwendung  notwendig.Aber auch zur Korrektur oder zur Vollständigkeitskontrolle ist eine Konvertierung in ein Präsentationsformat von Nutzem. Es können u. a. folgende Präsentionsformate erstellt werden:  html5  pdf  troff  xhtmlBeispiel Kommandoaufruf für DITA-OT  Für die Erstellung einer DITA-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=dita --output=output/dita       Für die Erstellung einer html5-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=html5 --output=output/html5Impressum und und DatenschutzerklärungFolgendes Impressum ist der Dokumentation anzufügen:Impressum und DatenschutzerklärungNachstehend finden Sie die gesetzlich geregelten Pflichtangaben zur Anbieterkennzeichnung sowie rechtliche Hinweise zur Dokumentation des Modulprojektes: XXX des OCR-D Projektes.AnbieterAnbieter dieser Internetpräsenz ist im Rechtssinne XXX[es folgt die Adresse][es folgt der Vertreter]Das Modul-Projekt wird vertreten durch XXX.[es folgt der Redaktionsverantwortliche mit Angabe der Persion und Adresse]Lizenz der DokumentationDie Dokumentation liegt unter dem xmlbasierten Format DITA [http://docs.oasis-open.org/dita/dita/v1.3/dita-v1.3-part3-all-inclusive.html] vor und kann unter der Creative Commons-Lizenz CC BY-SA 4.0 DE (https://creativecommons.org/licenses/by-sa/4.0/de/) genutzt werden.            siehe: Seite „Darwin Information Typing Architecture“. In: Wikipedia, Die freie Enzyklopädie. Bearbeitungsstand: 5. April 2018, 15:34 UTC. URL: https://de.wikipedia.org/w/index.php?title=Darwin_Information_Typing_Architecture&amp;amp;oldid=175806494 (Abgerufen: 23. Mai 2018, 10:40 UTC) &amp;#8617;              siehe https://de.wikipedia.org/wiki/Organization_for_the_Advancement_of_Structured_Information_Standards &amp;#8617;      ",
      "url": " /de/dita.html"
    },
  

    {
      "slug": "en-spec-docker-html",
      "title": "Dockerfile provided by MP",
      "content"	 : "Dockerfile provided by MPMP should provide aDockerfile that shouldresult in a container which bundles the tools developed by the MP alongwith all requirements.Based on ocrd:coreDocker containers should be based on the ocrd baseimage which itself is based on Ubuntu18.04. For one, this allows MP to use the ocrd tool to handle recurrent tasksin a spec-conformant way. Besides, it locally installed and containerizedCLI interchangeable.Naming imagesImage tags MUST be the same as the project name but with underscore (_)replaced with forward slash (/).Examples:            project name      docker tag                  ocrd_tesserocr      ocrd/tesserocr              ocrd_calamari      ocrd/calamari              ocrd_olena      ocrd/olena      Labelling imagesThe Dockerfile MUST accept build args VCS_REF and BUILD_DATE.VCS_REF contains the short id of the latest commit this image was built upon.BUILD_DATE contains an ISO-8601 date.From these build args, the image shall be labelled with this command:LABEL     maintainer=&quot;https://github.com/YOUR/PROJECT/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/YOUR/PROJECT&quot;     org.label-schema.build-date=$BUILD_DATEmaintainer and org.label-schema.cvs-url shall point to the issues andlanding page of the GitHub project resp.Shell entrypointNo CMD should be provided.No ENTRYPOINT should be provided.If CMD or ENTRYPOINT are provided, they should be empty arrays./data as volumeThe directory /data in the the container should be marked as a volume toallow processing host data in the container in a uniform way.ExampleDockerfileFROM ocrd:coreVOLUME [&quot;/data&quot;]ARG VCS_REFARG BUILD_DATELABEL     maintainer=&quot;https://github.com/bar/ocrd_foo/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/bar/ocrd_foo&quot;     org.label-schema.build-date=$BUILD_DATE# RUN-commands to install requirements, build and install# e.g.# apt-get install -y curlENTRYPOINT []Command to build docker imagedocker build   -t &#39;ocrd/foo&#39; --build-arg VCS_REF=$(git rev-parse --short HEAD) --build-arg BUILD_DATE=$(date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;)",
      "url": " /en/spec/docker.html"
    },
  

    {
      "slug": "de-spec-docker-html",
      "title": "",
      "content"	 : "Dockerfile provided by MPMP should provide aDockerfile that shouldresult in a container which bundles the tools developed by the MP alongwith all requirements.Based on ocrd:coreDocker containers should be based on the ocrd baseimage which itself is based on Ubuntu18.04. For one, this allows MP to use the ocrd tool to handle recurrent tasksin a spec-conformant way. Besides, it locally installed and containerizedCLI interchangeable.Naming imagesImage tags MUST be the same as the project name but with underscore (_)replaced with forward slash (/).Examples:            project name      docker tag                  ocrd_tesserocr      ocrd/tesserocr              ocrd_calamari      ocrd/calamari              ocrd_olena      ocrd/olena      Labelling imagesThe Dockerfile MUST accept build args VCS_REF and BUILD_DATE.VCS_REF contains the short id of the latest commit this image was built upon.BUILD_DATE contains an ISO-8601 date.From these build args, the image shall be labelled with this command:LABEL     maintainer=&quot;https://github.com/YOUR/PROJECT/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/YOUR/PROJECT&quot;     org.label-schema.build-date=$BUILD_DATEmaintainer and org.label-schema.cvs-url shall point to the issues andlanding page of the GitHub project resp.Shell entrypointNo CMD should be provided.No ENTRYPOINT should be provided.If CMD or ENTRYPOINT are provided, they should be empty arrays./data as volumeThe directory /data in the the container should be marked as a volume toallow processing host data in the container in a uniform way.ExampleDockerfileFROM ocrd:coreVOLUME [&quot;/data&quot;]ARG VCS_REFARG BUILD_DATELABEL     maintainer=&quot;https://github.com/bar/ocrd_foo/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/bar/ocrd_foo&quot;     org.label-schema.build-date=$BUILD_DATE# RUN-commands to install requirements, build and install# e.g.# apt-get install -y curlENTRYPOINT []Command to build docker imagedocker build   -t &#39;ocrd/foo&#39; --build-arg VCS_REF=$(git rev-parse --short HEAD) --build-arg BUILD_DATE=$(date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;)",
      "url": " /de/spec/docker.html"
    },
  

    {
      "slug": "en-example-mets-html",
      "title": "Example METS",
      "content"	 : "Example METS&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;mets:mets xmlns:mets=&quot;http://www.loc.gov/METS/&quot;           xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;           xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;           xsi:schemaLocation=&quot;info:lc/xmlns/premis-v2 http://www.loc.gov/standards/premis/v2/premis-v2-0.xsd                               http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd                                http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/version17/mets.v1-7.xsd                                http://www.loc.gov/mix/v10 http://www.loc.gov/standards/mix/mix10/mix10.xsd&quot;&amp;gt;  &amp;lt;mets:metsHdr CREATEDATE=&quot;2020-02-28T07:52:41.141812&quot;&amp;gt;    &amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;CREATOR&quot;&amp;gt;      &amp;lt;mets:name&amp;gt;ocrd/core v2.3.1&amp;lt;/mets:name&amp;gt;    &amp;lt;/mets:agent&amp;gt;  &amp;lt;/mets:metsHdr&amp;gt;  &amp;lt;mets:dmdSec ID=&quot;DMDLOG_0001&quot;&amp;gt;    &amp;lt;mets:mdWrap MDTYPE=&quot;MODS&quot;&amp;gt;      &amp;lt;mets:xmlData&amp;gt;        &amp;lt;mods:mods xmlns:mods=&quot;http://www.loc.gov/mods/v3&quot;&amp;gt;          &amp;lt;mods:identifier type=&quot;purl&quot;&amp;gt;uniqueID&amp;lt;/mods:identifier&amp;gt;        &amp;lt;/mods:mods&amp;gt;      &amp;lt;/mets:xmlData&amp;gt;    &amp;lt;/mets:mdWrap&amp;gt;  &amp;lt;/mets:dmdSec&amp;gt;  &amp;lt;mets:amdSec ID=&quot;AMD&quot;&amp;gt;    &amp;lt;/mets:amdSec&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file MIMETYPE=&quot;image/jpg&quot; ID=&quot;OCR-D-IMG_00001&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;OTHER&quot; OTHERLOCTYPE=&quot;FILE&quot; xlink:href=&quot;OCR-D-IMG/OCR-D-IMG_0001.jpg&quot;/&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;P_00001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_00001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /en/example_mets.html"
    },
  

    {
      "slug": "en-faq-html",
      "title": "FAQ",
      "content"	 : "FAQGeneralWhere can I start my journey into the OCR-D ecosphere?Who is the target audience of OCR-D?OCR-D’s primary target audience are libraries and archives, digitizinghistorical prints at scale.Where can I get support on OCR-D?  Open an issue at the OCR-D/core repository  Chat with OCR-D project members and other OCR-D users in OCR-D’s chat room.  Send an email to …What is the difference between OCR-D and ABBYY?ABBYY is a software developer producing the ABBYY Recognition Server whichoffers layout detection and text recognition with a pay-per-page pricing model.OCR-D is a project that integrates a wide variety of solutions for the fullgamut of possible OCR workflow steps. ABBYY is simple to use but offers fewoptions for customization whereas OCR-D workflows can be fine-tuned for bestrecognition of specific corpora. OCR-D has a strong focus on historical prints,trainable layout detection and text recognition and open interfaces toaccommodate future developments, whereas ABBYY performs more strongly for modernprint. Finally, OCR-D is a community effort with a strong focus on transparencyand Free Software.What is the difference between OCR-D and Tesseract?Tesseract is the leading FreeSoftware OCR solution and tightly integrated into OCR-D in both a technical andorganizational sense. Technically, Tesseract has been wrapped asocrd_tesserocr, an OCR-D-compliantprocessor that is more powerful than the command line tool bundled withTesseract. Organizationally, Tesseract maintainers and contributors have beenpart of the OCR-D project from the beginning and the originally OCR-D-developedTesseract training tooltesstrain has been adopted bythe wider Tesseract community.What is the difference between OCR-D and TRANSKRIBUS?TRANSKRIBUS is a software platform and serverinfrastructure to make it easier for Digital Humanities practitioners tocollaborate on Handwriting Text Recognition. Apart from the different use cases  historical prints for OCR-D, historical manuscripts for TRANSKRIBUS - thereare differences in philosophy: All components of OCR-D are freely available asApache-licensed Free Software whereas some core components of TRANSKRIBUS,particularly the recognition engine, are proprietary. TRANSKRIBUS is aserver-client architecture with an Eclipse-based graphical user interface atits core whereas OCR-D’s focus is on mass digitization and command lineinteraction.What is the difference between OCR-D and ocr4all?Is OCR-D production-ready?Which formats are supported by OCR-D?OCR-D is primarily based around METS as a container format and PAGE-XML forlayout detection and text recognition results. Other OCR formats such as ALTO,hOCR or ABBYY FineReader XML are supported through conversion withocrd_fileformat.The preferred image format within OCR-D is TIFF but PNG and JPEG are alsosupported. JPEG2000 is not currently supported but can be added in the futureif there is demand for it.Why does OCR-D need METS files? How can I process images without METS?The processes within OCR-D are designed around METS for the simple reason that it issuch an ubiquitous and well-defined format used in libraries and archivesaround the world. By relying on a container format instead of just images,processors can make use of more information and can store detailed results in awell-defined fashion.If the data to be processed isn’t already described by a METS file, the ocrd command linetool offers simple ways to create new METS files or augment existing ones.How much does it cost to deploy OCR-D?OCR-D is Free Software, licensed under the terms of the Apache 2.0 license andwill be free to use and adapt in perpetuity.How are the full texts produced by OCR-D presented to the (library) user? Are they integrated into the library catalog and can therefore be used for full text search in the catalog?What are the system requirements for OCR-D-software?The OCR-D/core framework is fairly lightcompared with other interoperability platforms. System requirements thereforedepend on the actual processors to be used and the scale of the operation. Itis possible to use OCR-D on commodity hardware such as desktop PCs and laptopsbut can also be deployed to massive servers or even single-board computers.However, OCR workflows can be very memory-intensive, in particular when workingwith large neural network models that have to be loaded into memory. We recommendat least 16 GB of RAM to support even the most demanding workflow steps.Another bottleneck for OCR workflows is input/output. We recommend storing dataon SSD instead of HDD.CLIHow can I find out the version of OCR-D software?To find the version of the OCR-D/core framework installed, run the ocrd CLIwith the --version flag:$ ocrd --versionocrd, version 2.2.2All OCR-D processors also support the --version flag, e.g.:ocrd-tesserocr-recognize --versionVersion 0.7.0, ocrd/core 2.2.2How do I get help on ocrd CLI commands?Every command and subcommand of the ocrd CLI tool supports the --helpoption to print a description, arguments and options:ocrd --helpocrd workspace --helpocrd workspace add --helpHow do I get help on OCR-D processors?All OCR-D-compliant processors support the -h/--help flag as well:$ ocrd-tesserocr-recognize --helpHow can I specify parameters on the command line?Parameters to an OCR-D-compliant processor must be specified in the JSON syntax. The JSON datacan be passed to a processor with the -p CLI option, which can be either the filename of a file containing the JSON data or the JSON data itself:ocrd-tesseract-recognize -I IN -O OUT -p &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39;# same effect:echo  &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39; &amp;gt; /tmp/params.jsonocrd-tesseract-recognize -I IN -O OUT -p /tmp/params.jsonHow do I specify multiple input/output file groups?You can specify multiple file group names for both input and output by joiningthe names with a comma (,).ocrd-tesserocr-recognize -I DEFAULT,REGIONS -O OCR-TESSSERACTThis would instruct ocrd-tesserocr-recognize to take images from theDEFAULT group and region-segmented layout information from the REGIONSgroup.How to configure logging?How to stop tensorflow logging spam  @bertsky  Another thing that needs to be added to tame TF isos.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; – before the tensorflow modulegets imported.To achieve the same, run this before executing a TF-based processor in theshell (or even add it to your $HOME/.bashrc to set this permanently):export TF_CPP_MIN_LOG_LEVEL=3OCR-D module project softwareWhere can I find official OCR-D module project software?Which third-party OCR-D-compatible software exists?Which processors are available?Workflows and processorsHow can I define workflows?Where can I find sample workflows to experiment with?How to handle failing workflows?Why do some processors have multiple input or output file groups?Where can I learn about the input and output file groups of a processor?How can I validate my workflow is correctly wired?Where can I learn about the parameters of a processor?ocrd_allWhat is ocrd_all?How to update ocrd_all?How to debug ocrd_all problems?I used sudo and now everything is brokenTrainingI want to train a custom OCR model. Where do I start?OCR-D-Ground TruthWhich of the three transcription levels specified in the Transcription Guidelines was used for the GT of OCR-D?Are the three transcription levels designed hierarchically? Meaning, does level 3 include level 2 and level 1?I want to make some GT myself. Which level should I use? Can I mix levels?I have some transcriptions of early modern books, but I didn’t stick to the OCR-D GT guidelines. Would my transcriptions still be useful for OCR-D?",
      "url": " /en/faq.html"
    },
  

    {
      "slug": "de-faq-html",
      "title": "FAQ",
      "content"	 : "Bitte lesen sie die englische Version der Häufig Beantworteten Fragen.",
      "url": " /de/faq.html"
    },
  

    {
      "slug": "en-spec-glossary-html",
      "title": "OCR-D Glossary",
      "content"	 : "OCR-D Glossary  Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D frameworkThis section is non-normative.Layout and TypographyBlockSee RegionBorderFrom the PAGE-XML content schema documentation  Border of the actual page (if the scanned image contains parts not belonging to the page).Font familyWithin OCR-D, font family refers to grouping elements by font similarity. Thesemantics of a font family are up to the data producer.GlyphWithin OCR-D, a glyph is the atomic unit within a word.Grapheme ClusterSee GlyphLineSee TextLineReading OrderReading order describes the logical sequence of regions within a document.RegionA region is described by a polygon inside a page.Region typeThe semantics or function of a region such as heading, page number, column, table…SymbolSee GlyphTextLineA text line is a single row of words within a text region. (Depending on the region’s or page’s orientation, and the script’s writing direction, it can be horizontal or vertical.)Print spaceFrom the PAGE-XML content schema documentation  Determines the effective area on the paper of a printed page. Its size is equal for all pages of a book (exceptions: titlepage, multipage pictures).  It contains all living elements (except marginalia) like paragraphs and headings, as well as footnotes, headings, running titles.  It does not contain pagenumber (if not part of running title), marginalia, signature mark, preview words.WordA word is a sequence of glyphs within a line which does not contain any word-bounding whitespace. (That is, it includes punctuation and is synonym to token in NLP.)DataGround TruthGround truth (GT) in the context of OCR-D aretranscriptions, specific structure descriptions and word lists. These areessentially available in PAGE XML format in combination with the originalimage. Essential parts of the GT were created manually.We distinguish different usage scenarios for GT:Reference dataWith the term reference data, we refer to data that illustratesdifferent stages of an OCR/OLR process on representative materials. They aresupposed to support the assessment of commonly encountered difficulties and challenges whenrunning certain analysis operations and are therefore manually annotatedat all levels.Evaluation dataEvaluation data are used to quantitatively evaluate the performance of OCR toolsand/or algorithms. Parts of these data which correspond to the tool(s) under considerationare guaranteed to be recorded manually.Training dataMany OCR-related tools need to be adapted to the specific domain of the works which are tobe processed. This domain adaptation is called training. Data used to guide this processare called training data. It is essential that those parts of these data which are fedto the training algorithm are captured manually.ActivitiesBinarizationBinarization means converting all color or grayscale pixels in an image to either black or white.Controlled term: binarized (comments of a mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)See Felix’ Niklas interactive demoDewarpingManipulate an image in such a way that all text lines arestraightened and any geometrical distortions have been corrected.Controlled term: preprocessing/optimization/dewarpingSee Matt Zucker’s entry on Dewarping.DespecklingRemove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to remove “salt-and-pepper” noise resulting from Binarization.Controlled term: preprocessing/optimization/despecklingDeskewingRotate an image so that all text lines are horizontal.Controlled term: preprocessing/optimization/deskewingFont identificationDetect the font type(s) used in the document, either before or after an OCR run.Controlled term: recognition/font-identificationGrayscale normalization  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled term:  gray_normalized (comments in file)  preprocessing/optimization/cropping (step)Gray normalization is similar to binarization but instead of a purely bitonalimage, the output can also contain shades of gray to avoid inadvertentlycombining glyphs when they are very close together.Document analysisDocument analysis is the detection of structure on the document level to e.g. create a table of contents.Reading order detectionDetect the reading order of regions.CroppingDetecting the print space in a page, as opposed to the margins. It is a form ofregion segmentation.Controlled term: preprocessing/optimization/cropping.Border removal–&amp;gt; CroppingSegmentationSegmentation means detecting areas within an image.Specific segmentation algorithms are labelled by the semantics of the regionsthey detect not the semantics of the input, i.e. an algorithm that detectsregions is called region segmentation.Region segmentationSegment an image into regions. Also determines whether this is a textor non-text region (e.g. images).Controlled term:  SEG-REGION (USE)  layout/segmentation/region (step)Region classificationDetermine the type of a detected region.Line segmentationSegment text regions into textlines.Controlled term:  SEG-LINE (USE)  layout/segmentation/line (step)Line recognitionSee OCR.OCRMap pixel areas to glyphs and words.Word segmentationSegment a textline into wordsControlled term:  SEG-LINE (USE)  layout/segmentation/word (step)Glyph segmentationSegment a textline into glyphsControlled term: SEG-GLYPHText recognitionSee OCR.Text optimizationText optimization encompasses the manipulations to the text based on the stepsup to and including text recognition. This includes (semi-)automatically correctingrecognition errors, orthographical harmonization, fixing segmentation errors etc.Data PersistenceSoftware repositoryThe software repository contains all OCR-D algorithms and tools developedduring the project including tests. It will also contain the documentation andinstallation instructions for deploying a document analysis workflow.Ground Truth repositoryContains all the ground truth data.Research data repositoryThe research data repository may contain the results of allactivities during document analysis. At least it contains theend results of every processed document and its full provenance. The researchdata repository must be available locally.Model repositoryContains all trained (OCR) models for text recognition. The model repositoryhas to be available at least locally. Ideally, a publicly available model repository willbe developed.WorkspaceA workspace is a representation for some document in the local file system. Minimally it consists of a directory with a copy of the METS file. Additionally, that directory may contain physical data files and sub-directories belonging to the document (required or generated by run-time OCR-D processing), as referenced by the METS via mets:file/mets:FLocat/@href and mets:fileGrp/@USE. Files and sub-directories without reference (like log or config files) are not part of the workspace, as are references to remote locations. They can be added to the workspace by referencing them in the METS via their relative local path names.Workflow modulesThe OCR-D project divided the various elements of an OCRworkflow into six abstract modules.Image preprocessingManipulating the input images for subsequent layout analysis and text recognition.Layout analysisDetection of structure within the page.Text recognition and optimizationRecognition of text and post-correction of recognition errors.Model trainingGenerating data files from aligned ground truth text and images to configurethe prediction of text and layout recognition engines.Long-term preservation and persistenceStoring results of OCR and OLR indefinitely, taking into account versioning,multiple runs, provenance/parametrization and providing access to these savedsnapshots in a granular fashion.Quality assuranceProviding measures, algorithms and software to estimate the quality of the individual processes within the OCR-D domain.Component architecture(OCR-D) ApplicationApplication composed of various servers that can execute processors; can be a desktop computer or workstation, a distributed system comprising a controller and multiple processing servers, or an HPC cluster.OCR-D Web APIAs proposed in OCR-D/spec#173, the OCR-D Web API defines uniform and interdependent services that can be distributed across network components, depending on the use case.(OCR-D) ServiceGroup of endpoints of the OCR-D Web API; discovery/workspace/processing/workflow/…(OCR-D) ServerConcrete implementation of a subset of OCR-D services, or the network host providing it.(OCR-D) ControllerOCR-D Server (implementing at least discovery, workspace and workflow services) executing workflows (a single workflow or multiple workflows simultaneously), distributing tasks to configured processing servers, managing workspace data management. Should also manage load balancing.(OCR-D) Processing ServerOCR-D server (implementing at least discovery and processing services) that can execute one or more (locally installed) processors or evaluators, manages workspace data; implementor should consider whether a single OCR-D processing server (with page-parallel processing) best fits the use case, or multiple OCR-D processing servers (with document-parallel processing), or even dedicated OCR-D processing servers with GPU/CUDA support.(OCR-D) BackendSoftware component of a server concerned with network operations; e.g. Python library with request handlers, implementing service discovery and network-capable workspace data management.(OCR-D) Workflow Runtime LibrarySoftware component of a server or processor concerned with OCR systems modelling; e.g. Python library in OCR-D/core providing classes for all essential functional components (OcrdPage, OcrdMets, Workspace, Resolver, Processor, ProcessorTask, Workflow, WorkflowTask …), including mechanisms for signalling and orchestration of workflows, on top of which components (from processor to controller) can be implemented.(OCR-D) Workflow EngineCentral software component of the controller, executing workflows, including control structures (in a linear/parallel/incremental way). Also needed in single-host CLI deployments (where it can be based on inter-process communication and file system I/O alone), like ocrd process.(OCR-D) ProcessorA processor is a tool that implements the uniform OCR-D command-line-interface for run-time data processing. That is, it executes a single workflow step, or a combination of multiple workflow steps, on the workspace (represented by local METS), reading input files for all or requested physical pages of the input fileGrp(s), and writing output files for them into the output fileGrp(s). It may take a number of optional or mandatory parameters.→ OCR-D Workflow Guide(OCR-D) EvaluatorAn evaluator is a tool that implements the uniform OCR-D CLI for run-time quality estimation, assessing an activity’s annotation (i.e. a processor’s output) with some quality metric to yield a score and applying a given threshold against it to signal full or partial success/failure.(OCR-D) ModuleSoftware package/repository providing one or more processors or evaluators, possibly encompassing additional areas of functionality (training, format conversion, creation of GT, visualization)Modules can comprise multiple methods/activities that are called processorsfor OCR-D. There were eight MP in thesecond phase of OCR-D (2018-2020).MessagingMessaging service on the basis of Publish/Subscribe architecture (or similar) to coordinate network components, in particular for the distribution of tasks and load balancing, as well as signalling processor/evaluator results.OCR-D WorkflowCombination of activities via concrete processors and evaluators and their parameterization configured as a sequence or lattice, depending on their success or failure. Implemented in the OCR-D Workflow Runtime Library and serializable in a yet-to-specifcy format (as of 2020/10).The term Workflow is understood to encompass more features in other contexts, such as manual intervention by the user. In contrast to the terminology in workflow engines like Taverna or digitization frameworks like Kitodo, an OCR-D workflow is a fully automatic process.",
      "url": " /en/spec/glossary.html"
    },
  

    {
      "slug": "de-spec-glossary-html",
      "title": "",
      "content"	 : "OCR-D Glossary  Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D frameworkThis section is non-normative.Layout and TypographyBlockSee RegionBorderFrom the PAGE-XML content schema documentation  Border of the actual page (if the scanned image contains parts not belonging to the page).Font familyWithin OCR-D, font family refers to grouping elements by font similarity. Thesemantics of a font family are up to the data producer.GlyphWithin OCR-D, a glyph is the atomic unit within a word.Grapheme ClusterSee GlyphLineSee TextLineReading OrderReading order describes the logical sequence of regions within a document.RegionA region is described by a polygon inside a page.Region typeThe semantics or function of a region such as heading, page number, column, table…SymbolSee GlyphTextLineA text line is a single row of words within a text region. (Depending on the region’s or page’s orientation, and the script’s writing direction, it can be horizontal or vertical.)Print spaceFrom the PAGE-XML content schema documentation  Determines the effective area on the paper of a printed page. Its size is equal for all pages of a book (exceptions: titlepage, multipage pictures).  It contains all living elements (except marginalia) like paragraphs and headings, as well as footnotes, headings, running titles.  It does not contain pagenumber (if not part of running title), marginalia, signature mark, preview words.WordA word is a sequence of glyphs within a line which does not contain any word-bounding whitespace. (That is, it includes punctuation and is synonym to token in NLP.)DataGround TruthGround truth (GT) in the context of OCR-D aretranscriptions, specific structure descriptions and word lists. These areessentially available in PAGE XML format in combination with the originalimage. Essential parts of the GT were created manually.We distinguish different usage scenarios for GT:Reference dataWith the term reference data, we refer to data that illustratesdifferent stages of an OCR/OLR process on representative materials. They aresupposed to support the assessment of commonly encountered difficulties and challenges whenrunning certain analysis operations and are therefore manually annotatedat all levels.Evaluation dataEvaluation data are used to quantitatively evaluate the performance of OCR toolsand/or algorithms. Parts of these data which correspond to the tool(s) under considerationare guaranteed to be recorded manually.Training dataMany OCR-related tools need to be adapted to the specific domain of the works which are tobe processed. This domain adaptation is called training. Data used to guide this processare called training data. It is essential that those parts of these data which are fedto the training algorithm are captured manually.ActivitiesBinarizationBinarization means converting all color or grayscale pixels in an image to either black or white.Controlled term: binarized (comments of a mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)See Felix’ Niklas interactive demoDewarpingManipulate an image in such a way that all text lines arestraightened and any geometrical distortions have been corrected.Controlled term: preprocessing/optimization/dewarpingSee Matt Zucker’s entry on Dewarping.DespecklingRemove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to remove “salt-and-pepper” noise resulting from Binarization.Controlled term: preprocessing/optimization/despecklingDeskewingRotate an image so that all text lines are horizontal.Controlled term: preprocessing/optimization/deskewingFont identificationDetect the font type(s) used in the document, either before or after an OCR run.Controlled term: recognition/font-identificationGrayscale normalization  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled term:  gray_normalized (comments in file)  preprocessing/optimization/cropping (step)Gray normalization is similar to binarization but instead of a purely bitonalimage, the output can also contain shades of gray to avoid inadvertentlycombining glyphs when they are very close together.Document analysisDocument analysis is the detection of structure on the document level to e.g. create a table of contents.Reading order detectionDetect the reading order of regions.CroppingDetecting the print space in a page, as opposed to the margins. It is a form ofregion segmentation.Controlled term: preprocessing/optimization/cropping.Border removal–&amp;gt; CroppingSegmentationSegmentation means detecting areas within an image.Specific segmentation algorithms are labelled by the semantics of the regionsthey detect not the semantics of the input, i.e. an algorithm that detectsregions is called region segmentation.Region segmentationSegment an image into regions. Also determines whether this is a textor non-text region (e.g. images).Controlled term:  SEG-REGION (USE)  layout/segmentation/region (step)Region classificationDetermine the type of a detected region.Line segmentationSegment text regions into textlines.Controlled term:  SEG-LINE (USE)  layout/segmentation/line (step)Line recognitionSee OCR.OCRMap pixel areas to glyphs and words.Word segmentationSegment a textline into wordsControlled term:  SEG-LINE (USE)  layout/segmentation/word (step)Glyph segmentationSegment a textline into glyphsControlled term: SEG-GLYPHText recognitionSee OCR.Text optimizationText optimization encompasses the manipulations to the text based on the stepsup to and including text recognition. This includes (semi-)automatically correctingrecognition errors, orthographical harmonization, fixing segmentation errors etc.Data PersistenceSoftware repositoryThe software repository contains all OCR-D algorithms and tools developedduring the project including tests. It will also contain the documentation andinstallation instructions for deploying a document analysis workflow.Ground Truth repositoryContains all the ground truth data.Research data repositoryThe research data repository may contain the results of allactivities during document analysis. At least it contains theend results of every processed document and its full provenance. The researchdata repository must be available locally.Model repositoryContains all trained (OCR) models for text recognition. The model repositoryhas to be available at least locally. Ideally, a publicly available model repository willbe developed.WorkspaceA workspace is a representation for some document in the local file system. Minimally it consists of a directory with a copy of the METS file. Additionally, that directory may contain physical data files and sub-directories belonging to the document (required or generated by run-time OCR-D processing), as referenced by the METS via mets:file/mets:FLocat/@href and mets:fileGrp/@USE. Files and sub-directories without reference (like log or config files) are not part of the workspace, as are references to remote locations. They can be added to the workspace by referencing them in the METS via their relative local path names.Workflow modulesThe OCR-D project divided the various elements of an OCRworkflow into six abstract modules.Image preprocessingManipulating the input images for subsequent layout analysis and text recognition.Layout analysisDetection of structure within the page.Text recognition and optimizationRecognition of text and post-correction of recognition errors.Model trainingGenerating data files from aligned ground truth text and images to configurethe prediction of text and layout recognition engines.Long-term preservation and persistenceStoring results of OCR and OLR indefinitely, taking into account versioning,multiple runs, provenance/parametrization and providing access to these savedsnapshots in a granular fashion.Quality assuranceProviding measures, algorithms and software to estimate the quality of the individual processes within the OCR-D domain.Component architecture(OCR-D) ApplicationApplication composed of various servers that can execute processors; can be a desktop computer or workstation, a distributed system comprising a controller and multiple processing servers, or an HPC cluster.OCR-D Web APIAs proposed in OCR-D/spec#173, the OCR-D Web API defines uniform and interdependent services that can be distributed across network components, depending on the use case.(OCR-D) ServiceGroup of endpoints of the OCR-D Web API; discovery/workspace/processing/workflow/…(OCR-D) ServerConcrete implementation of a subset of OCR-D services, or the network host providing it.(OCR-D) ControllerOCR-D Server (implementing at least discovery, workspace and workflow services) executing workflows (a single workflow or multiple workflows simultaneously), distributing tasks to configured processing servers, managing workspace data management. Should also manage load balancing.(OCR-D) Processing ServerOCR-D server (implementing at least discovery and processing services) that can execute one or more (locally installed) processors or evaluators, manages workspace data; implementor should consider whether a single OCR-D processing server (with page-parallel processing) best fits the use case, or multiple OCR-D processing servers (with document-parallel processing), or even dedicated OCR-D processing servers with GPU/CUDA support.(OCR-D) BackendSoftware component of a server concerned with network operations; e.g. Python library with request handlers, implementing service discovery and network-capable workspace data management.(OCR-D) Workflow Runtime LibrarySoftware component of a server or processor concerned with OCR systems modelling; e.g. Python library in OCR-D/core providing classes for all essential functional components (OcrdPage, OcrdMets, Workspace, Resolver, Processor, ProcessorTask, Workflow, WorkflowTask …), including mechanisms for signalling and orchestration of workflows, on top of which components (from processor to controller) can be implemented.(OCR-D) Workflow EngineCentral software component of the controller, executing workflows, including control structures (in a linear/parallel/incremental way). Also needed in single-host CLI deployments (where it can be based on inter-process communication and file system I/O alone), like ocrd process.(OCR-D) ProcessorA processor is a tool that implements the uniform OCR-D command-line-interface for run-time data processing. That is, it executes a single workflow step, or a combination of multiple workflow steps, on the workspace (represented by local METS), reading input files for all or requested physical pages of the input fileGrp(s), and writing output files for them into the output fileGrp(s). It may take a number of optional or mandatory parameters.→ OCR-D Workflow Guide(OCR-D) EvaluatorAn evaluator is a tool that implements the uniform OCR-D CLI for run-time quality estimation, assessing an activity’s annotation (i.e. a processor’s output) with some quality metric to yield a score and applying a given threshold against it to signal full or partial success/failure.(OCR-D) ModuleSoftware package/repository providing one or more processors or evaluators, possibly encompassing additional areas of functionality (training, format conversion, creation of GT, visualization)Modules can comprise multiple methods/activities that are called processorsfor OCR-D. There were eight MP in thesecond phase of OCR-D (2018-2020).MessagingMessaging service on the basis of Publish/Subscribe architecture (or similar) to coordinate network components, in particular for the distribution of tasks and load balancing, as well as signalling processor/evaluator results.OCR-D WorkflowCombination of activities via concrete processors and evaluators and their parameterization configured as a sequence or lattice, depending on their success or failure. Implemented in the OCR-D Workflow Runtime Library and serializable in a yet-to-specifcy format (as of 2020/10).The term Workflow is understood to encompass more features in other contexts, such as manual intervention by the user. In contrast to the terminology in workflow engines like Taverna or digitization frameworks like Kitodo, an OCR-D workflow is a fully automatic process.",
      "url": " /de/spec/glossary.html"
    },
  

    {
      "slug": "de-impressum-html",
      "title": "Impressum",
      "content"	 : "ImpressumNachstehend finden Sie die gesetzlich geregelten Pflichtangaben zur Anbieterkennzeichnung sowie rechtliche Hinweise zur Internetpräsenz des OCR-D-Projekts.Angaben gemäß § 5 TMGAnbieterAnbieter dieser Internetpräsenz ist im Rechtssinne die Herzog August Bibliothek Wolfenbüttel.Herzog August Bibliothek,Lessingplatz 1,D-38304 WolfenbüttelTel.: +49(0)5331/808-0,Fax: +49(0)5331/808-302,e-mail: mailto:auskunft@hab.dehttp://www.hab.deVertreterDie HAB Wolfenbüttel wird vertreten durch ihren Direktor, Herrn Prof. Dr. Peter BurschelUmsatzsteueridentifikationsnummer: DE811255517Verantwortlich für den Inhalt nach § 55 Abs. 2 RStV:Andrea OpitzHerzog August Bibliothek,Lessingplatz 1,D-38304 WolfenbüttelTel.: +49(0)5331/808-314,e-mail: mailto:opitz@hab.deHaftungsausschluss:Haftung für InhalteDie Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit,Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen.Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nachden allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieterjedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachenoder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungenzur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleibenhiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniseiner konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungenwerden wir diese Inhalte umgehend entfernen.Haftung für LinksUnser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einflusshaben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für dieInhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seitenverantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf möglicheRechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nichterkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkreteAnhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungenwerden wir derartige Links umgehend entfernen.UrheberrechtDie durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen demdeutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art derVerwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung desjeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten,nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreibererstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritterals solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksamwerden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungenwerden wir derartige Inhalte umgehend entfernen.DatenschutzDie Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich.Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift odereMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis.Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben.Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation perE-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriffdurch Dritte ist nicht möglich.Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Drittezur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wirdhiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklichrechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durchSpam-Mails, vor.Verwendete BilderDiese Website nutzt die folgenden Abbildungen aus Wikimedia Commons:Kupferstich “Le cabinet de la Bibliotheque de Sainte Genevieve”, Paris: 1692, von Franz ErtingerTyp 615.92.341, Houghton Library, Harvard UniversityKupferstich “Calculating machine designed by Pascal”. Louvet sculpt. Rue Galande No. 51https://lccn.loc.gov/2006690493, Library of CongressWebsite Impressum erstellt durch impressum-generator.de von der Kanzlei Hasselbach",
      "url": " /de/impressum.html"
    },
  

    {
      "slug": "en-imprint-html",
      "title": "Imprint",
      "content"	 : "ImprintIn the following you will find the legally regulated obligatory information on provideridentification as well as legal information on the Internet presence of the OCR-D project.Information according to § 5 TMGProviderThe provider of this internet presence in the legal sense is the Duke August LibraryWolfenbüttel.Duke August Library,Lessingplatz 1,D-38304 WolfenbüttelPhone: +49(0)5331/808-0,Fax: +49(0)5331/808-302,e-mail: mailto:auskunft@hab.dehttp://www.hab.deRepresentativeThe HAB Wolfenbüttel is represented by its director, Prof. Dr. Peter BurschelSales tax identification number: DE811255517Responsible for the content according to § 55 Abs. 2 RStV:Andrea OpitzDuke August Library,Lessingplatz 1,D-38304 WolfenbüttelPhone: +49(0)5331/808-314,e-mail: mailto:opitz@hab.deDisclaimer:Liability for contentsThe contents of our pages were created with the greatest care. However, we cannot guaranteethat the contents are correct, complete and up-to-date. As a service provider, we areresponsible for our own content on these pages according to § 7 para.1 TMG (German TelemediaAct) and general laws. According to §§ 8 to 10 TMG we are not obliged to monitor transmittedor stored information from third parties or to investigate circumstances that indicateillegal activity. Obligations for the removal or blocking of the use of information accordingto the general laws remain unaffected. However, liability in this respect is only possiblefrom the time of knowledge of a concrete infringement. If we become aware of any such legalinfringements, we will remove these contents immediately.Liability for linksOur offer contains links to external websites of third parties, on whose contents we haveno influence. Therefore we cannot assume any liability for these external contents. As theContents of the linked pages are always the responsibility of the respective provider oroperator of the pages.The linked pages were checked for possible legal violations at thetime of linking. Illegal contents were not identified at the time of linking. However, apermanent control of the contents of the linked pages is not reasonable without concreteevidence of a violation of the law.  If infringements of rights become knownwe will remove such links immediately.CopyrightThe contents and works on these pages created by the site operators are subject to theGerman copyright law. The duplication, processing, distribution and any kind ofuse outside the limits of copyright law requires the written consent of thethe respective author or creator.Downloads and copies of these pages are only permittedfor private, non-commercial use. Insofar as the contents on this site were not created bythe operator, the copyrights of third parties are observed. In particular, third-partycontent is identified as such. Should you nevertheless become aware of a copyrightinfringement, please inform us accordingly. If we become aware of any infringements, wewill remove such contents immediately.Privacy policyThe use of our website is usually possible without providing personal data.As far as personal data (e.g. name, address or e-mail addresses) is collected on our website,this is always done on a voluntary basis, as far as possible.This data will not be passed on to third parties without your express consent.We would like to point out that data transmission on the Internet (e.g. communication viaE-Mail) can have security gaps. A complete protection of the data against accessby third parties is not possible.The usage of contact data published within the scope of the imprint obligation by thirdparties for the transmission of not expressly requested advertising and information materialis hereby expressly contradicted. The operators of the pages expressly reserve the right totake legal action in the event of the unsolicited sending of advertising information, forexample through spam mails.Images usedThis website uses the following images from Wikimedia Commons:Copper engraving “Le cabinet de la Bibliotheque de Sainte Genevieve”, Paris: 1692, byFranz ErtingerType 615.92.341, Houghton Library, Harvard UniversityCopper engraving “Calculating machine designed by Pascal”. Louvet sculpt. Rue Galande No. 51https://lccn.loc.gov/2006690493, Library of CongressWebsite imprint created by impressum-generator.de of the law firm Hasselbach",
      "url": " /en/imprint.html"
    },
  

    {
      "slug": "en",
      "title": "",
      "content"	 : "                                                                                                                                About OCR-D            Learn more about the OCR-D project                                                                                Technical Resources              OCR-D Specifications  OCR-D Specifications for CLI, METS, PAGE etc.    OCR-D/core API documentation  Python API documentation of core implementation    OCR-D development best practices  Practical information on distributed development of software and specifications in OCR-D    Ground Truth Guidelines  Guidelines for Ground Truth    PAGE-XML format documentation  Documentation for the PAGE-XML format                                                              User Guides &amp; Info  Setup Guide  How to setup/install the OCR-D stack    User Guide  Instructions how to use OCR-D components    Workflows  Steps of an OCR-D-workflow with sample workflows    Models  Overview of models for different OCR-engines    Glossary  Glossary of technical terms used in OCR-D                                ",
      "url": " /en/"
    },
  

    {
      "slug": "de",
      "title": "",
      "content"	 : "                                                                                                                                Über OCR-D            Erfahren Sie mehr über das Projekt OCR-D                                                                    Technische Dokumentation              OCR-D Spezifikationen  Spezifikationen für CLI, METS, PAGE, etc.    OCR-D/core API Dokumentation  Python API Dokumentation für die core Referenzimplementierung    Best practices für Software Entwicklung in OCR-D  Praktische Informationen zur verteilten Entwicklung von Software und Spezifikationen in OCR-D    Ground Truth Richtlinien  Transkriptionsrichtlinien für Ground Truth    PAGE-XML Formatdokumentation  Dokumentation zum PAGE-XML Format                                                              Nutzungsinformationen und Anleitungen  Setup Anleitung  Schritt-für-Schritt Anleitung zur Installation von OCR-D (aktuell nur auf Englisch verfügbar)    Nutzeranleitung  Instruktionen zum Arbeiten mit OCR-D (aktuell nur auf Englisch verfügbar)    Workflows  Schritte eines OCR-D-Workflows mit Beispielworkflows (aktuell nur auf Englisch verfügbar)    Modelle  Überblick zu Modellen verschiedener OCR-Engines (aktuell nur auf Englisch verfügbar)    Glossar  Fachbegriffe aus dem Bereich der OCR erklärt (aktuell nur auf Englisch verfügbar)                                ",
      "url": " /de/"
    },
  

    {
      "slug": "",
      "title": "",
      "content"	 : "                                                      Deutsch                                                                                          English                                                ",
      "url": " /"
    },
  

    {
      "slug": "en-spec",
      "title": "Specifications",
      "content"	 : "            Intro      Overview of OCR-D technical documentation                  CLI      Command line tools provided by MP                  METS      OCR-D METS conventions                  OCRD-ZIP      METS workspace serialized as ZIP                  ocrd-tool.json      OCR-D tool description                  PAGE      PAGE conventions                  GT Guidelines      OCR-D Ground Truth transcription guidelines                  Dockerfile      OCR-D Dockerfile conventions      ",
      "url": " /en/spec/"
    },
  

    {
      "slug": "slides-2019-03-25-dhd",
      "title": "",
      "content"	 : "Workshop DHD 2019Theorie: Einführung in OCR allgemein20’ @kbaSlidesTheorie: OCR-D Projektuebersicht10’ @ehrmnSlidesTheorie: OCR-D Ground Truth10’ @tboenigSlidesTheorie: OCR-D Repository15’ @VolkerHartmannSlides (Google Slides)Slides (PowerPoint)Demo (hackmd)Theorie: OCR-D Spezifikationen und Software15’ @kbaSlidesPraxis: Installation des OCR-D Stack30’ @kba, @VolkerHartmannSetup guidePause (30’)Praxis: Erstellen von Ground Truth30’ @tboenigGuidePraxis: OCR-D auf existierendes METS anwenden30’ @kbaGuidePraxis: OCR-D auf willkürliche Bilder anwenden15’ @kbaGuidePraxis: OCRD-ZIP erzeugen und untersuchenGuide",
      "url": " /slides/2019-03-25-dhd/"
    },
  

    {
      "slug": "de-spec",
      "title": "",
      "content"	 : "            Intro      Übersicht über die technische Dokumentation zu OCR-D                  CLI      Anforderungen an Kommandozeilentools                  METS      OCR-D METS Konventionen                  OCRD-ZIP      ZIP-Serialisierung METS-basierter Workspaces                  ocrd-tool.json      Die Beschreibungssprache der OCR-D Werkzeuge                  PAGE      PAGE conventions                  GT Richtlinien      OCR-D Ground Truth Transkriptionsrichtlinien                  Dockerfile      OCR-D Dockerfile conventions      ",
      "url": " /de/spec/"
    },
  

    {
      "slug": "en-initial-tests-html",
      "title": "Results and findings of the first OCR-D test",
      "content"	 : "Results and findings of the first OCR-D testBackgroundAt the turn of the year 2019/2020, the OCR-D software was tested for the firsttime in nine pilot libraries. This was to ensure the practical acceptance ofthe software by future, potential users. Therefore the focus was on itsfunctionality and usability in practice. In addition to the partners of thecoordination project, two libraries involved in the module projects as well asfour other libraries took part in the testing. The findings of this first testrun will be incorporated into the further development of the OCR-D prototype.All pilot libraries have some know-how and experience with OCR, as they havealready produced full texts at least on project level or via commercial serviceproviders. The extent to which OCR, which is regarded as important, should becarried out in-house in future and firmly anchored in the digitisationworkflow, is currently discussed in the libraries. Concerning the target groupsof the full texts there are different opinions in the testing institutions.While one third name humanities scholars in general, another third would liketo serve a very broad target group (humanities, digital humanities, computerlinguistics and economics). The remaining libraries only see relatively few,specialised users (Digital Humanities or Computational Linguistics) as thetarget group for OCR texts.The pilot libraries demand the following features of an OCR software:  High recognition rate of layout and text  Cost-effective use  Quick adaptability/troubleshooting  Modularity  Output in standard formats  Connection to existing workflows  Well-documented interfaces  Word coordinates  Trainability  Extensive GT corporaHigh quality of text recognition is most important, whereas the othercharacteristics are only named each by some of the pilot libraries and shouldbe considered as desired but subordinate optional features.Evaluation of the software testsIn order to ensure the comparability of the individual tests in the pilotlibraries, a questionnaire distributed to the pilot libraries at the beginningof the test. In this questionnaire, the general conditions of the test run,e.g. the technical equipment used and the tested OCR-D processors, as well asthe documentation of the software, interfaces, functionality and usability ofthe software, its possibilities for being integrated into existing workflowsand the required output formats are recorded. Furthermore, recognition quality,functionality and usability, open requirements and positive features of theOCR-D software as well as the results of the test were to be described.During the test phase, the various options for installing the OCR-D softwarewith and without a Docker container were tried out and the software wassuccessfully installed on a wide range of servers with varying processingpower, some of them virtual. For Non-Intel CPU architectures (ARM, PowerPC64)this was more complicated and time consuming, as individual Python packageswere not packaged for these computers and had to be adapted manually. Thecomplete installation of all available OCR-D processors (ocrd_all), which wasdeveloped during the test phase, was confirmed as the least complex variant andwas therefore the most recommendable. No pilot library integrated the OCR-Dsoftware In a workflow software such as Kitodo, as this would have meant toomuch work for a simple test run.The usage of the numerous OCR-D processors was described as a challenge.Calling the processors was not so much of a problem than understanding theirrespective areas of application and, in particular, selecting and combining theprocessors to form meaningful workflows. For the first test, besides thetechnical documentation of the software, there was no overall documentation onits use available yet, which is also aimed at users inexperienced in OCR. Therequirements and wishes of the testers for such a documentation were taken intoaccount when preparing the manuals, which are now available in the user areaof the OCR-D website and will be improvedcontinuously.The OCR-D software runs very stable, no library reported any crashes. All ofthe required output formats are already provided, whereas requested changes onOCR-D’s interfaces are already planned for the further development of theprototype.The recognition quality was only checked on individual pages by the respectivepilot libraries, as there is no Ground Truth available for the test books.Overall, the results of this first test run are promising. The MannheimUniversity Library has tested OCR-D with a focus on Tesseract processors onfive prints from the 16th to 19th century. On antiqua prints from the 17th and18th centuries and a blackletter (Fraktur) text from the 19th century, asexpected, the best results of - in the case of the antiqua - significantly lessthan 0.1 CER were achieved for the raw data, whereas the blackletter printsfrom the 17th century was slightly above 0.1 CER. The greatest challenge wasthe 16th century blackletter, where only a CER of just under 0.16 was reached.The BBAW provides a comprehensive insight into their testing, by making theirreport and data publiclyavailable.The OCR-D testers formulate some desiderata especially with regard todocumentation, quality and usability of the processors as well as their futurescalability. The requirements for the documentation of the OCR-D software havealready been implemented to a large extent, though documentation as a whole isregarded as a continuous task which must successively include above allpractical experience in the application of the OCR-D software. For theprocessors, some improvements would still be desirable, especially in layoutrecognition and post-correction. The corresponding developments of the OCR-Dmodule projects could only be tested to a limited extent due to theirdevelopment stage or their special technical requirements (GPU). Hopefully, theabove mentioned desiderata can be met with their results or with furthermodels. For the use of the OCR-D software in mass digitisation, the runtime ofseveral processors - as originally planned for the third project phase - stillneeds to be optimised, and the possibilities for parallelisation should also beexpanded.The testers positively emphasized the modular and transparent structure of theOCR-D software, which distinguishes it in particular from other OCR solutionsand allows the its users to configure optimal workflows for particular usecases. Furthermore, all OCR-D source code is freely available and can befurther developed by experts specialized in their respective fields and can beadapted in-house for usage in experiments on the OCR workflow without extensiveprogramming work. In case of questions and problems, the developers quicklyprovide low-threshold support. All in all, it is comparatively easy to initiatethe robustly running OCR-D full-text generation process, which is still in needof further optimization but already delivers promising results.",
      "url": " /en/initial-tests.html"
    },
  

    {
      "slug": "en-spec-intro-html",
      "title": "OCR-D Specs Overview",
      "content"	 : "OCR-D Specs OverviewSince OCR-D focuses on improving access to mass digitization for historicalprints, it is important that its tools are sufficiently uniform in their interfacesand data access patterns to support the widest possible application withinGLAM digitization workflows.This website lays out a set of conventions and interface definitions that mustbe implemented by the OCR-D module projects (MP) to be usable within the OCR-D ecosphere.CLISoftware developed by MP must be executable with acommand line interface (CLI) on a Linux OS. CLI are straightforward to run andtest and can be easily embedded in automated setups. The mechanics of OCR-Dconformant CLI tools are laid out in the CLI specs.METSTo allow processing OCR-related data in a digitization workflow, a uniform dataexchange format is necessary. OCR-D decided to use the widely used METS formatand has developed conventions on how MP must access and manipulateMETS data in order to be interoperable.ocrd-tool.jsonInteroperability needs metadata, both descriptive and technical. OCR-D hasdeveloped a format that allows MP to express general informationabout themselves and detailed information about the tools they develop.RESTOCR-D will offer RESTful access to the MP CLI based on HTTP, usingthe Open API / Swagger set of tools.DockerfileDocker is a widely used system for containerization of software. MPs areencouraged to package the tools they develop as a docker image by providing aDockerfile. OCR-D offers recommendations on how the Dockerfile should bestructured.",
      "url": " /en/spec/intro.html"
    },
  

    {
      "slug": "de-spec-intro-html",
      "title": "",
      "content"	 : "OCR-D Specs OverviewSince OCR-D focuses on improving access to mass digitization for historicalprints, it is important that its tools are sufficiently uniform in their interfacesand data access patterns to support the widest possible application withinGLAM digitization workflows.This website lays out a set of conventions and interface definitions that mustbe implemented by the OCR-D module projects (MP) to be usable within the OCR-D ecosphere.CLISoftware developed by MP must be executable with acommand line interface (CLI) on a Linux OS. CLI are straightforward to run andtest and can be easily embedded in automated setups. The mechanics of OCR-Dconformant CLI tools are laid out in the CLI specs.METSTo allow processing OCR-related data in a digitization workflow, a uniform dataexchange format is necessary. OCR-D decided to use the widely used METS formatand has developed conventions on how MP must access and manipulateMETS data in order to be interoperable.ocrd-tool.jsonInteroperability needs metadata, both descriptive and technical. OCR-D hasdeveloped a format that allows MP to express general informationabout themselves and detailed information about the tools they develop.RESTOCR-D will offer RESTful access to the MP CLI based on HTTP, usingthe Open API / Swagger set of tools.DockerfileDocker is a widely used system for containerization of software. MPs areencouraged to package the tools they develop as a docker image by providing aDockerfile. OCR-D offers recommendations on how the Dockerfile should bestructured.",
      "url": " /de/spec/intro.html"
    },
  

    {
      "slug": "en-kwalitee-html",
      "title": "Kwalitee",
      "content"	 : "           GitHub              Last update              Number of contributors                 cor-asv-ann    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-annn    OCR post-correction with encoder-attention-decoder LSTMsnn[![CircleCI](https://circleci.com/gh/ASVLeipzig/cor-asv-ann.svg?style=svg)](https://circleci.com/gh/ASVLeipzig/cor-asv-ann)nn## IntroductionnnThis is a tool for automatic OCR _post-correction_ (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the _character level_ with a model architecture akin to neural machine translation, i.e. a stacked **encoder-decoder** network with attention mechanism. nnThe **attention model** always applies to full lines (in a _global_ configuration), and uses a linear _additive_ alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a _soft alignment_ between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start &quot;forgetting&quot; the input altogether at some point in the line and behave like unconditional LM generators.)nn...FIXME: mention: n- stacked architecture (with bidirectional bottom and attentional top), configurable depth/widthn- weight tyingn- underspecification and gapn- confidence input and alternative inputn- CPU/GPU optionn- incremental training, LM transfer, shallow transfern- evaluation (CER, PPL)nn### Processing PAGE annotationsnnWhen applied on PAGE-XML (as OCR-D workspace processor), this component also allows processing below the `TextLine` hierarchy level, i.e. on `Word` or `Glyph` level. For that it uses the soft alignment scores to calculate an optimal hard alignment path for characters, and thereby distributes the transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary.nn...nn### Architecturenn...FIXME: show!nn### Input with confidence and/or alternativesnn...FIXME: explain!nn### Multi-OCR inputnnnot yet!nn### ModesnnWhile the _encoder_ can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a **beam-search** _decoder_ requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also **greedily** use the best output only for each position (without beam search). And in doing so, another option is to feed back the softmax output directly into the decoder input instead of its argmax unit vector. This effectively passes the full probability distribution from state to state, which (not very surprisingly) can increase correction accuracy quite a lot – it can get as good as a medium-sized beam search results. This latter option also allows to run in parallel again, which is also much faster – consuming up to ten times less CPU time.nnThererfore, the backend function `lib.Sequence2Sequence.correct_lines` can operate the encoder-decoder network in either of the following modes:nn#### _fast_nnDecode greedily, but feeding back the full softmax distribution in batch mode.nn#### _greedy_nnDecode greedily, but feeding back the argmax unit vectors for each line separately.nn#### _default_nnDecode beamed, feeding back the argmax unit vectors for the best history/output hypotheses of each line. More specifically:nn&amp;gt; Start decoder with start-of-sequence, then keep decoding untiln&amp;gt; end-of-sequence is found or output length is way off, repeatedly.n&amp;gt; Decode by using the best predicted output characters and several next-bestn&amp;gt; alternatives (up to some degradation threshold) as next input.n&amp;gt; Follow-up on the N best overall candidates (estimated by accumulatedn&amp;gt; score, normalized by length and prospective cost), i.e. do A*-liken&amp;gt; breadth-first search, with N equal `batch_size`.n&amp;gt; Pass decoder initial/final states from character to character,n&amp;gt; for each candidate respectively.n&amp;gt; Reserve 1 candidate per iteration for running through `source_seq`n&amp;gt; (as a rejection fallback) to ensure that path does not fall off then&amp;gt; beam and at least one solution can be found within the search limits.nn### EvaluationnnText lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (`cor-asv-ann-eval`) and PAGE-XML annotations (`ocrd-cor-asv-ann-evaluate`). nnDistances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.nnThere are a number of distance metrics available (all operating on grapheme clusters, not mere codepoints):n- `Levenshtein`:  n  simple unweighted edit distance (fastest, standard; GT level 3)n- `NFC`:  n  like `Levenshtein`, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)n- `NFKC`:  n  like `Levenshtein`, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for `ſ`, which is already normalized to `s`)n- `historic_latin`:  n  like `Levenshtein`, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter `e` as in `Wuͤſte` instead of  to `Wüſte`, `ſ` vs `s`, or quotation/citation marks; GT level 1)nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-venv`` or ``python3-venv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies:n```shellnmake depsn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtn```nnTo install this module, then do:n```shellnmake installn```nWhich is the equivalent of:n```shellnpip install .n```nn## UsagennThis packages has the following user interfaces:nn### command line interface `cor-asv-ann-train`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-eval`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-repl`nninteractivenn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-process`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). nn...nn```jsonn    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    }n```nn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-evaluate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Inputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.nn...nn```jsonn    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;n        }n      }n    }n```nn...nn## Testingnnnot yet!n...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;,n  &quot;version&quot;: &quot;0.1.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    },n    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;n        },n        &quot;confusion&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;minimum&quot;: 0,n          &quot;default&quot;: 0,n          &quot;description&quot;: &quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-ann-trainn    - cor-asv-ann-evaln    - cor-asv-ann-repln    - ocrd-cor-asv-ann-processn    - ocrd-cor-asv-ann-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_ann&#39;,n    version=&#39;0.1.2&#39;,n    description=&#39;sequence-to-sequence translator for noisy channel error correction&#39;,n    long_description=README,n    author=&#39;Robert Sachunsky&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-ann&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-ann-train=ocrd_cor_asv_ann.scripts.train:cli&#39;,n            &#39;cor-asv-ann-eval=ocrd_cor_asv_ann.scripts.eval:cli&#39;,n            &#39;cor-asv-ann-repl=ocrd_cor_asv_ann.scripts.repl:cli&#39;,n            &#39;ocrd-cor-asv-ann-process=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_process&#39;,n            &#39;ocrd-cor-asv-ann-evaluate=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 00:58:56 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;49&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-ann&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-ann-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align different textline annotations and compute distance&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-evaluate&quot;, &quot;parameters&quot;=&amp;gt;{&quot;confusion&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;minimum&quot;=&amp;gt;0, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;metric&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;Levenshtein&quot;, &quot;description&quot;=&amp;gt;&quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;, &quot;enum&quot;=&amp;gt;[&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/evaluation&quot;]}, &quot;ocrd-cor-asv-ann-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cor-asv-ann-evaluate] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cor-asv-ann-evaluate.parameters.confusion] Additional properties are not allowed (&#39;minimum&#39; was unexpected)n  [tools.ocrd-cor-asv-ann-evaluate.steps.0] &#39;recognition/evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-ann&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_ann&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}         cor-asv-fst    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-fstn    OCR post-correction with error/lexicon Finite State Transducers andn    chararacter-level LSTM language modelsnn## Introductionnnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnIn addition to the requirements listed in `requirements.txt`, the toolnrequires then[pynini](http://www.opengrm.org/twiki/bin/view/GRM/Pynini)nlibrary, which has to be installed from source.nn## UsagennThe package has two user interfaces:nn### Command Line InterfacennThe package contains a suite of CLI tools to work with plaintext data (prefix:n`cor-asv-fst-*`). The minimal working examples and data formats are describednbelow. Additionally, each tool has further optional parameters - for a detailedndescription, call the tool with the `--help` option.nn#### `cor-asv-fst-train`nnTrain FST models. The basic invocation is as follows:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -t TRAINING_FILEn```nnThis will create two transducers, which will be stored in `LEXICON_FILE` andn`ERROR_MODEL_FILE`, respectively. As the training of the lexicon and the errornmodel is done independently, any of them can be skipped by omitting thenrespective parameter.nn`TRAINING_FILE` is a plain text file in tab-separated, two-column formatncontaining a line of OCR-output and the corresponding ground truth line:nn```n» Bergebt mir, daß ih niht weiß, wiet»Vergebt mir, daß ich nicht weiß, wienaus dem (Geiſte aller Nationen Mahrunqtaus dem Geiſte aller Nationen NahrungnKannſt Du mir die re&amp;lt;hée Bahn niché zeigen ?tKannſt Du mir die rechte Bahn nicht zeigen?nfrag zu bringen. —ttrag zu bringen. —nſie ins irdij&amp;lt;he Leben hinein, Mit leichtem,tſie ins irdiſche Leben hinein. Mit leichtem,n```nnEach line is treated independently. Alternatively to the above, the trainingndata may also be supplied as two files:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -i INPUT_FILE -g GT_FILEn```nnIn this variant, `INPUT_FILE` and `GT_FILE` are both in tab-separated,ntwo-column format, in which the first column is the line ID and the second thenline:nn```n&amp;gt;=== INPUT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ih denke. Aber was die ſelige Frau Geheimräth1nnalexis_ruhe01_1852_0035_019     „Das fann ich niht, c’esl absolument impos-nalexis_ruhe01_1852_0087_027     rend. In dem Augenbli&amp;gt; war 1hr niht wohl zunalexis_ruhe01_1852_0099_012     ür die fle ſich ſchlugen.“nalexis_ruhe01_1852_0147_009     ſollte. Nur Über die Familien, wo man ſie einführennn&amp;gt;=== GT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ich denke. Aber was die ſelige Frau Geheimräthinnalexis_ruhe01_1852_0035_019     „Das kann ich nicht, c&#39;est absolument impos—nalexis_ruhe01_1852_0087_027     rend. Jn dem Augenblick war ihr nicht wohl zunalexis_ruhe01_1852_0099_012     für die ſie ſich ſchlugen.“nalexis_ruhe01_1852_0147_009     ſollte. Nur über die Familien, wo man ſie einführenn```nn#### `cor-asv-fst-process`nnThis tool applies a trained model to correct plaintext data on a line basis.nThe basic invocation is:nn```shellncor-asv-fst-process -i INPUT_FILE -o OUTPUT_FILE -l LEXICON_FILE -e ERROR_MODEL_FILE (-m LM_FILE)n```nn`INPUT_FILE` is in the same format as for the training procedure. `OUTPUT_FILE`ncontains the post-correction results in the same format.nn`LM_FILE` is a `ocrd_keraslm` language model - if supplied, it is used fornrescoring.nn#### `cor-asv-fst-evaluate`nnThis tool can be used to evaluate the post-correction results. The minimalnworking invocation is:nn```shellncor-asv-fst-evaluate -i INPUT_FILE -o OUTPUT_FILE -g GT_FILEn```nnAdditionally, the parameter `-M` can be used to select the evaluation measuren(`Levenshtein` by default). The files should be in the same two-column formatnas described above.nn### [OCR-D processor](https://ocr-d.github.io/cli) interface `ocrd-cor-asv-fst-process`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML)ndocuments in an [OCR-D](https://ocr-d.github.io) annotation workflow.nInput files need a textual annotation (`TextEquiv`) on the givenn`textequiv_level` (currently _only_ `word`!).nn...nn```jsonn  &quot;tools&quot;: {n    &quot;cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n```nn...nn## Testingnn...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;,n  &quot;version&quot;: &quot;0.1.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-fst-trainn    - cor-asv-fst-processn    - cor-asv-fst-evaluaten    - ocrd-cor-asv-fst-processn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_fst&#39;,n    version=&#39;0.2.0&#39;,n    description=&#39;OCR post-correction with error/lexicon Finite State &#39;n                &#39;Transducers and character-level LSTMs&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Maciej Sumalvico, Robert Sachunsky&#39;,n    author_email=&#39;sumalvico@informatik.uni-leipzig.de, &#39;n                 &#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-fst&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    test_suite=&#39;tests&#39;,n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-fst-train=ocrd_cor_asv_fst.scripts.train:main&#39;,n            &#39;cor-asv-fst-process=ocrd_cor_asv_fst.scripts.process:main&#39;,n            &#39;cor-asv-fst-evaluate=ocrd_cor_asv_fst.scripts.evaluate:main&#39;,n            &#39;ocrd-cor-asv-fst-process=ocrd_cor_asv_fst.wrapper.cli:ocrd_cor_asv_fst&#39;,n        ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 17:54:58 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;178&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-fst&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-fst-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-fst-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;100, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during beam search in language modelling&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;errorfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for error model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;keraslm_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for language model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lexiconfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for lexicon model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the FST output confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pruning_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;transition weight for pruning the hypotheses in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rejection_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;transition weight (per character) for unchanged input in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;, &quot;enum&quot;=&amp;gt;[&quot;word&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-fst&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Maciej Sumalvico, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sumalvico@informatik.uni-leipzig.de, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_fst&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}         ocrd_calamari    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:edgenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /buildnCOPY Makefile .nCOPY setup.py .nCOPY ocrd-tool.json .nCOPY requirements.txt .nCOPY ocrd_calamari ocrd_calamarinnRUN make calamari/buildnRUN pip3 install .nnENTRYPOINT [&quot;/usr/local/bin/ocrd-calamari-recognize&quot;]nn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_calamarinn&amp;gt; Recognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn[![image](https://circleci.com/gh/OCR-D/ocrd_calamari.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_calamari)n[![image](https://img.shields.io/pypi/v/ocrd_calamari.svg)](https://pypi.org/project/ocrd_calamari/)n[![image](https://codecov.io/gh/OCR-D/ocrd_calamari/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_calamari)nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Installationnn### From PyPInn```npip install ocrd_calamarin```nn### From Reponn```shnpip install .n```nn## Install modelsnnDownload models trained on GT4HistOCR data:nn```nmake gt4histocr-calamarinls gt4histocr-calamarin```nn## Example Usagenn~~~nocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn~~~nnWith `test-parameters.json`:n~~~n{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n~~~nn## Development &amp;amp; TestingnFor information regarding development and testing, please seen[README-DEV.md](README-DEV.md).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/kba/ocrd_calamari&quot;,n  &quot;version&quot;: &quot;0.0.3&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-calamari-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-calamari-recognize&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Recognize lines with Calamari&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-CALAMARI&quot;n      ],n      &quot;parameters&quot;: {n        &quot;checkpoint&quot;: {n          &quot;description&quot;: &quot;The calamari model files (*.ckpt.json)&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;file&quot;, &quot;cacheable&quot;: truen        },n        &quot;voter&quot;: {n          &quot;description&quot;: &quot;The voting algorithm to use&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;confidence_voter_default_ctc&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom pathlib import Pathnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_calamari&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;Calamari bindings&#39;,n    long_description=Path(&#39;README.md&#39;).read_text(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Mike Gerber&#39;,n    author_email=&#39;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&#39;,n    url=&#39;https://github.com/kba/ocrd_calamari&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=Path(&#39;requirements.txt&#39;).read_text().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-calamari-recognize=ocrd_calamari.cli:ocrd_calamari_recognize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 16:14:13 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;84&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-calamari-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize lines with Calamari&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-calamari-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-CALAMARI&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;The calamari model files (*.ckpt.json)&quot;, &quot;format&quot;=&amp;gt;&quot;file&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;voter&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;confidence_voter_default_ctc&quot;, &quot;description&quot;=&amp;gt;&quot;The voting algorithm to use&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_calamari&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_calamarinnRecognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Example Usagenn```shnocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn```nnWith `test-parameters.json`:nn```jsonn{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n```nnTODOn----nn* Support Calamari&#39;s &quot;extended prediction data&quot; outputn* Currently, the processor only supports a prediction using confidence voting of multiple models. While this isn  superior, it makes sense to support single model prediction, too.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-calamari&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;numpy&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;calamari-ocr (==0.3.5)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;click&quot;, &quot;ocrd (&amp;gt;=1.0.0b11)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Calamari bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;6229919, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;sha256&quot;=&amp;gt;&quot;cf08ec027390519d465f6be861e5672b48e7b39b3d1f8e13e54cb401034355b6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9320, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:11&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:11.044376Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/30/62/d8efee35233443d444fc49f7f89792979234c1d735285d599f989e63cee1/ocrd_calamari-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;sha256&quot;=&amp;gt;&quot;51a09088d677799258d8c796dbaba8a1b44a318d06c060314499f708fa37bdd4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3884, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:13&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:13.643406Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/46/1a/b5f02d113aa7810cb773f0b586d1202c254d22e4bf3c6b829d937da2c1b0/ocrd_calamari-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;sha256&quot;=&amp;gt;&quot;454164c6b1c063b76c5189ae596115499bffd6e944c896dee3b03f08852f5680&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5247, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:56.460224Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/53/c05186a309284a22d4f1f0399a5fb241d7b11fb0e5b94c33fa8ae229a6fc/ocrd_calamari-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;sha256&quot;=&amp;gt;&quot;39e0f5b334a735fb8fa20e5490dcd07a96a620bc785c8e2b31f64a23fa13a6fe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3952, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:57.972949Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/9d/cc/de53bfd3c2b666cab5ef199c93902c85bb83ee03d923e9ef7abe87377857/ocrd_calamari-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari&quot;}         ocrd_im6convert    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivennENV PREFIX=/usr/localnnWORKDIR /buildnCOPY ocrd-im6convert .nCOPY ocrd-tool.json .nCOPY Makefile .nnRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install apt-utils &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    makennRUN make deps-ubuntu installnnENV DEBIAN_FRONTEND teletypenn# no fixed entrypoint (e.g. also allow `convert` etc)nCMD [&quot;/usr/local/bin/ocrd-im6convert&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_imageconvertnn&amp;gt; Thin wrapper around convert(1)nn## Introductionnn[ImageMagick&#39;s](https://imagemagick.org) `convert` CLI contains a treasure trove of image operations. This wrapper aims to provide much of that as an [OCR-D compliant processor](https://ocr-d.github.io/CLI).nn## InstallationnnThis module requires GNU make (for installation) and the ImageMagick command line tools (at runtime). On Ubuntu 18.04 (or similar), you can install them by running:nn    sudo apt-get install maken    sudo make deps-ubuntu # or: apt-get install imagemagicknnMoreover, an installation of [OCR-D core](https://github.com/OCR-D/core) is needed:nn    make deps # or: pip install ocrdnnThis will install the Python package `ocrd` in your current environment. (Setting up a [venv](https://ocr-d.github.io/docs/guide#python-setup) is strongly recommended.)nnLastly, the provided shell script `ocrd-im6convert` works best when copied into your `PATH`, referencing its ocrd-tool.json under a known path. This can be done by running:nn    make installnnThis will copy the binary and JSON file under `$PREFIX`, which variable you can override to your needs. The default value is to use `PREFIX=$VIRTUAL_ENV` if you have already activated a venv, or `PREFIX=$PWD/.local` (i.e. under the current working directory).nn## UsagennThis package provides `ocrd-im6convert` as a [OCR-D processor](https://ocr-d.github.com/cli) (command line interface). It uses the following parameters:nn```JSONn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }n```nnCf. [IM documentation](https://imagemagick.org/script/command-line-options.php) or man-page `convert(1)` for formats and options.nn### Examplenn    ocrd-im6convert -I OCR-D-IMG -O OCR-D-IMG-SMALL -p &#39;{ &quot;output-format&quot;: &quot;image/png&quot;, &quot;output-options&quot;: &quot;-resize 24%&quot; }&#39;nn(This downscales the images in the input file group `OCR-D-IMG` to 24% and stores them as PNG files under the output file group `OCR-D-IMG-SMALL`.)nn## TestingnnNone yetn&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_im6convert&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {nn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Dec 27 13:38:58 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;27&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_im6convert&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-im6convert&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Convert and transform images&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-im6convert&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;input-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-format&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Desired media type of output&quot;, &quot;enum&quot;=&amp;gt;[&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_im6convert&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;}         ocrd_keraslm    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnn[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_keraslm.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_keraslm)nn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;für die Wiſſen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so schädlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_keraslm&quot;,n  &quot;version&quot;: &quot;0.3.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 10n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the input confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - keraslm-raten    - ocrd-keraslm-raten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_keraslm&#39;,n    version=&#39;0.3.2&#39;,n    description=&#39;character-level language modelling in Keras&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_keraslm&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    extras_require={n        &#39;plotting&#39;: [n            &#39;sklearn&#39;,n            &#39;matplotlib&#39;,n            ]n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;keraslm-rate=ocrd_keraslm.scripts.run:cli&#39;,n            &#39;ocrd-keraslm-rate=ocrd_keraslm.wrapper.cli:ocrd_keraslm_rate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 9 10:13:52 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;0.3.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;91&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-keraslm-rate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-keraslm-rate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;, &quot;OCR-D-COR-CIS&quot;, &quot;OCR-D-COR-ASV&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-LM&quot;], &quot;parameters&quot;=&amp;gt;{&quot;alternative_decoding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the input confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.3.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_keraslm&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnnn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;für die Wiſſen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so schädlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-keraslm&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/0.3.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0)&quot;, &quot;click&quot;, &quot;keras (&amp;gt;=2.2.4)&quot;, &quot;numpy&quot;, &quot;tensorflow (&amp;lt;2.0)&quot;, &quot;h5py&quot;, &quot;networkx (&amp;gt;=2.0)&quot;, &quot;sklearn; extra == &#39;plotting&#39;&quot;, &quot;matplotlib; extra == &#39;plotting&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;character-level language modelling in Keras&quot;, &quot;version&quot;=&amp;gt;&quot;0.3.2&quot;}, &quot;last_serial&quot;=&amp;gt;6158523, &quot;releases&quot;=&amp;gt;{&quot;0.3.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3ec82a615434e90028722586c6123e4a1887e36b0a57f06566a291892280e88&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2.py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34192, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:09&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:09.567407Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/ba/8f5f0f1801ea99221c772357e2c79d9935a88e89873924e557e24aea6c33/ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;sha256&quot;=&amp;gt;&quot;665a9bf1d7bc46f497d71638b2d33608062edd16ac11b9cff05be56eacda53c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32287, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:12.437293Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/79/0e/744edc5497d706ac558b90d8d85b2e52ad5fb6b794c6f9cb44fc0aaa341a/ocrd_keraslm-0.3.1.tar.gz&quot;}], &quot;0.3.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}         ocrd_kraken    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_kraken ./ocrd_krakennRUN pip3 install --upgrade pipnRUN pip3 install .nnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_kraken&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-kraken-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-binarize&quot;,n      &quot;input_file_grp&quot;: &quot;OCR-D-IMG&quot;,n      &quot;output_file_grp&quot;: &quot;OCR-D-IMG-BIN&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;description&quot;: &quot;Binarize images with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;level-of-operation&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;page&quot;,n          &quot;enum&quot;: [&quot;page&quot;, &quot;block&quot;, &quot;line&quot;]n        }n      }n    },n    &quot;ocrd-kraken-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-segment&quot;,n      &quot;categories&quot;: [n        &quot;Layout analysis&quot;n      ],n      &quot;steps&quot;: [n        &quot;layout/segmentation/region&quot;n      ],n      &quot;description&quot;: &quot;Block segmentation with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;text_direction&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Sets principal text direction&quot;,n          &quot;enum&quot;: [&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;],n          &quot;default&quot;: &quot;horizontal-lr&quot;n        },n        &quot;script_detect&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;Enable script detection on segmenter output&quot;,n          &quot;default&quot;: falsen        },n        &quot;maxcolseps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2},n        &quot;scale&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0},n        &quot;black_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false},n        &quot;white_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false}n      }n    },n    &quot;ocrd-kraken-ocr&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-ocr&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;OCR with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;lines-json&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;url&quot;,n          &quot;required&quot;: &quot;true&quot;,n          &quot;description&quot;: &quot;URL to line segmentation in JSON&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls two binaries:nn    - ocrd-kraken-binarizen    - ocrd-kraken-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_kraken&#39;,n    version=&#39;0.1.1&#39;,n    description=&#39;kraken bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael Würzner&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_kraken&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0a4&#39;,n        &#39;kraken == 0.9.16&#39;,n        &#39;click &amp;gt;= 7&#39;,n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-kraken-binarize=ocrd_kraken.cli:ocrd_kraken_binarize&#39;,n            &#39;ocrd-kraken-segment=ocrd_kraken.cli:ocrd_kraken_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Oct 21 20:52:26 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;85&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-kraken-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG&quot;, &quot;output_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG-BIN&quot;, &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;block&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-kraken-ocr&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;OCR with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-ocr&quot;, &quot;parameters&quot;=&amp;gt;{&quot;lines-json&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;URL to line segmentation in JSON&quot;, &quot;format&quot;=&amp;gt;&quot;url&quot;, &quot;required&quot;=&amp;gt;&quot;true&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-kraken-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Block segmentation with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-segment&quot;, &quot;parameters&quot;=&amp;gt;{&quot;black_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;script_detect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Enable script detection on segmenter output&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;text_direction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;horizontal-lr&quot;, &quot;description&quot;=&amp;gt;&quot;Sets principal text direction&quot;, &quot;enum&quot;=&amp;gt;[&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;white_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-kraken-binarize.input_file_grp] &#39;OCR-D-IMG&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.output_file_grp] &#39;OCR-D-IMG-BIN&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.parameters.level-of-operation] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.maxcolseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.scale] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.black_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.white_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-ocr] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-ocr.parameters.lines-json.required] &#39;true&#39; is not of type &#39;boolean&#39;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_kraken&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-kraken&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/0.1.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0a4)&quot;, &quot;kraken (==0.9.16)&quot;, &quot;click (&amp;gt;=7)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;kraken bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;last_serial&quot;=&amp;gt;6008613, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;sha256&quot;=&amp;gt;&quot;a0de30df5e8b7d9fe1ed3343a8fa3a413620828a2cdf46bcab8d77e864869d53&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10691, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:30&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:30.728403Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b4/52/aea22b8cfab48546e10118e0eb7e70dc108fe633af3e07194dfd04e00fb2/ocrd_kraken-0.0.2-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;sha256&quot;=&amp;gt;&quot;805fb1aa976f9ee1275e347b1fee2413af3ea7cc8972af84464c6f4253ebdd6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9634, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:32.808242Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/00/a9843c2c73a086c1f66e28d6b0d64053ecd66995daddfb5c0f28e566c9f7/ocrd_kraken-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;sha256&quot;=&amp;gt;&quot;a6464f3559acfb36947687d4e2e70cd7cb7e655d70234696e2e7c1b07f99bab8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5003, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:34.101144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/bb/9e4299ec1d5f494e7bf14de447f361455f36ea0255181871ee937aae0528/ocrd_kraken-0.0.2.tar.gz&quot;}], &quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;sha256&quot;=&amp;gt;&quot;4e6b7e9d1930de1f0bd57dfd63f9418c4345842e7cc8fdd9b147e7d378b8fe51&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10442, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:43.225080Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/4b/d7027ac27e1228cf9aa3ecd94e412b371b2a63ab2c93c1b77ad5414380c1/ocrd_kraken-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;sha256&quot;=&amp;gt;&quot;9bec610685e29d29e0614f2dfc300d201fbbff3f728140536031f14e4e65584c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4121, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:44.655031Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cb/35/7be3dd70b97e276ce2300dddf165bfc21c0e469c2626d7d531a07b8bf0fb/ocrd_kraken-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}         ocrd_ocropy    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_ocropy ./ocrd_ocropynRUN pip3 install --upgrade pipnRUN make deps installnnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginen&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_ocropy&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-ocropy-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-ocropy-segment&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;description&quot;: &quot;Segment page&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE&quot;],n      &quot;parameters&quot;: {n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.1},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;,&quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: false},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.2},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 8},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.0},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0}n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls one binary:nn    - ocrd-ocropy-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setupnnsetup(n    name=&#39;ocrd_ocropy&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;ocropy bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_ocropy&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=[&#39;ocrd_ocropy&#39;],n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0b8&#39;,n        &#39;ocrd-fork-ocropy &amp;gt;= 1.4.0a3&#39;,n        &#39;click&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-ocropy-segment=ocrd_ocropy.cli:ocrd_ocropy_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jun 11 14:51:00 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;66&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_ocropy&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginennn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-ocropy&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b8)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;click&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;ocropy bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;4979689, &quot;releases&quot;=&amp;gt;{&quot;0.0.1a1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;sha256&quot;=&amp;gt;&quot;1dc3926e7c28ecb52260c42d0b3b6b3cc3d2964b13ea994601219269c8072d89&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6462, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:48&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:48.327057Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c7/ce/9f578c500afbffba6de78fb1fb0d881c23ddb794256a276e4277d5ad7c25/ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fc72a46a9e3bc7fd601aa6c00992debe566f1838b95bbd61e8c746b3abd0d673&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6105, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:50&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:50.204116Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8f/a1/2030fb1c2c08cac624a7640daa6a12c3d115a52a9d7d66de5c6b427bbbde/ocrd_ocropy-0.0.1a1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;sha256&quot;=&amp;gt;&quot;a1827b7fb49a27e297fb01ceea45c2272d996f498c576637e42d8008d28dfe9b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10625, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:23&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:23.779614Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7f/46/222d127fe28c522ab65448bd552f9b9b66ec6e5582f8cc7e2ee57f5450a5/ocrd_ocropy-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;sha256&quot;=&amp;gt;&quot;d1e4cd90fff395e332814f51de1b46533ac88ea72f99f4502524c0c659572519&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5855, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:25.438144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/18/c634cc95db36cfa523a75f3ae4e5ee3055b8bcf56969bc3231cdddb3d082/ocrd_ocropy-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}         ocrd_olena    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# Patch and build Olena from Git, thenn# Install OCR-D wrapper for binarizationnFROM ocrd/corennMAINTAINER OCR-DnnENV PREFIX=/usr/localnnWORKDIR /build-olenanCOPY .gitmodules .nCOPY Makefile .nCOPY ocrd-tool.json .nCOPY ocrd-olena-binarize .nnENV DEPS=&quot;g++ make automake git&quot;nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends $DEPS &amp;amp;&amp;amp; n    make deps-ubuntu &amp;amp;&amp;amp; n    git init &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/olena.git repo/olena &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/assets.git repo/assets &amp;amp;&amp;amp; n    make build-olena install clean-olena &amp;amp;&amp;amp; n    apt-get -y remove $DEPS &amp;amp;&amp;amp; n    apt-get -y autoremove &amp;amp;&amp;amp; apt-get clean &amp;amp;&amp;amp; n    rm -fr /build-olenannWORKDIR /datanVOLUME /datann#ENTRYPOINT [&quot;/usr/bin/ocrd-olena-binarize&quot;]n#CMD [&quot;--help&quot;]nCMD [&quot;/usr/bin/ocrd-olena-binarize&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_olenann&amp;gt; Binarize with Olena/scribonn[![Build Status](https://travis-ci.org/OCR-D/ocrd_olena.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_olena)n[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_olena.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_olena)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/core.svg)](https://hub.docker.com/r/ocrd/olena/tags/)nn## Requirementsnn```nmake deps-ubuntun```nn...will try to install the required packages on Ubuntu.nn## Installationnn```nmake build-olenan```nn...will download, patch and build Olena/scribo from source, and install locally (in VIRTUAL_ENV or in CWD/local).nn```nmake installn```nn...will do that, but additionally install `ocrd-binarize-olena` (the OCR-D wrapper).nn## Testingnn```nmake testn```nn...will clone the assets repository from Github, make a workspace copy, and run checksum tests for binarization on them.nn## UsagennThis package has the following user interfaces:nn### command line interface `scribo-cli`nnConverts images in any format to netpbm (monochrome portable bitmap).nn```nUsage: scribo-cli [version] [help] COMMAND [ARGS]nnList of available COMMAND argument:nn  Full Toolchainsn  ---------------nnn   * On documentsnn     doc-ppct       Common preprocessing before looking for text.nn     doc-ocr           Find and recognize text. Output: the actual textn     tt       and its location.nn     doc-dia           Analyse the document structure and extract then     tt       text. Output: an XML file with region and textn     tt       information.nnnn   * On picturesnn     pic-loc           Try to localize text if there&#39;s any.nn     pic-ocr           Localize and try to recognize text.nnnn  Toolsn  -----nnn     * xml2doct       Convert the XML results of document toolchainsn       tt       into user documents (HTML, PDF...).nnn  Algorithmsn  ----------nnn   * Binarizationnn     sauvola           Sauvola&#39;s algorithm.nn     sauvola-ms        Multi-scale Sauvola&#39;s algorithm.nn     sauvola-ms-fg     Extract foreground objects and run multi-scalen                       Sauvola&#39;s algorithm.nn     sauvola-ms-split  Run multi-scale Sauvola&#39;s algorithm on each colorn                       component and merge results.nn---------------------------------------------------------------------------nSee &#39;scribo-cli COMMAND --help&#39; for more information on a specific command.n```nnFor example:nn```shnscribo-cli sauvola-ms path/to/input.tif path/to/output.png --enable-negate-outputn```nn### [OCR-D processor](https://ocr-d.github.com/cli) interface `ocrd-olena-binarize`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.github.io) annotation workflow. Input could be any valid workspace with source images available. Currently covers the `Page` hierarchy level only. Uses either (the last) `AlternativeImage`, if any, or `imageFilename`, otherwise. Adds an `AlternativeImage` with the result of binarization for every page.nn```jsonn    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n```nn## LicensennCopyright 2018-2020 Project OCR-DnnLicensed under the Apache License, Version 2.0 (the &quot;License&quot;);nyou may not use this file except in compliance with the License.nYou may obtain a copy of the License atnn   http://www.apache.org/licenses/LICENSE-2.0nnUnless required by applicable law or agreed to in writing, softwarendistributed under the License is distributed on an &quot;AS IS&quot; BASIS,nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.nSee the License for the specific language governing permissions andnlimitations under the License.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;1.1.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_olena&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 18:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v1.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;117&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_olena&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-olena-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-olena-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;impl&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The name of the actual binarization algorithm&quot;, &quot;enum&quot;=&amp;gt;[&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;k&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.34, &quot;description&quot;=&amp;gt;&quot;Sauvola&#39;s formulae parameter&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;win-size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;101, &quot;description&quot;=&amp;gt;&quot;Window size&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;1.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_olena&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;}         ocrd_segment    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_segment&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-segment-repair&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-repair&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Analyse and repair region segmentation&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-EVAL-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;sanitize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;n        },n        &quot;plausibilize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;n        },n        &quot;plausibilize_merge_min_overlap&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 0.90,n          &quot;description&quot;: &quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-regions&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-regions&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract region segmentation as image+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-lines&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-lines&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract line segmentation as image+txt+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-evaluate&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Compare region segmentations&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:nn    - ocrd-segment-repairn    - ocrd-segment-extract-pagesn    - ocrd-segment-extract-regionsn    - ocrd-segment-extract-linesn    - ocrd-segment-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_segment&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Page segmentation and segmentation evaluation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    author=&#39;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_segment&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-segment-repair=ocrd_segment.cli:ocrd_segment_repair&#39;,n            &#39;ocrd-segment-extract-pages=ocrd_segment.cli:ocrd_segment_extract_pages&#39;,n            &#39;ocrd-segment-extract-regions=ocrd_segment.cli:ocrd_segment_extract_regions&#39;,n            &#39;ocrd-segment-extract-lines=ocrd_segment.cli:ocrd_segment_extract_lines&#39;,n            &#39;ocrd-segment-evaluate=ocrd_segment.cli:ocrd_segment_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 10:42:42 2020 +0000&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;60&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-segment-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Compare region segmentations&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-evaluate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-lines&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract line segmentation as image+txt+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-lines&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-regions&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract region segmentation as image+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-regions&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-repair&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analyse and repair region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-repair&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-EVAL-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;plausibilize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;plausibilize_merge_min_overlap&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.9, &quot;description&quot;=&amp;gt;&quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sanitize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_segment&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-segment&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b19)&quot;, &quot;click&quot;, &quot;shapely&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Page segmentation and segmentation evaluation&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6235446, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}         ocrd_tesserocr    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .nCOPY requirements_test.txt .nCOPY ocrd_tesserocr ./ocrd_tesserocrnCOPY Makefile .nRUN make deps-ubuntu &amp;amp;&amp;amp; n    apt-get install -y --no-install-recommends n    g++ n    tesseract-ocr-script-frak n    tesseract-ocr-deu n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; rm -rf /build-ocrd n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove g++ libtesseract-dev maken&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / tables / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, table, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some Tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nnTo run with docker:nn```ndocker run -v path/to/workspaces:/data ocrd/tesserocr ocrd-tesserocrd-crop ...n```nnn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnsudo make deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-table](ocrd_tesserocr/segment_table.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.8.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,n  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-tesserocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-DESKEW-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        },n        &quot;min_orientation_confidence&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 1.5,n          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;description&quot;: &quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-SEG-GLYPH&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;n      ],n      &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;n        },n        &quot;raw_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;n        },n        &quot;char_whitelist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;n        },n        &quot;char_blacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;n        },n        &quot;char_unblacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;n        },n        &quot;model&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-region&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-PAGE&quot;,n        &quot;OCR-D-GT-SEG-PAGE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,n          &quot;default&quot;: 0n        },n        &quot;crop_polygons&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;n        },n        &quot;find_tables&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-table&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the region level&quot;n        }n      }n     },n     &quot;ocrd-tesserocr-segment-line&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextRegion level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-segment-word&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextLine level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,n      &quot;input_file_grp&quot;: [nt&quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [nt&quot;OCR-D-SEG-PAGE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;parameters&quot; : {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,n          &quot;default&quot;: 4n        }n      }n    },n    &quot;ocrd-tesserocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-BIN-BLOCK&quot;,n        &quot;OCR-D-BIN-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls five executables:nn    - ocrd_tesserocr_recognizen    - ocrd_tesserocr_segment_regionn    - ocrd_tesserocr_segment_tablen    - ocrd_tesserocr_segment_linen    - ocrd_tesserocr_segment_wordn    - ocrd_tesserocr_cropn    - ocrd_tesserocr_deskewn    - ocrd_tesserocr_binarizen&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_tesserocr&#39;,n    version=&#39;0.8.0&#39;,n    description=&#39;Tesserocr bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_tesserocr&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-tesserocr-recognize=ocrd_tesserocr.cli:ocrd_tesserocr_recognize&#39;,n            &#39;ocrd-tesserocr-segment-region=ocrd_tesserocr.cli:ocrd_tesserocr_segment_region&#39;,n            &#39;ocrd-tesserocr-segment-table=ocrd_tesserocr.cli:ocrd_tesserocr_segment_table&#39;,n            &#39;ocrd-tesserocr-segment-line=ocrd_tesserocr.cli:ocrd_tesserocr_segment_line&#39;,n            &#39;ocrd-tesserocr-segment-word=ocrd_tesserocr.cli:ocrd_tesserocr_segment_word&#39;,n            &#39;ocrd-tesserocr-crop=ocrd_tesserocr.cli:ocrd_tesserocr_crop&#39;,n            &#39;ocrd-tesserocr-deskew=ocrd_tesserocr.cli:ocrd_tesserocr_deskew&#39;,n            &#39;ocrd-tesserocr-binarize=ocrd_tesserocr.cli:ocrd_tesserocr_binarize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 15:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.8.0&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;334&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;dockerhub&quot;=&amp;gt;&quot;ocrd/tesserocr&quot;, &quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-tesserocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BIN-BLOCK&quot;, &quot;OCR-D-BIN-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-tesserocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Poor man&#39;s cropping via region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-PAGE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;extend detected border by this many (true) pixels on every side&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-tesserocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Detect script, orientation and skew angle for pages or regions&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-DESKEW-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_orientation_confidence&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;Minimum confidence score to apply orientation as detected by OSD&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-tesserocr-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;], &quot;parameters&quot;=&amp;gt;{&quot;char_blacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_unblacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_whitelist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;raw_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-tesserocr-segment-line&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment regions into lines with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-line&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextRegion level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-tesserocr-segment-region&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-region&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-PAGE&quot;, &quot;OCR-D-GT-SEG-PAGE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;crop_polygons&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;annotate polygon coordinates instead of bounding box rectangles&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;find_tables&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;extend detected region rectangles by this many (true) pixels&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-table&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment table regions into cell text regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-table&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the region level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-word&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment lines into words with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-word&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextLine level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/word&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.8.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_tesserocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnmake deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## TestingnnTo run with docker:nn```ndocker run ocrd/tesserocr ocrd-tesserocrd-crop ...n```nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nn## DevelopmentnnLatest changes that require pre-release of [ocrd &amp;gt;= 2.0.0](https://github.com/OCR-D/core/tree/edge) are kept in branch [`edge`](https://github.com/OCR-D/ocrd_tesserocr/tree/edge).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-tesserocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/0.7.0/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;tesserocr (&amp;gt;=2.4.1)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Tesserocr bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.7.0&quot;}, &quot;last_serial&quot;=&amp;gt;6506849, &quot;releases&quot;=&amp;gt;{&quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;sha256&quot;=&amp;gt;&quot;64ec4e7a43ddaf199af7da8966996e260454dae4d30f79cb112149cddf5b8fd2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17089, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:24&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:24.592860Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/07/63/e617002f9c2013f8a9ce10baeab48acffc0dff3d21ab160ee67428e08ebd/ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2a7fd61a97bb222f2ac5a6f85b3d2ce43da843509993eef189f09b48f44027f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15424, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:25.913866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4d/48/282d1d793137f1ec30118a9a0bd48534a6a8053bc74a830b6c4eb389653f/ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;sha256&quot;=&amp;gt;&quot;3fecd0a93d9a711552fbd2cf15af1f150f04f503f7b3f09d9c025267601bb42d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9234, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:27.040863Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/a7/66775daafba5937821fd643b6d1069570b262af3a48d701712d2a94350a2/ocrd_tesserocr-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;sha256&quot;=&amp;gt;&quot;d474e372af4266ab4343570c47a448f9f68b3c002f970717663b64acabe1dbe4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:51.905308Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/5c/95/7f29b87ff5be4fdd149400855862840de4681b669d3fda60a2ce8bf24127/ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;sha256&quot;=&amp;gt;&quot;3c0f56fc2c88ec1ea2461eb0610763443b9af279c5260b08a1be079c92bed5c6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:53&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:53.535866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/da/23/fb5e1e125f1fda3b1069960426c5b40a9c5e12fe8f73ac29244888cf110b/ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;sha256&quot;=&amp;gt;&quot;2de460c4d3218ac6e3133b498c01ee7428770edcd60a02f65793ae4006f3db82&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9251, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:54.917641Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/31/73/c2044ae57f402e21947ceb97f574625cf534eccbf432f6916c419cf3d7e7/ocrd_tesserocr-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2409adbb5c529b05eba8be5a9d1c7e11660dc2626bcaf61b407b617d5c7c99e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15453, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:20&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:20.618650Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c1/ca/38355a461d8e29d7039391f5051be291d6a425b078783adb1ebb6ba10e55/ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;sha256&quot;=&amp;gt;&quot;fbde4fc1a5a0340507b6d96bd529a42162e732b7cca31e968b28f6a4fcdccd12&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9242, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:21.805810Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1b/fe/b365c2ffddea53e616408f0213e45614ce3791ead2058df33a795ddc3d21/ocrd_tesserocr-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;sha256&quot;=&amp;gt;&quot;1549fbf8d314dc1f5ea20b45842e971a97b3c276f78d4d167a463432d5b77b18&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17420, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:12.698851Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/18/7f/fd08ca819e6f3980220ac680b5c931080247544c2704963e518db6f7a3d0/ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;sha256&quot;=&amp;gt;&quot;1648df71d28a9b3388f1e701256037eb9023f149a17a22d0a9c2dec4a0510002&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15729, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:14&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:14.276437Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/34/08/ea3ebc9476e1d28672e23b8d1332dbbc95ac9a3246cd7d02be2375995da6/ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;sha256&quot;=&amp;gt;&quot;6ec1b6c5cb4395f6f4e7356219e7019612fdcda685b511de7171dcaf4f39a439&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9442, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:15&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:15.802793Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/10/d1b3c66b891193ccc07200d93391cbcfe9c4c5ea2bb1cac045e7d1cf1fa6/ocrd_tesserocr-0.1.3.tar.gz&quot;}], &quot;0.2.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;sha256&quot;=&amp;gt;&quot;f61661e4cba7b77336dcabc6117d1e4fa90357ec98f263eacfc2c836e3a477f4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;16547, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:21.318896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d1/94/606de830cdba1f81928dc42a71f7e58cc6510d6a8b0f9e945c01f56ee3e7/ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;sha256&quot;=&amp;gt;&quot;05cc4be3ae1404afd45d8b9278d19fcd6a1ea86d376f52f571fefc4af4d96b86&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10356, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:22.854225Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/50/1c/eda34c75846857877176db4f4f0564e8b7c979a872e4c2a521fa8c389fbb/ocrd_tesserocr-0.2.0.tar.gz&quot;}], &quot;0.2.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fd8c18ce5d170e766bccd34c2214e5de22ea13f795bc79642e8be2414c550f2a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15963, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:44.123075Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/af/10f4d710bde5515131fc16ea3408670af8e786998a1e0f6d127e800fbc17/ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;sha256&quot;=&amp;gt;&quot;40f4776bc548be14245de726e744f827742f02e568f6062cc465d6a585624cae&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9534, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:45&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:45.820115Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/df/cc/fd5b999abcae94ff2116a25e31f593b95f0dda4486d89bd4e83d6671b805/ocrd_tesserocr-0.2.1.tar.gz&quot;}], &quot;0.2.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;sha256&quot;=&amp;gt;&quot;7ccdeb2a24f9d93ec6668d02807a4f5fa31d88789a3101ad1fd4ea003128ca65&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;18334, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:06.855632Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4e/5f/37ec32a07681542a1d34fa9764c76ef34d201a82489335d154d34e8b46b2/ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;sha256&quot;=&amp;gt;&quot;ad96c009bcf39b8f9e99f3e58b736ab385e5683935b9146ed9e39e8e8883b4c2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10990, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:08&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:08.563041Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/53/c0186de6ad8429e6b8e0f5e5ac51a8a3d51a2c71bcb597a5879313bf2a2d/ocrd_tesserocr-0.2.2.tar.gz&quot;}], &quot;0.3.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;sha256&quot;=&amp;gt;&quot;09f23770905034ed00f7cb516a907288512a4d21305914b6e2dd7215b9138c6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34706, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:39.261053Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b2/b5/8a890997a3f874498a1f596f3ebdb765daa181858a46cc5a66949945adf8/ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;sha256&quot;=&amp;gt;&quot;11b6742c4c398ea800d0b17276f0efd8a91ccbd6f0c1df05d7046c3e401a33c8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;22743, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:40.918776Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/fa/10af8e05b04c55680b20582c18bed55ffa846bfa65948c6b6138252a8434/ocrd_tesserocr-0.3.0.tar.gz&quot;}], &quot;0.4.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;sha256&quot;=&amp;gt;&quot;4822713547e696dbb327a80f9dd5bad705be4b7dc1f44fdef1d44f9e03c21c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37231, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:05&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:05.083051Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/ee/2b/483b44bf3180e81aa8a5bf7307ae47da4d1656e69dec1a704f9a8d558b88/ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;sha256&quot;=&amp;gt;&quot;616bf420794ef71bcc372fa4c29775c48d6909d01b6849e2d0be83766cd0ed90&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;19943, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:06.605798Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/87/09/b994a5d7310f73b04b7dd840a5fbdd726da42b7980ac0a07595b6c56ef00/ocrd_tesserocr-0.4.0.tar.gz&quot;}], &quot;0.4.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;sha256&quot;=&amp;gt;&quot;d21818eceac8bcdc1fdb38d4a58bfd1620cef8e7a5d0e6276afbd7695c2cac31&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38864, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:27.102775Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1d/78/93c90d9593f62546fea5e2ef9b5edbb5a47121582db724ca41f93830ec87/ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;sha256&quot;=&amp;gt;&quot;bbf3843361c4807c5790790d8a8fc0a0325b2fb9817cd4fa70210659dde8c8cb&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20535, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:28.641792Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/a7/2e/de857738105ed9f1888d3f6724c0c314404b67582652a91b060d25cff808/ocrd_tesserocr-0.4.1.tar.gz&quot;}], &quot;0.5.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3bca0adcb9fce640a010d38d7e1d04b4fc423ec0cc958ff3980afbf74a5711f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;33343, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:17&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:17.958444Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/36/98/a6c6b46903a3b25b1740cde4aedaf62de6441ac887536e36ad24a3c3bf12/ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;sha256&quot;=&amp;gt;&quot;aaf012b2c6adcd9a34b6fa9351dcd16fed3ab848d4d8a563b3825f9b7103be42&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;21170, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:19&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:19.386827Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/85/5b/7c5c21b78ccd00d49f7747ad5b2a381d9860aeed41fe545a24a361544837/ocrd_tesserocr-0.5.0.tar.gz&quot;}], &quot;0.5.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;sha256&quot;=&amp;gt;&quot;18cef805014268db86fd6c32bca83069cdf536298fe8151f59f9197d255a9d14&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38309, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:42&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:42.078476Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/84/b5aca7d06e31dcb91683ab60e154b73a8d0e1cb4d5ae22debf55922573df/ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;sha256&quot;=&amp;gt;&quot;7dd6a5fd556395deb58070d5f6196871a241d89434a26d0a0fc7e106404aa90a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20350, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:43.864345Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/1f/ed95415ee91659222301aa77e4f8c27be33df8e258972059bc031a2c0e3b/ocrd_tesserocr-0.5.1.tar.gz&quot;}], &quot;0.6.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;sha256&quot;=&amp;gt;&quot;41d5309efc4f886569d47dede504cea5e14ffd8e27a33acb69e15c775d34f754&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37693, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:55.328581Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/a9/431c3ad62ac4612b6be3f5cad58b49910a9c00b5f28dd62f8d535ed0c0cf/ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;sha256&quot;=&amp;gt;&quot;3a1aeff23dbf42cc8c003039cc8695cd4e01807245f935c9323e6df2832855a7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20588, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:57.128983Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/48/30/6c8253739ee61d4a42b6512be3fcfe0ce7190ff2835ee1210b1c483da025/ocrd_tesserocr-0.6.0.tar.gz&quot;}], &quot;0.7.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}         ocrd_cis    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:latestnENV VERSION=&quot;Mi 9. Okt 13:26:16 CEST 2019&quot;nENV GITURL=&quot;https://github.com/cisocrgroup&quot;nENV DOWNLOAD_URL=&quot;http://cis.lmu.de/~finkf&quot;nENV DATA=&quot;/apps/ocrd-cis-post-correction&quot;nn# depsnCOPY data/docker/deps.txt ${DATA}/deps.txtnRUN apt-get update nt&amp;amp;&amp;amp; apt-get -y install --no-install-recommends $(cat ${DATA}/deps.txt)nn# localesnRUN sed -i -e &#39;s/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/&#39; /etc/locale.gen n    &amp;amp;&amp;amp; dpkg-reconfigure --frontend=noninteractive locales n    &amp;amp;&amp;amp; update-locale LANG=en_US.UTF-8nn# install the profilernRUNtgit clone ${GITURL}/Profiler --branch devel --single-branch /tmp/profiler nt&amp;amp;&amp;amp; cd /tmp/profiler nt&amp;amp;&amp;amp; mkdir build nt&amp;amp;&amp;amp; cd build nt&amp;amp;&amp;amp; cmake -DCMAKE_BUILD_TYPE=release .. nt&amp;amp;&amp;amp; make compileFBDic trainFrequencyList profiler nt&amp;amp;&amp;amp; cp bin/compileFBDic bin/trainFrequencyList bin/profiler /apps/ nt&amp;amp;&amp;amp; cd / n    &amp;amp;&amp;amp; rm -rf /tmp/profilernn# install the profiler&#39;s language backendnRUNtgit clone ${GITURL}/Resources --branch master --single-branch /tmp/resources nt&amp;amp;&amp;amp; cd /tmp/resources/lexica nt&amp;amp;&amp;amp; make FBDIC=/apps/compileFBDic TRAIN=/apps/trainFrequencyList nt&amp;amp;&amp;amp; mkdir -p /${DATA}/languages nt&amp;amp;&amp;amp; cp -r german latin greek german.ini latin.ini greek.ini /${DATA}/languages nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/resourcesnn# install ocrd_cis (python)nCOPY Manifest.in Makefile setup.py ocrd-tool.json /tmp/build/nCOPY ocrd_cis/ /tmp/build/ocrd_cis/nCOPY bashlib/ /tmp/build/bashlib/n# COPY . /tmp/ocrd_cisnRUN cd /tmp/build nt&amp;amp;&amp;amp; make install nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/buildnn# download ocr models and pre-trainded post-correction modelnRUN mkdir /apps/models nt&amp;amp;&amp;amp; cd /apps/models nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/model.zip &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur1-00085000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur2-00062000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1nnVOLUME [&quot;/data&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{nt&quot;git_url&quot;: &quot;https://github.com/cisocrgroup/ocrd_cis&quot;,nt&quot;version&quot;: &quot;0.0.6&quot;,nt&quot;tools&quot;: {ntt&quot;ocrd-cis-ocropy-binarize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-binarize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/binarization&quot;,ntttt&quot;preprocessing/optimization/grayscale_normalization&quot;,ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-BIN&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;method&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;],nttttt&quot;description&quot;: &quot;binarization method to use (only ocropy will include deskewing)&quot;,nttttt&quot;default&quot;: &quot;ocropy&quot;ntttt},ntttt&quot;grayscale&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;description&quot;: &quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;,nttttt&quot;default&quot;: falsentttt},ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 0.0ntttt},ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-deskew&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-deskew&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 5.0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-denoise&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-denoise&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/despeckling&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-DESPECK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Despeckle pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 3.0ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-clip&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-clip&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Clip text regions / lines at intersections with neighbours&quot;,nttt&quot;parameters&quot;: {ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.7ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-resegment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-resegment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.8ntttt},ntttt&quot;extend_margins&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;,nttttt&quot;default&quot;: 3ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-dewarp&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-dewarp&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/dewarping&quot;nttt],nttt&quot;description&quot;: &quot;Dewarp line images with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;range&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;,nttttt&quot;default&quot;: 4.0ntttt},ntttt&quot;max_neighbour&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;,nttttt&quot;default&quot;: 0.05ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-recognize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-recognize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;,ntttt&quot;OCR-D-SEG-WORD&quot;,ntttt&quot;OCR-D-SEG-GLYPH&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-OCR-OCRO&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;,nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-rec&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-rec&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text snippets&quot;,nttt&quot;parameters&quot;: {ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-segment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-segment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-GT-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Segment pages into regions or regions into lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level to read images from&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;maxcolseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 2,nttttt&quot;description&quot;: &quot;number of white/background column separators to try (when operating on the page level)&quot;ntttt},ntttt&quot;maxseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 5,nttttt&quot;description&quot;: &quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_regions&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextRegion elements (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_lines&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextLine elements (when operating on the region level)&quot;ntttt},ntttt&quot;spread&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;default&quot;: 2.4,nttttt&quot;description&quot;: &quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-train&quot;,nttt&quot;categories&quot;: [ntttt&quot;lstm ocropy model training&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;training&quot;nttt],nttt&quot;description&quot;: &quot;train model with ground truth from mets data&quot;,nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;ntttt},ntttt&quot;ntrain&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;lines to train before stopping&quot;,nttttt&quot;default&quot;: 1000000ntttt},ntttt&quot;outputpath&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;(existing) path for the trained model&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-align&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-align&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Align multiple OCRs and/or GTs&quot;ntt},ntt&quot;ocrd-cis-wer&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-wer&quot;,nttt&quot;categories&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;description&quot;: &quot;calculate the word error rate for aligned page xml files&quot;,nttt&quot;parameters&quot;: {ntttt&quot;testIndex&quot;: {nttttt&quot;description&quot;: &quot;text equiv index for the test/ocr tokens&quot;,nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;gtIndex&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;text equiv index for the gt tokens&quot;,nttttt&quot;default&quot;: -1ntttt}nttt}ntt},ntt&quot;ocrd-cis-jar&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-jar&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Output path to the ocrd-cis.jar file&quot;ntt},ntt&quot;ocrd-cis-profile&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-profile&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Add a correction suggestions and suspicious tokens (profile)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;executable&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;backend&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;language&quot;: {ntttt    &quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;german&quot;ntttt},ntttt&quot;additionalLexicon&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-train.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Train post correction model&quot;,nttt&quot;parameters&quot;: {ntttt&quot;gtArchives&quot;: {nttttt&quot;description&quot;: &quot;List of ground truth archives&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Path (or URL) to a ground truth archive&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;imagePreprocessingSteps&quot;: {nttttt&quot;description&quot;: &quot;List of image preprocessing steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;training&quot;: {nttttt&quot;description&quot;: &quot;Configuration of training command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;trigrams&quot;,ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;leFeatures&quot;,ntttttt&quot;rrFeatures&quot;,ntttttt&quot;dmFeatures&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;trigrams&quot;: {nttttttt&quot;description&quot;: &quot;Path to character trigrams csv file (format: n,trigram)&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;filterClasses&quot;: {nttttttt&quot;description&quot;: &quot;List of filtered feature classes&quot;,nttttttt&quot;required&quot;: false,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Class name of feature class to filter&quot;,ntttttttt&quot;type&quot;: &quot;string&quot;nttttttt}ntttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;leFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the lexicon extension features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;rrFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the reranker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;dmFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the desicion maker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt}nttttt}ntttt}nttt}ntt},ntt&quot;ocrd-cis-post-correct&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-post-correct.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Post correct OCR results&quot;,nttt&quot;parameters&quot;: {ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of additional ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;postCorrection&quot;: {nttttt&quot;description&quot;: &quot;Configuration of post correction command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;model&quot;,ntttttt&quot;runLE&quot;,ntttttt&quot;runDM&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;model&quot;: {nttttttt&quot;description&quot;: &quot;Path to the post correction model file&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;runLE&quot;: {nttttttt&quot;description&quot;: &quot;Do run the lexicon extension step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt},ntttttt&quot;runDM&quot;: {nttttttt&quot;description&quot;: &quot;Do run the ranking and the decision step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt}nttttt}ntttt}nttt}ntt}nt}n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;&quot;&quot;&quot;nInstalls:n    - ocrd-cis-alignn    - ocrd-cis-trainingn    - ocrd-cis-profilen    - ocrd-cis-wern    - ocrd-cis-datan    - ocrd-cis-ocropy-clipn    - ocrd-cis-ocropy-denoisen    - ocrd-cis-ocropy-deskewn    - ocrd-cis-ocropy-binarizen    - ocrd-cis-ocropy-resegmentn    - ocrd-cis-ocropy-segmentn    - ocrd-cis-ocropy-dewarpn    - ocrd-cis-ocropy-recognizen    - ocrd-cis-ocropy-trainn&quot;&quot;&quot;nnimport codecsnfrom setuptools import setupnfrom setuptools import find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cis&#39;,n    version=&#39;0.0.6&#39;,n    description=&#39;CIS OCR-D command line tools&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Florian Fink, Tobias Englmeier, Christoph Weber&#39;,n    author_email=&#39;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&#39;,n    url=&#39;https://github.com/cisocrgroup/ocrd_cis&#39;,n    license=&#39;MIT&#39;,n    packages=find_packages(),n    include_package_data=True,n    install_requires=[n        &#39;ocrd&amp;gt;=2.0.0&#39;,n        &#39;click&#39;,n        &#39;scipy&#39;,n        &#39;numpy&amp;gt;=1.17.0&#39;,n        &#39;pillow&amp;gt;=6.2.0&#39;,n        &#39;shapely&#39;,n        &#39;matplotlib&amp;gt;3.0.0&#39;,n        &#39;python-Levenshtein&#39;,n        &#39;calamari_ocr == 0.3.5&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;, &#39;*.csv.gz&#39;, &#39;*.jar&#39;],n    },n    scripts=[n        &#39;bashlib/ocrd-cis-lib.sh&#39;,n        &#39;bashlib/ocrd-cis-train.sh&#39;,n        &#39;bashlib/ocrd-cis-post-correct.sh&#39;,n    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-cis-align=ocrd_cis.align.cli:ocrd_cis_align&#39;,n            &#39;ocrd-cis-profile=ocrd_cis.profile.cli:ocrd_cis_profile&#39;,n            &#39;ocrd-cis-wer=ocrd_cis.wer.cli:ocrd_cis_wer&#39;,n            &#39;ocrd-cis-data=ocrd_cis.data.__main__:main&#39;,n            &#39;ocrd-cis-ocropy-binarize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_binarize&#39;,n            &#39;ocrd-cis-ocropy-clip=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_clip&#39;,n            &#39;ocrd-cis-ocropy-denoise=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_denoise&#39;,n            &#39;ocrd-cis-ocropy-deskew=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_deskew&#39;,n            &#39;ocrd-cis-ocropy-dewarp=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_dewarp&#39;,n            &#39;ocrd-cis-ocropy-recognize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_recognize&#39;,n            &#39;ocrd-cis-ocropy-rec=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_rec&#39;,n            &#39;ocrd-cis-ocropy-resegment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_resegment&#39;,n            &#39;ocrd-cis-ocropy-segment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_segment&#39;,n            &#39;ocrd-cis-ocropy-train=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_train&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 23 15:42:32 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;436&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cis-align&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align multiple OCRs and/or GTs&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-align&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-jar&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Output path to the ocrd-cis.jar file&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-jar&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-ocropy-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;grayscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;method&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;ocropy&quot;, &quot;description&quot;=&amp;gt;&quot;binarization method to use (only ocropy will include deskewing)&quot;, &quot;enum&quot;=&amp;gt;[&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;, &quot;preprocessing/optimization/grayscale_normalization&quot;, &quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-clip&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Clip text regions / lines at intersections with neighbours&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-clip&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.7, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-denoise&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Despeckle pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-denoise&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESPECK&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/despeckling&quot;]}, &quot;ocrd-cis-ocropy-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Dewarp line images with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;max_neighbour&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4.0, &quot;description&quot;=&amp;gt;&quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-cis-ocropy-rec&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text snippets&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-rec&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-OCRO&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-resegment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-resegment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;extend_margins&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.8, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment pages into regions or regions into lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read images from&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;number of white/background column separators to try (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextLine elements (when operating on the region level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextRegion elements (when operating on the page level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;spread&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2.4, &quot;description&quot;=&amp;gt;&quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;lstm ocropy model training&quot;], &quot;description&quot;=&amp;gt;&quot;train model with ground truth from mets data&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-train&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;ntrain&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1000000, &quot;description&quot;=&amp;gt;&quot;lines to train before stopping&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;outputpath&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;(existing) path for the trained model&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;training&quot;]}, &quot;ocrd-cis-post-correct&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Post correct OCR results&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-post-correct.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of additional ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;postCorrection&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of post correction command&quot;, &quot;properties&quot;=&amp;gt;{&quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the post correction model file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;runDM&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the ranking and the decision step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;runLE&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the lexicon extension step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;required&quot;=&amp;gt;[&quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;model&quot;, &quot;runLE&quot;, &quot;runDM&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-profile&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Add a correction suggestions and suspicious tokens (profile)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-profile&quot;, &quot;parameters&quot;=&amp;gt;{&quot;additionalLexicon&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;backend&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;executable&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;language&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;german&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Train post correction model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-train.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtArchives&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ground truth archives&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path (or URL) to a ground truth archive&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;imagePreprocessingSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of image preprocessing steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;training&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of training command&quot;, &quot;properties&quot;=&amp;gt;{&quot;dmFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the desicion maker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;filterClasses&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of filtered feature classes&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of feature class to filter&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;leFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the lexicon extension features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;rrFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the reranker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;trigrams&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to character trigrams csv file (format: n,trigram)&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;trigrams&quot;, &quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;leFeatures&quot;, &quot;rrFeatures&quot;, &quot;dmFeatures&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-wer&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;evaluation&quot;], &quot;description&quot;=&amp;gt;&quot;calculate the word error rate for aligned page xml files&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-wer&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;text equiv index for the gt tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;testIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;text equiv index for the test/ocr tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;evaluation&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.6&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cis-ocropy-rec] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.textequiv_level] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.ntrain.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-ocropy-train.categories.0] &#39;lstm ocropy model training&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-ocropy-train.steps.0] &#39;training&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-align] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-align.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-wer] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-wer.parameters.testIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.parameters.gtIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.categories.0] &#39;evaluation&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-wer.steps.0] &#39;evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-jar] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-jar.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-profile] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.executable] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.backend] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.language] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.additionalLexicon] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-train.parameters.gtArchives] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.gtArchives.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.training.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training.required] [&#39;trigrams&#39;, &#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;leFeatures&#39;, &#39;rrFeatures&#39;, &#39;dmFeatures&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-train.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-post-correct] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-post-correct.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.postCorrection.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection.required] [&#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;model&#39;, &#39;runLE&#39;, &#39;runDM&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-post-correct.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;cisocrgroup/ocrd_cis&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author-email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author_email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;MIT&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-cis&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/0.0.7/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;scipy&quot;, &quot;numpy (&amp;gt;=1.17.0)&quot;, &quot;pillow (&amp;gt;=6.2.0)&quot;, &quot;matplotlib (&amp;gt;3.0.0)&quot;, &quot;python-Levenshtein&quot;, &quot;calamari-ocr (==0.3.5)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;CIS OCR-D command line tools&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.7&quot;}, &quot;last_serial&quot;=&amp;gt;6235442, &quot;releases&quot;=&amp;gt;{&quot;0.0.6&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;sha256&quot;=&amp;gt;&quot;ac2ada13f48b301831e41cba1e9a86b8e10ac2e8f4036ecdda9eb3524e36461c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044792, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:33.819139Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f7/e0/5e3953c9243d05859e679bb83bef9c6f08e10fe0eef736fce90bc42657bc/ocrd_cis-0.0.6-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;sha256&quot;=&amp;gt;&quot;97aea3f172a5eda7272113eb99d55fddda0a96069a20173ea17563d0532bbd55&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96645, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:38.406783Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8a/a9/1fab502623c41529c13b4ecbedfe224f35843160ddcef4c527a18cfe73b8/ocrd_cis-0.0.6.tar.gz&quot;}], &quot;0.0.7&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}         ocrd_anybaseocr    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-layouterkennungnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nCOPY ocrd_anybaseocr ./ocrd_anybaseocrnRUN pip3 install .n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.n n #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.n n #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)n n #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.n n #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.n n #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.n n #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.n n #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.n n #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/mjenckel/LAYoutERkennung/&quot;,n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-anybaseocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-binarize&quot;,n      &quot;description&quot;: &quot;Binarize images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;parameters&quot;: {n        &quot;nocheck&quot;:         {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;disable error checking on inputs&quot;},n        &quot;show&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;display final results&quot;},n        &quot;raw_copy&quot;:        {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;also copy the raw image&quot;},n        &quot;gray&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;force grayscale processing even if image seems binary&quot;},n        &quot;bignore&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1,   &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;debug&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,     &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;escale&quot;:          {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0,   &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;hi&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 90,    &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;lo&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 5,     &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;perc&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 80,    &quot;description&quot;: &quot;percentage for filters&quot;},n        &quot;range&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 20,    &quot;description&quot;: &quot;range for filters&quot;},n        &quot;threshold&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;zoom&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;zoom for page background estimation, smaller=faster&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-deskew&quot;,n      &quot;description&quot;: &quot;Deskew images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;parameters&quot;: {n        &quot;escale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;bignore&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1, &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;threshold&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5, &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;maxskew&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;skew angle estimation parameters (degrees)&quot;},n        &quot;skewsteps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8,   &quot;description&quot;: &quot;steps for skew angle estimation (per degree)&quot;},n        &quot;debug&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;parallel&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;???&quot;},n        &quot;lo&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 5,   &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;hi&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 90,   &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-crop&quot;,n      &quot;description&quot;: &quot;Image crop using non-linear processing&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;parameters&quot;: {n        &quot;colSeparator&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.04, &quot;description&quot;: &quot;consider space between column. 25% of width&quot;},n        &quot;maxRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Consider maximum rular area&quot;},n        &quot;minArea&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.05, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;minRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Consider minimum rular area&quot;},n        &quot;positionBelow&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.75, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;positionLeft&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.4, &quot;description&quot;: &quot;rular position in left&quot;},n        &quot;positionRight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.6, &quot;description&quot;: &quot;rular position in right&quot;},n        &quot;rularRatioMax&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 10.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularRatioMin&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 3.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularWidth&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.95, &quot;description&quot;: &quot;maximum rular width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-dewarp&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-dewarp&quot;,n      &quot;description&quot;: &quot;dewarp image with anyBaseOCR&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/dewarping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DEWARP&quot;],n      &quot;parameters&quot;: {n        &quot;imgresize&quot;:    { &quot;type&quot;: &quot;string&quot;,                      &quot;default&quot;: &quot;resize_and_crop&quot;, &quot;description&quot;: &quot;run on original size image&quot;},n        &quot;pix2pixHD&quot;:    { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;: &quot;Path to pix2pixHD library&quot;},n        &quot;model_name&quot;:t{ &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models&quot;, &quot;description&quot;: &quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;},n        &quot;checkpoint_dir&quot;:   { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;./&quot;, &quot;description&quot;: &quot;Path to where to look for dir with model name&quot;},n        &quot;gpu_id&quot;:       { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,    &quot;description&quot;: &quot;gpu id&quot;},n        &quot;resizeHeight&quot;: { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image height&quot;},n        &quot;resizeWidth&quot;:  { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-tiseg&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-tiseg&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;separate text and non-text part with anyBaseOCR&quot;,n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-textline&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-textline&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE-ANY&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;description&quot;: &quot;separate each text line&quot;,n      &quot;parameters&quot;: {n        &quot;minscale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 12.0, &quot;description&quot;: &quot;minimum scale permitted&quot;},n        &quot;maxlines&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 300, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.0, &quot;description&quot;: &quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.0, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.7, &quot;description&quot;: &quot;non-standard scaling of vertical parameters&quot;},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.2, &quot;description&quot;: &quot;baseline threshold&quot;},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8, &quot;description&quot;: &quot;noise threshold for removing small components from lines&quot;},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;use gaussian instead of uniform&quot;},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum black column separators&quot;},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 10, &quot;description&quot;: &quot;widen black separators (to account for warping)&quot;},n        &quot;blackseps&quot;:   {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;also check for black column separators&quot;},n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum # whitespace column separators&quot;},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.1, &quot;description&quot;: &quot;minimum aspect ratio for column separators&quot;},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 6.5, &quot;description&quot;: &quot;minimum column height (units=scale)&quot;},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;padding for extracted lines&quot;},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;expand mask for grayscale extraction&quot;},n        &quot;parallel&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0, &quot;description&quot;: &quot;number of CPUs to use&quot;},n        &quot;libpath&quot;:     {&quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;.&quot;, &quot;description&quot;: &quot;Library Path for C Executables&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-layout-analysis&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-layout-analysis&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LAYOUT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {n        &quot;batch_size&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 4, &quot;description&quot;: &quot;Batch size for generating test images&quot;},n        &quot;model_path&quot;:         { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/structure_analysis.h5&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classification Model&quot;},n        &quot;class_mapping_path&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/mapping_DenseNet.pickle&quot;,&quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classes&quot;}n      }n    },n    &quot;ocrd-anybaseocr-block-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-block-segmentation&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-BLOCK-SEGMENT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {        n        &quot;block_segmentation_model&quot;:   { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to block segmentation Model&quot;},n        &quot;block_segmentation_weights&quot;: { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/block_segmentation_weights.h5&quot;,  &quot;required&quot;: false, &quot;description&quot;: &quot;Path to model weights&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }       n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd-anybaseocr&#39;,n    version=&#39;0.0.1&#39;,n    author=&quot;DFKI&quot;,n    author_email=&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;,n    url=&quot;https://github.com/mjenckel/LAYoutERkennung&quot;,n    license=&#39;Apache License 2.0&#39;,n    long_description=open(&#39;README.md&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    packages=find_packages(exclude=[&quot;work_dir&quot;, &quot;src&quot;]),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;]n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-anybaseocr-binarize           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_binarize&#39;,n            &#39;ocrd-anybaseocr-deskew             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_deskew&#39;,n            &#39;ocrd-anybaseocr-crop               = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_cropping&#39;,        n            &#39;ocrd-anybaseocr-dewarp             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_dewarp&#39;,n            &#39;ocrd-anybaseocr-tiseg              = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_tiseg&#39;,n            &#39;ocrd-anybaseocr-textline           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_textline&#39;,n            &#39;ocrd-anybaseocr-layout-analysis    = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_layout_analysis&#39;,n            &#39;ocrd-anybaseocr-block-segmentation = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_block_segmentation&#39;n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Dec 17 13:28:07 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;111&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_anybaseocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung/&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-anybaseocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;gray&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;force grayscale processing even if image seems binary&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;nocheck&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;disable error checking on inputs&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;perc&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;80, &quot;description&quot;=&amp;gt;&quot;percentage for filters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;20, &quot;description&quot;=&amp;gt;&quot;range for filters&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;raw_copy&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also copy the raw image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;show&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;display final results&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;zoom&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;zoom for page background estimation, smaller=faster&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-anybaseocr-block-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-block-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BLOCK-SEGMENT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;block_segmentation_model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/&quot;, &quot;description&quot;=&amp;gt;&quot;Path to block segmentation Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;block_segmentation_weights&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/block_segmentation_weights.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to model weights&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Image crop using non-linear processing&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;colSeparator&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.04, &quot;description&quot;=&amp;gt;&quot;consider space between column. 25% of width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.3, &quot;description&quot;=&amp;gt;&quot;Consider maximum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.01, &quot;description&quot;=&amp;gt;&quot;Consider minimum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;positionBelow&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.75, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionLeft&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.4, &quot;description&quot;=&amp;gt;&quot;rular position in left&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionRight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.6, &quot;description&quot;=&amp;gt;&quot;rular position in right&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMax&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMin&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.95, &quot;description&quot;=&amp;gt;&quot;maximum rular width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-anybaseocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;skew angle estimation parameters (degrees)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;???&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;skewsteps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;steps for skew angle estimation (per degree)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-anybaseocr-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;dewarp image with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DEWARP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint_dir&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;./&quot;, &quot;description&quot;=&amp;gt;&quot;Path to where to look for dir with model name&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;gpu_id&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;gpu id&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;imgresize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;resize_and_crop&quot;, &quot;description&quot;=&amp;gt;&quot;run on original size image&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_name&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models&quot;, &quot;description&quot;=&amp;gt;&quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pix2pixHD&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;=&amp;gt;&quot;Path to pix2pixHD library&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;resizeHeight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image height&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;resizeWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image width&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-anybaseocr-layout-analysis&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-layout-analysis&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LAYOUT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;batch_size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;Batch size for generating test images&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;class_mapping_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/mapping_DenseNet.pickle&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classes&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/structure_analysis.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classification Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-textline&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate each text line&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-textline&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE-ANY&quot;], &quot;parameters&quot;=&amp;gt;{&quot;blackseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also check for black column separators&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;minimum aspect ratio for column separators&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;6.5, &quot;description&quot;=&amp;gt;&quot;minimum column height (units=scale)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;expand mask for grayscale extraction&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;libpath&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;.&quot;, &quot;description&quot;=&amp;gt;&quot;Library Path for C Executables&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum # whitespace column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxlines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum black column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;12.0, &quot;description&quot;=&amp;gt;&quot;minimum scale permitted&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;noise threshold for removing small components from lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;padding for extracted lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;number of CPUs to use&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;widen black separators (to account for warping)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;baseline threshold&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;use gaussian instead of uniform&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.7, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of vertical parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-anybaseocr-tiseg&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate text and non-text part with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-tiseg&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-anybaseocr-tiseg.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-layout-analysis.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-block-segmentation.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_anybaseocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author-email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author_email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.nn #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.nn #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)nn #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.nn #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.nn #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.nn #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.nn #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.nn #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/0.0.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;opencv-python-headless (&amp;gt;=3.4)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;ocrd-fork-pylsd (&amp;gt;=0.0.3)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;torch (&amp;gt;=1.1.0)&quot;, &quot;torchvision&quot;, &quot;pandas&quot;, &quot;keras&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;scikit-image&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;last_serial&quot;=&amp;gt;6317222, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr&quot;}         ocrd_pc_segmentation    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# WORK IN PROGRESS - NOT READYnFROM ocrd/corenVOLUME [&quot;/data&quot;]nMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .n#COPY requirements_test.txt .nCOPY ocrd_pc_segmentation ./ocrd_pc_segmentationnCOPY Makefile .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n        build-essential n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove build-essentialn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depsn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;,n  &quot;version&quot;: &quot;0.1.0&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-pixelclassifier-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-pc-segmentation&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG-BIN&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;xheight&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;height of character x in pixels used during training&quot;,n          &quot;default&quot;: 8n        },n        &quot;model&quot;:  {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;trained model for pixel classifier&quot;,n          &quot;default&quot;: &quot;__DEFAULT__&quot;n        },n        &quot;gpu_allow_growth&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;nn        },n        &quot;resize_height&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;default&quot;: 300,n          &quot;description&quot;: &quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;n        }nn      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_pc_segmentation&#39;,n    version=&#39;0.1.3&#39;,n    description=&#39;pixel-classifier based page segmentation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Alexander Gehrke, Christian Reul, Christoph Wick&#39;,n    author_email=&#39;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&#39;,n    url=&#39;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&quot;requirements.txt&quot;).read().split(),n    extras_require={n        &#39;tf_cpu&#39;: [&#39;ocr4all_pixel_classifier[tf_cpu]&amp;gt;=0.0.1&#39;],n        &#39;tf_gpu&#39;: [&#39;ocr4all_pixel_classifier[tf_gpu]&amp;gt;=0.0.1&#39;],n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    classifiers=[n        &quot;Programming Language :: Python :: 3&quot;,n        &quot;License :: OSI Approved :: Apache Software License&quot;,n        &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;nn    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-pc-segmentation=ocrd_pc_segmentation.cli:ocrd_pc_segmentation&#39;,n        ]n    },n    data_files=[(&#39;&#39;, [&quot;requirements.txt&quot;])],n    include_package_data=True,n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Jan 20 10:00:24 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;29&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd-pixelclassifier-segmentation.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-pixelclassifier-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;gpu_allow_growth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;__DEFAULT__&quot;, &quot;description&quot;=&amp;gt;&quot;trained model for pixel classifier&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;resize_height&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;xheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;height of character x in pixels used during training&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-pixelclassifier-segmentation.parameters.xheight.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-pixelclassifier-segmentation.parameters.resize_height.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author-email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author_email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[&quot;License :: OSI Approved :: Apache Software License&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;], &quot;description&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/0.1.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0a1)&quot;, &quot;click&quot;, &quot;ocr4all-pixel-classifier (&amp;gt;=0.1.3)&quot;, &quot;numpy&quot;, &quot;ocr4all-pixel-classifier[tf_cpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_cpu&#39;&quot;, &quot;ocr4all-pixel-classifier[tf_gpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_gpu&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;pixel-classifier based page segmentation&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.3&quot;}, &quot;last_serial&quot;=&amp;gt;6169845, &quot;releases&quot;=&amp;gt;{&quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;sha256&quot;=&amp;gt;&quot;c22e9fad55a01f29bea78943c8ac93bc1a0780cbc6b606cbf81bac5f888d2294&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559195, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:31&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:31.585016Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/72/f6/5936ad2bdc878920ae26b448bd68eb580f04632b373d5fba62c79a8c8148/ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;sha256&quot;=&amp;gt;&quot;9f908f54f86d85a10b5d1d339e9f964f1b2ade3b4032ee8dadeeaa474dc299b7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547673, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:39.690671Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d9/82/c3fee56b73554529fe319dd596df56758e5429b1d5ee4b8603d404f7c94e/ocrd_pc_segmentation-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;sha256&quot;=&amp;gt;&quot;026be378afb3104e0f2367254da1da0f3ba212f5d4d5c8f6a7880b4eddc5b9a5&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559196, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:53:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:53:55.306178Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cc/d6/396ad6297c509445f03fddedc5efcd6f882ce5bb223c050157d675574858/ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;sha256&quot;=&amp;gt;&quot;e2dcd0b641accb8c6594d6dd24dcf1899c3cefbc033c5860b4ff72c20f1ad4ca&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547671, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:54:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:54:12.122137Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0c/47/46c39455cc4c5739e4599f7715c4b618193b561885aa302777fd7b11c1b5/ocrd_pc_segmentation-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;}         dinglehopper    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;dinglehoppern============nndinglehopper is an OCR evaluation tool and reads [ALTO](https://github.com/altoxml), [PAGE](https://github.com/PRImA-Research-Lab/PAGE-XML) and text files.nn[![Build Status](https://travis-ci.org/qurator-spk/dinglehopper.svg?branch=master)](https://travis-ci.org/qurator-spk/dinglehopper)nnGoalsn-----n* Usefuln  * As a UI tooln  * For an automated evaluationn  * As a libraryn* Unicode supportnnInstallationn------------nIt&#39;s best to use pip, e.g.:n~~~nsudo pip install .n~~~nnUsagen-----n~~~ndinglehopper some-document.gt.page.xml some-document.ocr.alto.xmln~~~nThis generates `report.html` and `report.json`.nnnAs a OCR-D processor:n~~~nocrd-dinglehopper -m mets.xml -I OCR-D-GT-PAGE,OCR-D-OCR-TESS -O OCR-D-OCR-TESS-EVALn~~~nThis generates HTML and JSON reports in the `OCR-D-OCR-TESS-EVAL` filegroup.nnn![dinglehopper displaying metrics and character differences](.screenshots/dinglehopper.png?raw=true)nnTestingn-------nUse `pytest` to run the tests in [the tests directory](qurator/dinglehopper/tests):n~~~nvirtualenv -p /usr/bin/python3 venvn. venv/bin/activatenpip install -r requirements.txtnpip install pytestnpytestn~~~n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/qurator-spk/dinglehopper&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-dinglehopper&quot;: {n      &quot;executable&quot;: &quot;ocrd-dinglehopper&quot;,n      &quot;description&quot;: &quot;Evaluate OCR text against ground truth with dinglehopper&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-PAGE&quot;,n        &quot;OCR-D-OCR&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-EVAL&quot;n      ],n      &quot;categories&quot;: [n        &quot;Quality assurance&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ]n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;from io import opennfrom setuptools import find_packages, setupnnwith open(&#39;requirements.txt&#39;) as fp:n    install_requires = fp.read()nnsetup(n    name=&#39;dinglehopper&#39;,n    author=&#39;Mike Gerber, The QURATOR SPK Team&#39;,n    author_email=&#39;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&#39;,n    description=&#39;The OCR evaluation tool&#39;,n    long_description=open(&#39;README.md&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    keywords=&#39;qurator ocr&#39;,n    license=&#39;Apache&#39;,n    namespace_packages=[&#39;qurator&#39;],n    packages=find_packages(exclude=[&#39;*.tests&#39;, &#39;*.tests.*&#39;, &#39;tests.*&#39;, &#39;tests&#39;]),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;templates/*&#39;],n    },n    entry_points={n      &#39;console_scripts&#39;: [n        &#39;dinglehopper=qurator.dinglehopper.cli:main&#39;,n        &#39;ocrd-dinglehopper=qurator.dinglehopper.ocrd_cli:ocrd_dinglehopper&#39;,n      ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jan 14 13:22:42 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;56&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper.git&quot;}, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-dinglehopper&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Quality assurance&quot;], &quot;description&quot;=&amp;gt;&quot;Evaluate OCR text against ground truth with dinglehopper&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-dinglehopper&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-PAGE&quot;, &quot;OCR-D-OCR&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-EVAL&quot;], &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [] &#39;version&#39; is a required propertyn&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;qurator-spk/dinglehopper&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Mike Gerber, The QURATOR SPK Team&quot;, &quot;author-email&quot;=&amp;gt;&quot;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;UNKNOWN&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;}         ocrd_typegroups_classifier    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebräisch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;git_url&quot;: &quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-typegroups-classifier&quot;: {n      &quot;executable&quot;: &quot;ocrd-typegroups-classifier&quot;,n      &quot;description&quot;: &quot;Classification of 15th century type groups&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/font-identification&quot;n      ],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;parameters&quot;: {n        &quot;network&quot;: {n          &quot;description&quot;: &quot;The file name of the neural network to use, including sufficient path information&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: truen        },n        &quot;stride&quot;: {n          &quot;description&quot;: &quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 112n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_typegroups_classifier&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Typegroups classifier for OCR&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Matthias Seuret, Konstantin Baierer&#39;,n    author_email=&#39;seuretm@users.noreply.github.com&#39;,n    url=&#39;https://github.com/seuretm/ocrd_typegroups_classifier&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    include_package_data=True,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.tgc&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;typegroups-classifier=ocrd_typegroups_classifier.cli.simple:cli&#39;,n            &#39;ocrd-typegroups-classifier=ocrd_typegroups_classifier.cli.ocrd_cli:cli&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 11:38:59 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;77&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-typegroups-classifier&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Classification of 15th century type groups&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;network&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The file name of the neural network to use, including sufficient path information&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;stride&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;112, &quot;description&quot;=&amp;gt;&quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/font-identification&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_typegroups_classifier&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebräisch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.1)&quot;, &quot;pandas&quot;, &quot;scikit-image&quot;, &quot;torch (&amp;gt;=1.4.0)&quot;, &quot;torchvision (&amp;gt;=0.5.0)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Typegroups classifier for OCR&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6465300, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;sha256&quot;=&amp;gt;&quot;d469964e37069a2dab403bbf7400eec4ddabcf4ee83c86d6e88bda1bd96e9c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26290742, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:55.449239Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/e6/1b/5d0e6967985a7e23d01f558677bd7de4385dacc0186e4896ad23cb4e2f0d/ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;sha256&quot;=&amp;gt;&quot;6b339f6b52cb62acc93f64d11637aa895a2cfbe7958df3391e4d6480d8c87d28&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15969, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:59&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:59.723574Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6a/d0/620fd50f319ef68ec959b67d0c048bb0f1d602ca5cc0baa0ff46fd235382/ocrd_typegroups_classifier-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier&quot;}         ",
      "url": " /en/kwalitee.html"
    },
  

    {
      "slug": "en-spec-logging-html",
      "title": "Conventions for LOGGING",
      "content"	 : "THIS IS JUST A FIRST DRAFT!Conventions for LOGGINGThis section specifies how the output of the digitization workflow is logged.Target AudienceUsers and developers of digitization workflows in libraries and/or digitization centers.IntroductionLogging is essential for developers and users to debug applications.You always have to choose between two contradictory goals:  Runtime  InformationMany issues make troubleshooting easier but have a negative effect on the runtime.Therefore, log levels have been introduced to customize the output to suit your needs.When a workflow is executed, the output of the applications MAY be stored in files.All log files were stored in a subfolder ‘metadata/log’.The file name SHOULD contain the ID of the activity in the provenance and the name of the stream.FILENAME := ACTIVITY_ID + “_” + OUTPUT_STREAM + “.log”‘&#39;’Example:’’’ ocrd-kraken-bin_0001_stdout.logLog LevelsA more detailed description will be found hereTRACE / ALLThis is the most verbose logging level to trace the path of the algorithm.E.g.: Start/End/Duration of a method, even loops may be logged. Note that logging within loops can dramatically affect performance.DEBUGThis level is used if parameters and or the status of an algorithm/method should be logged.INFOLog information about used settings on application level.WARNThis level is used to log information about missing/wrong configurations which may lead to errors.ERRORLog all events that produce no or a wrong result. It is useful to output all the information that helps to determine the cause without the need for further investigation.FormatThe format of the logging output has to be formatted like this:TIMESTAMP LEVEL LOGGERNAME - MESSAGEExample:08:03:40.017 WARN edu.kit.ocrd.MyTestClass - A warn messageMETSThe log files are not referenced inside METS.If they are listed in the provenance, their content must be included in the provenance.Ingest Workspace to OCR-D RepositoriumNo log files will be stored in repositoryUse CasesLog during the WorkflowAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are also stored in the provenance, only information that is important for later analysis should be provided.STDERR only contains error messages that cause the program to terminate (see Loglevel ERROR).STDOUT should only contain outputs that are maximum of the log level INFO (see Loglevel INFO).For automated workflows it is recommended to save STDERR only.Analyze applicationsAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are used to analyze the program flow and possible errors and the performance is not important, all information should be output here.",
      "url": " /en/spec/logging.html"
    },
  

    {
      "slug": "de-spec-logging-html",
      "title": "",
      "content"	 : "THIS IS JUST A FIRST DRAFT!Conventions for LOGGINGThis section specifies how the output of the digitization workflow is logged.Target AudienceUsers and developers of digitization workflows in libraries and/or digitization centers.IntroductionLogging is essential for developers and users to debug applications.You always have to choose between two contradictory goals:  Runtime  InformationMany issues make troubleshooting easier but have a negative effect on the runtime.Therefore, log levels have been introduced to customize the output to suit your needs.When a workflow is executed, the output of the applications MAY be stored in files.All log files were stored in a subfolder ‘metadata/log’.The file name SHOULD contain the ID of the activity in the provenance and the name of the stream.FILENAME := ACTIVITY_ID + “_” + OUTPUT_STREAM + “.log”‘&#39;’Example:’’’ ocrd-kraken-bin_0001_stdout.logLog LevelsA more detailed description will be found hereTRACE / ALLThis is the most verbose logging level to trace the path of the algorithm.E.g.: Start/End/Duration of a method, even loops may be logged. Note that logging within loops can dramatically affect performance.DEBUGThis level is used if parameters and or the status of an algorithm/method should be logged.INFOLog information about used settings on application level.WARNThis level is used to log information about missing/wrong configurations which may lead to errors.ERRORLog all events that produce no or a wrong result. It is useful to output all the information that helps to determine the cause without the need for further investigation.FormatThe format of the logging output has to be formatted like this:TIMESTAMP LEVEL LOGGERNAME - MESSAGEExample:08:03:40.017 WARN edu.kit.ocrd.MyTestClass - A warn messageMETSThe log files are not referenced inside METS.If they are listed in the provenance, their content must be included in the provenance.Ingest Workspace to OCR-D RepositoriumNo log files will be stored in repositoryUse CasesLog during the WorkflowAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are also stored in the provenance, only information that is important for later analysis should be provided.STDERR only contains error messages that cause the program to terminate (see Loglevel ERROR).STDOUT should only contain outputs that are maximum of the log level INFO (see Loglevel INFO).For automated workflows it is recommended to save STDERR only.Analyze applicationsAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are used to analyze the program flow and possible errors and the performance is not important, all information should be output here.",
      "url": " /de/spec/logging.html"
    },
  

    {
      "slug": "en-spec-mets-html",
      "title": "Requirements on handling METS/PAGE",
      "content"	 : "Requirements on handling METS/PAGEOCR-D has decided to base its data exchange format on top of METS.For layout and text recognition results, the primary exchange format is PAGEThis document defines a set of conventions and mechanism for using METS.Conventions for PAGE are outlined in a separate documentPixel density of images must be explicit and high enoughThe pixel density is the ratio of the number of pixels that represent a a unit of measure of the scanned object. It is typically measured in pixels per inch (PPI, a.k.a. DPI).The original input images MUST have &amp;gt;= 150 ppi.Every processing step that generates new images and changes their dimensions MUST make sure to adapt the density explicitly when serialising the image.$&amp;gt; exiftool input.tif |grep &#39;X Resolution&#39;&quot;300&quot;# WRONG (ppi unchanged)$&amp;gt; convert input.tif -resize 50% output.tif# RIGHT:$&amp;gt; convert input.tif -resize 50% -density 150 -unit inches output.tif$&amp;gt; exiftool output.tif |grep &#39;X Resolution&#39;&quot;150&quot;However, since technical metadata about pixel density is so often lost inconversion or inaccurate, processors should assume 300 ppi for images withmissing or suspiciously low pixel density metadata.No multi-page imagesImage formats like TIFF support encoding multiple images in a single file.Data providers MUST provide single-image TIFF files.OCR-D processors MUST raise an exception if they encounter multi-image TIFF files.Unique ID for the document processedMETS provided to the MP must be uniquely addressable within the global library community.For this purpose, the METS file MUST contain a mods:identifier that must contain a globally unique identifier for the document and have a type attribute with a value of, in order of preference:  purl  urn  handle  urlFile Group USE syntaxAll mets:fileGrp MUST have a unique USE attribute that hints at the provenance of the files.It SHOULD have the structureID := &quot;OCR-D-&quot; + PREFIX? + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?PREFIX := (&quot;&quot; | &quot;GT-&quot;)WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}PREFIX can be GT- to indicate that these files are ground truth.WORKFLOW_STEP can be one of:  IMG: Image(s)  SEG: Segmented regions / lines / words  OCR: OCR produced from image  COR: Post-correctionPROCESSOR should be a mnemonic of the processor or result type in a terse,all-caps form, such as the name of the tool (KRAKEN) or the organisationCIS or the type of manipulation (CROP) or a combination of both startingwith the type of manipulation (BIN-KRAKEN).Examples            &amp;lt;mets:fileGrp USE&amp;gt;      Type of use for OCR-D                  &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      The unmanipulated source images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-BIN&quot;&amp;gt;      Binarization preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-CROP&quot;&amp;gt;      Cropping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESKEW&quot;&amp;gt;      Deskewing preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESPECK&quot;&amp;gt;      Despeckling preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DEWARP&quot;&amp;gt;      Dewarping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-REGION&quot;&amp;gt;      Region segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-LINE&quot;&amp;gt;      Line segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-WORD&quot;&amp;gt;      Word segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-TESS&quot;&amp;gt;      Tesseract OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-OCRO&quot;&amp;gt;      Ocropus OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-CIS&quot;&amp;gt;      CIS post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-ASV&quot;&amp;gt;      ASV post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-REGION&quot;&amp;gt;      Region segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-LINE&quot;&amp;gt;      Line segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-WORD&quot;&amp;gt;      Word segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation ground truth      File ID syntaxEach mets:file must have an ID attribute. The ID attribute of a mets:file SHOULD be the USE of the containing mets:fileGrp combined with a 4-zero-padded number.The ID MUST be unique inside the METS file.FILEID := ID + &quot;_&quot; + [0-9]{4}ID := FILEGRP + (&quot;.IMG&quot;)?Examples            &amp;lt;mets:file ID&amp;gt;      ID of the file for OCR-D                  &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;      The unmanipulated source image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN_0001&quot;&amp;gt;      PAGE encapsulating the result from binarization              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN.IMG_0001&quot;&amp;gt;      Black-and-white image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP_0001&quot;&amp;gt;      PAGE encapsulating the result from (binarization and) cropping              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP.IMG_0001&quot;&amp;gt;      Cropped black-and-white image      Grouping files by pageEvery METS file MUST have exactly one physical map that contains a singlemets:div[@TYPE=&quot;physSequence&quot;] which in turn must contain amets:div[@TYPE=&quot;page&quot;] for every page in the work.These mets:div[@TYPE=&quot;page&quot;] can contain an arbitrary number of mets:fptrpointers to mets:file elements to signify that all the files within a div areencodings of the same page.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;Images and coordinatesCoordinates are always absolute, i.e. relative to extent defined in the imageWidth/imageHeight attribute of the nearest &amp;lt;pc:Page&amp;gt;.When a processor wants to access the image of a layout element like a TextRegion or TextLine, the algorithm should be:  If the element in question has an attribute imageFilename, resolve this value  If the element has a &amp;lt;pc:Coords&amp;gt; subelement, resolve by passing the attribute imageFilename of the nearest &amp;lt;pc:Page&amp;gt; and the points attribute of the &amp;lt;pc:Coords&amp;gt; elementMedia Type for PAGE XMLEvery &amp;lt;mets:file&amp;gt; representing a PAGE document MUST have its MIMETYPE attribute set to application/vnd.prima.page+xml.Always use URL or relative filenamesAlways use URL, except for files located in the directory or any subdirectories of the METS file.Example/tmp/foo/ws1├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  /tmp/foo/ws1/foo.xml (absolute path)  file:///tmp/foo/ws1/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)If in PAGE then in METSAll URL used in imageFilename and filename attributes of&amp;lt;pc:Page&amp;gt;/&amp;lt;pc:AlternativeImage&amp;gt; MUST be referenced in a mets:fileGrp as the@xlink:href attribute of a mets:file. This MUST be the same file group asthe PAGE-XML that was the result of the processing step that produced the&amp;lt;pg:AlternativeImage&amp;gt;. In other words: &amp;lt;pg:AlternativeImage&amp;gt; should bewritten to the same mets:fileGrp as its source PAGE-XML, which in mostimplementations will mean the same folder.Recording processing information in METSProcessors should add information to the METS metadata header to indicate thatthey changed the METS. This information is mainly for human consumption to getan overview of the software agents involved in the METS file’s creation. Moredetailed or machine-actionable provenance information is outside the scope ofthe processor.To add agent information, a processor must:1) locate the first mets:metsHdr M.2) Add to M a new mets:agent A with these attributes  TYPE must be the string OTHER  OTHERTYPE must be the string SOFTWARE  ROLE must be the string OTHER  OTHERROLE must be the processing step this processor provided, from the list in the ocrd-tool.json spec3) Add to A a mets:name N that should include, in free-text form, these data points  Name of the processor, e.g. the name of the executable from ocrd-tool.json  Version of the processor, e.g. from ocrd-tool.jsonExample:&amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;OTHER&quot; OTHERROLE=&quot;preprocessing/optimization/binarization&quot;&amp;gt;  &amp;lt;mets:name&amp;gt;ocrd_tesserocr v0.1.2&amp;lt;/mets:name&amp;gt;&amp;lt;/mets:agent&amp;gt;",
      "url": " /en/spec/mets.html"
    },
  

    {
      "slug": "de-spec-mets-html",
      "title": "",
      "content"	 : "Requirements on handling METS/PAGEOCR-D has decided to base its data exchange format on top of METS.For layout and text recognition results, the primary exchange format is PAGEThis document defines a set of conventions and mechanism for using METS.Conventions for PAGE are outlined in a separate documentPixel density of images must be explicit and high enoughThe pixel density is the ratio of the number of pixels that represent a a unit of measure of the scanned object. It is typically measured in pixels per inch (PPI, a.k.a. DPI).The original input images MUST have &amp;gt;= 150 ppi.Every processing step that generates new images and changes their dimensions MUST make sure to adapt the density explicitly when serialising the image.$&amp;gt; exiftool input.tif |grep &#39;X Resolution&#39;&quot;300&quot;# WRONG (ppi unchanged)$&amp;gt; convert input.tif -resize 50% output.tif# RIGHT:$&amp;gt; convert input.tif -resize 50% -density 150 -unit inches output.tif$&amp;gt; exiftool output.tif |grep &#39;X Resolution&#39;&quot;150&quot;However, since technical metadata about pixel density is so often lost inconversion or inaccurate, processors should assume 300 ppi for images withmissing or suspiciously low pixel density metadata.No multi-page imagesImage formats like TIFF support encoding multiple images in a single file.Data providers MUST provide single-image TIFF files.OCR-D processors MUST raise an exception if they encounter multi-image TIFF files.Unique ID for the document processedMETS provided to the MP must be uniquely addressable within the global library community.For this purpose, the METS file MUST contain a mods:identifier that must contain a globally unique identifier for the document and have a type attribute with a value of, in order of preference:  purl  urn  handle  urlFile Group USE syntaxAll mets:fileGrp MUST have a unique USE attribute that hints at the provenance of the files.It SHOULD have the structureID := &quot;OCR-D-&quot; + PREFIX? + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?PREFIX := (&quot;&quot; | &quot;GT-&quot;)WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}PREFIX can be GT- to indicate that these files are ground truth.WORKFLOW_STEP can be one of:  IMG: Image(s)  SEG: Segmented regions / lines / words  OCR: OCR produced from image  COR: Post-correctionPROCESSOR should be a mnemonic of the processor or result type in a terse,all-caps form, such as the name of the tool (KRAKEN) or the organisationCIS or the type of manipulation (CROP) or a combination of both startingwith the type of manipulation (BIN-KRAKEN).Examples            &amp;lt;mets:fileGrp USE&amp;gt;      Type of use for OCR-D                  &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      The unmanipulated source images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-BIN&quot;&amp;gt;      Binarization preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-CROP&quot;&amp;gt;      Cropping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESKEW&quot;&amp;gt;      Deskewing preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESPECK&quot;&amp;gt;      Despeckling preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DEWARP&quot;&amp;gt;      Dewarping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-REGION&quot;&amp;gt;      Region segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-LINE&quot;&amp;gt;      Line segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-WORD&quot;&amp;gt;      Word segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-TESS&quot;&amp;gt;      Tesseract OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-OCRO&quot;&amp;gt;      Ocropus OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-CIS&quot;&amp;gt;      CIS post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-ASV&quot;&amp;gt;      ASV post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-REGION&quot;&amp;gt;      Region segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-LINE&quot;&amp;gt;      Line segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-WORD&quot;&amp;gt;      Word segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation ground truth      File ID syntaxEach mets:file must have an ID attribute. The ID attribute of a mets:file SHOULD be the USE of the containing mets:fileGrp combined with a 4-zero-padded number.The ID MUST be unique inside the METS file.FILEID := ID + &quot;_&quot; + [0-9]{4}ID := FILEGRP + (&quot;.IMG&quot;)?Examples            &amp;lt;mets:file ID&amp;gt;      ID of the file for OCR-D                  &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;      The unmanipulated source image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN_0001&quot;&amp;gt;      PAGE encapsulating the result from binarization              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN.IMG_0001&quot;&amp;gt;      Black-and-white image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP_0001&quot;&amp;gt;      PAGE encapsulating the result from (binarization and) cropping              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP.IMG_0001&quot;&amp;gt;      Cropped black-and-white image      Grouping files by pageEvery METS file MUST have exactly one physical map that contains a singlemets:div[@TYPE=&quot;physSequence&quot;] which in turn must contain amets:div[@TYPE=&quot;page&quot;] for every page in the work.These mets:div[@TYPE=&quot;page&quot;] can contain an arbitrary number of mets:fptrpointers to mets:file elements to signify that all the files within a div areencodings of the same page.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;Images and coordinatesCoordinates are always absolute, i.e. relative to extent defined in the imageWidth/imageHeight attribute of the nearest &amp;lt;pc:Page&amp;gt;.When a processor wants to access the image of a layout element like a TextRegion or TextLine, the algorithm should be:  If the element in question has an attribute imageFilename, resolve this value  If the element has a &amp;lt;pc:Coords&amp;gt; subelement, resolve by passing the attribute imageFilename of the nearest &amp;lt;pc:Page&amp;gt; and the points attribute of the &amp;lt;pc:Coords&amp;gt; elementMedia Type for PAGE XMLEvery &amp;lt;mets:file&amp;gt; representing a PAGE document MUST have its MIMETYPE attribute set to application/vnd.prima.page+xml.Always use URL or relative filenamesAlways use URL, except for files located in the directory or any subdirectories of the METS file.Example/tmp/foo/ws1├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  /tmp/foo/ws1/foo.xml (absolute path)  file:///tmp/foo/ws1/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)If in PAGE then in METSAll URL used in imageFilename and filename attributes of&amp;lt;pc:Page&amp;gt;/&amp;lt;pc:AlternativeImage&amp;gt; MUST be referenced in a mets:fileGrp as the@xlink:href attribute of a mets:file. This MUST be the same file group asthe PAGE-XML that was the result of the processing step that produced the&amp;lt;pg:AlternativeImage&amp;gt;. In other words: &amp;lt;pg:AlternativeImage&amp;gt; should bewritten to the same mets:fileGrp as its source PAGE-XML, which in mostimplementations will mean the same folder.Recording processing information in METSProcessors should add information to the METS metadata header to indicate thatthey changed the METS. This information is mainly for human consumption to getan overview of the software agents involved in the METS file’s creation. Moredetailed or machine-actionable provenance information is outside the scope ofthe processor.To add agent information, a processor must:1) locate the first mets:metsHdr M.2) Add to M a new mets:agent A with these attributes  TYPE must be the string OTHER  OTHERTYPE must be the string SOFTWARE  ROLE must be the string OTHER  OTHERROLE must be the processing step this processor provided, from the list in the ocrd-tool.json spec3) Add to A a mets:name N that should include, in free-text form, these data points  Name of the processor, e.g. the name of the executable from ocrd-tool.json  Version of the processor, e.g. from ocrd-tool.jsonExample:&amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;OTHER&quot; OTHERROLE=&quot;preprocessing/optimization/binarization&quot;&amp;gt;  &amp;lt;mets:name&amp;gt;ocrd_tesserocr v0.1.2&amp;lt;/mets:name&amp;gt;&amp;lt;/mets:agent&amp;gt;",
      "url": " /de/spec/mets.html"
    },
  

    {
      "slug": "de-modelle-html",
      "title": "OCR-Modelle",
      "content"	 : "OCR-ModelleFür die Texterkennung wird ein geeignetes OCR-D-Modul und ein dazu passendesSprach-/Schriftmodell benötigt. Diese Seite gibt einen Überblick über diewichtigsten Modelle und Modell-Repositorien.ocrd-tesserocr-recognizeDieser Prozessor verwendet Tesseract (ab Version 4.1) für die Texterkennung. Tesseract benötigtSprach- oder Schriftmodelle. Dies sind Dateien in einem speziellen Format (*.traineddata). Sie enthaltenmindestens eine Liste mit dem Erkennungs-Zeichensatz (“unicharset”) und die Gewichte des neuronalen Erkennungs-Modells (“lstm”), optional auch noch Wörterbücher (“wordlist”/”dawg”) und weitere Komponenten.Sprachmodelle sind im Zeichensatz und im Wörterbuch auf eine Muttersprache (z. B. deu = Deutsch) beschränkt.Schriftmodelle dagegen enthalten einen umfangreicheren Zeichensatz und Wörterbücher aus mehreren Sprachen mit der gleichen Schrift (z. B. Latin = lateinische Schrift mit Englisch, Deutsch, Französisch,Spanisch, Italienisch, …).Für Tesseract gibt es mehr als 100 Sprach- und Schriftmodelle, die von Google mittels synthetischer Daten(d.h. per Rasterung großer Mengen von Text mit vielen verschiedenen Vektorfonts) erzeugt (“trainiert”)wurden. Daneben gibt es aber auch noch weitere Modelle von anderen Anbietern, und man kann auch eigeneModelle entweder komplett neu oder auf Basis vorhandener Modelle erstellen. Eigenes Training wird durchtesstrain gut unterstützt.Die Modelle von Google gibt es jeweils in drei Varianten:tessdata_fast Diese Variante wird auch von den meistenLinux-Distributionen angeboten und ist besonders schnell bei der Texterkennung. Sie verwendet neuronale Netzwerke.tessdata_best Diese Variante braucht deutlich mehr Zeit bei derTexterkennung, kann aber im Einzelfall(nicht generell!) bessere Ergebnisse liefern. Sie verwendet neuronale Netzwerke.Eigenes Training neuer Modelle auf Basis vorhandener Modelle setzt ebenfalls diese Variante voraus.tessdata Diese Variante ist ähnlich schnell wie tessdata_fast, enthältaber zusätzlich zu den neuronalen Netzwerken auch noch die musterbasierte Zeichenerkennung von Tesseract 3.Man kann damit also zwei unterschiedliche Texterkennungsmethoden kombinieren, was in Einzelfällen zu besseren Ergebnissenführen kann.Schrift- und Sprachmodelle für historische DruckeDie folgenden Modelle für Tesseract gibt es:  deu_frak Älteres Sprachmodell für deutsche Fraktur. Dieses Modell war mit Tesseract 3 gebräuchlich, ist aber heute nicht mehr zu empfehlen.  deu Sprachmodell für deutsche Antiqua, das aber auch etwas Fraktur erkennen kann.  frk Sprachmodell für deutsche Fraktur, das aber auch etwas Antiqua erkennen kann.  Latin Schriftmodell für lateinische Antiqua-Schriften, das aber auch etwas Fraktur erkennen kann.  Fraktur Schriftmodell für Fraktur-Schriften, das aber auch Antiqua-Schriften ganz gut erkennt. Fehler beimErzeugen dieses Modells haben zur Folge, dass es kein Paragraphzeichen kennt und die Ligaturen ch und ckhäufig als Kleiner- und Größerzeichen “erkennt”.Weitere Frakturmodelle. Ausgehend von Fraktur sind mit Hilfe von GT4HistOCRweitere Modelle der UB Mannheimerzeugt worden, die für ein breites Spektrum historischer Drucke gute Ergebnisse liefern.Schrift- und Sprachmodelle können in Tesseract auch kombiniert werden, was inder Regel noch bessere Ergebnisse bringt, allerdings dann auch mehr Zeitkostet.",
      "url": " /de/modelle.html"
    },
  

    {
      "slug": "en-models-html",
      "title": "Models for OCR-D processors",
      "content"	 : "Models for OCR-D processorsOCR engines rely on pre-trained models for their recognition. Every engine hasits own internal format(s) for models. Some support central storage of modelsat a specific location (tesseract, ocropy, kraken) while others require the fullpath to a model (calamari).Since v2.22.0, OCR-D/corecomes with a framework for managing processor resources uniformly. This meansthat processors can delegate to OCR-D/core to resolve specific file resources by name,looking in well-defined places in the filesystem. This also includes downloading and cachingfile parameters passed as a URL. Furthermore, OCR-D/core comes with a bundled databaseof known resources, such as models, dictionaries, configurations and otherprocessor-specific data files. This means that OCR-D users should be able toconcentrate on fine-tuning their OCR workflows and not bother with implementationdetails like “where do I get models from and where do I put them”.In particular, users can reference file parameters by name now.All of the above mentioned functionality can be accessed using the ocrdresmgr command line tool.What models are available?To get a list of the resources that the OCR-D/core is awareof:ocrd resmgr list-availableThe output will look similar to this:ocrd-calamari-recognize- qurator-gt4hist-0.3 (https://qurator-data.de/calamari-models/GT4HistOCR/2019-07-22T15_49+0200/model.tar.xz)  Calamari model trained with GT4HistOCR- qurator-gt4hist-1.0 (https://qurator-data.de/calamari-models/GT4HistOCR/2019-12-11T11_10+0100/model.tar.xz)  Calamari model trained with GT4HistOCRocrd-cis-ocropy-recognize- LatinHist.pyrnn.gz (https://github.com/chreul/OCR_Testdata_EarlyPrintedBooks/raw/master/LatinHist-98000.pyrnn.gz)  ocropy historical latin model by github.com/chreulAs you can see, resources are grouped by the processors which make use of them.The word after the list symbol, e.g. qurator-gt4hist-0.3,LatinHist.pyrnn.gz, defines the name of the resource, which is a shorthand you canuse in parameters without having to specify the full URL (in brackets after thename).The second line of each entry contains a short description of the resource.Installing known resourcesYou can install resources with the ocrd resmgr download command. It expectsthe name of the processor as the first argument and either the name or URL of aresource as a second argument.Although model distribution is not currently centralised within OCR-D, weare working towards a central model repository.For example, to install the LatinHist.pyrnn.gz resource for ocrd-cis-ocropy-recognize:ocrd resmgr download ocrd-cis-ocropy-recognize LatinHist.pyrnn.gz# orocrd resmgr download ocrd-cis-ocropy-recognize https://github.com/chreul/OCR_Testdata_EarlyPrintedBooks/raw/master/LatinHist-98000.pyrnn.gzThis will look up the resource in the bundled resource and user databases, download,unarchive (where applicable) and store it in the proper location.NOTE: The special name * can be used instead of a resource name/url todownload all known resources for this processor. To download all tesseract models:ocrd resmgr download ocrd-tesserocr-recognize &#39;*&#39;NOTE: Equally, the special processor * can be used instead of a processor and a resourceto download all known resources for all installed processors:ocrd resmgr download &#39;*&#39;(In either case, * must be in quotes or escaped to avoid wildcard expansion by the shell.)Installing unknown resourcesIf you need to install a resource which OCR-D doesn’t know of, that can be achieved by passings its URL in combination with the --any-url/-n flag to ocrd resmgr download:To install a model for ocrd-tesserocr-recognize that is located at https://my-server/mymodel.traineddata.ocrd resmgr download -n ocrd-tesserocr-recognize https://my-server/mymodel.traineddataThis will download and store the resource in the proper location and create a stub entry in theuser database.  You can then use it as the parameter value for the model parameter:ocrd-tesserocr-recognize -P model mymodelList installed resourcesThe ocrd resmgr list-installed command has the same output format as ocrd resmgr list-available. But insteadof the database, it scans the filesystem locations where data is searched for existingresources and lists URL and description if a database entry exists.User databaseWhenever the OCR-D/core resource manager encounters an unknown resource in the filesystem or when you installa resource with ocrd resmgr download, it will create a new stub entry in the user database, which is found at$HOME/.config/ocrd/resources.yml and created if it doesn’t exist.This allows you to use the OCR-D/core resource manager mechanics, includinglookup of known resources by name or URL, without relying (only) on thedatabase maintained by the OCR-D/core developers.NOTE: If you produced or found resources that are interesting for the widerOCR(-D) community, please tell us in the OCR-D gitterchat so we can add it to the database.Where is the dataThe lookup algorithm is defined in our specificationsIn order of preference, a resource &amp;lt;name&amp;gt; for a processor ocrd-foo is searched at:  $PWD/ocrd-resources/ocrd-foo/&amp;lt;name&amp;gt;  $XDG_DATA_HOME/ocrd-resources/ocrd-foo/&amp;lt;name&amp;gt;  /usr/local/share/ocrd-resources/ocrd-foo/&amp;lt;name&amp;gt;(where XDG_DATA_HOME defaults to $HOME/.local/share if unset).We recommend using the $XDG_DATA_HOME location, which is also the default. Butyou can override the location to store data with the --location option, which canbe cwd, data and system resp.# will download to $PWD/ocrd-resources/ocrd-anybaseocr-dewarp/latest_net_G.pthocrd resmgr download --location cwd ocrd-anybaseocr-dewarp latest_net_G.pth# will download to /usr/local/share/ocrd-resources/ocrd-anybaseocr-dewarp/latest_net_G.pthocrd resmgr download --location system ocrd-anybaseocr-dewarp latest_net_G.pthChanging the default resource directoryThe $XDG_DATA_HOME default location is reasonable becausemodels are usually large files which should persist across different deployments,both native and containerized, both single-module and ocrd_all.Moreover, that variable can easily be overridden during installation.However, there are use cases where system or even cwd should beused as location to store resources, hence the --location option.Notes on specific processorsOcropy / ocrd_cisAn Ocropy model is simply the neural network serialized with Python’s picklemechanism and is generally distributed in a gzipped form, with a .pyrnn.gzextension and can be used as such, no need to unarchive.To use a specific model with OCR-D’s ocropus wrapper inocrd_cis and more specifically, theocrd-cis-ocropy-recognize processor, use the model parameter:ocrd-cis-ocropy-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-OCRO -P model fraktur-jze.pyrnn.gzNOTE: Model must be downloade before withocrd resmgr download ocrd-cis-ocropy-recognize fraktur-jze.pyrnn.gzCalamari / ocrd_calamariCalamari models are Tensorflow model directories. For distribution, thisdirectory is usually packed to a tarball or ZIP file. Once downloaded, thesecontainers must be unpacked to a directory again. ocrd resmgr handles thisfor you, so you just need the name of the resource in the database.The Calamari-OCR project also maintains a repository of models.To use a specific model with OCR-D’s calamari wrapperocrd_calamari and more specifically,the ocrd-calamari-recognize processor, use the checkpoint_dir parameter:# To use the &quot;default&quot; model, i.e. the one trained on GT4HistOCR by QURATORocrd-calamari-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-CALA# To use your own trained modelocrd-calamari-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-CALA -P checkpoint_dir /path/to/modeldir# or, to be able to control which checkpoints to use:ocrd-calamari-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-CALA -P checkpoint &#39;/path/to/modeldir/*.ckpt.json&#39;Tesseract / ocrd_tesserocrTesseract models are single files with a .traineddata extension.Since tesseract only supports model lookup in a single directory, models shouldonly be stored in a single location. If the default location (virtualenv) isnot the place you want to use for tesseract models, consider changing the default locationin the OCR-D config file.NOTE: For reasons of effiency and to avoid duplicate models, all ocrd-tesserocr-* processorsreuse the resource directory for ocrd-tesserocr-recognize.If the TESSDATA_PREFIX environemnt variable is set when any of the tesseract processorsare called, it will be the location to look for resources instead of the default.OCR-D’s Tesseract wrapper,ocrd_tesserocr and morespecifically, the ocrd-tesserocr-recognize processor, expects the name of themodel(s) to be provided as the model parameter. Multiple models can becombined by concatenating with + (which generally improves accuracy but always slows processing):# Use the deu and frk modelsocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS -P model &#39;deut+frk&#39;# Use the Fraktur modelocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS -P FrakturModels and DockerWe recommend keeping all downloaded resources in a persistent host directory,separate of the ocrd/* Docker container and data directory, and mounting thatresource directory into a specific path in the container alongside the data directory.The host resource directory can be empty initially. Each time you run the Docker container,your processors will access the host directory to resolve resources, and you can downloadadditional models into that location using ocrd resmgr.The following will assume (without loss of generality) that your host-side datapath is under ./data, and the host-side resource path is under ./models:To download models to ./models in the host FS and /usr/local/share/ocrd-resources in Docker:docker run --user $(id -u)   --volume $PWD/models:/usr/local/share/ocrd-resources ocrd/all ocrd resmgr download ocrd-tesserocr-recognize eng.traineddata; ocrd resmgr download ocrd-calamari-recognize default; ...To run processors, as usual do:docker run --user $(id -u) --workdir /data   --volume $PWD/data:/data   --volume $PWD/models:/usr/local/share/ocrd-resources   ocrd/all ocrd-tesserocr-recognize -I IN -O OUT -P model engThis principle applies to all ocrd/* Docker images, e.g. you can replace ocrd/all above with ocrd/tesserocr as well.Model trainingWith the pretrained models mentioned above, good results can be obtained for many originals. Nevertheless, therecognition rate can usually be improved significantly by fine-tuning an existing model or even training a newmodel on your own particular originals.TesstrainFor training Tesseract models, tesstrain can be used. As it isnot included in ocrd_all, you will still have to install it, first. For information on the setup and the training process itself see the Readme in the GithHub Repository.okralactWhile tesstrain only allows you to train models for Tesseract, with okralactyou can train models for four engines compatible with OCR-D - namely Tesseract, Ocropus, Kraken and Calamari - at once. Especially if you want to use several OCR engines for your workflows or are not sure which OCR engine will give you the bestresults, this might be particularly effective for you. Just like tesstrain it is not included in ocrd_all, meaning you will still have to install it, first. For information on the setup and the training process itself see theReadme in the GithHub Repository.",
      "url": " /en/models.html"
    },
  

    {
      "slug": "en-module-processors-html",
      "title": "",
      "content"	 : "cor-asv-ann          cor-asv-fst          ocrd_calamari          ocrd_im6convert          ocrd_keraslm          ocrd_kraken        ocrd_ocropy          ocrd_olena          ocrd_segment          ocrd_tesserocr          ocrd_cis        ocrd_anybaseocr        ocrd_pc_segmentation          dinglehopper        ocrd_typegroups_classifier          ",
      "url": " /en/module-processors.html"
    },
  

    {
      "slug": "en-module-processors-html",
      "title": "Module Processors",
      "content"	 : "            cor-asv-ann                                                        cor-asv-fst                                                        ocrd_calamari                                                        ocrd_im6convert                                                        ocrd_keraslm                                                        ocrd_kraken                                                        ocrd_ocropy                                                        ocrd_olena                                                        ocrd_segment                                                        ocrd_tesserocr                                                        ocrd_cis                                                        ocrd_anybaseocr                                                        ocrd_pc_segmentation                                                        dinglehopper                                                        ocrd_typegroups_classifier                                            ",
      "url": " /en/module-processors.html"
    },
  

    {
      "slug": "en-ocrd-gt-repo-html",
      "title": "Working with OCR-D-(Ground-Truth)-Repository",
      "content"	 : "Working with OCR-D-(Ground-Truth)-RepositoryUpload bagit container from scratch to OCR-D(-GT)-RepositoryExample to upload a scanned page to OCR-D-Repo.Preparation: Create WorkspaceRequirements: ocrd (Version 1.0.0) See Setup OCR-D StackActivate virtualenvuser@hostname:~$source ~/env-ocrd/bin/activate(env-ocrd) user@hostname:~$Initialize Workspace(env-ocrd) user@hostname:~$ ocrd workspace init communist_manifesto(env-ocrd) user@hostname:~$ cd communist_manifestoCreate Folder for Scanned Page(env-ocrd) user@hostname:~/communist_manifesto$ mkdir OCR-D-IMGDownload Image (Google)(env-ocrd) user@hostname:~/communist_manifesto$ wget https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Manifesto_of_the_Communist_Party.djvu/page15-2745px-Manifesto_of_the_Communist_Party.djvu.jpg -O OCR-D-IMG/OCR-D-IMG_0015.jpgAdd Image to Workspace(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace add -g P0015 -G OCR-D-IMG -i OCR-D-IMG_0015 -m image/jpg OCR-D-IMG/OCR-D-IMG_0015.jpgSet Unique ID for Workspace(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace set-id &#39;communist_manifesto&#39;Validate WorkspaceFor some images, the resolution of the image is not set. To avoid validation errors, the resolution check is skipped.For further details see ‘ocrd workspace validate –help’.(env-ocrd) user@hostname:~/communist_manifesto$ ocrd workspace validate --skip pixel_density mets.xmlCreate BagIt Container(env-ocrd) user@hostname:~/communist_manifesto$ cd ..(env-ocrd) user@hostname:~/$ ocrd zip bag -i communist_manifesto -d communist_manifesto/Validate BagIt Container(env-ocrd) user@hostname:~/$ ocrd zip validate communist_manifesto.ocrd.zip[...]OKUpload BagIt Containeruser@hostname:~/$ curl -u ingest:GENERATED_PASSWORD -v -F &quot;file=@communist_manifesto.ocrd.zip&quot; http://localhost:8080/api/v1/metastore/bagit [...]OKDownload all BagIt Containersuser@hostname:~/Download$ wget -O listOfContainers.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagituser@hostname:~/Download$ ocrdzips=$(cat listOfContainers.json | tr &quot;,[]&quot;&quot; &quot;n&quot;)user@hostname:~/Download$ for addr in $ocrdzipsdo  wget $addr  filename=$(basename -- &quot;$addr&quot;)  directory=&quot;${filename%.*}&quot;  mkdir $directory  cd $directory  unzip ../$filename  cd ..doneList all Documents (in Browser)https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagitThe list shows all ingested documents with its  ‘Upload Date’  ‘Version’  ‘OCR-D Identifier’  ‘Link for Download’  ‘Referenced Files’  ‘Metadata’  and ‘Semantic Labeling’(Upload is only available for authorized users)Download Documenthttps://ocr-d-repo.scc.kit.edu/api/v1/dataresources/71e19490-343a-4d68-a5a7-7cf4c725c843/data/arent_dichtercharaktere_1885.zipDownload of the complete document as bagit container.List all Files inside Documenthttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/filesAll files of given resourceID referenced inside the mets.xml are listed here.Download Single Filehttps://ocr-d-repo.scc.kit.edu/api/v1/dataresources/71e19490-343a-4d68-a5a7-7cf4c725c843/data/bagit/data/DEFAULT/DEFAULT_0002Download/view single file (Tiff) of given resourceID, file group and fileID.List Metadatahttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/metadataList metadata of the document (e.g.: title, author, year, identifier, languages, classifications) of given resourceID.List Ground Truth Metadatahttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/71e19490-343a-4d68-a5a7-7cf4c725c843/groundtruthList all semantic labels of given resourceID.Search Inside RepositoryAll searches will return a list of fitting resourceIDs. In order to further investigate the found resources, the listings above can be used.Search via browserhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/searchSearch on command lineSearch for Semantic Labelhttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/labeling?label=condition/acquisition/method-flaws/imaging/uneven-illuminationSearch for documents with e.g. uneven illumination.Search for Documents Containing Multiple Semantic Labels at Oncehttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/labeling?label=condition/acquisition/method-flaws/imaging/uneven-illumination,condition/acquisition/content-or-background/included-objects/preceeding-or-proceedingSearch for Documents with Classification ‘Fachtext’https://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/classification?class=FachtextSearch for Documents with Language ‘deu’https://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/language?lang=deuSearch for Documents with Identifier ‘16488’https://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/identifier?identifier=16529Search for Documents with Specific Identifier and Typehttps://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/identifier?identifier=urn:nbn:de:kobv:b4-200905196929&amp;amp;type=urnSearch for document with specific identifier of a specific type.Possible types are:  purl  urn  handle  url  dtaid  …    Download selected BagIt Containers    E.g.: All with Classification ‘Belletristik’```bash=bash    Get all containers    user@hostname:~/Download$ wget -O listOfAllContainers.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit              user@hostname:~/Download$ allocrdzips=$(cat listOfAllContainers.json      tr “,[]&quot;” “n”)      Get IDs of fitting containersuser@hostname:~/Download$ wget -O filteredList.json https://ocr-d-repo.scc.kit.edu/api/v1/metastore/mets/classification?class=Belletristik            user@hostname:~/Download$ filteredIds=$(cat filteredList.json      tr “,[]&quot;” “n”)      user@hostname:~/Download$ for bagitid in $filteredIdsdo  for addr in $allocrdzips  do    if echo “$addr” | grep -q “$bagitid”; then      wget $addr      filename=$(basename – “$addr”)      directory=”${filename%.*}”  mkdir $directory  cd $directory  unzip ../$filename  cd ..fi   done done ```",
      "url": " /en/ocrd-gt-repo.html"
    },
  

    {
      "slug": "en-spec-ocrd-tool-html",
      "title": "ocrd-tool.json",
      "content"	 : "ocrd-tool.jsonTools MUST be described in a file ocrd-tool.json in the root of the repository.It must contain a JSON object adhering to the ocrd-tool JSON Schema.In particular, every tool provided must be described in an array item under thetools key. These definitions drive the CLI and the webservices.To validate a ocrd-tool.json file, use ocrd ocrd-tool /path/to/ocrd-tool.json validate.File parametersTo mark a parameter as expecting the address of a file, it must declare thecontent-type property as a valid mediatype.Optionally, workflow processors can be notified that this file is potentiallylarge and static (e.g. a fixed dataset or a precomputed model) and should be cachedindefinitely after download by setting the cacheable property to true.The filename itself, i.e. the concrete value &amp;lt;fpath&amp;gt; of a file parametershould be resolved in the following way:  If &amp;lt;fpath&amp;gt; is an http/https URL: Download to a temporary directory (ifcacheable==False) or a semi-temporary cache directory (if cacheable==True)  If &amp;lt;fpath&amp;gt; is an absolute path: Use as-is.  If &amp;lt;fpath&amp;gt; is a relative path, try resolving the following paths and returnthe first one found if any, otherwise abort with an error message stating so:          $CWD/ocrd-resources/&amp;lt;fpath&amp;gt;      If an environment variable is defined that has the name of the processor inupper-case and with - replaced with - and followed by _PATH (e.g. for a processorocrd-dummy, the variable would need to be called OCRD_DUMMY_PATH):                  Split the variable value at : and try to resolve by appending &amp;lt;fpath&amp;gt;to each token and return the first found file if any                    $XDG_DATA_HOME/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt; (with $HOME/.local/share instead of $XDG_DATA_HOME if unset)      /usr/local/share/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt;      Input / Output file groupsTools should define the names of both expected input and produced output filegroups as a list of USE attributes of mets:fileGrp elements. If more thanone file group is expected or produced, this should be explained in thedescription of the tool.NOTE: Both input and output file groups can be overridden atruntime. Tools must therefore ensure not tohardcode file group names. When multiple groups are expected, the order of theoverride reflects the order in which they are defined in the ocrd-tool.json.Definitiontype: objectdescription: Schema for tools by OCR-D MPrequired:  - version  - git_url  - toolsadditionalProperties: falseproperties:  version:    description: &quot;Version of the tool, expressed as MAJOR.MINOR.PATCH.&quot;    type: string    pattern: &#39;^[0-9]+.[0-9]+.[0-9]+$&#39;  git_url:    description: Github/Gitlab URL    type: string    format: url  dockerhub:    description: DockerHub image    type: string  tools:    type: object    additionalProperties: false    patternProperties:      &#39;ocrd-.*&#39;:        type: object        additionalProperties: false        required:          - description          - steps          - executable          - categories          - input_file_grp          # Not required because not all processors produce output files          # - output_file_grp        properties:          executable:            description: The name of the CLI executable in $PATH            type: string          input_file_grp:            description: Input fileGrp@USE this tool expects by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          output_file_grp:            description: Output fileGrp@USE this tool produces by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          parameters:            description: Object describing the parameters of a tool. Keys are parameter names, values sub-schemas.            type: object            patternProperties:              &quot;.*&quot;:                type: object                additionalProperties: false                required:                  - description                  - type                  # also either &#39;default&#39; or &#39;required&#39;                properties:                  type:                    type: string                    description: Data type of this parameter                    enum:                      - string                      - number                      - boolean                      - object                      - array                  format:                    description: Subtype, such as `float` for type `number` or `uri` for type `string`.                  description:                    description: Concise description of syntax and semantics of this parameter                  required:                    type: boolean                    description: Whether this parameter is required                  default:                    description: Default value when not provided by the user                  enum:                    type: array                    description: List the allowed values if a fixed list.                  content-type:                    type: string                    description: &quot;If parameter is reference to file: Media type of the file&quot;                  cacheable:                    type: boolean                    description: &quot;If parameter is reference to file: Whether the file should be cached, e.g. because it is large and won&#39;t change.&quot;                    default: false          description:            description: Concise description what the tool does          categories:            description: Tools belong to this categories, representing modules within the OCR-D project structure            type: array            items:              type: string              enum:                - Image preprocessing                - Layout analysis                - Text recognition and optimization                - Model training                - Long-term preservation                - Quality assurance          steps:            description: This tool can be used at these steps in the OCR-D functional model            type: array            items:              type: string              enum:                - preprocessing/characterization                - preprocessing/optimization                - preprocessing/optimization/cropping                - preprocessing/optimization/deskewing                - preprocessing/optimization/despeckling                - preprocessing/optimization/dewarping                - preprocessing/optimization/binarization                - preprocessing/optimization/grayscale_normalization                - recognition/text-recognition                - recognition/font-identification                - recognition/post-correction                - layout/segmentation                - layout/segmentation/text-nontext                - layout/segmentation/region                - layout/segmentation/line                - layout/segmentation/word                - layout/segmentation/classification                - layout/analysisExampleThis is from the ocrd_tesserocr project:{  &quot;version&quot;: &quot;0.10.0&quot;,  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,  &quot;tools&quot;: {    &quot;ocrd-tesserocr-deskew&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-DESKEW-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        },        &quot;min_orientation_confidence&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;default&quot;: 1.5,          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;        }      }    },    &quot;ocrd-tesserocr-fontshape&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-fontshape&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Recognize font shapes (family/monospace/bold/italic) and size in segmented words with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons), annotating TextStyle&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-OCR&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-OCR-STYLE&quot;      ],      &quot;steps&quot;: [&quot;recognition/font-identification&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Number of background-filled pixels to add around the word image (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) on each side before recognition.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;osd&quot;,          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or osd); must be an old (pre-LSTM) model&quot;        }      }    },    &quot;ocrd-tesserocr-recognize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Segment and/or recognize text with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons) on any level of the PAGE hierarchy.&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-SEG-GLYPH&quot;,        &quot;OCR-D-OCR-TESS&quot;      ],      &quot;steps&quot;: [        &quot;layout/segmentation/region&quot;,        &quot;layout/segmentation/line&quot;,        &quot;recognition/text-recognition&quot;      ],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Extend detected region/cell/line/word rectangles by this many (true) pixels, or extend existing region/line/word images (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) by this many (background/white) pixels on each side before recognition.&quot;        },        &quot;segmentation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Highest PAGE XML hierarchy level to remove existing annotation from and detect segments for (before iterating downwards); if ``none``, does not attempt any new segmentation; if ``cell``, starts at table regions, detecting text regions (cells). Ineffective when lower than ``textequiv_level``.&quot;        },        &quot;textequiv_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to re-use or detect segments for and add the TextEquiv results to (before projecting upwards); if ``none``, adds segmentation down to the glyph level, but does not attempt recognition at all; if ``cell``, stops short before text lines, adding text of text regions inside tables (cells) or on page level only.&quot;        },        &quot;overwrite_segments&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;If ``segmentation_level`` is not none, but an element already contains segments, remove them and segment again. Otherwise use the existing segments of that element.&quot;        },        &quot;overwrite_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;If ``textequiv_level`` is not none, but a segment already contains TextEquivs, remove them and replace with recognised text. Otherwise add new text as alternative. (Only the first entry is projected upwards.)&quot;        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, annotate polygon coordinates instead of bounding box rectangles.&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;When detecting regions, recognise tables as table regions (Tesseract&#39;s ``textord_tabfind_find_tables=1``).&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, use &#39;sparse text&#39; page segmentation mode (finding as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space.&quot;        },        &quot;raw_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting lines, do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) on line images. Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;        },        &quot;char_whitelist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;        },        &quot;char_blacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;        },        &quot;char_unblacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow inclusively.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;description&quot;: &quot;The tessdata text recognition model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur).&quot;        }      }    },     &quot;ocrd-tesserocr-segment&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions and lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 4        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }   },   &quot;ocrd-tesserocr-segment-region&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_regions&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the Page level (otherwise skip page; no incremental annotation yet).&quot;        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 0        },        &quot;crop_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }    },     &quot;ocrd-tesserocr-segment-table&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_cells&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TableRegion level (otherwise skip table; no incremental annotation yet).&quot;        }      }     },     &quot;ocrd-tesserocr-segment-line&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextRegion level (otherwise skip region; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-segment-word&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-GT-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_words&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (otherwise skip line; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-crop&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;      ],      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-PAGE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],      &quot;parameters&quot; : {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,          &quot;default&quot;: 4        }      }    },    &quot;ocrd-tesserocr-binarize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-BIN-BLOCK&quot;,        &quot;OCR-D-BIN-LINE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],      &quot;parameters&quot;: {        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        }      }    }  }}",
      "url": " /en/spec/ocrd_tool.html"
    },
  

    {
      "slug": "de-spec-ocrd-tool-html",
      "title": "",
      "content"	 : "ocrd-tool.jsonTools MUST be described in a file ocrd-tool.json in the root of the repository.It must contain a JSON object adhering to the ocrd-tool JSON Schema.In particular, every tool provided must be described in an array item under thetools key. These definitions drive the CLI and the webservices.To validate a ocrd-tool.json file, use ocrd ocrd-tool /path/to/ocrd-tool.json validate.File parametersTo mark a parameter as expecting the address of a file, it must declare thecontent-type property as a valid mediatype.Optionally, workflow processors can be notified that this file is potentiallylarge and static (e.g. a fixed dataset or a precomputed model) and should be cachedindefinitely after download by setting the cacheable property to true.The filename itself, i.e. the concrete value &amp;lt;fpath&amp;gt; of a file parametershould be resolved in the following way:  If &amp;lt;fpath&amp;gt; is an http/https URL: Download to a temporary directory (ifcacheable==False) or a semi-temporary cache directory (if cacheable==True)  If &amp;lt;fpath&amp;gt; is an absolute path: Use as-is.  If &amp;lt;fpath&amp;gt; is a relative path, try resolving the following paths and returnthe first one found if any, otherwise abort with an error message stating so:          $CWD/ocrd-resources/&amp;lt;fpath&amp;gt;      If an environment variable is defined that has the name of the processor inupper-case and with - replaced with - and followed by _PATH (e.g. for a processorocrd-dummy, the variable would need to be called OCRD_DUMMY_PATH):                  Split the variable value at : and try to resolve by appending &amp;lt;fpath&amp;gt;to each token and return the first found file if any                    $XDG_DATA_HOME/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt; (with $HOME/.local/share instead of $XDG_DATA_HOME if unset)      /usr/local/share/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt;      Input / Output file groupsTools should define the names of both expected input and produced output filegroups as a list of USE attributes of mets:fileGrp elements. If more thanone file group is expected or produced, this should be explained in thedescription of the tool.NOTE: Both input and output file groups can be overridden atruntime. Tools must therefore ensure not tohardcode file group names. When multiple groups are expected, the order of theoverride reflects the order in which they are defined in the ocrd-tool.json.Definitiontype: objectdescription: Schema for tools by OCR-D MPrequired:  - version  - git_url  - toolsadditionalProperties: falseproperties:  version:    description: &quot;Version of the tool, expressed as MAJOR.MINOR.PATCH.&quot;    type: string    pattern: &#39;^[0-9]+.[0-9]+.[0-9]+$&#39;  git_url:    description: Github/Gitlab URL    type: string    format: url  dockerhub:    description: DockerHub image    type: string  tools:    type: object    additionalProperties: false    patternProperties:      &#39;ocrd-.*&#39;:        type: object        additionalProperties: false        required:          - description          - steps          - executable          - categories          - input_file_grp          # Not required because not all processors produce output files          # - output_file_grp        properties:          executable:            description: The name of the CLI executable in $PATH            type: string          input_file_grp:            description: Input fileGrp@USE this tool expects by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          output_file_grp:            description: Output fileGrp@USE this tool produces by default            type: array            items:              type: string              pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          parameters:            description: Object describing the parameters of a tool. Keys are parameter names, values sub-schemas.            type: object            patternProperties:              &quot;.*&quot;:                type: object                additionalProperties: false                required:                  - description                  - type                  # also either &#39;default&#39; or &#39;required&#39;                properties:                  type:                    type: string                    description: Data type of this parameter                    enum:                      - string                      - number                      - boolean                      - object                      - array                  format:                    description: Subtype, such as `float` for type `number` or `uri` for type `string`.                  description:                    description: Concise description of syntax and semantics of this parameter                  required:                    type: boolean                    description: Whether this parameter is required                  default:                    description: Default value when not provided by the user                  enum:                    type: array                    description: List the allowed values if a fixed list.                  content-type:                    type: string                    description: &quot;If parameter is reference to file: Media type of the file&quot;                  cacheable:                    type: boolean                    description: &quot;If parameter is reference to file: Whether the file should be cached, e.g. because it is large and won&#39;t change.&quot;                    default: false          description:            description: Concise description what the tool does          categories:            description: Tools belong to this categories, representing modules within the OCR-D project structure            type: array            items:              type: string              enum:                - Image preprocessing                - Layout analysis                - Text recognition and optimization                - Model training                - Long-term preservation                - Quality assurance          steps:            description: This tool can be used at these steps in the OCR-D functional model            type: array            items:              type: string              enum:                - preprocessing/characterization                - preprocessing/optimization                - preprocessing/optimization/cropping                - preprocessing/optimization/deskewing                - preprocessing/optimization/despeckling                - preprocessing/optimization/dewarping                - preprocessing/optimization/binarization                - preprocessing/optimization/grayscale_normalization                - recognition/text-recognition                - recognition/font-identification                - recognition/post-correction                - layout/segmentation                - layout/segmentation/text-nontext                - layout/segmentation/region                - layout/segmentation/line                - layout/segmentation/word                - layout/segmentation/classification                - layout/analysisExampleThis is from the ocrd_tesserocr project:{  &quot;version&quot;: &quot;0.10.0&quot;,  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,  &quot;tools&quot;: {    &quot;ocrd-tesserocr-deskew&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-DESKEW-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        },        &quot;min_orientation_confidence&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;default&quot;: 1.5,          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;        }      }    },    &quot;ocrd-tesserocr-fontshape&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-fontshape&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Recognize font shapes (family/monospace/bold/italic) and size in segmented words with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons), annotating TextStyle&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-OCR&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-OCR-STYLE&quot;      ],      &quot;steps&quot;: [&quot;recognition/font-identification&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Number of background-filled pixels to add around the word image (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) on each side before recognition.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;osd&quot;,          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or osd); must be an old (pre-LSTM) model&quot;        }      }    },    &quot;ocrd-tesserocr-recognize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Segment and/or recognize text with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons) on any level of the PAGE hierarchy.&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-SEG-GLYPH&quot;,        &quot;OCR-D-OCR-TESS&quot;      ],      &quot;steps&quot;: [        &quot;layout/segmentation/region&quot;,        &quot;layout/segmentation/line&quot;,        &quot;recognition/text-recognition&quot;      ],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Extend detected region/cell/line/word rectangles by this many (true) pixels, or extend existing region/line/word images (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) by this many (background/white) pixels on each side before recognition.&quot;        },        &quot;segmentation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Highest PAGE XML hierarchy level to remove existing annotation from and detect segments for (before iterating downwards); if ``none``, does not attempt any new segmentation; if ``cell``, starts at table regions, detecting text regions (cells). Ineffective when lower than ``textequiv_level``.&quot;        },        &quot;textequiv_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to re-use or detect segments for and add the TextEquiv results to (before projecting upwards); if ``none``, adds segmentation down to the glyph level, but does not attempt recognition at all; if ``cell``, stops short before text lines, adding text of text regions inside tables (cells) or on page level only.&quot;        },        &quot;overwrite_segments&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;If ``segmentation_level`` is not none, but an element already contains segments, remove them and segment again. Otherwise use the existing segments of that element.&quot;        },        &quot;overwrite_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;If ``textequiv_level`` is not none, but a segment already contains TextEquivs, remove them and replace with recognised text. Otherwise add new text as alternative. (Only the first entry is projected upwards.)&quot;        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, annotate polygon coordinates instead of bounding box rectangles.&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;When detecting regions, recognise tables as table regions (Tesseract&#39;s ``textord_tabfind_find_tables=1``).&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, use &#39;sparse text&#39; page segmentation mode (finding as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space.&quot;        },        &quot;raw_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting lines, do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) on line images. Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;        },        &quot;char_whitelist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;        },        &quot;char_blacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;        },        &quot;char_unblacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow inclusively.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;description&quot;: &quot;The tessdata text recognition model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur).&quot;        }      }    },     &quot;ocrd-tesserocr-segment&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions and lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 4        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }   },   &quot;ocrd-tesserocr-segment-region&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_regions&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the Page level (otherwise skip page; no incremental annotation yet).&quot;        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 0        },        &quot;crop_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }    },     &quot;ocrd-tesserocr-segment-table&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_cells&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TableRegion level (otherwise skip table; no incremental annotation yet).&quot;        }      }     },     &quot;ocrd-tesserocr-segment-line&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextRegion level (otherwise skip region; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-segment-word&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-GT-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_words&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (otherwise skip line; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-crop&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;      ],      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-PAGE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],      &quot;parameters&quot; : {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,          &quot;default&quot;: 4        }      }    },    &quot;ocrd-tesserocr-binarize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-BIN-BLOCK&quot;,        &quot;OCR-D-BIN-LINE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],      &quot;parameters&quot;: {        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        }      }    }  }}",
      "url": " /de/spec/ocrd_tool.html"
    },
  

    {
      "slug": "en-spec-ocrd-zip-html",
      "title": "OCRD-ZIP",
      "content"	 : "OCRD-ZIPThis document describes an exchange format to bundle a workspace described by aMETS file following OCR-D’s conventions.RationaleMETS is the exchange format of choice by OCR-D for describing relations offiles such as images and metadata about those images such as PAGE or ALTOfiles. METS is a textual format, not suitable for embedding arbitrary,potentially binary, data. For various use cases (such as transfer via network,long-term preservation, reproducible tests etc.) it is desirable to have aself-contained representation of a workspace.With such a representation, data producers are not forced to providedereferenceable HTTP-URL for the files they produce and data consumers are notforced to dereference all HTTP-URL.While METS does have mechanisms for embedding XML data and even base64-encodedbinary data, the tradeoffs in file size, parsing speed and readability are toogreat to make this a viable solution for a mass digitization scenario.Instead, we propose an exchange format (“OCRD-ZIP”) based on the BagIt specused for data ingestion adopted in the web archiving community.BagIt profileAs a baseline, an OCRD-ZIP must adhere to v0.97+ of the BagItspecs, i.e.  all files in data/  a file bagit.txt  a file bag-info.txtIn accordance with the BagIt standard, bagit.txt MUST consist of exactlythese two lines:BagIt-Version: 1.0Tag-File-Character-Encoding: UTF-8In addition, OCRD-ZIP adhere to a BagItprofile (see Appendix A forthe full definition):  bag-info.txt MUST additionally contain these tags:          BagIt-Profile-Identifier: URL of the OCR-D BagIt profile      Ocrd-Identifier: A globally unique identifier for this bag      Ocrd-Base-Version-Checksum: Checksum of the version this bag is based on        bag-info.txt MAY additionally contain these tags:                Ocrd-Manifestation-Depth: Whether all URL are dereferenced as files or only some      BagIt-Profile-IdentifierThe BagIt-Profile-Identifier must be the string https://ocr-d.de/bagit-profile.json.Ocrd-MetsOcrd-Mets can be provided to declare that the METS file will not be thestandard mets.xml but another path relative to /data/.Implementations MUST check for the Ocrd-Mets tag: If it has a value, look for theMETS file at that location, relative to /data. Otherwise, assume the defaultmets.xml.Ocrd-Manifestation-DepthSpecify whether the bag contains the full manifestation of the data referenced in the METS (full)or only those files that were file:// URLs before (partial). Default: partial.Ocrd-IdentifierA globally unique identifier identifying the work/works/parts of works thisbundle of file represents.This is to be used for repositories to identify new ingestions of existing works.To ensure global uniqueness, the identifier should be prefixed with anidentifier of the organization, e.g. an ISIL or domain name.Ocrd-Base-Version-ChecksumThe SHA512 checksum of the manifest-sha512.txt file of the version this bagwas based on, if any.InvariantsZIPAn OCRD-ZIP MUST be a serialized as a ZIP file.manifest-sha512.txtChecksums for the files in /data must be calculated with the SHA512algorithm only and provided as manifest-sha512.txt.Since the checksum of this manifest file can be relevant (seeOcrd-Base-Version-Checksum), in addition to the requirementsof the BagIt spec, the entries MUST be sorted.NOTE: These checksums can be generated with find data -type f | sort -sf |xargs sha512sum &amp;gt; manifest-sha512.txt.File names must be relative to METSWithin an OCRD-ZIP, all local file resources referenced in the METS (andconsequently all those referenced in other files within the workspace – seerule “If in PAGE then in METS” must berelative to the location of the METS file.Example/tmp/foo/ws1/data├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  /tmp/foo/ws1/data/foo.xml (absolute path)  file:///tmp/foo/ws1/data/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)When in data then in METSAll files except mets.xml itself that are contained in data directory mustbe referenced in a mets:file/mets:Flocat in the mets.xml.When in METS and not in dataDue to partial OCRD-ZIP not all files may be part of the payload. If so they have to be mentioned in fetch.txt and in all payload manifest files.Optional metadata about the payloadIn addition to the actual data files in /data, the following metadata filesare allowed to be present in the root of the bag:  README.md: An extended, human-readable description of the dataset in the Markdown syntax  Makefile: A GNU make build file to reproduce the data in /data.  build.sh: A bash script to reproduce the data in /data.  sources.csv: A comma-separated values list to be used in the scripts. For straightforward HTTP downloads, prefer fetch.txt.These files are purely for documentation and should not be used by processors in any way.AlgorithmsPacking a workspace as OCRD-ZIPTo pack a workspace to OCRD-ZIP:  Create a temporary folder TMP  Foreach mets:file f in the source METS:          Strip file:// from the beginning of the xlink:href of f      If it is not a file path (begins with http:// or https://):                  If Ocrd-Manifestation-Depth is partial,continue                    Download/Copy the file to a location within TMP/data. The structure SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt; where                  &amp;lt;USE&amp;gt; is the USE attribute of the parent mets:fileGrp          &amp;lt;ID&amp;gt; is the ID attribute of the mets:file                    Replace the URL of f with the path relative to /data (SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt;) in                  all mets:FLocat of the METS          all other files in the workspace, esp. PAGE-XML                      Write out the changed METS to TMP/data/mets.xml  Package TMP as a BagIt bagUnpacking OCRD-ZIP to a workspace  Unzip OCRD-ZIP z to a folder TMP  If the value M of Ocrd-Mets is different from mets.xml:          Rename TMP/data/mets.xml to TMP/data/ + M        Move TMP/data to an appropriate location to use as a workspaceAppendix A - BagIt profile definitionBagIt-Profile-Info:  BagIt-Profile-Identifier: https://ocr-d.de/bagit-profile.json  BagIt-Profile-Version: &#39;1.2.0&#39;  Source-Organization: OCR-D  External-Description: BagIt profile for OCR data  Contact-Name: Konstantin Baierer  Contact-Email: konstantin.baierer@sbb.spk-berlin.de  Version: 0.1Bag-Info:  Bagging-Date:    required: false  Source-Organization:    required: false  Ocrd-Mets:    required: false    default: &#39;mets.xml&#39;  Ocrd-Manifestation-Depth:    required: false    default: partial    values: [&quot;partial&quot;, &quot;full&quot;]  Ocrd-Identifier:    required: true  Ocrd-Checksum:    required: false    # echo -n | sha512sum    default: &#39;cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e&#39;Manifests-Required: [&#39;sha512&#39;]Tag-Manifests-Required: []Tag-Files-Required: []Tag-Files-Allowed:  - README.md  - Makefile  - build.sh  - sources.csv  - metadata/*.xml  - metadata/*.txtAllow-Fetch.txt: trueSerialization: requiredAccept-Serialization: application/zipAccept-BagIt-Version:  - &#39;1.0&#39;Appendix B - IANA considerationsProposed media type of OCRD-ZIP: application/vnd.ocrd+zipProposed extension: .ocrd.zip",
      "url": " /en/spec/ocrd_zip.html"
    },
  

    {
      "slug": "de-spec-ocrd-zip-html",
      "title": "",
      "content"	 : "OCRD-ZIPThis document describes an exchange format to bundle a workspace described by aMETS file following OCR-D’s conventions.RationaleMETS is the exchange format of choice by OCR-D for describing relations offiles such as images and metadata about those images such as PAGE or ALTOfiles. METS is a textual format, not suitable for embedding arbitrary,potentially binary, data. For various use cases (such as transfer via network,long-term preservation, reproducible tests etc.) it is desirable to have aself-contained representation of a workspace.With such a representation, data producers are not forced to providedereferenceable HTTP-URL for the files they produce and data consumers are notforced to dereference all HTTP-URL.While METS does have mechanisms for embedding XML data and even base64-encodedbinary data, the tradeoffs in file size, parsing speed and readability are toogreat to make this a viable solution for a mass digitization scenario.Instead, we propose an exchange format (“OCRD-ZIP”) based on the BagIt specused for data ingestion adopted in the web archiving community.BagIt profileAs a baseline, an OCRD-ZIP must adhere to v0.97+ of the BagItspecs, i.e.  all files in data/  a file bagit.txt  a file bag-info.txtIn accordance with the BagIt standard, bagit.txt MUST consist of exactlythese two lines:BagIt-Version: 1.0Tag-File-Character-Encoding: UTF-8In addition, OCRD-ZIP adhere to a BagItprofile (see Appendix A forthe full definition):  bag-info.txt MUST additionally contain these tags:          BagIt-Profile-Identifier: URL of the OCR-D BagIt profile      Ocrd-Identifier: A globally unique identifier for this bag      Ocrd-Base-Version-Checksum: Checksum of the version this bag is based on        bag-info.txt MAY additionally contain these tags:                Ocrd-Manifestation-Depth: Whether all URL are dereferenced as files or only some      BagIt-Profile-IdentifierThe BagIt-Profile-Identifier must be the string https://ocr-d.de/bagit-profile.json.Ocrd-MetsOcrd-Mets can be provided to declare that the METS file will not be thestandard mets.xml but another path relative to /data/.Implementations MUST check for the Ocrd-Mets tag: If it has a value, look for theMETS file at that location, relative to /data. Otherwise, assume the defaultmets.xml.Ocrd-Manifestation-DepthSpecify whether the bag contains the full manifestation of the data referenced in the METS (full)or only those files that were file:// URLs before (partial). Default: partial.Ocrd-IdentifierA globally unique identifier identifying the work/works/parts of works thisbundle of file represents.This is to be used for repositories to identify new ingestions of existing works.To ensure global uniqueness, the identifier should be prefixed with anidentifier of the organization, e.g. an ISIL or domain name.Ocrd-Base-Version-ChecksumThe SHA512 checksum of the manifest-sha512.txt file of the version this bagwas based on, if any.InvariantsZIPAn OCRD-ZIP MUST be a serialized as a ZIP file.manifest-sha512.txtChecksums for the files in /data must be calculated with the SHA512algorithm only and provided as manifest-sha512.txt.Since the checksum of this manifest file can be relevant (seeOcrd-Base-Version-Checksum), in addition to the requirementsof the BagIt spec, the entries MUST be sorted.NOTE: These checksums can be generated with find data -type f | sort -sf |xargs sha512sum &amp;gt; manifest-sha512.txt.File names must be relative to METSWithin an OCRD-ZIP, all local file resources referenced in the METS (andconsequently all those referenced in other files within the workspace – seerule “If in PAGE then in METS” must berelative to the location of the METS file.Example/tmp/foo/ws1/data├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  /tmp/foo/ws1/data/foo.xml (absolute path)  file:///tmp/foo/ws1/data/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)When in data then in METSAll files except mets.xml itself that are contained in data directory mustbe referenced in a mets:file/mets:Flocat in the mets.xml.When in METS and not in dataDue to partial OCRD-ZIP not all files may be part of the payload. If so they have to be mentioned in fetch.txt and in all payload manifest files.Optional metadata about the payloadIn addition to the actual data files in /data, the following metadata filesare allowed to be present in the root of the bag:  README.md: An extended, human-readable description of the dataset in the Markdown syntax  Makefile: A GNU make build file to reproduce the data in /data.  build.sh: A bash script to reproduce the data in /data.  sources.csv: A comma-separated values list to be used in the scripts. For straightforward HTTP downloads, prefer fetch.txt.These files are purely for documentation and should not be used by processors in any way.AlgorithmsPacking a workspace as OCRD-ZIPTo pack a workspace to OCRD-ZIP:  Create a temporary folder TMP  Foreach mets:file f in the source METS:          Strip file:// from the beginning of the xlink:href of f      If it is not a file path (begins with http:// or https://):                  If Ocrd-Manifestation-Depth is partial,continue                    Download/Copy the file to a location within TMP/data. The structure SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt; where                  &amp;lt;USE&amp;gt; is the USE attribute of the parent mets:fileGrp          &amp;lt;ID&amp;gt; is the ID attribute of the mets:file                    Replace the URL of f with the path relative to /data (SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt;) in                  all mets:FLocat of the METS          all other files in the workspace, esp. PAGE-XML                      Write out the changed METS to TMP/data/mets.xml  Package TMP as a BagIt bagUnpacking OCRD-ZIP to a workspace  Unzip OCRD-ZIP z to a folder TMP  If the value M of Ocrd-Mets is different from mets.xml:          Rename TMP/data/mets.xml to TMP/data/ + M        Move TMP/data to an appropriate location to use as a workspaceAppendix A - BagIt profile definitionBagIt-Profile-Info:  BagIt-Profile-Identifier: https://ocr-d.de/bagit-profile.json  BagIt-Profile-Version: &#39;1.2.0&#39;  Source-Organization: OCR-D  External-Description: BagIt profile for OCR data  Contact-Name: Konstantin Baierer  Contact-Email: konstantin.baierer@sbb.spk-berlin.de  Version: 0.1Bag-Info:  Bagging-Date:    required: false  Source-Organization:    required: false  Ocrd-Mets:    required: false    default: &#39;mets.xml&#39;  Ocrd-Manifestation-Depth:    required: false    default: partial    values: [&quot;partial&quot;, &quot;full&quot;]  Ocrd-Identifier:    required: true  Ocrd-Checksum:    required: false    # echo -n | sha512sum    default: &#39;cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e&#39;Manifests-Required: [&#39;sha512&#39;]Tag-Manifests-Required: []Tag-Files-Required: []Tag-Files-Allowed:  - README.md  - Makefile  - build.sh  - sources.csv  - metadata/*.xml  - metadata/*.txtAllow-Fetch.txt: trueSerialization: requiredAccept-Serialization: application/zipAccept-BagIt-Version:  - &#39;1.0&#39;Appendix B - IANA considerationsProposed media type of OCRD-ZIP: application/vnd.ocrd+zipProposed extension: .ocrd.zip",
      "url": " /de/spec/ocrd_zip.html"
    },
  

    {
      "slug": "en-spec-page-html",
      "title": "Conventions for PAGE",
      "content"	 : "Conventions for PAGEIn addition to these conventions, refer to the PAGE APIdocs for extensivedocumentation on the PAGE XML format itself.Media TypeThe preliminary media type of a PAGEdocument is application/vnd.prima.page+xml, which MUST be used as the MIMETYPE of a &amp;lt;mets:file&amp;gt;representing a PAGE document.One page in one PAGEA single PAGE XML file represents one page in the original document.Every &amp;lt;pc:Page&amp;gt; element MUST have an attribute image which MUST always be the source image.The PAGE XML root element &amp;lt;pc:PcGts&amp;gt; MUST have exactly one &amp;lt;pc:Page&amp;gt;.ImagesURL for imageFilename / filenameThe imageFilename of the &amp;lt;pg:Page&amp;gt; and filename of the&amp;lt;pg:AlternativeImage&amp;gt; element MUST be a filename relative to the mets.xml.All URL used in imageFilename and filename MUST be referenced in a fileGrpin METS. This MUST bethe same file group as the PAGE-XML that was the result of the processing stepthat produced the &amp;lt;pg:AlternativeImage&amp;gt;. In other words:&amp;lt;pg:AlternativeImage&amp;gt; should be written to the same &amp;lt;mets:fileGrp&amp;gt; as its sourcePAGE-XML, which in most implementations will mean the same folder.Original image as imageFilenameThe imageFilename attribute of the &amp;lt;pg:Page&amp;gt; MUST reference the originalimage and MUST NOT change between processing steps.AlternativeImage for derived imagesTo encode images derived from the original image, the &amp;lt;pc:AlternativeImage&amp;gt;should be used. Its filename attribute should reference the URL of thederived image.The comments attribute SHOULD be used according to the AlternativeImage classification.AlternativeImage: classificationThe comments attribute of the &amp;lt;pg:AlternativeImage&amp;gt; attribute should be used  binarized  grayscale_normalized  deskewed  despeckled  cropped  rotated-90 / rotated-180 / rotated-270  dewarpedAlternativeImage on sub-page level elementsFor the results of image processing that changes the positions of pixels (e.g.cropping, rotation, dewarping), AlternativeImage on page level and polygon ofrecognized zones is not sufficient for accessing the section of the image that a region is based onsince coordinates are always relative to the original image.For such use cases, &amp;lt;pg:AlternativeImage&amp;gt; may be used as a child of&amp;lt;pg:TextRegion&amp;gt;, &amp;lt;pg:TextLine&amp;gt;, &amp;lt;pg:Word&amp;gt; or &amp;lt;pg:Glyph&amp;gt;.Attaching text recognition results to elementsA PAGE document can attach recognized text to typographical units ofa page at different levels, such as region (&amp;lt;pg:TextRegion&amp;gt;), line(&amp;lt;pg:TextLine&amp;gt;), word (&amp;lt;pg:Word&amp;gt;) or glyph (&amp;lt;pg:Glyph&amp;gt;).To attach recognized text to an element E, it must be encoded asUTF-8 in a single &amp;lt;pg:Unicode&amp;gt; element U within a &amp;lt;pg:TextEquiv&amp;gt;element T of E.T must be the last element of E.Leading and trailing whitespace (U+0020, U+000A) in the content of a&amp;lt;pg:Unicode&amp;gt; is not significant and must be removed from the string byprocessors.To encode an actual space character at the start or end of the content&amp;lt;pg:Unicode&amp;gt;, use a non-breaking space U+00A0.Text recognition confidenceThe confidence score describing the assumed correctness of the text recognition results in a&amp;lt;pg:TextEquiv&amp;gt; can be expressed in an attribute @conf as a float valuebetween 0 and 1, where 0 means “certainly wrong” and 1 means “certainlycorrect”.Attaching multiple text recognition results to elementsAlternative text recognition results can be expressed by using multiple&amp;lt;pg:TextEquiv&amp;gt; wherever a single &amp;lt;pg:TextEquiv&amp;gt; would be allowed. Whenusing multiple &amp;lt;pg:TextEquiv&amp;gt;, they each must have an attribute @index withan integer number unique per set of &amp;lt;pg:TextEquiv&amp;gt; that allows ranking themin order of preference. @index of the first (preferred) &amp;lt;pg:TextEquiv&amp;gt; must bethe value 1.Consistency of text results on different levelsSince text results can be defined on different levels and those levels can benested, text results information can be redundant. To avoid inconsistencies,the following assertions must be true:  text of &amp;lt;pg:Word&amp;gt; must be equal to the text of all &amp;lt;pg:Glyph&amp;gt;    contained within, concatenated with empty string  text of &amp;lt;pg:TextLine&amp;gt; must be equal to the text of all    &amp;lt;pg:Word&amp;gt; contained  within, concatenated with a single space (U+0020).  text of &amp;lt;pg:TextRegion&amp;gt; must be equal to the text of all    &amp;lt;pg:TextLine&amp;gt; contained within, concatenated with a newline (U+000A).NOTE: “Concatenation” means joining a list of strings with a separator, noseparator is added to the start or end of the resulting string.These assertions are only to be enforced for the first &amp;lt;pg:TextEquiv&amp;gt; of bothcontaining and contained elements, i.e. the only &amp;lt;pg:TextEquiv&amp;gt; of an elementor the &amp;lt;pg:TextEquiv&amp;gt; with @index = 1 if multiple textresults are attached.Consistency strictnessA consistency checker must support four levels of strictness:strictIf any of the assertions fail for a PAGE document, an exceptionshould be raised and the document no further processedlaxIf any of the assertions fail for a PAGE document, another comparisondisregarding all whitespace shall be made. If this still fails, an exceptionshould be raised and the document no further processedfixIf any of the assertions fail for a specific element in PAGE document, the textresults of this element must be recreated, by concatenating the text results ofits children elements. This algorithm needs to be recursive, i.e. if any of thechildren elements is itself inconsistent, its text results must be recreated inthe same way before concatenation.offThese consistency checks are so restrictive to spot data that cannot beunambigiously processed. However, there are valid use cases where the“index-1-consistency” is too narrow, esp. in post-correction with languagemodels. For such use cases, it must be possible to disable the consistencyvalidation altogether in the workflow.Example&amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;f&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;    &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;foof&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;toot&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;&amp;lt;/Word&amp;gt;In this example, the &amp;lt;pg:Word&amp;gt; has text foof butthe concatenation of the first text results of the contained &amp;lt;pg:Glyphs&amp;gt;spells foot. As a result:  Validation should raise an exception for inconsistency.  Data consumers should assume the text result to be foot.TextStyleTypographical information (type, cut etc.) must be documented in PAGE XML using the&amp;lt;TextStyle&amp;gt; element.See the PAGE documentation on TextStyle for all possible values.The &amp;lt;TextStyle&amp;gt; element can be used in all relevant elements:  &amp;lt;TextRegion&amp;gt;  &amp;lt;TextLine&amp;gt;  &amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;Example:&amp;lt;Word&amp;gt;  &amp;lt;TextStyle fontFamily=&quot;Arial&quot; fontSize=&quot;17.0&quot; bold=&quot;true&quot;/&amp;gt;  &amp;lt;!-- [...] --&amp;gt;&amp;lt;/Word&amp;gt;Font familiesThe pg:TextStyle/@fontFamily attribute can list one or more fontfamilies, separated by comma (,).font-families    := font-family (&quot;,&quot; font-family)*font-family      := font-family-name (&quot;:&quot; confidence)?font-family-name := [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot;]+ | &#39;&quot;&#39; [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot; | &quot; &quot;]+ &#39;&quot;&#39;confidence       := (&quot;0&quot; | &quot;1&quot;)? &quot;.&quot; [&quot;0&quot; - &quot;9&quot;]+Font family names that contain a space must be quoted with double quotes (&quot;).Clusters of typesetsSometimes it is necessary to not express that an element is typeset in aspecific font family but in font family from a cluster of related font groups.For such typeset clusters, the pg:TextStyle/@fontFamily attribute should be re-used.This specification doesn’t restrict the naming of font families.However, we recommend to choose one of the following list of type groups names ifapplicable:  textura  rotunda  bastarda  antiqua  greek  hebrew  italic  frakturFont families and confidenceProviding multiple font families means that the element inquestion is set in one of the font families listed.It is not possible to declare that multiple font families are used in anelement. Instead, data producers are advised to increase output granularityuntil every element is set in a single font family.The degree of confidence in the font family can be expressed by concatenatingfont family names with colon (:) followed by a float between 0 (informationis certainly wrong) and 1 (information is certainly correct).If a font family is not suffixed with a confidence value, the confidence isconsidered to be 1.Examples&amp;lt;TextStyle fontFamily=&quot;Arial:0.8, Times:0.7, Courier:0.4&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:.8, Times:0.5&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:1&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial&quot;/&amp;gt;ColumnsTo model columns, use constructs in the &amp;lt;pg:ReadingOrder&amp;gt; of the PAGEdocument.A grid layout must be wrapped in a &amp;lt;pg:OrderedGroup&amp;gt; with a@caption that has the form column_&amp;lt;horizontal&amp;gt;_&amp;lt;vertical&amp;gt; where&amp;lt;vertical&amp;gt; is the number of columns and &amp;lt;horizontal&amp;gt; is the number of rows.&amp;lt;OrderedGroup caption=&quot;column_1_1&quot;&amp;gt; &amp;lt;!-- the default: single column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_2&quot;&amp;gt; &amp;lt;!-- two-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_3&quot;&amp;gt; &amp;lt;!-- three-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_2_3&quot;&amp;gt; &amp;lt;!-- three-column layout split in top and bottom --&amp;gt;Regions that belong to the same column must be grouped within&amp;lt;pg:OrderedGroupIndexed&amp;gt; with a caption that begins with column_&amp;lt;y&amp;gt;_&amp;lt;x&amp;gt;where &amp;lt;y&amp;gt; is the row position and &amp;lt;x&amp;gt; is the column position (counting starts at 1):&amp;lt;OrderedGroup caption=&quot;column_2_2&quot;&amp;gt; &amp;lt;!-- two-column two-row layout --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-right column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-right column --&amp;gt;&amp;lt;/OrderedGroup&amp;gt;",
      "url": " /en/spec/page.html"
    },
  

    {
      "slug": "de-spec-page-html",
      "title": "",
      "content"	 : "Conventions for PAGEIn addition to these conventions, refer to the PAGE APIdocs for extensivedocumentation on the PAGE XML format itself.Media TypeThe preliminary media type of a PAGEdocument is application/vnd.prima.page+xml, which MUST be used as the MIMETYPE of a &amp;lt;mets:file&amp;gt;representing a PAGE document.One page in one PAGEA single PAGE XML file represents one page in the original document.Every &amp;lt;pc:Page&amp;gt; element MUST have an attribute image which MUST always be the source image.The PAGE XML root element &amp;lt;pc:PcGts&amp;gt; MUST have exactly one &amp;lt;pc:Page&amp;gt;.ImagesURL for imageFilename / filenameThe imageFilename of the &amp;lt;pg:Page&amp;gt; and filename of the&amp;lt;pg:AlternativeImage&amp;gt; element MUST be a filename relative to the mets.xml.All URL used in imageFilename and filename MUST be referenced in a fileGrpin METS. This MUST bethe same file group as the PAGE-XML that was the result of the processing stepthat produced the &amp;lt;pg:AlternativeImage&amp;gt;. In other words:&amp;lt;pg:AlternativeImage&amp;gt; should be written to the same &amp;lt;mets:fileGrp&amp;gt; as its sourcePAGE-XML, which in most implementations will mean the same folder.Original image as imageFilenameThe imageFilename attribute of the &amp;lt;pg:Page&amp;gt; MUST reference the originalimage and MUST NOT change between processing steps.AlternativeImage for derived imagesTo encode images derived from the original image, the &amp;lt;pc:AlternativeImage&amp;gt;should be used. Its filename attribute should reference the URL of thederived image.The comments attribute SHOULD be used according to the AlternativeImage classification.AlternativeImage: classificationThe comments attribute of the &amp;lt;pg:AlternativeImage&amp;gt; attribute should be used  binarized  grayscale_normalized  deskewed  despeckled  cropped  rotated-90 / rotated-180 / rotated-270  dewarpedAlternativeImage on sub-page level elementsFor the results of image processing that changes the positions of pixels (e.g.cropping, rotation, dewarping), AlternativeImage on page level and polygon ofrecognized zones is not sufficient for accessing the section of the image that a region is based onsince coordinates are always relative to the original image.For such use cases, &amp;lt;pg:AlternativeImage&amp;gt; may be used as a child of&amp;lt;pg:TextRegion&amp;gt;, &amp;lt;pg:TextLine&amp;gt;, &amp;lt;pg:Word&amp;gt; or &amp;lt;pg:Glyph&amp;gt;.Attaching text recognition results to elementsA PAGE document can attach recognized text to typographical units ofa page at different levels, such as region (&amp;lt;pg:TextRegion&amp;gt;), line(&amp;lt;pg:TextLine&amp;gt;), word (&amp;lt;pg:Word&amp;gt;) or glyph (&amp;lt;pg:Glyph&amp;gt;).To attach recognized text to an element E, it must be encoded asUTF-8 in a single &amp;lt;pg:Unicode&amp;gt; element U within a &amp;lt;pg:TextEquiv&amp;gt;element T of E.T must be the last element of E.Leading and trailing whitespace (U+0020, U+000A) in the content of a&amp;lt;pg:Unicode&amp;gt; is not significant and must be removed from the string byprocessors.To encode an actual space character at the start or end of the content&amp;lt;pg:Unicode&amp;gt;, use a non-breaking space U+00A0.Text recognition confidenceThe confidence score describing the assumed correctness of the text recognition results in a&amp;lt;pg:TextEquiv&amp;gt; can be expressed in an attribute @conf as a float valuebetween 0 and 1, where 0 means “certainly wrong” and 1 means “certainlycorrect”.Attaching multiple text recognition results to elementsAlternative text recognition results can be expressed by using multiple&amp;lt;pg:TextEquiv&amp;gt; wherever a single &amp;lt;pg:TextEquiv&amp;gt; would be allowed. Whenusing multiple &amp;lt;pg:TextEquiv&amp;gt;, they each must have an attribute @index withan integer number unique per set of &amp;lt;pg:TextEquiv&amp;gt; that allows ranking themin order of preference. @index of the first (preferred) &amp;lt;pg:TextEquiv&amp;gt; must bethe value 1.Consistency of text results on different levelsSince text results can be defined on different levels and those levels can benested, text results information can be redundant. To avoid inconsistencies,the following assertions must be true:  text of &amp;lt;pg:Word&amp;gt; must be equal to the text of all &amp;lt;pg:Glyph&amp;gt;    contained within, concatenated with empty string  text of &amp;lt;pg:TextLine&amp;gt; must be equal to the text of all    &amp;lt;pg:Word&amp;gt; contained  within, concatenated with a single space (U+0020).  text of &amp;lt;pg:TextRegion&amp;gt; must be equal to the text of all    &amp;lt;pg:TextLine&amp;gt; contained within, concatenated with a newline (U+000A).NOTE: “Concatenation” means joining a list of strings with a separator, noseparator is added to the start or end of the resulting string.These assertions are only to be enforced for the first &amp;lt;pg:TextEquiv&amp;gt; of bothcontaining and contained elements, i.e. the only &amp;lt;pg:TextEquiv&amp;gt; of an elementor the &amp;lt;pg:TextEquiv&amp;gt; with @index = 1 if multiple textresults are attached.Consistency strictnessA consistency checker must support four levels of strictness:strictIf any of the assertions fail for a PAGE document, an exceptionshould be raised and the document no further processedlaxIf any of the assertions fail for a PAGE document, another comparisondisregarding all whitespace shall be made. If this still fails, an exceptionshould be raised and the document no further processedfixIf any of the assertions fail for a specific element in PAGE document, the textresults of this element must be recreated, by concatenating the text results ofits children elements. This algorithm needs to be recursive, i.e. if any of thechildren elements is itself inconsistent, its text results must be recreated inthe same way before concatenation.offThese consistency checks are so restrictive to spot data that cannot beunambigiously processed. However, there are valid use cases where the“index-1-consistency” is too narrow, esp. in post-correction with languagemodels. For such use cases, it must be possible to disable the consistencyvalidation altogether in the workflow.Example&amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;f&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;    &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;foof&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;toot&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;&amp;lt;/Word&amp;gt;In this example, the &amp;lt;pg:Word&amp;gt; has text foof butthe concatenation of the first text results of the contained &amp;lt;pg:Glyphs&amp;gt;spells foot. As a result:  Validation should raise an exception for inconsistency.  Data consumers should assume the text result to be foot.TextStyleTypographical information (type, cut etc.) must be documented in PAGE XML using the&amp;lt;TextStyle&amp;gt; element.See the PAGE documentation on TextStyle for all possible values.The &amp;lt;TextStyle&amp;gt; element can be used in all relevant elements:  &amp;lt;TextRegion&amp;gt;  &amp;lt;TextLine&amp;gt;  &amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;Example:&amp;lt;Word&amp;gt;  &amp;lt;TextStyle fontFamily=&quot;Arial&quot; fontSize=&quot;17.0&quot; bold=&quot;true&quot;/&amp;gt;  &amp;lt;!-- [...] --&amp;gt;&amp;lt;/Word&amp;gt;Font familiesThe pg:TextStyle/@fontFamily attribute can list one or more fontfamilies, separated by comma (,).font-families    := font-family (&quot;,&quot; font-family)*font-family      := font-family-name (&quot;:&quot; confidence)?font-family-name := [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot;]+ | &#39;&quot;&#39; [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot; | &quot; &quot;]+ &#39;&quot;&#39;confidence       := (&quot;0&quot; | &quot;1&quot;)? &quot;.&quot; [&quot;0&quot; - &quot;9&quot;]+Font family names that contain a space must be quoted with double quotes (&quot;).Clusters of typesetsSometimes it is necessary to not express that an element is typeset in aspecific font family but in font family from a cluster of related font groups.For such typeset clusters, the pg:TextStyle/@fontFamily attribute should be re-used.This specification doesn’t restrict the naming of font families.However, we recommend to choose one of the following list of type groups names ifapplicable:  textura  rotunda  bastarda  antiqua  greek  hebrew  italic  frakturFont families and confidenceProviding multiple font families means that the element inquestion is set in one of the font families listed.It is not possible to declare that multiple font families are used in anelement. Instead, data producers are advised to increase output granularityuntil every element is set in a single font family.The degree of confidence in the font family can be expressed by concatenatingfont family names with colon (:) followed by a float between 0 (informationis certainly wrong) and 1 (information is certainly correct).If a font family is not suffixed with a confidence value, the confidence isconsidered to be 1.Examples&amp;lt;TextStyle fontFamily=&quot;Arial:0.8, Times:0.7, Courier:0.4&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:.8, Times:0.5&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:1&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial&quot;/&amp;gt;ColumnsTo model columns, use constructs in the &amp;lt;pg:ReadingOrder&amp;gt; of the PAGEdocument.A grid layout must be wrapped in a &amp;lt;pg:OrderedGroup&amp;gt; with a@caption that has the form column_&amp;lt;horizontal&amp;gt;_&amp;lt;vertical&amp;gt; where&amp;lt;vertical&amp;gt; is the number of columns and &amp;lt;horizontal&amp;gt; is the number of rows.&amp;lt;OrderedGroup caption=&quot;column_1_1&quot;&amp;gt; &amp;lt;!-- the default: single column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_2&quot;&amp;gt; &amp;lt;!-- two-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_3&quot;&amp;gt; &amp;lt;!-- three-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_2_3&quot;&amp;gt; &amp;lt;!-- three-column layout split in top and bottom --&amp;gt;Regions that belong to the same column must be grouped within&amp;lt;pg:OrderedGroupIndexed&amp;gt; with a caption that begins with column_&amp;lt;y&amp;gt;_&amp;lt;x&amp;gt;where &amp;lt;y&amp;gt; is the row position and &amp;lt;x&amp;gt; is the column position (counting starts at 1):&amp;lt;OrderedGroup caption=&quot;column_2_2&quot;&amp;gt; &amp;lt;!-- two-column two-row layout --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-right column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-right column --&amp;gt;&amp;lt;/OrderedGroup&amp;gt;",
      "url": " /de/spec/page.html"
    },
  

    {
      "slug": "en-phase2-html",
      "title": "Phase II",
      "content"	 : "Module Projects Phase IIIn the first project phase, a functional model for the OCR-D workflow wasdeveloped. Full text recognition is seen as a complex process that includesseveral upstream and downstream steps in addition to the actual textrecognition. First, a digital image is preprocessed for text recognition bycropping, deskewing, dewarping, despeckling and binarizing it into a black andwhite image. This is followed by layout recognition, which identifies the textareas of a page down to line level. The recognition of the lines or thebaseline is particularly important for the subsequent text recognition, whichis based on neural networks in all modern approaches. The individual structuresor elements of the fully text-recognized document are then classified accordingto their typographic function before the OCR result is improved in thepost-correction, if necessary. Finally the end result is transferred torepositories for long-term archiving.From the project proposals for the DFG’s module project call in March 2017,eight projects were approved:Scalable methods of text and structure recognition for full text digitization of historical prints: Image Optimisation        German Research Center for Artificial Intelligence (DFKI)Project participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungDFKI was involved in the OCR-D project with two modules: Image optimization andlayout recognition. In both modules several processors were developed andintegrated into the OCR-D software system.The first module project image optimization focused on the pre-processing ofthe digitized material with the aim of improving the image quality and thus theperformance of the subsequent OCR modules. For this purpose, tools forbinarization, deskewing, cropping and dewarping were implemented.The cropping tool based on computer vision is particularly noteworthy for itsperformance. It predominantly achieves very good results on the entire projectdata. The dewarping tool is also interesting due to its novel architecture.Generative neural networks are used to generate equalized variants of imagesinstead of determining explicit transformations for the equalization.Scalable text and structure recognition methods for the full text digitization of historical prints: Layout Recognition        DFKIProject participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungIn the second DFKI module project layout recognition, the aim was to extractthe document structure, both of individual document pages and the entiredocument. On the one hand, the metadata obtained in this way helps to digitizethe document as a whole, on the other hand, the extraction of certain documentstructures is necessary. For example, most OCR methods can only processindividual lines of text. The tools developed are used for text-non-textsegmentation, block segmentation and classification, text line detection andstructure analysis.One focus of development was the combined block segmentation and classificationbased on the MaskRCNN architecture known from video and image segmentation.This tool works with the unprocessed raw data, so that on the one hand nopre-processing is necessary and on the other hand the full information spectrumcan be used.Further development of a semi-automated open source tool for layout analysis and region extraction and classification (LAREX) of early printing        Julius-Maximilians-University of Würzburg   Institute of Computer Science: Chair of Artificial Intelligence and Applied Computer ScienceProject participants: Frank Puppe, Alexander GehrkeGitHub: ocr-d-modul-2-segmentierungAt the Department of Computer Science VI at the University of Würzburg, LAREXwas developed in the preliminary work. LAREX is a comfortable editor forannotating regions and layout elements on book pages. In the furtherdevelopment of the OCR-D module project, the focus was not only on improvingefficient operability but also on expanding automatic procedures.For this purpose a Convolutional-Neural-Net (CNN) was implemented and trained,which assigns each pixel of a page scan a classification in different classesin order to separate image and text. By considering the pixels of only oneclass each, a segmentation of the page is then carried out with classicalmethods. Another tested approach first used classical segmentation methods andthen classified the segments.The segmentation method based on CNN output was adapted to the OCR-Dinterfaces. Good results were achieved on pure text pages or pages with clearlyseparated images. There is potential for improvement especially in therecognition of decorative initials of older prints and other images close tothe text as well as multi-column layouts.NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers        University of Leipzig Institut für Informatik: Department of Automatic Language ProcessingProject participants: Gerhard Heyer, Robert SachunskyGitHub: ASVLeipzig/cor-asv-fstA fully automatic post-correction separately from the actual OCR only makessense if statistical knowledge about “correct text” and about typical OCRerrors is added a priori. Neural networks (NN) as well as weighted finitetransducers (WFST), which can be trained on corresponding additional data, aresuitable for this purpose.For the implementation of a combined architecture of NN and FST it was decided to implement three modules:  a pure NN solution with continuously (end-to-end) trained model on thecharacter level alone - as a deep (multi-layer), bidirectional recurrentnetwork according to the encoder-decoder scheme (for different input andoutput lengths) with an attention mechanism and A*-Beamsearch withadjustable rejection threshold (against overcorrection), i.e.post-correction of text lines is treated like machine translation,  a NN language model (LM) at the character level - as a deep (multi-layer),bidirectional recurrent network with interface for graph input andincremental decoding  a WFST component with an error model to be trained explicitly on glyph leveland word model/lexicon, as well as connection to 2. - via WFST compositionof input graph with error and word model according to the sliding windowprinciple, conversion of the single windows to one hypothesis graph per textline, and combination of the respective output weights with LM-evaluationsin an efficient search for the best path.The combination of 3. with 2. thus represents a hybrid solution. But also 1.can benefit from 2. (if the same network topology is used) by initializing theweights from a language model trained on larger amounts of pure text (transferlearning).Both approaches benefit from a close connection to the OCR search space, i.e. atransfer of alternative character hypotheses and their confidence (as so faronly possible with Tesseract and realized in cooperation with the moduleproject of the Mannheim University Library). However, they also deliver goodresults on pure full text (with CER reduction of up to 5%), provided thatsufficient suitable training data is available and the OCR itself deliversuseful results (below 10% CER).Command line interfaces for training and evaluation as well as full OCR-Dinterfaces for processing and evaluation are available for all modules.Optimized use of OCR processes – Tesseract as a component in the OCR-D workflow        University of Mannheim University Library MannheimProject participants: Stefan Weil, Noah MetzgerGitHub: tesseract-ocr/tesseract/The module project focused on the OCR software Tesseract, which has beendeveloped by Ray Smith since 1985, since 2005 as open source under a freelicense.The project had two main goals: The integration of Tesseract into the OCR-Dworkflow including support of the other module projects by providinginterfaces, and the general improvement of stability, code quality andperformance of Tesseract.The integration into the OCR-D workflow required much less effort thanoriginally planned; mainly because most of the work had already been doneoutside the module project and the existing Python interface tesserocr could beused.For the OCR-D module project of the University of Leipzig, Tesseract wasextended to generate alternative OCR results for the single characters. Asinput data for an OCR post-correction model, text recognition can thus befurther improved. A valuable side-effect of the new code are more accuratecharacter and word coordinates.With several hundred corrections, the code quality was significantly improvedand a much more stable program flow was achieved. Tesseract is now moremaintainable, requires less memory and is faster than before.A significant improvement in recognition accuracy for most of the printingunits relevant for OCR-D was achieved by new generic models for Tesseract.These were trained from September 2019 until January 2020 on the basis of thedata collection GT4HistOCR.Automatic post-correction of historical OCR captured prints with integrated optional interactive correction        Ludwig-Maximilians-University of MunichCentre for Information and Language Processing (CIS)Project participants: Klaus Schulz, Floran Fink, Tobias EnglmeierGitHub: https://github.com/cisocrgroup/ocrd-postcorrection, https://github.com/cisocrgroup/cis-ocrd-pyThe result of the project is a A-I-PoCoTo system integrated into the OCR-D workflow for fully automatic post-correction of full text recognized historical prints. The system also includes an optional interactive post-correction (I-PoCoTo), which is integrated into the interactive post-correction system PoCoWeb. The system can thus be used alternatively as a stand-alone tool for collaborative web-based post-correction of OCR documents.The basis of the fully automatic post-correction is a flexible, feature-based Machine Learning (ML) procedure for fully automatic OCR post-correction with a special focus on avoiding the problem of disimprovement. The system uses the document-dependent profiling technology developed at CIS to detect errors and to generate correction candidates. In addition to various confidence values, the features of the system also use information from additional auxiliary OCRs.The system logs all correction decisions. Via this protocol mechanism the automatic post correction in PoCoWeb can be checked interactively. You can manually undo individual correction decisions that have been made, and also subsequently execute correction decisions that have not been made.The entire system is integrated into the OCR-D workflow and follows the conventions valid there.Development of a model repository and an automatic font recognition for OCR-D        University LeipzigInstitute of Computer Science: Chair of Digital HumanitiesFriedrich Alexander University Erlangen-NurembergDepartment of Computer Science: Chair of Computer Science 5: Pattern RecognitionJohannes Gutenberg University MainzGutenberg Institute for World Literature and Writing-Oriented Media: Department of Book ScienceProject participants: Gregory Crane, Nikolaus Weichselbaumer, Saskia Limbach, Andreas Meier, Vincent Christlein, Mathias Seuret, Rui DongGitHub: OCR-D/okralact, https://github.com/seuretm/ocrd_typegroups_classifierThe recognition rates of OCR for prints produced before 1800 vary greatly, asthe diversity of historical fonts is either not taken into account at all oronly insufficiently in the training data. Therefore this module project,consisting of computer scientists and book historians, has set itself threegoals:On the one hand, we have developed a tool for the automatic recognition offonts in digitised images. Here, we have concentrated especially on brokenfonts besides fracture, which have received little attention so far, but werewidely used in the 15th and 16th centuries: Bastarda, Rotunda, Textura andSchwabacher. The tool has been trained with 35,000 images and achieves anaccuracy of 98% in determining fonts. Overall, it can not only differentiatebetween the above mentioned fonts, but also distinguish between Hebrew, Greek,Fraktur, Antiqua and Italic.In a second step, an online training infrastructure was created (Okralact). Itsimplifies the use of different OCR engines (Tesseract, Ocropus, Octopus,Calamari) and at the same time makes it possible to train specific models forcertain fonts.Finally, a model repository has been set up that contains already developedfont-specific OCR models. To lay a foundation here, we have transcribed a totalof about 2,500 lines for Bastarda, Textura and Schwabacher from a variety ofdifferent books.The high accuracy of the font recognition tool opens up the possibility ofhaving the tool even distinguish between the fonts of individual printers inthe future through further training data, which would address severaldesiderata of historical research.OLA-HD – An OCR-D long-term archive for historical books        Georg-August-University of GöttingenState and University Library of Lower SaxonySociety for Scientific Data Processing mbH GöttingenProject participants: Mustafa Dogan, Kristine Schima-Voigt, Philip Wieder, Triet Doan, Jörg-Holger PanzerGitHub: subugoe/OLA-HD-IMPLIn September 2018 the Digital Library Department of the State and University Library of Lower Saxony and the Gesellschaft für wissenschaftliche Datenverarbeitung Göttingen started the DFG project OLA-HD - Ein OCR-D Langzeitarchiv für historische Drucke.The aim of OLA-HD is to develop an integrated concept for long-term archivingand persistent identification of OCR objects, as well as a prototypicalimplementation.In regular exchange with the project partners, the basic requirements forlong-term archiving and persistent identification were determined and recordedin the form of a specification for technical and economic-organizationalimplementation.With the prototype the user can upload OCR results of a work as OCRD-ZIP intothe system. The system validates the zip file, assigns a PID and sends the fileto the archive manager (CDSTAR - GWDG Common Data StorageArchitecture). This writes the Zip file to thearchive (tape storage). Depending on the configuration (file type, file size,etc.), files are also written to an online storage (hard disk) for fast access.The user has access to all OCR versions and can download versions as BagIt-Zipfiles. All works and versions have their own PIDs. The PIDs are generated bythe European Persistent Identifier Consortium(ePIC) service. The different OCR versions ofa work are linked via the PID, so that the system can map the versioning in atree structure.Users who are not logged in can browse the inventory and preview text and - ifavailable - images in the file structure or navigate through the differentversions. Users can register and log in via the GWDG portal and manage theirfiles via a dashboard.By March 2020, minor optimizations will be made to the user interface and theconcept will be finalized. The concept will describe further expansion stagesthat may be useful for transferring the prototype software into a product.",
      "url": " /en/phase2.html"
    },
  

    {
      "slug": "de-phase2-html",
      "title": "Phase II",
      "content"	 : "Modulprojekte Phase IIAus den Projektanträgen für die Modulprojektausschreibung der DFG im März 2017 wurden acht Projekte bewilligt:Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Bildoptimierung        Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)Das DFKI war als Projektpartner im OCR-D Projekt mit zwei Modulen vertreten:Bildoptimierung und Layouterkennung. In beiden Modulen wurden mehrereProzessoren entwickelt und in das OCR-D-Softwaresystem integriert.Das erste Modul-Projekt Bildoptimierung fokussierte sich auf dieVorverarbeitung der Digitalisate mit dem Ziel, die Bildqualität und somit auchdie Performanz der nachfolgenden OCR-Module zu verbessern. Dafür wurdenWerkzeuge für die Binarisierung, das Deskewing, das Cropping und das Dewarpingimplementiert.Das auf Computer Vision basierte Cropping-Werkzeug ist als besonders performanthervorzuheben. Es erzielt auf den gesamten Projektdaten vorwiegend sehr guteErgebnisse. Auch das Dewarping-Werkzeug ist aufgrund seiner neuartigenArchitektur interessant. Mit Hilfe generativer neuronaler Netze werdenentzerrte Varianten von Bildern generiert, anstatt explizite Transformationenfür die Entzerrung zu bestimmen.Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Layouterkennung        DFKIGitHub: mjenckel/OCR-D-LAYoutERkennung/tree/masterIm zweiten Modul-Projekt des DFKI Layouterkennung galt es, dieDokumentstruktur, sowohl einzelner Dokumentseiten als auch im Gesamtdokument,zu extrahieren. Die dabei gewonnenen Metadaten helfen zum einen, das Dokumentals Ganzes zu digitalisieren, zum anderen ist das Extrahieren bestimmterDokumentstrukturen notwendig. Die meisten OCR-Methoden können z.B. nur einzelneTextzeilen verarbeiten. Die entwickelten Werkzeuge dienen derText-Nicht-Text-Segmentierung, der Blocksegmentierung und -klassifizierung, derTextzeilenerkennung sowie der Strukturanalyse.Ein Entwicklungsschwerpunkt war die kombinierte Blocksegmentierung und-klassifizierung, welche auf der, aus der Video- und Bildsegmentierungbekannten, MaskRCNN-Architektur basiert. Dieses Werkzeug arbeitet mit denunbearbeiteten Rohdaten, sodass einerseits keine Vorverarbeitung notwendig istund andererseits das volle Informationsspektrum ausgenutzt werden kann.Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken        Julius-Maximilians-Universität Würzburg Institut für Informatik: Lehrstuhl für Künstliche Intelligenz und angewandte InformatikGitHub: ocr-d-modul-2-segmentierungAm Lehrstuhl für Informatik VI der Uni Würzburg wurde in den Vorarbeiten LAREXentwickelt, ein komfortabler Editor zur Annotation von Regionen undLayout-Elementen auf Buchseiten. Bei der Weiterentwicklung imOCR-D-Modulprojekt lag der Schwerpunkt neben der Verbesserung der effizientenBedienbarkeit vor allem auch in dem Ausbau der automatischen Verfahren.Hierzu wurde ein Convolutional-Neural-Net (CNN) implementiert und trainiert,welches jedem Pixel eines Seitenscans eine Einordnung in verschiedene Klassenzuweist, um so Bild und Text zu trennen. Unter Betrachtung der Pixel je nureiner Klasse wird anschließend mit klassischen Verfahren eine Segmentierung derSeite durchgeführt. Ein weiterer getesteter Ansatz nutzte zuerst klassischeSegmentierungsverfahren und ordnete die Segmente anschließend ein.Das auf der CNN-Ausgabe basierende Segmentierungsverfahren wurde an dieOCR-D-Schnittstellen angepasst. Auf reinen Textseiten oder Seiten mit deutlichabgetrennten Bildern wurden gute Ergebnisse erzielt. Verbesserungspotentialbesteht vor allem bei der Erkennung von Zierinitialen älterer Drucke undweiteren nah am Text liegenden Bildern sowie mehrspaltigen Layouts.NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers        Universität Leipzig   Institut für Informatik: Abteilung Automatische Sprachverarbeitung_GitHub: ASVLeipzig/cor-asv-fstEine vollautomatische Nachkorrektur separat von der eigentlichen OCR ist immernur dann sinnvoll, wenn dabei statistisches Wissen über “richtigen Text” undüber typische OCR-Fehler a priori hinzukommt. Dafür eignen sich neuronaleNetze (NN) ebenso wie gewichtete endliche Transduktoren (WFST), die aufentsprechenden zusätzlichen Daten trainiert werden können.Für die Umsetzung einer kombinierten Architektur aus NN und FST wurdeentschieden, drei Module zu implementieren:  eine reine NN-Lösung mit durchgehend (end-to-end) trainiertem Modell  allein auf Zeichenebene – als tiefes (mehrschichtiges), bidirektionales  rekurrentes Netzwerk nach dem Encoder-Decoder-Schema (für verschiedene  Eingabe- und Ausgabelänge) mit Attention-Mechanismus und A*-Beamsearch mit  einstellbarer Rückweisungsschwelle (gegen Überkorrektur), d.h. die  Nachkorrektur von Textzeilen wird wie maschinelle Übersetzung behandelt,  ein NN-Sprachmodell (LM) auf Zeichenebene – als tiefes (mehrschichtiges),  bidirektionales rekurrentes Netzwerk mit Schnittstelle für Graph-Eingabe  und inkrementeller Dekodierung,  eine WFST-Komponente mit explizit zu trainierendem Fehlermodell auf  Zeichenebene und Wortmodell/Lexikon, sowie Anbindung an 2. – per  WFST-Komposition von Eingabegraph mit Fehler- und Wortmodell nach  Sliding-Window-Prinzip, Konversion der Einzelfenster zu einem  Hypothesengraph pro Textzeile, und Kombination der jeweiligen  Ausgabegewichte mit LM-Bewertungen in einer effizienten Suche nach dem  besten Pfad.Die Kombination von 3. mit 2. stellt also eine hybride Lösung dar. Aber auch 1.kann von 2. profitieren (sofern die gleiche Netzwerk-Topologie benutzt wird),indem die Gewichte aus einem auf größeren Mengen reinem Text trainiertenSprachmodell initialisiert werden (Transfer-Learning).Beide Ansätze profitieren von einer engen Anbindung an den OCR-Suchraum, d.h.eine Übergabe alternativer Zeichen-Hypothesen und ihrer Konfidenz (wie bishernur mit Tesseract möglich und in Zusammenarbeit mit dem Modulprojekt der UBMannheim realisiert). Sie liefern aber auch auf reinem Volltext bereits guteErgebnisse (mit CER-Reduktion von bis zu 5%), sofern genügend passendeTrainingsdaten zur Verfügung stehen und die OCR ihrerseits brauchbareErgebnisse (unterhalb 10% CER) liefert.Für alle Module stehen Kommandozeilen-Schnittstellen für Training undEvaluierung, sowie volle OCR-D-Schnittstellen für Prozessierung und Evaluierungzur Verfügung.Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow        Universität MannheimUniversitätsbibliothek MannheimGitHub: tesseract-ocr/tesseract/Im Fokus des Modulprojekts stand die OCR-Software Tesseract, die seit 1985 vonRay Smith entwickelt wurde, seit 2005 als Open Source unter einer freienLizenz.Das Projekt umfasste zwei Hauptziele: Die Einbindung von Tesseract in denOCR-D-Workflow inklusive Unterstützung der anderen Modulprojekte durch dieBereitstellung von Schnittstellen, sowie die allgemeine Verbesserung derStabilität, Codequalität und Performance von Tesseract.Die Einbindung in den OCR-D-Workflow erforderte wesentlich weniger Aufwand alsursprünglich geplant; hauptsächlich, weil die meiste Arbeit bereits außerhalbdes Modulprojekts geleistet war und dabei die schon vorhandenePython-Schnittstelle tesserocr genutzt werden konnte.Für das OCR-D-Modulprojekt der Universität Leipzig wurde Tesseract um dieGenerierung von alternativen OCR-Ergebnissen für die Einzelzeichen erweitert.Als Eingabedaten für ein OCR-Postkorrektur-Modell lässt sich damit dieTexterkennung weiter verbessern. Ein wertvoller Nebeneffekt des neuen Codessind genauere Zeichen- und Wortkoordinaten.Mit mehreren hundert Korrekturen konnte die Codequalität signifikant gesteigertund ein deutlich stabilerer Programmfluss erreicht werden. Tesseract ist jetztwartbarer, braucht weniger Speicher und ist schneller als zuvor.Eine wesentliche Verbesserung der Erkennungsgenauigkeit für die meisten der fürOCR-D relevanten Druckwerke konnte durch neue generische Modelle für Tesseracterreicht werden. Diese wurden ab Septem-ber 2019 bis Januar 2020 auf Basis derDatensammlung GT4HistOCR trainiert.Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur        Ludwig-Maximilians-Universität MünchenCentrum für Informations- und Sprachverarbeitung (CIS)GitHub: cisocrgroup/ocrd-postcorrection](https://github.com/cisocrgroup/ocrd-postcorrection), cisocrgroup/cis-ocrd-pyDas Ergebnis des Projekts ist ein in den OCR-D-Workflow integriertes SystemA-I-PoCoTo zur vollautomati-schen Nachkorrektur OCR-erfasster historischerDrucke. Das System beinhaltet zudem eine optional nachge-schaltete interaktiveNachkorrektur (I-PoCoTo), die in das interaktive NachkorrektursystemPoCoWeb einge-bunden ist. Das System kann damit auch alternativ alsStand-Alone-Tool zur gemeinschaftlichen webbasierten Nachkorrektur vonOCR-Dokumenten eingesetzt werden.Die Grundlage der vollautomatischen Nachkorrektur ist ein flexibles,featurebasiertes Machine-Learning (ML) Verfahren zur vollautomatischenOCR-Nachkorrektur mit einem besonderen Fokus auf die Vermeidung derVerschlimmbesserungsproblematik. Zur Erkennung von Fehlern und für dieErzeugung von Korrekturkandida-ten verwendet das System die am CIS entwickeltedokumentenabhängige Profilierungstechnologie. Die Fea-tures des Systemsverwenden neben verschiedenen Konfidenzwerten insbesondere auch Informationenaus zusätzlichen Hilfs-OCRs.Das System protokolliert sämtliche Korrekturentscheidungen. Über diesenProtokollmechanismus kann die automatische Postkorrektur in PoCoWebinteraktiv überprüft werden. Dabei können sowohl einzelne getätigteKorrekturentscheidungen manuell rückgängig gemacht werden, als auch nichtgetätigte Korrekturentschei-dungen nachträglich ausgeführt werden.Das gesamte System ist in den OCR-D-Workflow eingebunden und folgt den dortgültigen Konventionen.Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D        Universität LeipzigInstitut für Informatik: Lehrstuhl für Digital HumanitiesFriedrich-Alexander-Universität Erlangen-Nürnberg Department Informatik: Lehrstuhl für Informatik 5: MustererkennungJohannes Gutenberg-Universität Mainz Gutenberg-Institut für Weltliteratur und schriftorientierte Medien: Abteilung Buchwissenschaft_GitHub: OCR-D/okralact, seuretm/ocrd_typegroups_classifierDie Erkennungsquoten von OCR für Drucke, die vor 1800 produziert wurden,variieren sehr stark, da die Diversität historischer Schriftarten in denTrainingsdaten entweder gar nicht oder nur unzureichend berücksichtigt wird.Daher hat sich dieses Modulprojekt, bestehend aus Informatikerinnen undBuchhistorikerinnen, drei Ziele gesteckt:Zum einen haben wir ein Tool zur automatischen Erkennung von Schriftarten inBilddigitalisaten entwickelt. Hier haben wir uns besonders auf gebrocheneSchriften neben der Fraktur konzentriert, die bisher wenig Beachtung gefundenhaben, jedoch im 15. und 16. Jahrhundert weit verbreitet waren: Bastarda,Rotunda, Textura und Schwabacher. Das Tool wurde mit 35.000 Bildern trainiertund erreicht eine Genauigkeit von 98% bei der Bestimmung von Schriftarten.Insgesamt kann es nicht nur zwischen den o.g. Schriftarten differenzieren,sondern auch Hebräisch, Griechisch, Fraktur, Antiqua und Kursiv unterscheiden.In einem zweiten Schritt wurde eine Online-Trainingsinfrastruktur geschaffen(Okralact). Sie vereinfacht die Benutzung verschiedener OCR-engines (Tesseract,Ocropus, Kraken, Calamari) und ermöglicht es zugleich, spezifische Modelle fürbestimmte Schriftarten zu trainieren.Zum Abschluss wurde ein Modellrepositorium eingerichtet, das bereitserarbeitete schriftartspezifische OCR-Modelle enthält. Um hier einen Grundstockzu legen, haben wir insgesamt ca. 2.500 Zeilen für Bastarda, Textura undSchwabacher aus einer Vielzahl verschiedener Bücher transkribiert.Die hohe Genauigkeit des Tools zur Erkennung der Schriftarten eröffnet dieMöglichkeit, in Zukunft durch weitere Trainingsdaten das Tool sogar zwischenden Schriften einzelner Drucker unterscheiden zu lassen, was mehrere Desiderateder historischen Forschung adressieren würde.OLA-HD – Ein OCR-D-Langzeitarchiv für historische Drucke        Georg-August-Universität Göttingen  Niedersächsische Staats- und Universitätsbibliothek   Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen GitHub: subugoe/OLA-HD-IMPLIm September 2018 starteten die Abteilung Digitale Bibliothek derNiedersächsischen Staats- und Universi-tätsbibliothek und die Gesellschaft fürwissenschaftliche Datenverarbeitung Göttingen das DFG-Projekt OLA-HD – EinOCR-D Langzeitarchiv für historischeDrucke.Ziel von OLA-HD ist die Entwicklung eines integrierten Konzepts für dieLangzeitarchivierung und persistente Identifizierung von OCR-Objekten, sowieeine prototypische Implementierung.Im regelmäßigen Austausch mit den Projektpartnern wurden dieBasis-Anforderungen für die Langzeitarchivierung und persistente Identifikationermittelt und in Form einer Spezifikation zur technischen undwirtschaftlich-organisatorischen Umsetzung festgehalten.Mit dem Prototypen kann der Anwender OCR-Ergebnisse eines Werkes als OCRD-ZIPin das System laden. Das System validiert die Zip-Datei, vergibt eine PID undschickt die Datei an den Archiv-Manager (CDSTAR – GWDG Common Data StorageArchitecture). Dieser schreibt die Zip-Datei in dasArchiv (Bandspeicher). Abhängig von der Konfiguration (Datei-Typ, Datei-Größeetc.) werden Dateien zusätzlich in ein Online Storage geschrieben (Festplatte),um einen schnellen Zugriff zu ermöglichen. Der Nutzer hat Zugriff auf alleOCR-Versionen und kann Versionen als BagIt-Zip Dateien herunterladen. AlleWerke und Versionen haben eigene PIDs. Die PIDs werden vom European PersistentIdentifier Consortium (ePIC) Servicegeneriert. Die verschiedenen OCR-Versionen eines Werkes sind über die PIDverknüpft, sodass das System die Versionierung in einer Baumstruktur abbildenkann.Nicht angemeldete Anwender können den Bestand durchsuchen und in derDateistruktur eine Vorschau von Text und – sofern vorhanden – Bild erhaltenbzw. über die verschiedenen Versionen navigieren. Die Anwender können sich überdas GWDG-Portal registrieren und anmelden und können über ein Dashboard ihreDateien verwalten.Bis März 2020 werden kleinere Optimierungen am User-Interface vorgenommen unddas Konzept finalisiert. Im Konzept werden weitere Ausbaustufen beschrieben,die sinnvoll sein können, um die prototypische Soft-ware in ein Produkt zuüberführen.",
      "url": " /de/phase2.html"
    },
  

    {
      "slug": "en-phase3-html",
      "title": "",
      "content"	 : "OCR-D - Phase IIIIn February 2020, the DFG published a call for proposals to continue the OCR-D project in a third project phase. The goal of this phase is the implementation of the OCR-D software in institutions that maintain and process collections. Four implementation and three module projects were approved by the DFG.On 30 July, our kick-off workshop took place, heralding phase III of OCR-D. The team of the Coordination Project introduced objectives and communications channels of phase III, gave an insight into the current status and plans of the OCR-D software, the Web API and the handling of Ground Truth Data in OCR-D. Furthermore, the Coordination Project presented Best Practices of Software Developing in OCR-D, including ideas for the community on how to contribute. In addition, the implementation and module projects presented themselves to the interested community and to our cooperation partners.Implementation Projects  Integration of Kitodo and OCR-D for productive mass digitisation (UB Braunschweig, SLUB Dresden, UB Mannheim)  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR4all libraries – full text recognition of historical collections (GEI Braunschweig, HCI and ZPD of the University of Würzburg)  ODEM: OCR-D extension for mass digitization (ULB Sachsen-Anhalt)Module Projects  Workflow for work-specific training based on generic models with OCR-D and ground truth enhancement (UB Mannheim)  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen-Nürnberg)  OLA-HD Service - A Generic Service for Long-Term Archiving of Historical Prints (SUB Göttingen, GWDG)",
      "url": " /en/phase3.html"
    },
  

    {
      "slug": "de-phase3-html",
      "title": "OCR-D - Phase III",
      "content"	 : "OCR-D - Phase IIIIm Februar 2020 hat die DFG eine Ausschreibung zur Fortsetzung des OCR-D Projektes in einer dritten Projektphase veröffentlicht. Ziel dieser Phase ist die Implementierung der OCR-D-Software in bestandshaltenden und -verarbeitenden Einrichtungen. Vier Implementierungs- und drei Modulprojekte wurden von der DFG bewilligt.Am 30. Juli fand unser Kick-off-Workshop statt, der Phase III einläutete.Das Team gab eine Einführung in die Ziele und öffentlichen Kommunikationskanäle von OCR-D in Phase III, in Status und Pläne der OCR-Software und der Web-API und in den Umgang mit Ground Truth Daten in OCR-D. Zudem gab das Koordinierungsprojekt einen Einblick in die bisherige Praxis der Softwareentwicklung in OCR-D mit Möglichkeiten, mitzuwirken.Außerdem präsentierten sich unsere Implementierungs- und Modulprojekte gegenseitig sowie unseren Kooperationspartnern.Implementierungsprojekte  Integration von Kitodo und OCR-D zur produktiven Massendigitalisierung (UB Braunschweig, SLUB Dresden, UB Mannheim)  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR4all libraries – Volltexterkennung historischer Sammlungen (GEI Braunschweig, HCI und ZPD der Universität Würzburg)  ODEM: OCR-D Erweiterung für Massendigitalisierung (ULB Sachsen-Anhalt)Modulprojekte  Workflow für werkspezifisches Training auf Basis generischer Modelle mit OCR-D sowie Ground Truth Aufwertung (UB Mannheim)  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen-Nürnberg)  OLA-HD Service - Ein generischer Dienst für die Langzeitarchivierung historischer Drucke (SUB Göttingen, GWDG)",
      "url": " /de/phase3.html"
    },
  

    {
      "slug": "en-platforms-html",
      "title": "OCR-D Platforms",
      "content"	 : "OCR-D PlatformsIn OCR-D, we strive for maximum transparency and communication of our workinternally as well as externally. The project is active on various platforms,which are mostly open to the professional public to read and participate.GitHub – try it out yourself: On GitHub you can find the latest status of our developmental work on the OCR-D framework and the module projects.https://github.com/OCR-DGitter – follow up on our discussions and ask your own questions: We use the chat platform Gitter for short-term and low-threshold communication with each other.https://gitter.im/OCR-D/LobbyOCR-D TechCall - take part in our technical discussions about OCR-D tools and issues; every second Wednesday, 2 to 3 p.m.link and agenda/protocolOCR(-D) &amp;amp; Co - bring in your own questions and ideas and join our discussions on OCR(-D); every first Friday in a month, 10 to 11 p.m. in a BBB room. Topics for discussion can be suggested any time in this HedgeDoc.Ground Truth Repository – make use of our data: We collect our test and reference data in the Ground Truth Repository. You may use the data for your own training.https://ocr-d.de/en/dataTechnology Watch - In the Zotero group of OCR-D we collect relevant literature about OCR together with interested parties.Zotero list",
      "url": " /en/platforms.html"
    },
  

    {
      "slug": "de-platforms-html",
      "title": "OCR-D-Kanäle und -Plattformen",
      "content"	 : "OCR-D-Kanäle und -PlattformenIn OCR-D bemühen wir uns um maximale Transparenz und Kommunikation unserer Arbeit nach innen und außen. Das Projekt ist auf verschiedenen Plattformen aktiv,die größtenteils für die Fachöffentlichkeit offen sind zum Lesen und Mitmachen.GitHub - probieren Sie es selbst aus: Auf GitHub finden Sie den aktuellen Stand unserer Entwicklungsarbeit am OCR-D Framework und den Modulprojekten.https://github.com/OCR-DGitter - verfolgen Sie unsere Diskussionen und stellen Sie Ihre eigenen Fragen: Für die schnelle und niederschwellige Kommunikation untereinander nutzen wir den Gitter-Chat.https://gitter.im/OCR-D/LobbyOCR-D TechCall - nehmen Sie an unseren technischen Diskussionen über OCR-D-Werkzeuge und -Themen teil; jeden zweiten Mittwoch, 14 bis 15 Uhr.Link und Agenda/ProtokollOCR(-D) &amp;amp; Co - bringen Sie Ihre eigenen Fragen und Ideen ein und diskutieren Sie mit uns über OCR(-D); jeden ersten Freitag im Monat, 10 bis 11 Uhr in einem BBB-Raum. Diskussionsthemen können jederzeit in diesem HedgeDoc vorgeschlagen werden.Ground Truth Repository - nutzen Sie unsere Daten: Wir sammeln unsere Test- und Referenzdaten im Ground Truth Repository. Sie können diese Daten gerne auch für Ihr eigenes Training nutzen.https://ocr-d.de/de/dataTechnology Watch - In der Zotero-Gruppe von OCR-D sammeln wir gemeinsam mit Interessierten relevante Literatur über OCR.Zotero-Liste",
      "url": " /de/platforms.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-existing-mets-html",
      "title": "",
      "content"	 : "Praxis: OCR von digitalisierten BeständenDer Einfachheit halber verwenden wir ein Beispiel aus dem Bestand an Testdateninnerhalb von OCR-D, da diese weniger umfangreichsind. Prinzipiell funktionieren die OCR-D Werkzeuge aber mit allen METS-Dateien, dieauch im DFG-Viewer angezeigt werden können.Ist ocrd installiert?$ ocrd --helpUsage: ocrd [OPTIONS] COMMAND [ARGS]...  CLI to OCR-DOptions:  --version                       Show the version and exit.  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                  Log level  --help                          Show this message and exit.Commands:  bashlib    Work with bash library  ocrd-tool  Work with ocrd-tool.json JSON_FILE  process    Process a series of tasks  workspace  Working with workspace  zip        Bag/Spill/Validate OCRD-ZIP bagsFalls dieser Befehl nicht klappt, überprüfen Sie ob die Software richtig installiert wurde.METS herunterladenWir verwenden eine auf 2 Seiten reduzierte Erstausgabe von Kant “Kritik der reinen Vernunft” aus dem Bestanddes Deutschen Textarchivs, die in GitHub liegt: https://github.com/OCR-D/assets/blob/master/data/kant_aufklaerung_1784/data/mets.xmlDazu nutze den workspace Unterbefehl von ocrd, der es erlaubt, mit METS zuinteragieren, ohne das XML direkt bearbeiten zu müssen.Der Unterbefehl clone erlaubt das Herunterladen von METS und ggf. enthaltenen Dateien.Mit dem Parameter --download werden auch alle referenzierten Dateien mit heruntergeladen (das entfernte Verzeichnis wird “geklont”)$ ocrd workspace clone --download &quot;https://raw.githubusercontent.com/OCR-D/assets/master/data/kant_aufklaerung_1784/data/mets.xml&quot; kant20:47:48.914 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;/home/ich/kant$ find kantkantkant/OCR-D-IMGkant/OCR-D-IMG/INPUT_0017kant/OCR-D-IMG/INPUT_0020kant/OCR-D-GT-ALTOkant/OCR-D-GT-ALTO/PAGE_0020_ALTOkant/OCR-D-GT-ALTO/PAGE_0017_ALTOkant/OCR-D-GT-PAGEkant/OCR-D-GT-PAGE/PAGE_0017_PAGEkant/OCR-D-GT-PAGE/PAGE_0020_PAGEkant/mets.xmlDas METS liegt nun als mets.xml im Unterverzeichnis kant, die Bilder inUnterverzeichnissen, die Dateigruppen repräsentieren.TIPP Kopieren sie das geklonte Verzeichnis an diesem Punkt. Dann müssen Sie nichtalle Dateien neu laden, falls im weiteren Ablauf etwas schief läuft:$ cp -r kant kant.BAKMETS untersuchenDie METS-Datei könenn wir nun untersuchen.Welche Dateigruppen gibt es?$ cd kant$ ocrd workspace list-groupOCR-D-IMGOCR-D-GT-PAGEOCR-D-GT-ALTOEs gibt also drei Dateigruppen, wobei wir uns für die Bilder interessieren, die in OCR-D-IMG gelistet sind.Welche Seiten gibt es?Physische Seiten werden in METS in einer structMap aufgelistet. Wir können uns die IDs der Seiten ausgeben lassen:$ ocrd workspace list-pagePHYS_0017PHYS_0020Es gibt zwei Seiten.Dateien auflistenWir können uns die enthaltenen Dateien auflisten lassen mit dem find Unterbefehl:$ ocrd workspace findOCR-D-IMG/INPUT_0017OCR-D-IMG/INPUT_0020OCR-D-GT-PAGE/PAGE_0017_PAGEOCR-D-GT-PAGE/PAGE_0020_PAGEOCR-D-GT-ALTO/PAGE_0017_ALTOOCR-D-GT-ALTO/PAGE_0020_ALTOEs gibt sechs Dateien: Für jede der zwei Seiten Dateien für jede der drei Dateigruppen.Standardmäßig wird nur die Datei-ID ausgegeben, weitere Felder können über denParameter -k angegeben werden (ocrd workspace find --help liefert weitereInformationen):$ ocrd workspace find -k url -k mimetype -k pageId -k IDOCR-D-IMG/INPUT_0017    image/tiff      PHYS_0017       INPUT_0017OCR-D-IMG/INPUT_0020    image/tiff      PHYS_0020       INPUT_0020OCR-D-GT-PAGE/PAGE_0017_PAGE    application/vnd.prima.page+xml  PHYS_0017       PAGE_0017_PAGEOCR-D-GT-PAGE/PAGE_0020_PAGE    application/vnd.prima.page+xml  PHYS_0020       PAGE_0020_PAGEOCR-D-GT-ALTO/PAGE_0017_ALTO    application/alto+xml    PHYS_0017       PAGE_0017_ALTOOCR-D-GT-ALTO/PAGE_0020_ALTO    application/alto+xml    PHYS_0020       PAGE_0020_ALTOAnatomie eines ocrd-* KommandozeilenaufrufsAlle OCR-D Werkzeuge folgen einer einheitlichen Aufrufsyntax, die Teil derSpezifikationen ist.ocrd-*   -m mets.xml          # Pfad zur METS-Datei  -I INPUT-GROUP       # Eingabe-Dateigruppe  -O OUTPUT-GROUP      # Ausgabe-Dateigruppe  -l DEBUG|INFO|ERROR  # Optional: Logging Detail Level  -p parameter.json    # Optional: Parameter-Datei  -g PAGE-ID           # Optional: ID einer Seite falls nur eine Seite verarbeitet werden sollBinarisierung mit ocrd-kraken-binarizeDer erste Schritt für die OCR ist die Binarisierung des Bildes, bei der alleFarben durch schwarz oder weiss ersetzt werden.Dafür gibt es verschiedene Implementierungen im OCR-D-Ökosystem, wir verwendenhier die Binarisierung von kraken.$ ocrd-kraken-binarize -m mets.xml -I OCR-D-IMG -O OCR-D-IMG-BIN19:49:28.847 INFO processor.KrakenBinarize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=image/tiff, ID=INPUT_0017, url=OCR-D-IMG/INPUT_0017, local_filename=---]/&amp;gt;19:49:28.868 INFO processor.KrakenBinarize - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7effeb96aeb8&amp;gt;19:49:28.868 INFO processor.KrakenBinarize - About to binarize page &#39;None&#39;19:49:28.871 INFO kraken.binarization - Binarizing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG/INPUT_001719:49:32.928 INFO processor.KrakenBinarize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=image/tiff, ID=INPUT_0020, url=OCR-D-IMG/INPUT_0020, local_filename=---]/&amp;gt;19:49:32.931 INFO processor.KrakenBinarize - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7effeb8ac5f8&amp;gt;19:49:32.931 INFO processor.KrakenBinarize - About to binarize page &#39;None&#39;19:49:32.933 INFO kraken.binarization - Binarizing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG/INPUT_002019:49:36.986 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Damit werden alle Dateien in der OCR-D-IMG Dateigruppe binarisiert, dasErgebnis wird in die Dateigruppe OCR-D-IMG-BIN geschrieben.Zeilensegmentierung mit OcropyAls nächsten Schritt müssen die Zeilen bestimmt werden, damit OCR angewandt werden kann.Auch dafür gibt es verschiedene Implementierungen innerhalb von OCR-D, wir verwenden die Zeilensegmentierung von Ocropus.$ ocrd-ocropy-segment -m mets.xml -I OCR-D-IMG-BIN -O OCR-D-SEG-LINE19:52:45.364 INFO processor.ocropySegment - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0001, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001, local_filename=---]/&amp;gt;19:52:45.365 INFO processor.ocropySegment - downloaded_file &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0001, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001, local_filename=/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001]/&amp;gt;19:52:45.383 INFO processor.ocropySegment - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7fcf0fab2208&amp;gt;19:52:50.705 INFO processor.ocropySegment - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0002, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002, local_filename=---]/&amp;gt;19:52:50.705 INFO processor.ocropySegment - downloaded_file &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0002, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002, local_filename=/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002]/&amp;gt;19:52:50.706 INFO processor.ocropySegment - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7fcf0fa66358&amp;gt;19:52:55.739 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Ausgehend von den binarisierten Bilder in der Dateigruppe OCR-D-IMG-BIN erzeugen wir PAGE-XML mit Segmentierungsinformation in der Dateigruppe und dem lokalen Verzeichnis OCR-D-SEG-LINE.Das PAGE-XML sieht ausschnittsweise so aus:&amp;lt;pc:Page imageFilename=&quot;OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002&quot; imageWidth=&quot;1456&quot; imageHeight=&quot;2084&quot;&amp;gt;    &amp;lt;pc:TextRegion&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0002&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;103,136 122,136 122,188 103,188&quot;/&amp;gt;        &amp;lt;/pc:TextLine&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0003&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;126,125 166,125 166,185 126,185&quot;/&amp;gt;        &amp;lt;/pc:TextLine&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0004&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;166,124 209,124 209,168 166,168&quot;/&amp;gt;      &amp;lt;/pc:TextLine&amp;gt;      &amp;lt;!-- ... --&amp;gt;Wir können uns das Ergebnis auch im PAGE Viewer visualisieren:Die Zeilenerkennung ist nicht perfekt (Artefakte im Randbereich, ueberlangeZeilen), aber gut genug, um OCR durchzuführen.Texterkennung mit tesseractDas Projekt ocrd_tesserocr bindet tesseract über die tesserocrPython-Bindings an OCR-D an.Der Aufruf ist analog zu Binarisierung und Segmentierung:$ ocrd-tesserocr-recognize -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS21:09:52.910 INFO processor.TesserocrRecognize - Using model &#39;eng&#39; in /usr/share/tesseract-ocr/4.00/tessdata/ for recognition at the line level21:09:52.910 INFO processor.TesserocrRecognize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0001, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0001.xml, local_filename=---]/&amp;gt;21:09:53.071 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:10:24.127 INFO processor.TesserocrRecognize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0002, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0002.xml, local_filename=---]/&amp;gt;21:10:24.357 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:11:09.617 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Das Ergebnis ist allerdings nicht optimal. Ausschnitt aus kant/OCR-D-OCR-TESS/OCR-D-OCR-TESS_0002: &amp;lt;pc:TextLine id=&quot;line_0006&quot;&amp;gt;     &amp;lt;pc:Coords points=&quot;529,410 1342,410 1342,468 529,468&quot;/&amp;gt;     &amp;lt;pc:TextEquiv conf=&quot;0.47&quot;&amp;gt;         &amp;lt;pc:Unicode&amp;gt;geenegerc wogpe35tro rappucb धाम es Bgrenbeire ﺪﺷﺭ&amp;lt;/pc:Unicode&amp;gt;     &amp;lt;/pc:TextEquiv&amp;gt; &amp;lt;/pc:TextLine&amp;gt;Das liegt daran, dass wir nicht angegeben haben, welches Modell verwendet werden soll. Standardmäßig verwendettesseract daher eng, das Modell für englischsprachige Antiqua. Für deutsche Fraktur benötigen wir dasModell frk, das im tesseract-Modell-GitHub-Repository zu finden ist.$ cd /usr/share/tesseract-ocr/4.00/tessdata/$ sudo wget &#39;https://github.com/tesseract-ocr/tessdata_best/raw/master/frk.traineddata&#39;Nun führen wir den befehl noch einmal aus aber geben als Parameter das zu verwendende Modell mit an:$ ocrd-tesserocr-recognize -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-FRK -p &amp;lt;(echo &#39;{&quot;model&quot;: &quot;frk&quot;}&#39;)21:09:52.910 INFO processor.TesserocrRecognize - Using model &#39;frk&#39; in /usr/share/tesseract-ocr/4.00/tessdata/ for recognition at the line level21:09:52.910 INFO processor.TesserocrRecognize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0001, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0001.xml, local_filename=---]/&amp;gt;21:09:53.071 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:10:24.127 INFO processor.TesserocrRecognize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0002, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0002.xml, local_filename=---]/&amp;gt;21:10:24.357 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:11:09.617 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Vergleichen wir die Erkennung der Textzeile:&amp;lt;pc:TextLine id=&quot;line_0006&quot;&amp;gt;    &amp;lt;pc:Coords points=&quot;529,410 1342,410 1342,468 529,468&quot;/&amp;gt;    &amp;lt;pc:TextEquiv conf=&quot;0.9&quot;&amp;gt;        &amp;lt;pc:Unicode&amp;gt;gewiegelt worden ; ſo ſchädlich iſt es Vorurtheile zu&amp;lt;/pc:Unicode&amp;gt;    &amp;lt;/pc:TextEquiv&amp;gt;&amp;lt;/pc:TextLine&amp;gt;Das sieht nicht nur korrekt aus, auch tesseract ist sich sicherer: Konfidenz 0.9. vs 0.47 oben.“In einem Rutsch”Die einzelnen Prozessschritte nacheinander auszuführen wie beschrieben ist mühselig. Wenn an einemPunkt im Ablauf etwas schief läuft, muss der gesamte Vorgang abgebrochen werden. Daher bietet ocrdeinen Modus, in dem ein kompletter Ablauf von Prozesschritten angegeben werden kann. Wenn innerhalbdes Prozesses ein Fehler auftritt, endet der Gesamtprozess mit der Fehlermeldung.Die Syntax entsprocht weitgehend dem individuellen Aufruf mit der Einschränkung dass ocrd- weggelassenwird:$ ocrd process -m mets.xml   &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN&#39;   &#39;ocropy-segment -I OCR-D-IMG-BIN -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS&#39;AufgabeWenden Sie die gelernten Schritte an auf die erste Seite des Kommunistischen Manifests.Die URL der METS-Datei: https://raw.githubusercontent.com/OCR-D/assets/master/data/communist_manifesto/data/mets.xmlFalls Sie Python 3.7 verwenden und deshalb ocrd_kraken nicht installieren konnten: Hier ist ein OCRD-ZIPder Aufgabe bis einschliesslich Binarisierung: binarized.zip",
      "url": " /slides/2019-03-25-dhd/praxis-existing-mets.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-gt-html",
      "title": "",
      "content"	 : "OCR-D Ground Truth PraxisRepository-URL: https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagitWerkzeuge  TranskribusTranskribus ist eine Werkzeug mit der auf Basis des PAGE-Formates Transkribtionen erstellt werden können. Diese Transkribtionen  können für das Training von Texterkennungs-Software genutzt werden.  AletheiaAletheia ist die Refenenzsoftware des am PRIma-LAB (Pattern Recognition &amp;amp; Image Analysis Research Lab) entwickelten PAGE-Formates. Mit dieser Software werden Transkribtionen erstellt, die für das Training von Texterkennungs-Software genutzt werden können.Beide Werkzeuge können für die Erstellung von Ground Truth genutzt werden.GuidelinesOCR-D GroundTruth GuidelinesPräsentation: https://ocr-d.github.io/gt/trans_documentation/index.htmlDokumentation: https://github.com/OCR-D/gt-guidelinesPage-XMLDokumentation zum PAGE XML Format for Page Content im Rahmen von OCR-Dhttps://ocr-d.github.io/gt/trans_documentation/trPage.htmlGitHub: https://github.com/PRImA-Research-Lab/PAGE-XMLTools und HelferleinTanskribus Software: https://transkribus.euKurz-Hilfe: https://transkribus.eu/wiki/images/c/cf/Transkribus_in_10_Schritten.pdfAletheiaSoftware: https://www.primaresearch.org/tools/Aletheia/Editions~Hinweis~ Es liegt nur eine Windows-Version vor.makeAletheia_metsErstellung einer Mets-Datei (Page Collections-Datei), um einfach mit Aletheia zu arbeiten.https://github.com/tboenig/makeAletheia_metsTranskribus_mets2Aletheia_metsKonvertierung einer vorhandenen Transkribus-METS-Datei in eine  Aletheia-METS-Datei.https://github.com/tboenig/Transkribus_mets2Aletheia_metsWorkshopAufgabe:  Arbeiten in Transkribus: Erstellung von GroundTruth auf Basis von Digitalisaten          Erstellung einer Transkribtion und der Zuordnung von Seitenregionen.        Arbeiten in Altheia: Bearbeitung in Aletheia          Alternative Aufgabe: Öffnen der Datei im PAGE-Viewer        Arbeiten mit dem GT-OCR-D Repositorium: Download und Bearbeitung von GroundTruth aus dem GT-OCR-D Repositorium.Aufgabe 1:Voraussetzung: installierter Transkribus (https://transkribus.eu/Transkribus/)~Hinweis:~ Dazu ist ein Acount bei Transkribus notwendig.  Starten Sie Transkribus und loggen Sie sich ein.  Öffnen Sie die Collection OCR-D_DHD2019 Collection.  Sie finden darin: a_gehema_feldapotheke_1688_3, Öffnen Sie das Dokument.                  Das PAGE-Format bietet eine strukturelle Transkribtion von Regionen -&amp;gt; Zeilen -&amp;gt; Wörtern an. [Zeichen(Glyphen) können ebenfalls mit PAGE transkibiert werden.]                  Reduzieren Sie die Anzeigen nur auf die Zeilenebene.    Klicken Sie die erste Zeile an und beginnen Sie mit der Transkribtion des Textes.    Zeichen die sich außerhalb des Tastaturlayouts befinden, können Sie durch das Virtual keyboard eingeben.    Neben der Transkribtion ist die Markierung und Klassifizierung der Seitenregionen vorzunehmen.siehe: https://ocr-d.github.io/gt//trans_documentation/lyTextregionen.html  Folgende Regionen sind zu unterscheiden:          Textregion : TextRegion,      Abbildungen, Fotos : ImageRegion,      Buchschmuck, Zeichnungen : GraphicRegion,      Trennlinien, Separatoren : SeparatorRegion,      Tabellen : TableRegion,      Strichzeichnungen : LineDrawingRegion,      Mathematische Formeln : MathsRegion,      Chemische Formeln : ChemRegion,      Noten : MusicRegion,      Werbung : AdvertRegion und      Schmutz, Verfärbungen, Rauschen : NoiseRegion        Diese Regionen sind unter dem Metadaten-&amp;gt;Structural zu finden.                  Zum Abschluss der Arbeiten exportieren Sie Ihren Ground Truth.                Aufgabe 2:Um den GroundTruth aus Transkribus in Aletheia zu bearbeiten, kann die zur Verfügung gestellte METS-Datei genutzt werden. Jedoch muß diese in das Aletheia-Format konvertiert werden.Allgemeine Informationen zu METS: https://de.wikipedia.org/wiki/Metadata_Encoding_%26_Transmission_StandardMETS-Standard: http://www.loc.gov/standards/mets/Voraussetzung: Der SAXON : The XSLT and XQuery Processor ist auf Ihrem Rechner installiert.  Laden Sie sich die entsprechenden Tools (XSL-Transformation) von https://github.com/tboenig/Transkribus_mets2Aletheia_mets herunter  Entpacken Sie die Zip-Datei.  Öffnen Sie die Kommandozeile/Terminal          Windows:                  Tippen Sie in das Programm-Suche-Feld: cmd ein. Es öffnet sich ein DOS/Windows Terminal. In diesem können Sie mit dem SAXON Processor arbeiten.          Nutzen Sie Ihr Linux-Sub-System (Betrifft nur Nutzer von Windows10).                          Starten Sie die Linux-App.              Um Zugriff auf die Dateien Ihres Rechners zu bekommen nutzen Sie in Ihrem Linux-Subsystem das Kommando cd für Change Directory m cd /mnt/[Laufwerks-Angabe]/Users/Path zu den entsprechenden Ordnern/Dateien                                Für beide Systemvarianten gilt, passen Sie den Kommandozeilenaufruf an Ihr Dateisystem an.            java -jar ../saxon9he.jar -xsl:../xsl/Transkribus_mets2Aletheia_mets.xsl -s:../example/mets.xml -o: ../example/mets_aletheia.metsx                                Schreiben Sie den angpassten Kommandozeilenaufruf in das Kommandozeilenfenster/Terminal und starten Sie die Transformation durch drücken der Eingabetaste (Returntaste).                      Linux          Öffnen Sie einen Terminal      Passen Sie den Kommandozeilenaufruf an Ihr Dateisystem an.        java -jar ../saxon9he.jar -xsl:../xsl/Transkribus_mets2Aletheia_mets.xsl -s:../example/mets.xml -o: ../example/mets_aletheia.metsx                    Schreiben Sie den angepassten Kommandozeilenaufruf in das Kommandozeilenfenster/Terminal und starten Sie die Transformation durch drücken der Eingabetaste (Returntaste oder Entertaste).      Hinweis: Wo ist meine Eingabetaste (Returntaste oder Entertaste)?https://upload.wikimedia.org/wikipedia/commons/a/a5/Enter.png            Parameter      kurze Erklärung                  -xsl:      Angabe des XSL-Datei.              -s:      Angabe der Datei die transformiert werden soll.              -o:      Angabe der Ausgabedatei.      Alternative Aufgabe 2:Eine PAGE-Datei kann auch im PAGE-Viewer angezeigt werden. Dazu ist eine valide PAGE-Datei notwendig.Zur Zeit werden PAGE-Dateien in Transkribus mit einigen Erweiterungen ausgeliefert, die zum PAGE-Schema nicht konform sind.Deshalb müssen diese Erweiterungen vor dem Öffnen mit dem PAGE-Viewer zum Beispiel manuell entfernt werden.Ausschnitt aus der PAGE-Datei von Transkribus:&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&amp;gt;&amp;lt;PcGts xmlns=&quot;http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15 http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15/pagecontent.xsd&quot;&amp;gt;    &amp;lt;Metadata&amp;gt;        &amp;lt;Creator&amp;gt;OCR_D&amp;lt;/Creator&amp;gt;        &amp;lt;Created&amp;gt;2016-09-20T13:04:41.875+02:00&amp;lt;/Created&amp;gt;        &amp;lt;LastChange&amp;gt;2018-04-23T12:49:58.191+02:00&amp;lt;/LastChange&amp;gt;        &amp;lt;Comments&amp;gt;                Measurement unit: pixel                PrimaryLanguage: German                Language: GermanStandard                Producer: ABBYY FineReader Engine 11&amp;lt;/Comments&amp;gt;        &amp;lt;TranskribusMetadata docId=&quot;6557&quot; pageId=&quot;213761&quot; pageNr=&quot;1&quot; tsid=&quot;3193617&quot; status=&quot;GT&quot; userId=&quot;2082&quot; imgUrl=&quot;...&quot; xmlUrl=&quot;...&quot; imageId=&quot;205160&quot;/&amp;gt;    &amp;lt;/Metadata&amp;gt;    [...]&amp;lt;/PcGts&amp;gt;Der Eintrag:&amp;lt;TranskribusMetadata docId=&quot;6557&quot; pageId=&quot;213761&quot; pageNr=&quot;1&quot; tsid=&quot;3193617&quot; status=&quot;GT&quot; userId=&quot;2082&quot; imgUrl=&quot;...&quot; xmlUrl=&quot;...&quot; imageId=&quot;205160&quot;/&amp;gt;ist zu löschen.  Laden Sie eine Transkribus-PAGE-Datei in einen Text oder XML-Editor.  Markieren und löschen Sie die gesamten Zeile: &amp;lt;TranskribusMetadata...  Speichern Sie Datei unter einem neuen Namen zum Beispiel: [alter Name]_PAGEViewer.xml  Öffnen Sie den PAGE-Viewer und laden Sie die bearbeitete PAGE-Datei in den Viewer.  Aufgabe 3:Das OCR-D-GT-Repositorium speichert, verwaltet und archiviert GroundTruth-Daten. Für das Training und für die Evaluation können aus diesem Repositorium entsprechende Daten verwendet werden. Aber auch GroundTruth-Daten die in verschiedenen Kontexten erstellt wurden können in diesem Repositorium gespeichert und archiviert werden. Möchten Sie Ihren GroundTruth-Korpus zur Verfügung stellen dann nehmen Sie mit OCR-D Kontakt auf. Schreiben Sie eine E-Mail an ocrd@bbaw.de.  Öffnen Sie die Webseite: https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit.  In der Tabelle wählen Sie die erste Zeile aus und clicken Sie auf den Link zur zip-Datei in der Spalte URL.https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/f15fb8c8-3842-4314-9a44-5e8b472d7bfc/data/buerger_gedichte_1778.ocrd.zip  Speichern und entpacken Sie die zip-Datei  Starten Sie den PAGE-Viewer  Laden Sie eine PAGE-Datei, die sich im Ordner: buerger_gedichte_1778.ocrd-1/data/OCR-D-GT-SEG-BLOCK  Da die Dateien keine Dateiendungen besitzen, werden Sie im Datei-Öffnen-Fenster zuerst nicht angezeigt.            Windows-Explorer-Ansicht      Datei-Öffnen-Fenster im PAGE-Viewer                                Damit Sie alle Dateien im Select Document File-Fenster sehen können, geben Sie einen Stern () unter *Dateiname ein und bestätigen die Eingabe mit Enter.  Das Laden der Bilddateien erfolgt auf gleichem Weg.http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221",
      "url": " /slides/2019-03-25-dhd/praxis-gt.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-new-mets-html",
      "title": "",
      "content"	 : "Praxis: OCR von willkürlichen BildernDer Fokus von OCR-D ist auf Massendigitalisierung historischer Bestände.Deswegen wird konsequent immer METS verwendet.Um mit willkürlichen Bildern innerhalb OCR-D zu arbeiten, müssen wir daher METSerzeugen.Als Beispiel verwenden wir die erste Seite der englischen Ausgabe des Kommunistischen Manifests (Quelle: https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Manifesto_of_the_Communist_Party.djvu/page15-2745px-Manifesto_of_the_Communist_Party.djvu.jpg).Neues METS erzeugenDer workspace Unterbefehl erlaubt es, neues METS zu initiieren (init):$ ocrd workspace init communist_manifesto22:58:38.321 INFO ocrd.resolver - Writing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto/mets.xml22:58:38.322 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto/mets.xml&#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto$ find communist_manifestocommunist_manifestocommunist_manifesto/mets.xmlVerzeichnis für Bild anlegenPer Konvention heisst die Dateigruppe mit dem unkomprimierten Bild innerhalb von OCR-D immer OCR-D-IMG:$ cd communist_manifesto$ mkdir OCR-D-IMGBild herunterladen$ curl &#39;https://raw.githubusercontent.com/OCR-D/assets/master/data/communist_manifesto/data/OCR-D-IMG/OCR-D-IMG_0015&#39; &amp;gt; OCR-D-IMG/OCR-D-IMG_0015.pngBild zum METS hinzufügenDateien können mit dem add Unterbefehl von ocrd workspace hinzugefügt werden.Dafür sind eine Reihe von Parametern notwendig, die Sie der Hilfe entnehmen können:$ ocrd workspace add --helpUsage: ocrd workspace add [OPTIONS] LOCAL_FILENAME  Add a file LOCAL_FILENAME to METS in a workspace.Options:  -G, --file-grp TEXT  fileGrp USE  [required]  -i, --file-id TEXT   ID for the file  [required]  -m, --mimetype TEXT  Media type of the file  [required]  -g, --page-id TEXT   ID of the physical page  --force              If file with ID already exists, replace it  --help               Show this message and exit.Wir fügen also die Datei hinzu:$ ocrd workspace add -g P0015 -G OCR-D-IMG -i OCR-D-IMG_0015 -m image/png OCR-D-IMG/OCR-D-IMG_0015.pngUnd überprüfen ob sie wirklich hinzugefügt wurde:$ ocrd workspace findOCR-D-IMG/OCR-D-IMG_0015Eindeutigen Identifier hinzufügenNun setzen wir noch einen eindeutigen Identifier:$ ocrd workspace set-id &#39;1234567890&#39;ValidierenWir können nun überprüfen, ob das Verzeichnis den Anforderungen von OCR-Dgenügt. Dazu verwenden wir den validate Unterbefehl von ocrd workspace.Bei der Validierung überspringen wir die Untersuchung der Auflösung, dadiese aus technischen Gründen häufig nicht gegeben ist, was zu Fehlern führt, die keine sind.$ ocrd workspace validate --skip pixel_density mets.xml&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;Fertig!Das Verzeichnis ist nun in einer Form, dass es mit OCR-D Werkzeugen weiterverarbeitet werden kann.Im Ergebnis sollte das von Ihnen erstellte Verzeichnis dem Beispieldatensatz im OCR-D/assets Repository entsprechen.",
      "url": " /slides/2019-03-25-dhd/praxis-new-mets.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-ocrd-zip-html",
      "title": "",
      "content"	 : "Praxis: OCRD-ZIP erzeugenOCRD-ZIP ist das Austauschformat, in demWorkspaces archiviert werden. Dies ist das Format, mit dem alle Repostorienarbeiten (Ground Truth, Forschungsdaten, Langzeitarchivierung).OCRD-ZIP basiert auf dem [BagIt] Standard, der in der Archiv-Community verbreitet istund von der Library of Congress gepflegt wird.Ein OCRD-ZIP ist ein ZIP-Archiv, das neben den OCR-Daten auch Metadaten enthält,bspw. Prüfsummen für die enthaltenen Dateien und Provenienzinformationen.DateistrukturSiehe Folien vom OCR-D Entwicklerworkshop Februar 2019Erzeugen von OCRD-ZIPOCRD-ZIP kann aus bestehenden Verzeichnissen mit dem zip Unterbefehl von ocrd verarbeitet werden.Zum Erzeugen von OCRD-ZIP verwenden wir den bag Unterbefehl:$ ocrd zip --skip-zip --id foo out23:29:30.973 INFO ocrd.workspace_bagger - Bagging /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo to /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out (temp dir /tmp/ocrd-bagit-n_ag6g3e)23:29:30.974 INFO ocrd.workspace_bagger - Resolving OCR-D-IMG/OCR-D-IMG_0015 (partial)23:29:30.974 INFO ocrd.workspace_bagger - Resolved /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/OCR-D-IMG/OCR-D-IMG_001523:29:30.984 INFO bagit - Using 1 processes to generate manifests: sha51223:29:30.984 INFO bagit - Generating manifest lines for file data/mets.xml23:29:30.985 INFO bagit - Generating manifest lines for file data/OCR-D-IMG/OCR-D-IMG_001523:29:31.014 INFO bagit - Creating /tmp/ocrd-bagit-n_ag6g3e/tagmanifest-sha512.txt23:29:31.018 INFO ocrd.workspace_bagger - Created bag at /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/outDas Ergebnis liegt also nun im Verzeichnis out:$ find outoutout/tagmanifest-sha512.txtout/bagit.txtout/manifest-sha512.txtout/dataout/data/OCR-D-IMGout/data/OCR-D-IMG/OCR-D-IMG_0015out/data/mets.xmlout/bag-info.txtValidierenUm sicherzugehen, dass das OCRD-ZIP auch den Vorgaben entspricht, könen wir den validate Unterbefehl von ocrd zip verwenden.$ ocrd zip validate --skip-unzip out23:33:11.823 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/data/mets.xml23:33:11.824 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/data/OCR-D-IMG/OCR-D-IMG_001523:33:11.846 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/bagit.txt23:33:11.846 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/manifest-sha512.txt23:33:11.847 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/bag-info.txtOKDas OCRD-ZIP ist also valide und kann ins Repository!AufgabeErstellen Sie aus einem der Verzeichnisse aus den vorherigen Aufgaben ein OCRD-ZIP.Validieren Sie das OCRD-ZIP",
      "url": " /slides/2019-03-25-dhd/praxis-ocrd-zip.html"
    },
  

    {
      "slug": "en-project-summary-html",
      "title": "Brief Description &amp; Project Goals",
      "content"	 : "Brief Description &amp;amp; Project GoalsCoordinated Funding Initiative for the Further Development of Methods for Optical Character Recognition (OCR) “Short name: OCR-DProject PartnersDuke August Library Wolfenbüttel](http://www.hab.de/), Berlin-Brandenburg Academy of Sciences and Humanities, State Library of Berlin Prussian Cultural Heritage, Karlsruhe Institute of Technology(Project managers and contact persons see Contact)Project Duration2015–2020Funded byGerman Research Foundation Scientific Literature and Information Systems (LIS)Project ProgressThe “Coordinated Funding Initiative for the Further Development of Optical Character Recognition Methods” (OCR-D) began its first project phase in the third quarter of 2015. Requirements for the further development of automatic text recognition were collected and analyzed in six work packages. The work finally led to the DFG’s call for proposals “Scalable methods of text and structure recognition for the full text digitisation of historical prints” in March 2017. The approval of eight (module) projects by the DFG at the end of December 2017 marks the end of the first and the beginning of the second project phase, in which the module projects are coordinated and supported and their project results are tested and integrated. In order to be able to fulfill the tasks of the coordination project over the entire duration of all module projects, the DFG approved an extension of the project until July 2020.GoalsAn essential main goal of OCR-D is the conceptual preparation of the transformation of VD-prints (16th–19th century) into machine-readable form and the provision of the necessary tools.In order to achieve this, the coordination project and the module projects aim to meet the following objectives:  the creation of reference corpora for training and testing  the development of standards in the areas of metadata, documentation and ground truth  the further development of individual processing steps, with a particular focus on Optical Layout Recognition (OLR)  the analysis of existing tools and their further development  the creation of an OCR-D framework  the establishment of quality assurance procedures**At the end of the overall project, a software package for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries will be made available. Furthermore an accompanying concept will provide answers to technical, information scientifical and organisational questions regarding the possible mass processing of these data.",
      "url": " /en/project-summary.html"
    },
  

    {
      "slug": "en-project-html",
      "title": "The OCR-D Project",
      "content"	 : "The OCR-D Project**OCR-D is a coordination project aimed at the further development of Optical Character Recognition (OCR) techniques for historical prints.Workflows and methods of automatic text recognition are examined, described and, if necessary, optimized. An essential goal is the conceptual preparation of the transformation of German prints from the 16th to the 19th century into electronic full texts.This project involves the Duke August Library Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the State Library of Berlin Prussian Cultural Heritage and the Karlsruhe Institute of Technology. The Bavarian State Library was also involved until 31/08/2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have image digitised extensive holdings. With the help of OCR procedures, searchable full texts can be automatically generated from these image data. The use of digital full texts is indispensable today in many scientific disciplines, especially in the field of (digital) humanities.So far, however, access to the electronic full text has often been impossible, or only inadequately possible. Many historical holdings are available in digitised form through the “ Union Catalogues of Books Printed in German Speaking Countries “ (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially gothic types, are difficult to identify.There is a need for development, which we have uncovered in OCR-D. On the basis of existing tools and investigations, the OCR process is to be optimized for VD prints. In addition, answers will be found to the associated technical, information scientific and organizational problems. In contrast to other OCR projects, the focus is not on developing a new, powerful OCR engine. Instead, full text digitization is seen as a process that is implemented in modular open source software. The processes and parameters can be traced and, if required, tailor-made workflows can be defined that deliver optimal results for specific titles.The project is funded by the German Research Foundation](http://www.dfg.de/) and runs until July 2020. In the first phase, needs were identified and concepts for the further course were developed. The cooperation structure was consolidated and continued in the second phase. In this phase, the identified needs are addressed by eight module projects, which partly develop existing tools for the automated processing of early modern printing, partly set up new tools. In all steps, we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries is to be developed.",
      "url": " /en/project.html"
    },
  

    {
      "slug": "en-spec-provenance-html",
      "title": "Provenance",
      "content"	 : "ProvenanceProvenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. (Source: The PROV Data Model (PROV-DM))Data ModelThe PROV Data Model (PROV-DM) is used to store all provenance metadata.All provenance have to be stored in files. There’s provenance at the page level, but there’s also provenance at the document level.All files regarding provenance are stored in a subfolder metadata.FormatThe workflow provenance is stored in PROV-XML.TypesAll Activities, Entities belonging to the OCR-D workflow have the same namespace.Namespace  Prefix  ocrd  Namespace  http://www.ocr-d.de            Type      Data Type      Description                  Entity      ocrd:mets      Filename  of METS file              Entity      ocrd:mets_referencedFile      ID of the file referenced inside METS.              Entity      ocrd:parameter_file      Content of the parameter file.              Activity      ocrd:processor      Processor that was executed              Activity      ocrd:workflow      Workflow that was executed      ContentOnly the following information is stored for provenance:(a) General data  Workflow engine          Label including version      Start date      End date      (b) Processor data  Processor          Label including version, conforming to OCR-D mets:agent/mets:name (e.g.: ocrd-kraken-binarize_Version 0.1.0, ocrd/core 1.0.0)      Start date      End date        Content of METS file before executing the processor  Content of METS file after executing processor  ID of the input file(s)  ID of output file(s)  Content of parameter.json (optional)Input/OutputAll files referenced in METS must also be referenced in provenance by their mets:file/@ID.A file may be linked to its location (URL). The location may be replaced due to different uses:  local files  external filesAll files not referenced in METS must be linked to their content in provenance. (e.g.: parameter.json)Ingest Workspace to OCR-D RepositoriumAt least before ingesting into repository/LTA, the entire provenance must be stored in one file (metadata/ocrd_provenance.xml) to make the provenance searchable.Therefore all the provenance files are merged into one big file.This file replaces all provenance files stored in subfolder ‘metadata’ExampleThe file structure could look like this after a workflow with 4 steps has been executed.metadata/   |   +-- mets.xml.&#39;workflowid&#39;_0000   |   +-- mets.xml.&#39;workflowid&#39;_0001   |   +-- mets.xml.&#39;workflowid&#39;_0002   |   +-- mets.xml.&#39;workflowid&#39;_0003   |   +-- mets.xml.&#39;workflowid&#39;_0004   |   +-- ocrd_provenance.xml   |   +-- provenance_&#39;workflowid&#39;.xml (optional)Provenance and BagItThe provenance MAY be stored as tag directory in the bagIt container.E.g.:&amp;lt;base directory&amp;gt;/         |         +-- bagit.txt         |         +-- manifest-&amp;lt;algorithm&amp;gt;.txt         |         +-- [additional tag files]         |         +-- data/         |     |         |     +-- mets.xml         |     |         |     +-- ...         |         +-- metadata                |                +-- mets.xml.&#39;workflowid&#39;_0000                |                +-- ...                |                +-- mets.xml.&#39;workflowid&#39;_XXXX                |                +-- ocrd_provenance.xml",
      "url": " /en/spec/provenance.html"
    },
  

    {
      "slug": "de-spec-provenance-html",
      "title": "",
      "content"	 : "ProvenanceProvenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. (Source: The PROV Data Model (PROV-DM))Data ModelThe PROV Data Model (PROV-DM) is used to store all provenance metadata.All provenance have to be stored in files. There’s provenance at the page level, but there’s also provenance at the document level.All files regarding provenance are stored in a subfolder metadata.FormatThe workflow provenance is stored in PROV-XML.TypesAll Activities, Entities belonging to the OCR-D workflow have the same namespace.Namespace  Prefix  ocrd  Namespace  http://www.ocr-d.de            Type      Data Type      Description                  Entity      ocrd:mets      Filename  of METS file              Entity      ocrd:mets_referencedFile      ID of the file referenced inside METS.              Entity      ocrd:parameter_file      Content of the parameter file.              Activity      ocrd:processor      Processor that was executed              Activity      ocrd:workflow      Workflow that was executed      ContentOnly the following information is stored for provenance:(a) General data  Workflow engine          Label including version      Start date      End date      (b) Processor data  Processor          Label including version, conforming to OCR-D mets:agent/mets:name (e.g.: ocrd-kraken-binarize_Version 0.1.0, ocrd/core 1.0.0)      Start date      End date        Content of METS file before executing the processor  Content of METS file after executing processor  ID of the input file(s)  ID of output file(s)  Content of parameter.json (optional)Input/OutputAll files referenced in METS must also be referenced in provenance by their mets:file/@ID.A file may be linked to its location (URL). The location may be replaced due to different uses:  local files  external filesAll files not referenced in METS must be linked to their content in provenance. (e.g.: parameter.json)Ingest Workspace to OCR-D RepositoriumAt least before ingesting into repository/LTA, the entire provenance must be stored in one file (metadata/ocrd_provenance.xml) to make the provenance searchable.Therefore all the provenance files are merged into one big file.This file replaces all provenance files stored in subfolder ‘metadata’ExampleThe file structure could look like this after a workflow with 4 steps has been executed.metadata/   |   +-- mets.xml.&#39;workflowid&#39;_0000   |   +-- mets.xml.&#39;workflowid&#39;_0001   |   +-- mets.xml.&#39;workflowid&#39;_0002   |   +-- mets.xml.&#39;workflowid&#39;_0003   |   +-- mets.xml.&#39;workflowid&#39;_0004   |   +-- ocrd_provenance.xml   |   +-- provenance_&#39;workflowid&#39;.xml (optional)Provenance and BagItThe provenance MAY be stored as tag directory in the bagIt container.E.g.:&amp;lt;base directory&amp;gt;/         |         +-- bagit.txt         |         +-- manifest-&amp;lt;algorithm&amp;gt;.txt         |         +-- [additional tag files]         |         +-- data/         |     |         |     +-- mets.xml         |     |         |     +-- ...         |         +-- metadata                |                +-- mets.xml.&#39;workflowid&#39;_0000                |                +-- ...                |                +-- mets.xml.&#39;workflowid&#39;_XXXX                |                +-- ocrd_provenance.xml",
      "url": " /de/spec/provenance.html"
    },
  

    {
      "slug": "en-publications-html",
      "title": "Publications",
      "content"	 : "Publications  Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Dong, Rui; Burghardt, Manuel; Christlein, Vincent: New Approaches to OCR for Early Printed Books, in: DigItalia 15 (2), 12.2020, S. 74–87. Online: &amp;lt;https://doi.org/10.36181/digitalia-00015&amp;gt;.    Engl, Elisabeth: OCR-D kompakt: Ergebnisse und Stand der Forschung in der Förderinitiative, in: Bibliothek Forschung und Praxis 44 (2), 29.07.2020, S. 218–230. Online: &amp;lt;https://doi.org/10.1515/bfp-2020-0024&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Neudecker, Clemens: Volltexte – die Zukunft alter Drucke. Bericht zum Abschlussworkshop des OCR-D-Projekts, in: o-bib 7 (2), 05.05.2020, S. 1–4. Online: &amp;lt;https://doi.org/10.5282/o-bib/5600&amp;gt;.    Boenig, Matthias; Engl, Elisabeth; Baierer, Konstantin; Hartmann, Volker; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, in: DHd 2020: Spielräme - Digital Humanities zwischen Modellierung und Interpretation. Konferenzabstracts, Paderborn 05.03.2020, S. 244–247. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth; Boenig, Matthias; Baierer, Konstantin; Neudecker, Clemens; Hartmann, Volker: Volltexte für die Frühe Neuzeit. Der Beitrag des OCR-D-Projekts zur Volltexterkennung frühneuzeitlicher Drucke, in: Zeitschrift für Historische Forschung 47 (2), 2020, S. 223–250.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 25–30. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352638&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vincent: Dataset of Pages from Early Printed Books with Multiple Font Groups, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 1–6. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352640&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open source OCR framework for historical documents, in: EuropeanaTech Insight (13), 31.07.2019. Online: &amp;lt;https://pro.europeana.eu/page/issue-13-ocr#ocr-d-an-end-to-end-open-source-ocr-framework-for-historical-documents&amp;gt;.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 3–8. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322916&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 19.24. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322908&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 53–58. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322917&amp;gt;.    Engl, Elisabeth: Das Projekt OCR-D – Ein Fortschrittsbericht zur Volltextdigitalisierung frühneuzeitlicher Drucke, in: Medium Buch 1, 2019, S. 233–235.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, in: Conference Abstracts, Galway 08.12.2018. Online: &amp;lt;https://eadh2018.exordo.com/files/papers/92/final_draft/EADH_2018_Proposal_Brief_Final.pdf&amp;gt;.    Schulz, Klaus; Fink, Florian: Novel software for cleansing digitised historical texts, in: Scientia, 28.11.2018. Online: &amp;lt;https://doi.org/10.26320/SCIENTIA278&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, in: Konferenzabstracts, Köln 28.02.2018, S. 219–223. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D – Koordinierte Förderinitiative zur Weiterentwicklung von OCR-Verfahren, in: Bibliotheksdienst 52 (1), 05.12.2017. Online: &amp;lt;https://doi.org/10.1515/bd-2018-0007&amp;gt;.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, in: Konferenzabstracts, Leipzig 11.03.2016, S. 103–108. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.  Presentations  Vom Bild zum Text — praktische OCR für die DH, Event-Reihe, vDHd 2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/&amp;gt;.    Engl, Elisabeth: Massendigitalisierung alter Drucke – OCR-D in Bibliotheken, Vortrag, 3. Workshop zur Retrodigitalisierung. OCR – Prozesse und Entwicklungen 01.03.2021. Online: &amp;lt;https://wiki.zbw.eu/pages/viewpage.action?pageId=33620559&amp;amp;preview=/33620559/33620562/2021-02-24_Engl_Massendigitalisierung%20alter%20Drucke.pdf&amp;gt;.    Neudecker, Clemens: OCR-D: An open ecosystem for improving OCR on historical documents, Vortrag, Mini-ELAG 20.10.2020. Online: &amp;lt;https://elag.org/mini-elag-october-20-2020/ocr-d-an-open-ecosystem-for-improving-ocr-on-historical-documents/&amp;gt;.    Boenig, Matthias: Digitale Transformation: OCR-D, Angebot und Vision, Vortrag, FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft 08.10.2020. Online: &amp;lt;https://docs.google.com/presentation/d/1JCzfGq_Reze7R3TaecYyBocnkD6uNy94eukJTlYUbNI/edit#slide=id.g9d954d5829_0_469&amp;gt;.    Engl, Elisabeth: OCR-D in the wild: Erfahrungen und Erkenntnisse aus der Praxisphase mit Bibliotheken, Vortrag, vbib2020 26.05.2020. Online: &amp;lt;https://doi.org/10.5446/47151&amp;gt;.    Baierer, Konstantin; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, Paderborn 05.03.2020. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth: Die OCR-D-Workflowengine, Vortrag, 2. Workshop Retrodigitalisierung zu Effizienz und Qualitätssicherung in Digitalisierungsworkflows, Hannover 18.02.2020. Online: &amp;lt;/slides/Retrodigitalisierung-2020-02-18/TIB_Retrodigitalisierung.pdf&amp;gt;.    Boenig, Matthias: Spezifikationen und Lessons Learned, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_Spezifikationen_Lessons_Learned.pdf&amp;gt;.    Engl, Elisabeth: Bibliothekarische Digitalisierungspraxis und die OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_in_Bibliotheken.pdf&amp;gt;.    Engl, Elisabeth: OCR-D in a Nutshell, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/Abschlussworkshop_Überblick.pdf&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin: Funktionen und Möglichkeiten der OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;https://hackmd.io/@cneud/ocrd-bonn#/&amp;gt;.    Würzner, Kay-Michael: Multi-source OCR workflows with OCR-D, Vortrag, The Open Islamicate Texts Initiative Workshop, Berwyn 29.01.2020. Online: &amp;lt;https://wrznr.github.io/OpenITI-2020/#1&amp;gt;.    Baierer, Konstantin; Engl, Elisabeth; Luetgen, Michael: OCR(-D) und Kitodo, Vortrag, Kitodo Anwenderworkshop, Hamburg 19.11.2019. Online: &amp;lt;https://hackmd.io/@kba/S1peIVxhH#/&amp;gt;.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019. Online: &amp;lt;https://hackmd.io/@kba/SyiQKUCUH#/&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019.    Metzger, Noah: Projektabschlusspräsentation, Vortrag, Mannheim 19.09.2019. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/52213/&amp;gt;.    Sachunsky, Robert; Würzner, Kay-Michael: Flexible workflows with OCR-D, Vortrag, 3rd OCR-D developer workshop, Berlin 26.08.2019. Online: &amp;lt;https://hackmd.io/@FKFH0M1sR2SdJZwK5U8Cfg/S1YQ4NeNr#/&amp;gt;.    Metzger, Noah; Weil, Stefan: Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow, Workshop, MAD HD, Heidelberg 30.07.2019.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://hackmd.io/@QTT7e4hCTyWxVOvjiS61cA/B1nn7W7jV#/&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents&amp;gt;.    Weil, Stefan: Tesseract OCR – News, Vortrag, ELAG 2019, Berlin 09.05.2019. Online: &amp;lt;https://www.elag2019.de/talks/2019-05-09-tesseract-elag.pdf&amp;gt;.    Kamlah, Jan; Weil, Stefan: Forschungsdaten aus Digitalisaten, Vortrag, E-Science-Tage, Heidelberg 28.03.2019. Online: &amp;lt;https://heibox.uni-heidelberg.de/d/31bb269467/files/?p=%2FVortr%C3%A4ge%2FC3_2019-03-28-Kamlah-Weil.pdf&amp;gt;.    Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Herrmann, Elisa: Vom gedruckten Werk zu elektronischem Volltext, Workshop, DHd 2019, Mainz 25.03.2019. Online: &amp;lt;http://kba.cloud/2019-03-25-dhd/&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Christlein, Vincent; Maier, Andreas: Automatic Font Group Recognition in Early Printed Books, Vortrag, Mainz 25.03.2019.    Boenig, Matthias: OCR-D in der Praxis: Ein gemeinsamer Ausblick mit Dienstleistern und Anwendern, Öffentliche Arbeitssitzung, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/17097/docId/16357/start/0/rows/20&amp;gt;.    Herrmann, Elisa: Von der Vision zur Umsetzung: Der aktuelle Entwicklungsstand von OCR-D, Vortrag, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://www.researchgate.net/publication/332173701_Von_der_Vision_zur_Umsetzung_Der_aktuelle_Entwicklungsstand_von_OCR-D&amp;gt;.    Weil, Stefan: Hands-On Lab digital / Vom Bild zum Text. Automatisierte Texterkennung in historischen Drucken mit der freien Software Tesseract, Workshop, 108. Bibliothekartag, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16351&amp;gt;.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, Posterpräsentation, EADH 2018, Galway 08.12.2018. Online: &amp;lt;https://git.informatik.uni-leipzig.de/ocr-d/poster-eadh2018/blob/master/main.pdf&amp;gt;.    Baierer, Konstantin; Würzner, Kay-Michael: An open-source framework for integrating multi-source layout and text recognition tools into scalable OCR workflows, Vortrag, Bibliotheca Baltica Symposium, Rostock 05.10.2018. Online: &amp;lt;https://ocr-d.github.io/2018-10-05-baltica/index.html#/&amp;gt;.    Weil, Stefan: 126 Jahre Zeitung online – Fundgrube für historisch Interessierte und Motor für die Bibliotheks-IT, Vortrag, 107. Bibliothekartag, Berlin 15.06.2018. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/46507/&amp;gt;.    Herrmann, Elisa: Wieviel sind 85% wert: Qualität von OCR- und NER-Verfahren für die Forschung, Vortrag, MWW / DARIAH-DE Expertenworkshop Suchtechnologien, Weimar 24.05.2018. Online: &amp;lt;/slides/MWW-2018/MWW-Workshop_Wieviel sind 85% wert_2018-05-24.pdf&amp;gt;.    Würzner, Kay-Michael: Neues aus OCR-D, Vortrag, PhilTag 2018, Würzburg 10.04.2018. Online: &amp;lt;/slides/PhilTag-2018/content.md&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, Vortrag, DHd 2018, Köln 28.02.2018. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Perspektiven der automatischen Texterfassung als Grundlage wissenschaftlicher Editionen am Beispiel der Brief- und Schriftenausgabe der Bernd Alois Zimmermann-Gesamtausgabe, Workshop, Workshop der AG eHumanities Mainz. Geisteswissenschaftliche Forschungsdaten. Methoden zur digitalen Erfassung, Mainz 19.10.2017. Online: &amp;lt;/slides/Akademienunion_2017/slides/ocr-perspektiven.pdf&amp;gt;.    Prabhune, Ajinkya; Neudecker, Clemens: OCR-D Technische Systemarchitektur: Workflows, Repository, Schnittstellen, Vortrag, Karlsruhe 26.09.2017. Online: &amp;lt;/slides/OCR-Workshop-2017/slides/systemarchitektur/OCR-D-Workshop-Prabhune-Neudecker.pdf&amp;gt;.    Würzner, Kay-Michael: (Open-Source-)OCR-Workflows, Vortrag, Digital Humanities Kolloquium, Berlin 04.08.2017. Online: &amp;lt;https://edoc.bbaw.de/frontdoor/index/index/docId/2786&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D: Koordinierte Förderinitiative zur Weiterentwicklung von OCR für historische Dokumente, Vortrag, 106. Bibliothekartag, Frankfurt am Main 30.05.2017. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/16521/rows/10/start/0/facetNumber_author_facet/all/author_facetfq/St%C3%A4cker%2C+Thomas/docId/3004&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Compilation of a Large Ground-Truth Data Set: Using Transkribus, Vortrag, Transkribus User Conference, Wien 11.02.2017. Online: &amp;lt;/slides/Transkribus-WS-2017/slides/gt_compilation.pdf&amp;gt;.    Herrmann, Elisa: Aktuelle OCR-Entwicklungen und ihr Einsatz in der Praxis, Vortrag, Berliner Bibliothekswissenschaftliches Kolloquium, Berlin 17.01.2017.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, Vortrag, DHd 2016, Leipzig 11.03.2016. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.    Herrmann, Elisa: OCR-D: Koordinierungsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, Philtag 13, Würzburg 26.02.2016. Online: &amp;lt;/slides/PhilTag-2016/OCR-D_Wurzburg-13PhilTag.pdf&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: Kooperationsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, 12. Workshop zur Texterkennung in historischen Dokumenten, Rostock 09.02.2016. Online: &amp;lt;/slides/Rostock-2016-02-09/OCR-D_Rostock_09-02-2016.pdf&amp;gt;.  ",
      "url": " /en/publications.html"
    },
  

    {
      "slug": "de-publications-html",
      "title": "Publikationen",
      "content"	 : "Publikationen  Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Dong, Rui; Burghardt, Manuel; Christlein, Vincent: New Approaches to OCR for Early Printed Books, in: DigItalia 15 (2), 12.2020, S. 74–87. Online: &amp;lt;https://doi.org/10.36181/digitalia-00015&amp;gt;.    Engl, Elisabeth: OCR-D kompakt: Ergebnisse und Stand der Forschung in der Förderinitiative, in: Bibliothek Forschung und Praxis 44 (2), 29.07.2020, S. 218–230. Online: &amp;lt;https://doi.org/10.1515/bfp-2020-0024&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Neudecker, Clemens: Volltexte – die Zukunft alter Drucke. Bericht zum Abschlussworkshop des OCR-D-Projekts, in: o-bib 7 (2), 05.05.2020, S. 1–4. Online: &amp;lt;https://doi.org/10.5282/o-bib/5600&amp;gt;.    Boenig, Matthias; Engl, Elisabeth; Baierer, Konstantin; Hartmann, Volker; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, in: DHd 2020: Spielräme - Digital Humanities zwischen Modellierung und Interpretation. Konferenzabstracts, Paderborn 05.03.2020, S. 244–247. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth; Boenig, Matthias; Baierer, Konstantin; Neudecker, Clemens; Hartmann, Volker: Volltexte für die Frühe Neuzeit. Der Beitrag des OCR-D-Projekts zur Volltexterkennung frühneuzeitlicher Drucke, in: Zeitschrift für Historische Forschung 47 (2), 2020, S. 223–250.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 25–30. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352638&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 1–6. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352640&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open source OCR framework for historical documents, in: EuropeanaTech Insight (13), 31.07.2019. Online: &amp;lt;https://pro.europeana.eu/page/issue-13-ocr#ocr-d-an-end-to-end-open-source-ocr-framework-for-historical-documents&amp;gt;.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 3–8. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322916&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 19.24. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322908&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 53–58. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322917&amp;gt;.    Engl, Elisabeth: Das Projekt OCR-D – Ein Fortschrittsbericht zur Volltextdigitalisierung frühneuzeitlicher Drucke, in: Medium Buch 1, 2019, S. 233–235.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, in: Conference Abstracts, Galway 08.12.2018. Online: &amp;lt;https://eadh2018.exordo.com/files/papers/92/final_draft/EADH_2018_Proposal_Brief_Final.pdf&amp;gt;.    Schulz, Klaus; Fink, Florian: Novel software for cleansing digitised historical texts, in: Scientia, 28.11.2018. Online: &amp;lt;https://doi.org/10.26320/SCIENTIA278&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, in: Konferenzabstracts, Köln 28.02.2018, S. 219–223. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D – Koordinierte Förderinitiative zur Weiterentwicklung von OCR-Verfahren, in: Bibliotheksdienst 52 (1), 05.12.2017. Online: &amp;lt;https://doi.org/10.1515/bd-2018-0007&amp;gt;.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, in: Konferenzabstracts, Leipzig 11.03.2016, S. 103–108. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.  Vorträge  Vom Bild zum Text — praktische OCR für die DH, Event-Reihe, vDHd 2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/&amp;gt;.    Engl, Elisabeth: Massendigitalisierung alter Drucke – OCR-D in Bibliotheken, Vortrag, 3. Workshop zur Retrodigitalisierung. OCR – Prozesse und Entwicklungen 01.03.2021. Online: &amp;lt;https://wiki.zbw.eu/pages/viewpage.action?pageId=33620559&amp;amp;preview=/33620559/33620562/2021-02-24_Engl_Massendigitalisierung%20alter%20Drucke.pdf&amp;gt;.    Neudecker, Clemens: OCR-D: An open ecosystem for improving OCR on historical documents, Vortrag, Mini-ELAG 20.10.2020. Online: &amp;lt;https://elag.org/mini-elag-october-20-2020/ocr-d-an-open-ecosystem-for-improving-ocr-on-historical-documents/&amp;gt;.    Boenig, Matthias: Digitale Transformation: OCR-D, Angebot und Vision, Vortrag, FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft 08.10.2020. Online: &amp;lt;https://docs.google.com/presentation/d/1JCzfGq_Reze7R3TaecYyBocnkD6uNy94eukJTlYUbNI/edit#slide=id.g9d954d5829_0_469&amp;gt;.    Engl, Elisabeth: OCR-D in the wild: Erfahrungen und Erkenntnisse aus der Praxisphase mit Bibliotheken, Vortrag, vbib2020 26.05.2020. Online: &amp;lt;https://doi.org/10.5446/47151&amp;gt;.    Baierer, Konstantin; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, Paderborn 05.03.2020. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth: Die OCR-D-Workflowengine, Vortrag, 2. Workshop Retrodigitalisierung zu Effizienz und Qualitätssicherung in Digitalisierungsworkflows, Hannover 18.02.2020. Online: &amp;lt;/slides/Retrodigitalisierung-2020-02-18/TIB_Retrodigitalisierung.pdf&amp;gt;.    Boenig, Matthias: Spezifikationen und Lessons Learned, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_Spezifikationen_Lessons_Learned.pdf&amp;gt;.    Engl, Elisabeth: Bibliothekarische Digitalisierungspraxis und die OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_in_Bibliotheken.pdf&amp;gt;.    Engl, Elisabeth: OCR-D in a Nutshell, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/Abschlussworkshop_Überblick.pdf&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin: Funktionen und Möglichkeiten der OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;https://hackmd.io/@cneud/ocrd-bonn#/&amp;gt;.    Würzner, Kay-Michael: Multi-source OCR workflows with OCR-D, Vortrag, The Open Islamicate Texts Initiative Workshop, Berwyn 29.01.2020. Online: &amp;lt;https://wrznr.github.io/OpenITI-2020/#1&amp;gt;.    Baierer, Konstantin; Engl, Elisabeth; Luetgen, Michael: OCR(-D) und Kitodo, Vortrag, Kitodo Anwenderworkshop, Hamburg 19.11.2019. Online: &amp;lt;https://hackmd.io/@kba/S1peIVxhH#/&amp;gt;.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019. Online: &amp;lt;https://hackmd.io/@kba/SyiQKUCUH#/&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019.    Metzger, Noah: Projektabschlusspräsentation, Vortrag, Mannheim 19.09.2019. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/52213/&amp;gt;.    Sachunsky, Robert; Würzner, Kay-Michael: Flexible workflows with OCR-D, Vortrag, 3rd OCR-D developer workshop, Berlin 26.08.2019. Online: &amp;lt;https://hackmd.io/@FKFH0M1sR2SdJZwK5U8Cfg/S1YQ4NeNr#/&amp;gt;.    Metzger, Noah; Weil, Stefan: Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow, Workshop, MAD HD, Heidelberg 30.07.2019.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://hackmd.io/@QTT7e4hCTyWxVOvjiS61cA/B1nn7W7jV#/&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents&amp;gt;.    Weil, Stefan: Tesseract OCR – News, Vortrag, ELAG 2019, Berlin 09.05.2019. Online: &amp;lt;https://www.elag2019.de/talks/2019-05-09-tesseract-elag.pdf&amp;gt;.    Kamlah, Jan; Weil, Stefan: Forschungsdaten aus Digitalisaten, Vortrag, E-Science-Tage, Heidelberg 28.03.2019. Online: &amp;lt;https://heibox.uni-heidelberg.de/d/31bb269467/files/?p=%2FVortr%C3%A4ge%2FC3_2019-03-28-Kamlah-Weil.pdf&amp;gt;.    Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Herrmann, Elisa: Vom gedruckten Werk zu elektronischem Volltext, Workshop, DHd 2019, Mainz 25.03.2019. Online: &amp;lt;http://kba.cloud/2019-03-25-dhd/&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Christlein, Vincent; Maier, Andreas: Automatic Font Group Recognition in Early Printed Books, Vortrag, Mainz 25.03.2019.    Boenig, Matthias: OCR-D in der Praxis: Ein gemeinsamer Ausblick mit Dienstleistern und Anwendern, Öffentliche Arbeitssitzung, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/17097/docId/16357/start/0/rows/20&amp;gt;.    Herrmann, Elisa: Von der Vision zur Umsetzung: Der aktuelle Entwicklungsstand von OCR-D, Vortrag, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://www.researchgate.net/publication/332173701_Von_der_Vision_zur_Umsetzung_Der_aktuelle_Entwicklungsstand_von_OCR-D&amp;gt;.    Weil, Stefan: Hands-On Lab digital / Vom Bild zum Text. Automatisierte Texterkennung in historischen Drucken mit der freien Software Tesseract, Workshop, 108. Bibliothekartag, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16351&amp;gt;.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, Posterpräsentation, EADH 2018, Galway 08.12.2018. Online: &amp;lt;https://git.informatik.uni-leipzig.de/ocr-d/poster-eadh2018/blob/master/main.pdf&amp;gt;.    Baierer, Konstantin; Würzner, Kay-Michael: An open-source framework for integrating multi-source layout and text recognition tools into scalable OCR workflows, Vortrag, Bibliotheca Baltica Symposium, Rostock 05.10.2018. Online: &amp;lt;https://ocr-d.github.io/2018-10-05-baltica/index.html#/&amp;gt;.    Weil, Stefan: 126 Jahre Zeitung online – Fundgrube für historisch Interessierte und Motor für die Bibliotheks-IT, Vortrag, 107. Bibliothekartag, Berlin 15.06.2018. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/46507/&amp;gt;.    Herrmann, Elisa: Wieviel sind 85% wert: Qualität von OCR- und NER-Verfahren für die Forschung, Vortrag, MWW / DARIAH-DE Expertenworkshop Suchtechnologien, Weimar 24.05.2018. Online: &amp;lt;/slides/MWW-2018/MWW-Workshop_Wieviel sind 85% wert_2018-05-24.pdf&amp;gt;.    Würzner, Kay-Michael: Neues aus OCR-D, Vortrag, PhilTag 2018, Würzburg 10.04.2018. Online: &amp;lt;/slides/PhilTag-2018/content.md&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, Vortrag, DHd 2018, Köln 28.02.2018. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Perspektiven der automatischen Texterfassung als Grundlage wissenschaftlicher Editionen am Beispiel der Brief- und Schriftenausgabe der Bernd Alois Zimmermann-Gesamtausgabe, Workshop, Workshop der AG eHumanities Mainz. Geisteswissenschaftliche Forschungsdaten. Methoden zur digitalen Erfassung, Mainz 19.10.2017. Online: &amp;lt;/slides/Akademienunion_2017/slides/ocr-perspektiven.pdf&amp;gt;.    Prabhune, Ajinkya; Neudecker, Clemens: OCR-D Technische Systemarchitektur: Workflows, Repository, Schnittstellen, Vortrag, Karlsruhe 26.09.2017. Online: &amp;lt;/slides/OCR-Workshop-2017/slides/systemarchitektur/OCR-D-Workshop-Prabhune-Neudecker.pdf&amp;gt;.    Würzner, Kay-Michael: (Open-Source-)OCR-Workflows, Vortrag, Digital Humanities Kolloquium, Berlin 04.08.2017. Online: &amp;lt;https://edoc.bbaw.de/frontdoor/index/index/docId/2786&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D: Koordinierte Förderinitiative zur Weiterentwicklung von OCR für historische Dokumente, Vortrag, 106. Bibliothekartag, Frankfurt am Main 30.05.2017. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/16521/rows/10/start/0/facetNumber_author_facet/all/author_facetfq/St%C3%A4cker%2C+Thomas/docId/3004&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Compilation of a Large Ground-Truth Data Set: Using Transkribus, Vortrag, Transkribus User Conference, Wien 11.02.2017. Online: &amp;lt;/slides/Transkribus-WS-2017/slides/gt_compilation.pdf&amp;gt;.    Herrmann, Elisa: Aktuelle OCR-Entwicklungen und ihr Einsatz in der Praxis, Vortrag, Berliner Bibliothekswissenschaftliches Kolloquium, Berlin 17.01.2017.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, Vortrag, DHd 2016, Leipzig 11.03.2016. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.    Herrmann, Elisa: OCR-D: Koordinierungsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, Philtag 13, Würzburg 26.02.2016. Online: &amp;lt;/slides/PhilTag-2016/OCR-D_Wurzburg-13PhilTag.pdf&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: Kooperationsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, 12. Workshop zur Texterkennung in historischen Dokumenten, Rostock 09.02.2016. Online: &amp;lt;/slides/Rostock-2016-02-09/OCR-D_Rostock_09-02-2016.pdf&amp;gt;.  ",
      "url": " /de/publications.html"
    },
  

    {
      "slug": "search-index-json",
      "title": "",
      "content"	 : "[  {% for post in site.pages %}    {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    },  {% endfor %}  {% for post in site.posts %}  {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    }    {% unless forloop.last %},{% endunless %}  {% endfor %}]",
      "url": " /search-index.json"
    },
  

    {
      "slug": "search-html",
      "title": "Search",
      "content"	 : "Search",
      "url": " /search.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-setup-time-html",
      "title": "",
      "content"	 : "# Setup Time!## What operating system are you on?### LinuxYou&#39;re set, see [next section](#installation-ocr-d-stack)### Mac OS XWorks mostly like Linux for our purposes, see [next section](#installation-ocr-d-stack).However, you will need to have `homebrew` or `macports` on your system to beable to [install tesseract fromhomebrew](https://formulae.brew.sh/formula/tesseract_) or [withmacports](https://github.com/macports/macports-ports/blob/master/textproc/tesseract/Portfile).In case of problems, install VirtualBox and Ubuntu 18.04 inside.### Windows 7Please install VirtualBox and Ubuntu 18.04 inside### Windows 10Install Windows Subsystem for Linux (WSL)1. Open PowerShell as Administrator and run:    ```bash=bash    Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux    ```    :information_source: Needs reboot!2. Press &#39;Windows&#39;-Key3. Type &#39;Microsoft Store&#39;4. Start Microsoft Store5. Search for &#39;Ubuntu&#39;6. In category &#39;Apps&#39; select &#39;Ubuntu 18.04 LTS&#39;7. Select &#39;Download/Herunterladen&#39;8. Dialog &#39;Geräteübergreifend verwenden&#39;   a) Select &#39;Nein, danke&#39;9. Installation finished! -&gt; Start    ```    Installing, this may take a few minutes...    Please create a default UNIX user account. ...    Enter new UNIX username: ocrd    Enter new UNIX password: dhd2019    Retype new UNIX password: dhd2019    passwd: password updated successfully    [...]    ocrd@hostname:~$    ```10. Done**Link:** https://docs.microsoft.com/de-de/windows/wsl/install-win10## Installation OCR-D Stack### Python```bash=bash# Update package listuser@hostname:~$sudo apt-get update[...]Reading package lists... Done# Install Python3user@hostname:~$sudo apt-get install python3 python3-dev python3-virtualenv build-essential[...]Do you want to continue? [Y/n] &#39;Enter&#39;[...]user@hostname:~$```### Setup a virtualenv:information_source: Setting up virtual environment will take a while!```bash=bash# Setup virtual environment.user@hostname:~$python3 -m virtualenv ~/env-ocrd --python=python3Already using interpreter /usr/bin/python3Using base prefix &#39;/usr&#39;New python executable in /home/ocrd/env-ocrd/bin/python3Also creating executable in /home/ocrd/env-ocrd/bin/pythonInstalling setuptools, pkg_resources, pip, wheel...doneuser@hostname:~$```### Activate virtualenv```bash=bashuser@hostname:~$source ~/env-ocrd/bin/activate(env-ocrd) user@hostname:~$```### Install ocrd core Software:information_source: Setting up ocrd core software will take a while!```bash=bash(env-ocrd) user@hostname:~$pip install ocrd==1.0.0b6Collecting ocrd==1.0.0b5  Downloading https://files.pythonhosted.org/packages/cd/e4/9f56fe9971e04e2e97d6ad27457d6a1c17d798de9c15fd3030f194beca24/ocrd-0.15.2-py3-none-any.whl (138kB)   100% |████████████████████████████████| 143kB 3.0MB/s[...]Successfully installed Deprecated-1.2.0 Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.1.1 Pillow-5.4.1 Werkzeug-0.14.1 attrs-19.1.0 bagit-1.7.0 bagit-profile-1.3.0 certifi-2019.3.9 chardet-3.0.4 click-7.0 idna-2.8 itsdangerous-1.1.0 jsonschema-3.0.1 lxml-4.3.2 numpy-1.16.2 ocrd-0.15.2 opencv-python-4.0.0.21 pyrsistent-0.14.11 pyyaml-5.1 requests-2.21.0 six-1.12.0 urllib3-1.24.1 wrapt-1.11.1(env-ocrd) user@hostname:~$```#### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd --versionocrd, version 1.0.0b6```### Install OCR-D Modules- ocrd_ocropy- ocrd_kraken- ocrd_tesserocr#### ocrd_ocropy```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_ocropyCollecting ocrd_ocropy[...]Successfully installed cycler-0.10.0 imageio-2.5.0 kiwisolver-1.0.1 matplotlib-3.0.3 ocrd-fork-ocropy-1.4.0a3 ocrd-ocropy-0.0.1a1 pyparsing-2.3.1 python-dateutil-2.8.0(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-ocropy-segment --helpUsage: ocrd-ocropy-segment [OPTIONS]                                                                                                                   Options:  -V, --version                   Show version  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                  Log level  -J, --dump-json                 Dump tool description as JSON and exit  -p, --parameter PATH  -g, --page-id TEXT              ID(s) of the pages to process  -O, --output-file-grp TEXT      File group(s) used as output.  -I, --input-file-grp TEXT       File group(s) used as input.  -w, --working-dir TEXT          Working Directory  -m, --mets TEXT                 METS URL to validate  --help                          Show this message and exit.```#### ocrd_kraken```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_krakenCollecting ocrd_kraken[...]Successfully installed kraken-0.9.16 ocrd-kraken-0.1.0 regex-2019.3.12(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-kraken-binarize --helpUsage: ocrd-kraken-binarize [OPTIONS]Options:-V, --version                   Show version-l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]  Log level-J, --dump-json                 Dump tool description as JSON and exit-p, --parameter PATH-g, --page-id TEXT              ID(s) of the pages to process-O, --output-file-grp TEXT      File group(s) used as output.-I, --input-file-grp TEXT       File group(s) used as input.-w, --working-dir TEXT          Working Directory-m, --mets TEXT                 METS URL to validate--help                          Show this message and exit.(env-ocrd) user@hostname:~$```#### ocrd_tesserocr```bash=bash# Install tesseract(env-ocrd) user@hostname:~$sudo apt-get install libtesseract-dev tesseract-ocr[...]Do you want to continue? [Y/n] &#39;Enter&#39;[...](env-ocrd) user@hostname:~$```:information_source: Setting up virtual environment will take a while!```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_tesserocrCollecting ocrd_tesserocr[...]Successfully installed kraken-0.9.16 ocrd-kraken-0.1.0 regex-2019.3.12(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-tesserocr-recognize --helpUsage: ocrd-tesserocr-recognize [OPTIONS]Options:-V, --version                   Show version-l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                Log level-J, --dump-json                 Dump tool description as JSON and exit-p, --parameter PATH-g, --page-id TEXT              ID(s) of the pages to process-O, --output-file-grp TEXT      File group(s) used as output.-I, --input-file-grp TEXT       File group(s) used as input.-w, --working-dir TEXT          Working Directory-m, --mets TEXT                 METS URL to validate--help                          Show this message and exit.(env-ocrd) user@hostname:~$```## Install TranskribusTo use Transkribus a registration with Transkribus is necessary. Follow the *Register and Installation Guide* on: https://transkribus.eu/Transkribus/. In this case it is cited here in the following passage.**Register at the website**-  Go to: http://transkribus.eu/-  Read our user agreement: https://transkribus.eu/Transkribus/docs/TranskribusTermsOfUse_v04-2016.pdf-  All documents uploaded toTranskribus are “private”,which means that no one except you has access to them.-  The Transkribus team fully supports all EU directives on data protection and privacy. We will respect your privacy and only use the data to improve our services and support research in humanities and computer science!**Download Transkribus from the website** -  Go to the Transkribus website http://transkribus.eu/ and click “Download”.  -  Transkribus runs on Windows, MacOS and Linux.  If you need help installing the platform, consult the Transkribus wiki: https://transkribus.eu/wiki/index.php/Download_and_Installation -  If you use MacOS an error message may appear when you try to open Transkribus for the first time. To remedy this:-   right click the Track Pad to open the Context Menu and add a security exception for Transkribus.  -  Once you have downloaded Transkribus, make sure you unzip the file. The program cannot be started from the zipped file!## Install PAGE ViewerPAGE Viewer allows you to view a PAGE file together with the image.Download the appropriate version for your OS from the [PRIMA Labs Website](https://www.primaresearch.org/alternative_download_links.html)## Yay :fireworks: :tada::fireworks: :tada:If all steps were successful you are well prepared for the workshop.If not don&#39;t hesitate to contact us via [gitter](https://gitter.im/OCR-D/Lobby).",
      "url": " /slides/2019-03-25-dhd/setup-time.html"
    },
  

    {
      "slug": "en-setup-html",
      "title": "OCR-D setup guide",
      "content"	 : "# OCR-D setup guideOCR-D&#39;s software is a modular collection of many projects (called _modules_)with many tools per module (called _processors_) that you can combine freelyto achieve the workflow best suited for OCRing your content.## System requirementsMinimum system requirements- 8 GB RAM (more recommended)    - The more RAM is available, the more concurrent processes can be run  - Exceedingly large images (newspapers, folio-size books...) require a lot of RAM  - min. 20 GB free disk space for local installation (more recommended)    - How much disk space is needed depends mainly on the individual purposes of the installation. In addition to the installation itself   you will need space for various [pretrained models](https://ocr-d.de/en/models), training and evaluation data for training, and data to process.  - Operating system: Ubuntu 18.04    - Ubuntu 18.04 is our target platform because it was the most up-to-date Ubuntu LTS release when we started developing and [will be supported for the foreseeable future](https://ubuntu.com/about/release-cycle)  - Other Linux distributions or Ubuntu versions can also be used, though some instructions have to be adapted (e.g. package management, locations of some files)  - With Windows Subsystem for Linux (WSL), a feature of Windows 10, it is [also possible to set up an Ubuntu 18.04 installation within Microsoft Windows](https://github.com/OCR-D/ocrd-website/wiki/OCR-D-on-Windows)  - OCR-D can be deployed on an [Apple MacOSX machine using Homebrew](https://github.com/OCR-D/ocrd-website/wiki/OCR-D-on-macOS)  - Python 3.5, 3.6 or 3.7    - OCR-D&#39;s target Python version is currently Python 3.5 which we will continue to support until at least Q1 2021  - Python 3.6 and 3.7 are also tested and supported  - We currently **cannot fully support Python 3.8**, because there currently (July 2020) are no pre-built Python packages for Tensorflow and other GPU related software). We expect to unconditionally support Python 3.8 in Q1 2021 the latest.  For installation on Windows 10 (WSL) and macOS see the setup guides in the [OCR-D-Wiki](https://github.com/OCR-D/ocrd-website/wiki)Alternatively, you can use [Docker](https://hub.docker.com/u/ocrd). This way, you will only have to meet the minimum requirements for free disk space. But you can use any operating system you want and do not have to worry about the Python version. ## InstallationThere are two ways to install OCR-D modules:  1. [Using](#ocrd_all-via-docker) the [ocrd_all prebuilt Docker images `ocrd/all`](https://hub.docker.com/r/ocrd/all) to install a module collection (**recommended**)  2. [Using](#ocrd_all-natively) the [ocrd_all git repository](https://github.com/OCR-D/ocrd_all) to install selected modules nativelyWe recommend using the prebuilt Docker images since this does not require any changes tothe host system besides [installing Docker](https://hub.docker.com/r/ocrd/all).For developers it might be useful to [install the modules individually](#individual-installation), either via Docker or natively.Beware that for all other users and purposes we do not recommend installing modules individually, because it can be difficult to catch all dependencies, keep the software up-to-date and ensure that they are at usable and interoperable versions.## ocrd_allThe [`ocrd_all`](https://github.com/OCR-D/ocrd_all) project is an effort by theOCR-D community, now maintained by the OCR-D coordination team. It streamlinesthe native installation of OCR-D modules with a versatile Makefile approach.Besides allowing native installation of the full OCR-D stack (or any subset),it is also the base for the [`ocrd/all`](https://hub.docker.com/r/ocrd/all)Docker images available from DockerHub that contain the full stack (or certain subsets)of OCR-D modules ready for deployment.Technically, `ocrd_all` is a Git repository that keeps all the necessary softwareas Git submodules at specific revisions. This way, the software tools are knownto be at a stable version and guaranteed to be interoperable with one another.## ocrd_all via Docker### mini medi maxiThere are three versions of the[`ocrd/all`](https://hub.docker.com/r/ocrd/all) image:`minimum`, `medium` and `maximum`. They differ in which modules are includedand hence the size of the image. Only use the `minimum` or `medium` images ifyou are certain that you do not need the full OCR-D stack for your workflows, otherwisewe encourage you to use the large but complete `maximum` image.Check this table to see which modules are included in which version:| Module                      | `minimum` | `medium` | `maximum` || -----                       | ----      | ----     | ----      || core                        | ☑         | ☑        | ☑         || ocrd_cis                    | ☑         | ☑        | ☑         || ocrd_fileformat             | ☑         | ☑        | ☑         || ocrd_im6convert             | ☑         | ☑        | ☑         || ocrd_pagetopdf              | ☑         | ☑        | ☑         || ocrd_repair_inconsistencies | ☑         | ☑        | ☑         || ocrd_tesserocr              | ☑         | ☑        | ☑         || tesserocr                   | ☑         | ☑        | ☑         || workflow-configuration      | ☑         | ☑        | ☑         || cor-asv-ann                 | -         | ☑        | ☑         || dinglehopper                | -         | ☑        | ☑         || format-converters           | -         | ☑        | ☑         || ocrd_calamari               | -         | ☑        | ☑         || ocrd_keraslm                | -         | ☑        | ☑         || ocrd_olena                  | -         | ☑        | ☑         || ocrd_segment                | -         | ☑        | ☑         || tesseract                   | -         | ☑        | ☑         || ocrd_anybaseocr             | -         | -        | ☑         || ocrd_kraken                 | -         | -        | ☑         || ocrd_ocropy                 | -         | -        | ☑         || ocrd_pc_segmentation        | -         | -        | ☑         || ocrd_typegroups_classifier  | -         | -        | ☑         || sbb_textline_detector       | -         | -        | ☑         || cor-asv-fst                 | -         | -        | ☑         |### Fetch Docker imageTo fetch the `maximum` version of the `ocrd/all` Docker image:```shdocker pull ocrd/all:maximum```Replace `maximum` accordingly if you want the `minimum` or `medium` variant.(Also, if you want to keep the modules&#39; git repos inside the Docker images – so you can keep making fast updates, without waiting for a new pre-built image but also without building an image yourself –, then add the suffix `-git` to the variant, e.g. `maximum-git`. This will behave like the native installation, only inside the container. Yes, you can also [commit changes](https://rollout.io/blog/using-docker-commit-to-create-and-change-an-image/) made in containers back to your local Docker image.)### Testing the Docker installationFor example, let&#39;s fetch a document from the [OCR-D GT Repo](https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/):```shwget &#39;https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/736a2f9a-92c6-4fe3-a457-edfa3eab1fe3/data/wundt_grundriss_1896.ocrd.zip&#39;unzip wundt_grundriss_1896.ocrd.zipcd data```Let&#39;s segment the images in file group `OCR-D-IMG` from the zip file into regions (creating afirst [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) file group`OCR-D-SEG-BLOCK-DOCKER`)You can spin up a docker container, mounting the current working directory like this:```shdocker run --user $(id -u) --workdir /data --volume $PWD:/data -- ocrd/all:maximum ocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK-DOCKER```For instructions on how to process your own data, please see the [user guide](/en/user_guide). Make sure to also read [the notes on translating native command linecalls to docker calls above](/en/user_guide#translating-native-commands-to-docker-calls). Make sure the imagename matches the executable. ### Updating Docker imageTo update the docker images to their latest version, just run the `docker pull` command again:```shdocker pull ocrd/all:```This can even be set up as a cron-job to ensure the image is always up-to-date.## ocrd_all nativelyThe `ocrd_all` project contains a sophisticated Makefile to install or compileprerequisites as necessary, set up a virtualenv, install the core software,install OCR-D modules and more. Detailed documentation [can be found in itsREADME](https://github.com/OCR-D/ocrd_all).### InstallationThere are some [system requirements](https://github.com/OCR-D/ocrd_all#system-packages) for ocrd_all.You need to have `make` installed to make use of `ocrd_all`:```shsudo apt install make```Clone the repository (still without submodules) and change into the `ocrd_all` directory:```shgit clone https://github.com/OCR-D/ocrd_allcd ocrd_all```You should now be in a directory called `ocrd_all`.It is easiest to install all the possible system requirements by calling `make deps-ubuntu` as root:```shsudo make deps-ubuntu```This will install all system requirements.Now you are ready for the final step which will actually install the OCR-D-Software.You can either install  1. all the software at once with the `all` target (equivalent to the [`maximum` Docker version](#mini-medi-maxi)),  2. modules individually by using an executable from that module as the target, or  3. a subset of modules by listing the project names in the `OCRD_MODULES` variable (equivalent to a custom selection of the [`medium` Docker version](#mini-medi-maxi)):```shmake all                       # Installs all the software (recommended)make ocrd-tesserocr-binarize   # Install ocrd_tesserocr which contains ocrd-tesserocr-binarizemake ocrd-cis-ocropy-binarize  # Install ocrd_cis  which contains ocrd-cis-ocropy-binarizemake all OCRD_MODULES=&quot;core ocrd_tesserocr ocrd_cis&quot; # Will install only ocrd_tesserocr and ocrd_cis```(Custom choices for `OCRD_MODULES` and other control variables (cf. `make help`) can also be made permanent by writing them into `local.mk`.)**Note:** Never run `make all` as root unless you know *exactly* what you are doing!Installation is incremental, i.e. failed/interrupted attempts can be continued, and modules can be installed one at a time as needed.Running `make` will also take care of cloning and updating all required submodules.Especially running `make all` will take a while (between 30 and 60 minutes or more on slower machines). In the end, it should say that the last processor was installed successfully.Having installed `ocrd_all` successfully, `ocrd --version` should give you the current version of [OCR-D/core](https://github.com/OCR-D/core).Activate the virtual Python environment, which was created in the directory `venv`, before running any OCR-D command.```shsource venv/bin/activateocrd --versionocrd, version 2.13.2 # your version should be 2.13.2 or later``` ### Testing the native installationFor example, let&#39;s fetch a document from the [OCR-D GT Repo](https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit/):```shwget &#39;https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/736a2f9a-92c6-4fe3-a457-edfa3eab1fe3/data/wundt_grundriss_1896.ocrd.zip&#39;unzip wundt_grundriss_1896.ocrd.zipcd data```If you haven&#39;t done it already, activate your venv:```sh# Activate the venvsource /path/to/ocrd_all/venv/bin/activate```Let&#39;s segment the images in file group `OCR-D-IMG` from the zip file into regions (creating afirst [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) file group`OCR-D-SEG-BLOCK`):```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK```For instructions on how to process your own data, please see the [user guide](/en/user_guide).### Updating the softwareAs `ocrd_all` is in [activedevelopment](https://github.com/OCR-D/ocrd_all/commits/master), it is wise toregularly update the repository and its submodules:```shgit pull ```This will refresh the local clone of ocrd_all with the changes in the official ocrd_all GitHub repository.Now you can install the changes with```shmake all ```This will run the installation process for all submodules which have been changed. In the end, it shouldsay that the last processor was installed successfully. `--version` for the processors which have been changedshould give you its current version. ## Individual installation (experts only)For developing purposes it might be useful to install modules individually, either with Docker or natively. With all variants of individual module installation, it will be up to you tokeep the repositories up-to-date and installed. We therefore discourageindividual installation of modules and recommend using ocrd_all as outlined above..All [OCR-D modules](https://github.com/topics/ocr-d) follow the same[interface](https://ocr-d.github.io/cli) and common design patterns. So onceyou understand how to install and use one project, you know how to install anduse all of them.### Individual Docker containerThis is the best option if you want full control over which modules youactually intend to use while still profiting from the simple installation ofDocker containers.You need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)Many OCR-D modules are also [published as Docker containers to DockerHub](https://hub.docker.com/u/ocrd). To find the Dockerimage for a module, replace the `ocrd_` prefix with `ocrd/`:```shdocker pull ocrd/tesserocr  # Installs ocrd_tesserocrdocker pull ocrd/olena  # Installs ocrd_olena```Now you can [test your installation](#testing-the-docker-installation).### Native installationInstalling each module into your system natively requires you to know and install all its _dependencies_ first.That can be _system packages_ (or even system package repositories) or _Python packages_. To learn about system dependencies, consult the module&#39;s README files. In contrast, Python dependencies shouldbe resolved automatically by using the Python package manager `pip`.&gt; **NOTE**&gt; &gt; ocrd_tesserocr requires **tesseract-ocr &gt;= 4.1.0**. But the Tesseract packages&gt; bundled with **Ubuntu  please enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr),&gt; which has up-to-date builds of tesseract and its dependencies:&gt; &gt; ```sh&gt; sudo add-apt-repository ppa:alex-p/tesseract-ocr&gt; sudo apt-get update&gt; ```Next subsections:- For Python you also first need [virtualenv](#virtualenv). Then you have two options: - installing [via PyPI](#from-pypi) or - installing [via local git clone](#from-git).#### virtualenv* **Always install python modules into a virtualenv*** **Never run `pip`/`pip3` as root**First install Python 3 and `venv`:```shsudo apt install python3 python3-venv``````sh# If you haven&#39;t created the venv yet:python3 -m venv ~/venv# Activate the venvsource ~/venv/bin/activate```Once you have activated the virtualenv, you should see `(venv)` prepended toyour shell prompt.#### From PyPIThis is the best option if you want to use the stable, released version of individual modules.However, many modules require a number of non-Python (system) packages. For theexact list of packages you need to look at the README of the module inquestion. (If you are not on Ubuntu &gt;= 18.04, then your requirements maydeviate from that.)For example to install `ocrd_tesserocr` from PyPI:```shsudo apt-get install git python3 python3-pip python3-venv libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetpip3 install ocrd_tesserocr```Many ocrd modules conventionally contain a Makefile with a `deps-ubuntu` target that can handle calls to `apt-get` for you:```shsudo make deps-ubuntu```Now you can [test your installation](#testing-the-native-installation).#### From git This is the best option if you want to change the source code or install the latest, unpublished changes.```shgit clone https://github.com/OCR-D/ocrd_tesserocrcd ocrd_tesserocrsudo make deps-ubuntu # or manually with apt-getmake deps             # or pip3 install -r requirementsmake install          # or pip3 install .```If you intend to develop a module, it is best to install the module editable:```shpip install -e .```This way, you won&#39;t have to reinstall after making changes.Now you can [test your installation](#testing-the-native-installation).",
      "url": " /en/setup.html"
    },
  

    {
      "slug": "de-teststellung-html",
      "title": "Ergebnisse und Erkenntnisse der ersten OCR-D-Teststellung",
      "content"	 : "# Ergebnisse und Erkenntnisse der ersten OCR-D-Teststellung## HintergrundUm die Jahreswende 2019/2020 wurde die OCR-D-Software erstmals in neunPilotbibliotheken getestet. Damit sollte die praktische Akzeptanz der Softwarebei künftigen, potentiellen Nutzer*innen sichergestellt werden, weshalb der Fokus aufderen Funktionalität und Einsetzbarkeit in der Praxis lag. An der Teststellungteilgenommen haben neben den Häusern des Koordinierungsprojekts auch zwei anden Modulprojekten beteiligte Bibliotheken sowie vier weitere Bibliotheken. DieErkenntnisse dieses ersten Testlaufs fließen in die Weiterentwicklung desOCR-D-Prototypen ein.Alle Pilotbibliotheken verfügen über erste Kenntnisse und Erfahrungen zu OCR,da sie zumindest auf Projektebene bzw. über Dienstleister bereits Volltexteerstellt haben. Inwieweit die als wichtig angesehene OCR künftig eigenständigdurchgeführt und fest im Digitalisierungsworkflow verankert werden soll, wirdderzeit noch in den Häusern abgestimmt. Für welche Nutzergruppe Volltexteerstellt werden, wird von den einzelnen Häusern unterschiedlich angegeben.Während ein Drittel allgemein GeisteswissenschaftlerInnen nennt, möchte einweiteres Drittel eine sehr breite Zielgruppe (Geisteswissenschaft, DigitalHumanities, Computerlinguistik und Wirtschaftswissenschaften) bedienen. Dieübrigen Bibliotheken sehen lediglich einen kleinen Nutzerkreis (DigitalHumanities oder Computerlinguistik) als Zielgruppe der OCR-Texte.An eine OCR-Software stellen die Pilotbibliotheken die folgenden Anforderungen:* hohe Erkennungsrate von Layout und Text* kostengünstiger Einsatz* schnelle Adaptierbarkeit/Fehlerbehebung* Modularität* Ausgabe in Standardformate* Anbindung an existierende Workflows* gut dokumentierte Schnittstellen* Wortkoordinaten* Trainierbarkeit* umfangreiche GT KorporaAm wichtigsten ist die hohe Qualität der Texterkennung, die weiteren Merkmalewerden jeweils nur von einem Teil der Pilotbibliotheken angegeben und dürftenals gewünschte, aber untergeordnete fakultative Merkmale angesehen werden.## Auswertung der SoftwaretestsUm die Vergleichbarkeit der einzelnen Tests in den Pilotbibliothekengewährleisten zu können, wurde ein Fragebogen erstellt, der zu Beginn derTeststellung an die Pilotbibliotheken ausgegeben wurde. In diesem werden dieRahmenbedingungen des Testlaufs, bspw. die verwendete technische Ausstattungund die getesteten OCR-D-Prozessoren, sowie die Dokumentation der Software,Schnittstellen, Funktionalität bzw. Benutzbarkeit der Software, derenMöglichkeiten zur Einbindung in existierende Workflows und die jeweilsbenötigten Ausgabeformate erfasst. Mit Erkennungsqualität, Funktionalität bzw.Benutzbarkeit, offenen Anforderungen sowie positiven Merkmalen derOCR-D-Software wurde außerdem nach den Ergebnissen der Teststellung gefragt.In der Teststellung wurden die verschiedenen Möglichkeiten zur Installation derOCR-D-Software mit und ohne Docker-Container genutzt und die Softwareerfolgreich auf einer breiten Auswahl an unterschiedlich leistungsstarken,teils virtuellen Servern installiert. Bei Nicht-Intel-Rechnern (ARM, PowerPC64)war diese komplizierter und zeitaufwändiger, da einzelne Python-Pakete aufdiesen Rechnern nicht ausführbar waren und erst manuell angepasst werdenmussten. Die während der Teststellung entwickelte Gesamt-Installation allerverfügbarer OCR-D-Prozessoren (``ocrd_all``) wurde als einfachste undunaufwändigste Variante dabei als am empfehlenswertesten bestätigt. In eineWorkflow-Software wie bspw. Kitodo wurde die OCR-D-Software an keinerPilotbibliothek integriert, da der Aufwand für eine Einbindung der Software fürden Testlauf zu hoch gewesen wäre. Als Herausforderung wurde die Verwendung der zahlreichen OCR-D-Prozessorenbeschrieben. Hier bereitete weniger deren Aufruf Probleme, als das Verständnisvon deren jeweiligem Einsatzbereich und insbesondere Auswahl sowieZusammenstellung der Prozessoren zu sinnvollen Workflows. Für die ersteTeststellung lag neben der technischen Dokumentation der Software noch keineGesamtdokumentation zu deren Nutzung vor, die sich auch an im OCR-Bereichunerfahrene Anwender richtet. Die Anforderungen und Wünsche der Tester an einesolche Dokumentation wurden noch in der Ausarbeitung der inzwischen [imNutzerbereich der OCR-D-Website eingestelltenAnleitungen](https://ocr-d.de/de/use) berücksichtigt. Die OCR-D-Software läuft insgesamt sehr stabil, Abbrüche wurden von keinerBibliothek gemeldet. Die benötigten Ausgabeformate werden bereits alleangeboten, wohingegen die wenigen noch erforderlichen Arbeiten im Bereich derSchnittstellen für die Weiterentwicklung des Prototyps eingeplant sind.Die Erkennungsqualität wurde von den jeweiligen Pilotbibliotheken nur aneinzelnen Seiten überprüft, da zu den Testbüchern kein Ground Truth vorliegt.Insgesamt sind die Ergebnisse dieses ersten Testlaufs vielversprechend. Die UBMannheim hat OCR-D mit Fokus auf die Tesseract-Prozessoren bspw. an fünfDrucken des 16. bis 19. Jahrhunderts getestet. An Antiqua-Drucken des 17. und 18.Jahrhunderts sowie einem Fraktur-Text aus dem 19. Jahrhundert konntenerwartungsgemäß die besten Ergebnisse von - im Falle der Antiqua deutlich -unter 0,1 CER bei den Rohdaten erzielt werden, wohingen der Frakturdruck ausdem 17. Jahrhundert leicht über 0,1 CER lag. Die größte Herausforderung stelltedie Fraktur aus dem 16. Jahrhundert dar, bei der lediglich ein CER von knappunter 0,16 erreicht werden konnte. Einen umfassenden Einblick in ihreTeststellung gibt die BBAW, deren [Bericht und Daten öffentlicheinsehbar](https://github.com/tboenig/ocrd_bbaw_pilotbibliothek) sind.Desiderate formulieren die OCR-D-Tester insbesondere bei Dokumentation,Qualität bzw. Benutzbarkeit der Prozessoren sowie perspektivisch derenSkalierbarkeit. Die benannten Anforderungen an die Dokumentation derOCR-D-Software wurden bereits weitgehend umgesetzt, wobei Dokumentationinsgesamt als kontinuierliche Aufgabe aufgefasst wird in die sukzessive vorallem auch praktische Erfahrungen in der Anwendung der OCR-D-Softwareeinfließen müssen. Im Bereich der Prozessoren wären v.a. zu Layouterkennung undNachkorrektur noch Verbesserungen wünschenswert. Die entsprechendenEntwicklungen der OCR-D-Modulprojekte konnten durch ihr Entwicklungsstadiumbzw. ihre speziellen technischen Anforderungen (GPU) nur bedingt getestetwerden. Mit deren Ergebnissen oder auch weiteren Modellen können die obengenannten Desiderate hoffentlich bedient werden. Für den Einsatz derOCR-D-Software in der Massendigitalisierung ist die Laufzeit mehrererProzessoren - wie ursprünglich für die dritte Projektphase geplant - noch zuoptimieren, außerdem sollten die Möglichkeiten zur Parallelisierung ausgebautwerden. Positiv von den Testern hervorgehoben wird der modulare und transparente Aufbauder OCR-D-Software, der diese besonders auszeichnet und die Konfigurationoptimaler Workflows für konkrete Anwendungsfälle erlaubt. Außerdem können dieOpen Source verfügbaren Python-Programme bei Bedarf von jeweils daraufspezialisierten Experten weiterentwickelt werden und ohne aufwändigeProgrammierarbeiten für Experimente zum OCR-Workflow genutzt werden. Bei Fragenund Problemen leisten die Entwickler rasch niedrigschwelligen Support.Insgesamt ist es vergleichsweise einfach möglich, die robust laufende, wennauch noch weiter zu optimierende OCR-D-Volltexterstellung anzustoßen, diebereits vielversprechende Ergebnisse liefert.",
      "url": " /de/teststellung.html"
    },
  

    {
      "slug": "en-use-html",
      "title": "How to use OCR-D",
      "content"	 : "# Welcome to the OCR-D User Section!This section contains all information relevant for using the OCR-D-software in libraries and similar institutions. Learn how to install and use the software in your institution.* [Setup Guide](/en/setup)  * How to setup/install the OCR-D stack* [User Guide](/en/user_guide)  * Instructions how to use OCR-D components* [Workflows](/en/workflows)  * Steps of an OCR-D-workflow with sample workflows* [Models](/en/models)  * Overview of models for different OCR-engines* [Glossary](/en/spec/glossary)  * Glossary of technical terms used in OCR-D",
      "url": " /en/use.html"
    },
  

    {
      "slug": "de-use-html",
      "title": "Willkommen im Nutzerbereich von OCR-D!",
      "content"	 : "# Willkommen im Nutzerbereich von OCR-D!Dieser Bereich enthält alle Informationen, die für den Einsatz der OCR-D-Software in Bibliotheken und ähnlichen Einrichtungen relevant sind. Erfahren Sie, wie Sie die Software in Ihrer Einrichtung installieren und nutzen können.* [Setup Anleitung](/en/setup)  * Schritt-für-Schritt Anleitung zur Installation von OCR-D *(aktuell nur auf Englisch verfügbar)** [Nutzeranleitung](/en/user_guide)  * Instruktionen zum Arbeiten mit OCR-D *(aktuell nur auf Englisch verfügbar)** [Workflows](/en/workflows)  * Schritte eines OCR-D-Workflows mit Beispielworkflows *(aktuell nur auf Englisch verfügbar)** [Modelle](/en/models)  * Überblick zu Modellen verschiedener OCR-Engines *(aktuell nur auf Englisch verfügbar)** [Glossar](/de/spec/glossary)  * Fachbegriffe aus dem Bereich der OCR erklärt *(aktuell nur auf Englisch verfügbar)*",
      "url": " /de/use.html"
    },
  

    {
      "slug": "en-user-guide-html",
      "title": "User Guide for Non-IT Users",
      "content"	 : "# User Guide for Non-IT UsersThe following guide provides a detailed description on how to use the OCR-D-Software after it has been installed successfully. As explained in thesetup guide, you can either use the [OCR-D-Docker-solution](https://ocr-d.github.io/en/setup#ocrd_all-via-docker), or you can[install the Software locally](https://ocr-d.github.io/en/setup#ocrd_all-natively). Note that these two options require different prerequisites to get started with OCR-D after the installation as detailed in the very next two paragraphs. The [third preparatory step](#preparing-a-workspace) isobligatory for both Docker and Non-Docker users!Furthermore, Docker commands have a [different syntax than native calls](#translating-native-commands-to-docker-calls). This guide always states native calls first and then provides the respective command for Docker users. ## Prerequisites and Preparations### Setup dockerIf you want to use the OCR-D-Docker-solution, [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository) and [docker compose](https://docs.docker.com/compose/install/) have to be installed.  After installing docker you have to set up daemon and add user to  the group &#39;docker&#39;```sh# Start docker daemon at startupsudo systemctl enable docker# Add user to group &#39;docker&#39;sudo usermod -aG docker $USER``` Please log out and log in again.To test access to docker try the following command:```shdocker ps```Now you should see an (empty) list of available images.For installing docker images please refer to the [setup guide](setup.html).### Virtual environment (native installation)If you are using a native installation, you should activate thevirtualenv before starting to work with the OCR-D-software. This has either been installed automatically if you installed thesoftware via ocrd_all, or you should have [installed it yourself](https://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments) beforeinstalling the OCR-D-software individually. Note that you need to specify the path to your virtualenv. If you are simply using the `venv` is createdon-demand by `ocrd_all`, it is contained in your `ocrd_all` directory```shsource ~/venv/bin/activate# e.g. for your `ocrd_all` venvhabocr@ocrtest:~$ source ocrd_all/venv/bin/activate(venv) habocr@ocrtest:~$```Once you have activated the virtualenv, you should see `(venv)` prepended toyour shell prompt.When you are done with your OCR-D-work, you can use `deactivate` to deactivateyour venv.### Preparing a workspaceOCR-D processes digitized images in so-called workspaces, special directorieswhich contain the images to be processed and their corresponding METS file. Anyfiles generated while processing these images with the OCR-D-software will alsobe stored in this directory. How you prepare a workspace depends on whether you already have or don&#39;t have aMETS file with the paths to the images you want to process. For usage withinOCR-D your METS file should look similar to [this example](example_mets).#### Already existing METSIf you already have a METS file as indicated above, you can create a workspaceand load the pictures to be processed with the following command: ```shocrd workspace -d /path/to/workspace clone URL_OF_METS## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace clone -d /data URL_OF_METS```This will create a directory (called workspace in OCR-D) with your specified name which contains your METS file.In most cases, METS files indicate several picture formats. For OCR-D you willonly need one format. We strongly recommend using the format with the highestresolution. Optionally, you can specify to only load the file group needed:List all existing groups:```shocrd workspace -d [/path/to/your/workspace] list-group## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace -d /data list-group```This will provide you with the names of all the different file groups in your METS, e.g. THUMBNAILS,PRESENTATION, MAX.Download all files of one group:```shocrd workspace -d [/path/to/your/workspace] find --file-grp [selected file group] --download## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace -d /data find --file-grp [selected file group] --download```This will download all images in the specified file group and save them in a folder named accordinglyin your workspace. You are now ready to start processing your images with OCR-D.#### Non-existing METSIf you don&#39;t have a METS file or it doesn&#39;t suffice the OCR-D-requirements youcan generate it with the following commands. First, you have to create aworkspace:```shocrd workspace [-d /path/to/your/workspace] init # omit `-d` for current directory## alternatively using dockermkdir -p [/path/to/your/workspace]docker run --rm -u $(id -u) -v [/path/to/your/workspace]:/data -w /data -- ocrd/all:maximum ocrd workspace -d /data init```This should create a directory (called workspace in OCR-D) which contains a `mets.xml`.Then you can change into your workspace directory and set a unique ID```shcd /path/to/your/workspace # if not already thereocrd workspace set-id &#39;unique ID&#39;## alternatively using dockerdocker run --rm -u $(id -u) -v [/path/to/your/workspace]:/data -w /data -- ocrd/all:maximum ocrd workspace set-id &#39;unique ID&#39;```and copy the folder containing your pictures to be processed into the workspace:```shcp -r [/path/to/your/pictures] .```**Note:** All pictures must have the same format (tif, jpg, ...)In OCR-D we  name the image folder OCR-D-IMG, which is used throughout the documentation. Naming your image folder differently iscertainly possible, but you should be aware that you need to adapt the name of the image folder if copy and paste the samplecalls provided on this website. You should now have a workspace which contains the aforementioned `mets.xml` and a folder OCR-D-IMG with your images. Now you can add your pictures to the METS. When creating the workspace, a blankMETS file was created, too, to which you can add the pictures to be processed. You can do this manually with the following command:```shocrd workspace add -g [ID of the physical page, has to start with a letter] -G [name of picture folder in your workspace] -i [ID of the scanned page, has to start with a letter] -m image/[format of your picture] [/path/to/your/picture/in/workspace]## alternatively using dockerdocker run --rm -u $(id -u) -v [/path/to/workspace]:/data -w /data -- ocrd/all:maximum ocrd workspace add -g [ID of the physical page, has to start with a letter] -G [name of picture folder in your workspace] -i [ID of the scanned page, has to start with a letter] -m image/[format of your picture] [relative/path/to/your/picture/in/workspace]```Your command could e.g. look like this:```shocrd workspace add -g P_00001 -G OCR-D-IMG -i OCR-D-IMG_00001 -m image/tif OCR-D-IMG/00001.tif## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace add -g P_00001 -G OCR-D-IMG -i OCR-D-IMG_00001 -m image/tif OCR-D-IMG/00001.tif```If you have many pictures to be added to the METS, you can do this automatically with a for-loop:```shFILEGRP=&quot;YOUR-FILEGRP-NAME&quot;EXT=&quot;.tif&quot;  # the actual extension of the image filesMEDIATYPE=&#39;image/tiff&#39;  # the actual media type of the image files## using local ocrd CLIfor i in /path/to/your/picture/folder/in/workspace/*$EXT; do  base= `basename ${i} $EXT`;  ocrd workspace add -G $FILEGRP -i ${FILEGRP}_${base} -g P_${base} -m $MEDIATYPE ${i};done## alternatively using dockerfor i in /path/to/your/picture/folder/in/workspace/*$EXT; do  base= `basename ${i} $EXT`;  docker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace add -G $FILEGRP -i ${FILEGRP}_${base} -g P_${base} -m $MEDIATYPE ${i};done``` If the file names of the images starts with a number, at least one of the following characters must be placed in front of its name for parameter &#39;i&#39;: [a-z,A-Z,_,-] (e.g.: &#39;OCR-D-IMG__&#39;)Your for-loop could e.g. look like this:```shfor i in OCR-D-IMG/*.tif; do base=`basename ${i} .tif`; ocrd workspace add -G OCR-D-IMG -i OCR-D-IMG_${base} -g P_${base} -m image/tif ${i}; done## alternatively using dockerfor i in OCR-D-IMG/*.tif; do base=`basename ${i} .tif`;docker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd workspace add -G OCR-D-IMG -i OCR-D-IMG_${base} -g P_${base} -m image/tif ${i}; done```The log information should inform you about every image which was added to the `mets.xml`.In the end, your METS file should look like this [example METS](example_mets). You are now ready to start processing your images with OCR-D.Alternatively, `ocrd-import` from [workflow-configuration](#workflow-configuration) is a shell script which does all of the above (and can also convert arbitrary image formats) automatically. For usage options, see:```shocrd-import -h```For example, to search for all files under `path/to/your/pictures/` recursively, and add all image files under file group `OCR-D-IMG`, keeping their filename stem as page ID, and converting all unaccepted image file formats like JPEG2000, XPS or PDF (the latter rendered to bitmap at 300 DPI) to TIFF on the fly, and also add any PAGE-XML file of the same filename stem under file group `OCR-D-SEG-PAGE`, while ignoring other files, and finally write everything to `path/to/your/pictures/mets.xml`, do:```shocrd-import --nonnum-ids --ignore --render 300 path/to/your/pictures## alternatively using dockerdocker run --rm -u $(id -u) -v [/path/to/your/data]:/data -w /data -- ocrd/all:maximum ocrd-import -P -i -r 300 path/to/your/pictures```## Using the OCR-D-processors### OCR-D-SyntaxThere are several ways for invoking the OCR-D-processors. However, all of thoseways make use of the following syntax:```sh-I Input-Group    # folder of the files to be processed-O Output-Group   # folder for the output of your processor-P parameter      # indication of parameters for a particular processor```**Note:** The `-P` option accepts a parameter name and a parameter value. When we write `-P parameter`, we mean that `parameter` consists of`parameter name` and `parameter value`.For some processors parameters are purely optional, other processors as e.g. `ocrd-tesserocr-recognize` won&#39;t work without one or several parameters.### Calling a single processorIf you just want to call a single processor, e.g. for testing purposes, you can go into your workspace and use the following command:```shocrd-[processor needed] -I [Input-Group] -O [Output-Group] -P [parameter]## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd-[processor needed] -I [Input-Group] -O [Output-Group] -P [parameter]&#39;```Your command could e.g. look like this:```shocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN -P impl sauvola## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN -P impl sauvola```The specified processor will take the files in your Input-Group `-I`, process them and save the results in your Ouput-Group `-O`. It will also addthe information about this processing step and its results to METS file in your workspace. **Note:** For processors using multiple input-, or output groups you have to use a comma separated list. E.g.: ```shocrd-anybaseocr-crop  -I OCR-D-IMG -O OCR-D-CROP,OCR-D-IMG-CROP## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd-anybaseocr-crop  -I OCR-D-IMG -O OCR-D-CROP,OCR-D-IMG-CROP```**Note:** If multiple parameter key-value pairs are necessary, each of them has to be preceded by `-P`E.g.: ```sh-P param1 value1 -P param2 value2 -P param3 value3```**Note:** If a value consists of several words with whitespaces, they have to be enclosed in quotation marksE.g.: ```sh-P param &quot;value value&quot;```### Calling several processors#### ocrd-processIf you quickly want to specify a particular workflow on the CLI, you can useocrd-process, which has a similar syntax as calling single processors.```shocrd process   &#39;[processor needed without prefix &#39;ocrd-&#39;] -I [Input-Group] -O [Output-Group]&#39;   &#39;[processor needed without prefix &#39;ocrd-&#39;] -I [Input-Group] -O [Output-Group] -P [parameter]&#39;## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd process   &#39;[processor needed without prefix &#39;ocrd-&#39;] -I [Input-Group] -O [Output-Group]&#39;   &#39;[processor needed without prefix &#39;ocrd-&#39;] -I [Input-Group] -O [Output-Group] -P [parameter]&#39;```Your command could e.g. look like this:```shocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -P model Fraktur&#39;## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -P model Fraktur&#39;```Each specified processor will take all the files in your files in the respective Input-Group `-I`, process them and save theresults in the respective Ouput-Group `-O`. It will also add the information about this processing step and its results to the METS file in your workspace.The processors work on the files sequentially. So at first all files will be processed with the first processor (e.g. binarized), then all files will be processed by the second processor (e.g. segmented) etc. In the end your workspace should contain a folder for each Output-Group -O specifiedin your workflow, which contains the (intermediate) processing results. **Note:** In contrast to calling a single processor, for `ocrd process` you leaveout the prefix `ocrd-` before the name of a particular processor.#### TavernaTaverna is a more sophisticated workflow-software which allows you to specify aparticular workflow in a file and call this workflow, or rather its file, onseveral workspaces.Note that Taverna is not included in your [`ocrd_all`](https:/github.com/OCR-D/ocrd_all) installation. Therefore, you still might have to install it following this [setup guide](setup.md).Taverna comes with several predefined workflows which can help you getting started. These are stored in the `/conf` directory. 1. parameters.txt  (best results without gpu)2. parameters_fast.txt (good results for slower computers)3. parameters_gpu.txt (best results with gpu)**Note:** Those workflows are only tested with a limited set of pages of the 17./18. century. Results may be worse for other prints.For every workflow at least two files are needed: A `workflow_configuration` file contains a particular workflow which is invoked by a `parameters` file. For calling a workflow via Taverna, change into the `Taverna` folder and use the following command:```shbash startWorkflow.sh [particular parameters.txt] [/path/to/your/workspace]## alternatively using dockerdocker run --rm --network=&quot;host&quot; -v $PWD:/data -- ocrd/taverna process [particular parameters.txt] [relative/path/to/your/workspace]```The images in your indicated workspace will be processed and the respectiveoutput will be saved into the same workspace.When you want to adjust a workflow for better results on your particularimages, you should start off by copying the original `workflow_configuration`and `parameters` files. To this end, change to the `/conf` subdirectory of`Taverna` and use the following commands:```shconf$ cp workflow_configuration.txt [name of your new workflow_configuration.txt]conf$ cp parameters.txt [name of your new parameters.txt]```Open the new `parameters.txt` file with an editor like e.g. Nano and change thename of the old `workflow_configuration.txt` specified in this file to the nameof your new `workflow_configuration.txt` file: ```shnano [name of your new workflow_configuration.txt]```Then open your new `workflow_configuration.txt` file respectively and adjust it to your needs by exchanging or adding the specified processors of parameters. The first column contains the name of the processor, the following two columns indicate the names of the input and the output filegroups. The forth column for group-ID can be left blank. In the last column you can indicate the log level. If your processor requires a parameter, it has to be specified in the fith column. As with parameters when calling processors directly on the CLI, there are two ways how to specify them. You can either call a `json` file which should be stored in Taverna&#39;s subdirectory `models`. See [Calling a single processor](TODO) on how to create `json` files. Alternatively, you can directly write down the parameter needed using the following syntax:```sh{&quot;[param1]&quot;:&quot;[value1]&quot;,&quot;[param2]&quot;:&quot;[value2]&quot;,&quot;[param3]&quot;:&quot;[value3]&quot;}e.g.{&quot;level-of-operation&quot;:&quot;page&quot;}```**Note:** Avoid white spaces and escape double quotes with backslash.For information on the available processors see [section at the end](#get-more-information-about-processors).#### workflow-configurationworkflow-configuration is another tool for specifying OCR-D workflows and running them. It uses GNU make as workflow engine, treating document processing like software builds (including incremental and parallel computation). Configurations are just makefiles, targets are workspaces and their file groups.In contrast to Taverna it is included in ocrd_all, therefore you most likely already installed it with the other OCR-D-processors.The `workflow-configuration` directory already contains several workflows, which were tested against the Ground Truth provided by OCR-D. For the CER of those workflows in our tests see [the table on GitHub](https://github.com/bertsky/workflow-configuration#usage).**Note:** Most workflows are configured for GT data, i.e. they expect preprocessed images which were already segmented at least down to line level. If you want to run them on raw images, you have to add some preprocessing and segmentation steps first. Otherwise they will fail. In order to run a workflow, change into your data directory (that contains the workspaces) and call the desired configuration file on your workspace(s):```shocrd-make -f [name_of_your_workflow.mk] [/path/to/your/workspace1] [/path/to/your/workspace2]```As indicated in the command above, you can run a workflow on several workspaces by listing them after one another. Or use the special target `all` for all the workspaces in the current directory.The documents in those workspaces will be processed and the respectiveoutput along with the log files will be saved into the same workspace(s).For an overview of all available targets and workspaces:```shocrd-make help```For general info on `make` invocation, including the `-j` switch for parallel processing:```shmake --help```When you want to adjust a workflow for better results on your particularimages, you should start off by copying the original `workflow.mk` file:```shcp workflow.mk [name_of_your_new_workflow_configuration.mk]```Then open the new file with an editor which understands `make` syntax like e.g. `nano`, and exchange or add the processors or parameters to your needs: ```shnano [name_of_your_new_workflow_configuration.mk]```You can write new rules by using file groups as prerequisites/targets in the normal GNU make syntax. The first target defined must be the default goal that builds the very last file group for that configuration. Alternatively a variable `.DEFAULT_GOAL` pointing to that target can be set anywhere in the makefile. **Note:** Also see the [extensive Readme of workflow-configuration](https://bertsky.github.io/workflow-configuration) on how to adjust the preconfigured workflows to your needs.Each specified processor will take all the files in your files in the respective Input-Group `-I`, process them and save theresults in the respective Ouput-Group `-O`. It will also add the information about this processing step and its results to the METS file in your workspace.The processors work on the files sequentially. So at first all files will be processed with the first processor (e.g. binarized), then all files will be processed by the second processor (e.g. segmented) etc. In the end your workspace should contain a folder for each Output-Group -O specifiedin your workflow, which contains the (intermediate) processing results.#### Translating native commands to docker callsThe native calls presented above are simple to translate to commands based on thedocker images by prepending the boilerplate telling Docker which image to use,which user to run as, which files to bind to a container path etc.For example a call to[`ocrd-tesserocr-binarize`](https://github.com/OCR-D/tesserocr) might nativelylook like this:```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK```To run it with the [`ocrd/all:maximum`] Docker container:```shdocker run -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum ocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK           _________/ ___________/ ______/ _________________/ ___________________________________________________________/              (1)          (2)         (3)          (4)                            (5)```* (1) tells Docker to run the container as the calling user instead of root* (2) tells Docker to bind the current working directory as the `/data` folder in the container* (3) tells Docker to change the container&#39;s working directory to `/data`* (4) tells docker which image to run* (5) is the unchanged call to `ocrd-tesserocr-segment-region`It can also be useful to delete the container after creation with the `--rm`parameter.### Specifying New OCR-D-WorkflowsWhen you want to specify a new workflow adapted to the features of particularimages, we recommend using an existing workflow as specified in `Taverna` or`workflow-configuration` as starting point. You can adjust it to your needs byexchanging or adding the specified processors of parameters. For an overview onthe existing processors, their tasks and features, see the [next section](#get-more-information-about-processors) and our [workflow guide](workflows.html).### Get more Information about ProcessorsTo get all available processors you might use the autocomplete in your preferred console. **Note:** Activate virtual environment first.Type &#39;ocrd-&#39; followed by `TAB` to get a list of all available processors.To get further information about a particular processor, you can call `--help````sh[name_of_selected_processor] --help## alternatively using dockerdocker run --rm -u $(id -u) -v $PWD:/data -w /data -- ocrd/all:maximum [name_of_selected_processor] --help```### Usage of modelsSeveral processors rely on models which have to be downloaded beforehand. An overview on the existing model repositories and shortdescriptions on the most important models can be found [in our models documentation](https://ocr-d.de/en/models).We strongly recommend to use the [OCR-D resource manager](https://ocr-d.de/en/models) to download the models, as this makes iteasier to both download and use them.",
      "url": " /en/user_guide.html"
    },
  

    {
      "slug": "en-user-survey-html",
      "title": "Survey concerning usage of OCR texts",
      "content"	 : "# Survey concerning usage of OCR texts**Background**In spring of 2016 the BSB (back then part of the coordinating project) conducted a survey on the usage of OCR texts via the OCR-D-project-website which mainly addressed humanists. In total 139 researchers took part in the poll. 39 of those answers were partly illegible and some of the questions were only answered by a part of the participants. **Major findings**The survey shows that the great majority of the participants uses OCR texts for their research (cf. figure 1). Those texts are mainly used as search tools, but also as basis for the analysis of large amounts of text data (cf. figure 2). 60 % of the participants would also use dirty OCR texts for research purposes, whereas 40 % consider it useless data. Interestingly, only historians (87 %) show a significant preference of dirty OCR which is seen especially helpful as finding aid as one can still find information which would have been missed otherwise. Furthermore it facilitates citing by providing an initial text which can then be corrected so that the text doesn’t have to be typed completely manually. Overall however, the original image (61 %) is preferred to the OCR text (39 %) for citing, especially by librarians.Concerning the importance of versioning OCR texts there is much discord among the participating scholars. While almost three quarters of the researchers states that changes in the OCR text are important to their work, only half them wants to have access to earlier versions of OCR texts. These are mainly considered necessary for persistent quoting of the OCR text and for reproducing analyses conducted on those texts – though keeping track of all versions is rather seen as too laborious.**All in all, OCR texts are already widely used for research and also dirty OCR, as is currently the state of most OCRed early modern texts, is regarded as a valuable aid for particular parts of scientific work.**![](/assets/usage_OCR.png)![](/assets/usage_forms.png)",
      "url": " /en/user_survey.html"
    },
  

    {
      "slug": "de-user-survey-html",
      "title": "Umfrage zur Verwendung von OCR-Texten",
      "content"	 : "# Umfrage zur Verwendung von OCR-Texten**Hintergrund**Im Frühjahr 2016 führte die BSB (damals Teil des Koordinierungsprojekts) über die OCR-D-Projekt-Website eine Umfrage zur Verwendung von OCR-Texten durch, die sich hauptsächlich an Geisteswissenschaftler richtete. Insgesamt nahmen 139 Forscher an der Umfrage teil. 39 dieser Antworten waren teilweise unleserlich und einige der Fragen wurden nur von einem Teil der Teilnehmer beantwortet. **Hauptergebnisse**Die Umfrage zeigt, dass die grosse Mehrheit der Teilnehmerinnen und Teilnehmer OCR-Texte für ihre Forschung verwendet (vgl. Abb. 1). Diese Texte werden hauptsächlich als Suchwerkzeuge, aber auch als Grundlage für die Analyse großer Textdatenmengen verwendet (vgl. Abb. 2). 60 % der Teilnehmerinnen und Teilnehmer würden auch schmutzige OCR-Texte zu Forschungszwecken verwenden, während 40 % sie für nutzlose Daten halten. Interessanterweise zeigen nur die Historiker (87 %) eine signifikante Präferenz für schmutzige OCR, die als besonders hilfreich bei der Suche nach Informationen angesehen wird, die sonst übersehen worden wären. Darüber hinaus erleichtert sie das Zitieren, indem sie einen ersten Text liefert, der dann korrigiert werden kann, so dass der Text nicht vollständig manuell getippt werden muss. Insgesamt wird jedoch das Originalbild (61 %) dem OCR-Text (39 %) zum Zitieren vorgezogen, insbesondere von Bibliothekaren.Hinsichtlich der Bedeutung der Versionierung von OCR-Texten gibt es unter den teilnehmenden Wissenschaftlern große Meinungsverschiedenheiten. Während fast drei Viertel der Forscher angeben, dass Änderungen im OCR-Text für ihre Arbeit wichtig sind, möchte nur die Hälfte von ihnen Zugang zu früheren Versionen von OCR-Texten haben. Diese werden vor allem als notwendig erachtet, um den OCR-Text dauerhaft zitieren und die Analysen, die zu diesen Texten durchgeführt wurden, reproduzieren zu können - obwohl es eher als zu mühsam angesehen wird, alle Versionen im Auge zu behalten.**Insgesamt werden OCR-Texte bereits in großem Umfang für die Forschung verwendet, und auch  schmutzige OCR, wie sie derzeit für die meisten OCR-Texte frühneuzeitlicher Bücher vorliegt, wird als wertvolle Hilfe für bestimmte Teile der wissenschaftlichen Arbeit angesehen.**![](/assets/usage_OCR.png)![](/assets/usage_forms.png)",
      "url": " /de/user_survey.html"
    },
  

    {
      "slug": "en-workflows-html",
      "title": "OCR-D Workflow Guide",
      "content"	 : "# WorkflowsThere are several steps necessary to get the fulltext of a scanned print. The whole OCR process is shown in the following figure:![](https://ocr-d.de/assets/Funktionsmodell.png)The following instructions describe all steps of an OCR workflow. Depending on your particular print (or rather images), not all of thosesteps might be necessary to obtain good results. Whether a step is required or optional is indicated in the description of each step.This guide provides an overview of the available OCR-D processors and their required parameters. For more complex workflows and recommendationssee the [OCR-D-Website-Wiki](https://github.com/OCR-D/ocrd-website/wiki). Feel free to add your own experiences and recommendations in the Wiki!We will regularly amend this guide with valuable contributions from the Wiki.**Note:** In order to be able to run the workflows described in this guide, you need to have prepared your images in an [OCR-D-workspace](https://ocr-d.de/en/user_guide#preparing-a-workspace).We expect that you are familiar with the [OCR-D-user guide](https://ocr-d.de/en/user_guide) which explains all preparatory steps, syntax and differentsolutions for executing whole workflows.## Image Optimization (Page Level)At first, the image should be prepared for OCR.### Step 0.1: Image Enhancement (Page Level, optional)Optionally, you can start off your workflow by enhancing your images, which can be vital for the following binarization. In this processing step,the raw image is taken and enhanced by e.g. grayscale conversion, brightness normalization, noise filtering, etc.**Note:** `ocrd-preprocess-image` can be used to run arbitrary shell commands for preprocessing (original or derived) images, and can be seen as a generic OCR-D wrapper for many of the following workflow steps, provided a matching external tool exists. (The only restriction is that the tool must not change image size or the position/coordinates of its content.)#### Available processors            Processor      Parameter      Remark      Call                  ocrd-im6convert      -P output-format image/tiff      for output-options see IM Documentation      ocrd-im6convert -I OCR-D-IMG -O OCR-D-ENH -P output-format image/tiff              ocrd-preprocess-image            -P input_feature_filter binarized      -P output_feature_added binarized      -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            for parameters and command examples (presets) see the Readme          ocrd-preprocess-image -I OCR-D-IMG -O OCR-D-PREP -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                  ocrd-skimage-normalize                  ocrd-skimage-normalize -I OCR-D-IMG -O OCR-D-NORM            ocrd-skimage-denoise-raw                      ocrd-skimage-denoise-raw -I OCR-D-IMG -O OCR-D-DENOISE          ### Step 0.2: Font detectionOptionally, this processor can determine the font family (e.g. Antiqua, Fraktur,Schwabacher) to help select the right models for text detection.`ocrd-typegroups-classifier` annotates font families on pagelevel, including the confidence value (separated by colon). Supported `fontFamily` values:  * `Antiqua`  * `Bastarda`  * `Fraktur`  * `Gotico-Antiqua`  * `Greek`  * `Hebrew`  * `Italic`  * `Rotunda`  * `Schwabacher`  * `Textura`  * `other_font`  * `not_a_font`**Note:** `ocrd-typegroups-classifier` was trained on a very large and diversedataset, with both geometric and color-space random augmentation (contrast,brightness, hue, even compression artifacts and 2 different binarizationmethods), so it works best on the raw, *non-binarized* RGB image.**Note:** `ocrd-typegroups-classifier` comes with a non-OCR-D CLI that allowsfor the generation of &quot;heatmaps&quot; on the page to visualize which regions of the pageare classified as using a certain font with a certain confidence, see the[project&#39;s README for usage instructions](https://github.com/seuretm/ocrd_typegroups_classifier).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-typegroups-classifier      -P network /path/to/densenet121.tgc      Download densenet121.tgc from GitHub      ocrd-typegroups-classifier -I OCR-D-IMG -O OCR-D-IMG-FONTS      ### Step 1: Binarization (Page Level)All the images should be binarized right at the beginning of your workflow.Many of the following processors require binarized images. Some implementations(for deskewing, segmentation or recognition) may produce better results usingthe original image. But these can always retrieve the raw image instead of thebinarized version automatically.In this processing step, a scanned colored /gray scale document image is takenas input and a black and white binarized image is produced. This step shouldseparate the background from the foreground.**Note:** Binarization tools usually provide a threshold parameter which allowsyou to increase or decrease the weight of the foreground. This is optional andcan be especially useful for images which have not been enhanced.                                                    #### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            -P k 0.10            Recommended              ocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN                      ocrd-cis-ocropy-binarize        -P threshold 0.1        Fast        ocrd-cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN                ocrd-sbb-binarize      -P model      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname       ocrd-skimage-binarize      -P k 0.10      Slow      ocrd-skimage-binarize -I OCR-D-IMG -O OCR-D-BIN              ocrd-anybaseocr-binarize      -P threshold 0.1      Fast      ocrd-anybaseocr-binarize -I OCR-D-IMG -O OCR-D-BIN      ### Step 2: Cropping (Page Level)In this processing step, a document image is taken as input and the pageis cropped to the content area only (i.e. without noise at the margins or facing pages) by marking the coordinates of the page frame.We strongly recommend to execute this step if your images are not cropped already (i.e. only show the page of a book without a ruler,footer, color scale etc.). Otherwise you might run into severe segmentation problems.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-anybaseocr-crop            The input image has to be binarized and should be deskewed for the module to work.      ocrd-anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP              ocrd-tesserocr-crop            Cannot cope well with facing pages (textual noise is detected as text).      ocrd-tesserocr-crop -I OCR-D-BIN -O OCR-D-CROP      ### Step 3: Binarization (Page Level)For better results, the cropped images can be binarized again at this point or later on (on region level).#### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            Recommended      ocrd-olena-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-sbb-binarize      -P model      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)            ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname            ocrd-skimage-binarize                  ocrd-skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-cis-ocropy-binarize                  ocrd-cis-ocropy-binarize -I OCR-D-CROP -O OCR-D-BIN2      ### Step 4: Denoising (Page Level)In this processing step, artifacts like little specks (both in foreground or background) are removed from the binarized image. (Not to be confused with raw denoising in step 0.)This may not be necessary for all prints, and depends heavily on the selected binarization algorithm.            &amp;nbsp;      &amp;nbsp;                                                            #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-denoise      -P noise_maxsize 3.0            ocrd-cis-ocropy-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE              ocrd-skimage-denoise      -P maxsize 3.0      Slow      ocrd-skimage-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE      ### Step 5: Deskewing (Page Level)In this processing step, a document image is taken as input and the skew ofthat page is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.The input images have to be binarized for this module to work.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                 ocrd-cis-ocropy-deskew      -P level-of-operation page      Recommended      ocrd-cis-ocropy-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P level-of-operation page              ocrd-tesserocr-deskew      -P operation_level page      Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P operation_level page              ocrd-anybaseocr-deskew      &amp;nbsp;      &amp;nbsp;      ocrd-anybaseocr-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE      ### Step 6: Dewarping (Page Level)In this processing step, a document image is taken as input and the text lines are straightened or stretchedif they are curved. The input image has to be binarized for the module to work.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks      Call                  ocrd-anybaseocr-dewarp              -P model_path /path/to/latest_net_G.pth            For available models take a look at this site or use the [OCR-D resource manager](https://ocr-d.de/en/models)  Parameter model_path is optional if the model was installed via ocrd resmgr download ocrd-anybaseocr-dewarp &#39;*&#39;  GPU required!              ocrd-anybaseocr-dewarp -I OCR-D-DESKEW-PAGE -O OCR-D-DEWARP-PAGE            ## Layout AnalysisBy now the image should be well prepared for segmentation.### Step 7: Region segmentationIn this processing step, an (optimized) document image is taken as an input and theimage is segmented into the various regions, including columns.Segments are also classified, either coarse (text, separator, image, table, ...) or fine-grained (paragraph, marginalia, heading, ...).**Note:** The `ocrd-tesserocr-segment`, `ocrd-tesserocr-recognize`, `ocrd-eynollah segment`, `ocrd-sbb-textline-detector` and`ocrd-cis-ocropy-segment` processors do not only segment the page, butalso the text lines within the detected text regions in onestep. Therefore with those (and only with those!) processors you don&#39;t need tosegment into lines in an extra step and can continue with [step 13 - line-level dewarping](#step-13-dewarping-line-level).**Note:** If you use `ocrd-tesserocr-segment-region`, which uses only boundingboxes instead of polygon coordinates, then you should post-process via`ocrd-segment-repair` with `plausibilize=True` to obtain better results withoutlarge overlaps. _Alternatively_, consider using the all-in-one capabilities of`ocrd-tesserocr-segment` and `ocrd-tesserocr-recognize`, which can do regionsegmentation and line segmentation (and optionally also text recognition) inone step by querying Tesseract&#39;s internal iterator (accessing the more precisepolygon outlines instead of just coarse bounding boxes with lots ofhard-to-recover overlap). _Alternatively_, run with `shrink_polygons=True`(accessing that same iterator to calculate convex hull polygons).**Note:** All the `ocrd-tesserocr-segment*` processors internally delegate to`ocrd-tesserocr-recognize`, so you can replace calls to these task-specificprocessors with calls to `ocrd-tesserocr-recognize` with specific parameters:  processor callocrd-tesserocr-recognize parameters            ocrd-tesserocr-segment-region -P overwrite_regions true -P find_tables false      ocrd-tesserocr-recognize -P textequiv_level region -P segmentation_level region -P overwrite_segments true -P find_tables false              ocrd-tesserocr-segment-table -P overwrite_cells true      ocrd-tesserocr-recognize -P textequiv_level cell -P segmentation_level cell -P overwrite_segments true              ocrd-tesserocr-segment-line -P overwrite_lines true      ocrd-tesserocr-recognize -P textequiv_level line -P segmentation_level line -P overwrite_segments true              ocrd-tesserocr-segment-word -P overwrite_words true      ocrd-tesserocr-recognize -P textequiv_level word -P segmentation_level word -P overwrite_segments true      **Note:** The three parameters `segmentation_level`, `textequiv_level` and`model` define the behavior of `ocrd-tesserocr-recognize`:* `segmentation_level` determines the *highest level* to segment. Use `&quot;none&quot;` to disable segmentation altogether, i.e. only recognize existing segments.* `textequiv_level` determines the *lowest level* to segment. Use `&quot;none&quot;` to segment until the lowest level (`&quot;glyph&quot;`) and disable recognition altogether, only analyse layout.* `model` determines the model to use for text recognition. Use `&quot;&quot;` or do not set at all to disable recognition, i.e. only analyse layout.Examples:* To segment existing regions into lines (and only lines) only: `segmentation_level=&quot;line&quot;`, `textequiv_level=&quot;line&quot;`, `model=&quot;&quot;`* To segment existing regions into lines (and only lines) and recognize text: `segmentation_level=&quot;line&quot;`, `textequiv_level=&quot;line&quot;`, `model=&quot;Fraktur&quot;`For detailed descriptions of behaviour and options, see [tesserocr&#39;s README](https://github.com/OCR-D/ocrd_tesserocr/blob/master/README.md) and`ocrd-tesserocr-recognize/segment/segment-region/segment-table/segment-line/segment-word --help` help.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-segment      -P find_tables false -P shrink_polygons true      Recommended. Will reuse internal tesseract iterators to produce a complete segmentation with tight polygons instead of bounding boxes where possible      ocrd-tesserocr-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG -P find_tables false -P shrink_polygons true          ocrd-eynollah-segment      -P models      Models can be found here or downloaded with the [OCR-D resource manager](https://ocr-d.de/en/models);       If you didn&#39;t download the model with the `resource manager`, for model you need to pass the absolute path on your hard drive as parameter value.      ocrd-eynollah-segment -I OCR-D-IMG -O OCR-D-SEG -P models default              ocrd-sbb-textline-detector      -P model modelname      Models can be found here or downloaded with the [OCR-D resource manager](https://ocr-d.de/en/models);       If you didn&#39;t download the model with the `resource manager`, for model you need to pass the local path on your hard drive as parameter value.      ocrd-sbb-textline-detector -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P model /path/to/model              ocrd-cis-ocropy-segment      -P level-of-operation page          ocrd-cis-ocropy-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P level-of-operation page              ocrd-tesserocr-segment-region      -P find_tables false      Recommended      ocrd-tesserocr-segment-region -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P find_tables false -P shrink_polygons true              ocrd-segment-repair      -P plausibilize true      Only to be used after ocrd-tesserocr-segment-region      ocrd-segment-repair -I OCR-D-SEG-REG -O OCR-D-SEG-REPAIR -P plausibilize true              ocrd-anybaseocr-block-segmentation      -P block_segmentation_model mrcnn_name -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5      For available models take a look at this site ocr download them via [OCR-D resource manager](https://ocr-d.de/en/models);       If you didn&#39;t use the `resource manager`, you need to pass the local path on your hard drive as parameter value.      ocrd-anybaseocr-block-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P block_segmentation_model mrcnn_name -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5              ocrd-pc-segmentation                  ocrd-pc-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG      ## Image Optimization (Region Level)In the following steps, the text regions should be optimized for OCR.### Step 8:  Binarization (Region Level)In this processing step, a scanned colored /gray scale document image is taken as input and a blackand white binarized image is produced. This step should separate the background from the foreground.The binarization should be at least executed once (on page or region level). If you already binarizedyour image twice on page level, and have no large images, you can probably skip this step.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-skimage-binarize      -P level-of-operation region            ocrd-skimage-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region              ocrd-sbb-binarize      -P model -P operation_level region      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or with the [OCR-D resource manager](https://ocr-d.de/en/models)      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname -P operation_level region          ocrd-preprocess-image              -P level-of-operation region        -P &quot;output_feature_added&quot; binarized        -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            &amp;nbsp;          ocrd-preprocess-image -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                    ocrd-cis-ocropy-binarize      -P level-of-operation region-P &quot;noise_maxsize&quot;: float            ocrd-cis-ocropy-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region      ### Step 9:  Clipping (Region Level)In this processing step, intrusions of neighbouring non-text (e.g. separator) or text segments (e.g. ascenders/descenders) intotext regions of a page (or text lines or a text region) can be removed. A connected component analysis is run on every segment,as well as its overlapping neighbours. Now for each conflicting binary object,a rule based on majority and proper containment determines whether it belongs to the neighbour, and can thereforebe clipped to the background.This basic text-nontext segmentation ensures that for each text region there is a clean image without interference from separators and neighbouring texts. (On the region level, cleaning via coordinates would be impossible in many common cases.) On the line level, this can be seen as an alternative to _resegmentation_.Note: Clipping must be applied **before** any processor that produces derived images for the same hierarchy level (region/line). Annotations on the next higher level (page/region) are fine of course.#### Available processors            Processor      Parameter      Remarks    Call          &gt;      ocrd-cis-ocropy-clip      -P level-of-operation region      &amp;nbsp;      ocrd-cis-ocropy-clip -I OCR-D-DESKEW-REG -O OCR-D-CLIP-REG -P level-of-operation region      ### Step 10:  Deskewing (Region Level)In this processing step, text region images are taken as input and their skew is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-deskew      -P level-of-operation region            ocrd-cis-ocropy-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG -P level-of-operation region              ocrd-tesserocr-deskew            Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG      ### Step 11: Line segmentationIn this processing step, text regions are segmented into text lines.A line detection algorithm is run on every text region of every PAGE in theinput file group, and a TextLine element with the resulting polygonoutline is added to the annotation of the output PAGE.**Note:** If you use `ocrd-cis-ocropy-segment`, you can directly go on with [Step 13](#step-13-dewarping-on-line-level).**Note:** If you use `ocrd-tesserocr-segment-line`, which uses only boundingboxes instead of polygon coordinates, then you should post-process with theprocessors described in [Step 12](#step-12-resegmentation-line-level)._Alternatively_, consider using the all-in-one capabilities of[`ocrd-tesserocr-recognize`](#step-7-region-segmentation), which can do line segmentationand text recognition in one step by querying Tesseract&#39;s internal iterator(accessing the more precise polygon outlines instead of just coarse boundingboxes with lots of hard-to-recover overlap). _Alternatively_, run with`shrink_polygons=True` (accessing that same iterator to calculate convex hullpolygons)**Note:** As described in [Step 7](#step-7-page-segmentation),`ocrd-eynollah-segment`, `ocrd-sbb-textline-detector` and `ocrd-cis-ocropy-segment` do not only segmentthe page, but also the text lines within the detected text regions in one step. Therefore with those (and only with those!) processors you don’tneed to segment into lines in an extra step.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-segment      -P level-of-operation region      &amp;nbsp;      ocrd-cis-ocropy-segment -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE -P level-of-operation region              ocrd-tesserocr-segment-line      &amp;nbsp;      &amp;nbsp;      ocrd-tesserocr-segment-line -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE      ### Step 12: Resegmentation (Line Level)In this processing step the segmented text lines can be corrected in order to reduce their overlap. This can be done either via coordinates (polygonalizing the bounding boxes tightly around the glyphs) – which is what `ocrd-cis-ocropy-resegment` offers – or via derived images (clipping pixels that do not belong to a text line to the background color) – which is what `ocrd-cis-ocropy-clip` (on the `line` level) offers. The former is usually more accurate, but not always possible (for example, when neighbors intersect heavily, creating non-contiguous contours). The latter is only possible if no preceding workflow step has already annotated derived images (`AlternativeImage` references) on the line level (see also [region-level clipping](../Workflow-Guide-clipping)).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-clip      -P level-of-operation line            ocrd-cis-ocropy-clip -I OCR-D-SEG-LINE -O OCR-D-CLIP-LINE -P level-of-operation line              ocrd-cis-ocropy-resegment                  ocrd-cis-ocropy-resegment -I OCR-D-SEG-LINE -O OCR-D-RESEG      ### Step 13: Dewarping (Line Level)In this processing step, the text line images get vertically aligned if they are curved.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-dewarp      &amp;nbsp;      &amp;nbsp;      ocrd-cis-ocropy-dewarp -I OCR-D-CLIP-LINE -O OCR-D-DEWARP-LINE      ## Text Recognition### Step 14: Text recognitionThis processor recognizes text in segmented lines.An overview on the existing model repositories and short descriptions on the most important models can be found [here](https://ocr-d.de/en/models).We strongly recommend to use the [OCR-D resource manager](https://ocr-d.de/en/models) to download the models, as this way you don&#39;t have to specifythe path to each model.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-recognize      -P model GT4HistOCR_50000000.997_191951            Recommended Model can be found herea faster variant is here      TESSDATA_PREFIX=&quot;/test/data/tesseractmodels/&quot; ocrd-tesserocr-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P model Fraktur+Latin              ocrd-calamari-recognize              if you downloaded your model with the [OCR-D resource manager](https://ocr-d.de/en/models), use-P checkpoint_dir modelname        else use -P checkpoint_dir /path/to/models                    RecommendedModel can be found here;        For checkpoint you need to pass the local path on your hard drive as parameter value, and keep the verbatim asterisk (*).            ocrd-calamari-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P checkpoint_dir qurator-gt4histocr-1.0      **Note:** For `ocrd-tesserocr` the environment variable `TESSDATA_PREFIX` hasto be set to point to the directory where the used models are stored unlessthe default directory (normally $VIRTUAL_ENV/share/tessdata) is used.The directory should at least contain the following models:`deu.traineddata`, `eng.traineddata`, `osd.traineddata`.**Note:** Faster models for `tesserocr-recognize` are available fromhttps://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/.A good and currently the fastest model is[Fraktur-fast](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/Fraktur-fast.traineddata).UB Mannheim provides many more [models online](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/)which were trained on different GT data sets, for example from[Austrian Newspapers](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/ONB/tessdata_fast/).**Note:** If you want to go on with the optional post correction, you should also set the `textequiv_level` to `glyph` or in the case of`ocrd-calamari-recognize` at least `word` (which is already the default for `ocrd-tesserocr-recognize`).### Step 14.1: Font style annotationThis processor can determine the font style (e.g. *italic*, **bold**,underlined) and font family text recognition results.`ocrd-tesserocr-fontshape` can either use existing segmentation orsegment on-demand. It can detect the following font styles:  * `fontSize`  * `fontFamily`  * `bold`  * `italic`  * `underlined`  * `monospace`  * `serif`**Note:** `ocrd-tesserocr-fontshape` needs the old, pre-LSTM models to work atall. You can use the pre-installed `osd` (which is purely rule-based), butthere might be better alternatives for your language and script. You can stillget the old models from Tesseract&#39;s Github repo at the [lastrevision](https://github.com/tesseract-ocr/tessdata/commit/3cf1e2df1fe1d1da29295c9ef0983796c7958b7d)before the [LSTMmodels](https://github.com/tesseract-ocr/tessdata/commit/4592b8d453889181e01982d22328b5846765eaad)replaced them, usually under the same name. (Thus, `deu.traineddata` used to bea rule-based model but now is an LSTM model. `deu-frak.traineddata` is stillonly available as rule-based model and was complemented by the new LSTM models`frk.traineddata` and `script/Fraktur.traineddata`.) If you do need one of themodels that was replaced completely, then you should at least rename the oldone (e.g. to `deu3.traineddata`).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-fontshape      -P model osd -P padding 2      Download other pre-LSTM models from GitHub      ocrd-tesserocr-fontshape -I OCR-D-OCR -O OCR-D-OCR-FONT      ## Post Correction (Optional)### Step 15: Text alignmentIn this processing step, text results from multiple OCR engines (in different annotations sharing the same line segmentation) are alignedinto one annotation with `TextEquiv` alternatives.**Note:** This step is only required if you want to do post-correction afterwards,feeding alternative character hypotheses from several OCR-engines to improve the search space.The previous recognition step must be run on glyph or at least on word level.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-align      &amp;nbsp;      &amp;nbsp;    ocrd-cis-align -I OCR-D-OCR1,OCR-D-OCR2 -O OCR-D-ALIGN      ### Step 16: Post-correctionIn this processing step, the recognized text is corrected by statistical error modelling, language modelling, and word modelling (dictionaries,morphology and orthography).**Note:** Most tools benefit strongly from input which includes alternative OCR hypotheses. Currently, models for `ocrd-cor-asv-ann-process`are optimised for input from single OCR engines, whereas `ocrd-cis-postcorrect` expects input from multi-OCR alignment.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cor-asv-ann-process      -P textequiv_level word -P model_file modelname      Pre-trained models can be found here or downloaded via the [OCR-D resource manager](https://ocr-d.de/en/models);      If you didn&#39;t download the model with the `resource manager`, for model_file you need to pass the local path on your hard drive      as parameter value.     (Relative paths are resolved from the workspace directory or the environment variable CORASVANN_DATA.)     There is no default model_file.      ocrd-cor-asv-ann-process -I OCR-D-OCR -O OCR-D-PROCESS -P textequiv_level word -P model_file /path/to/model/model.h5              ocrd-cis-postcorrect      -P profilerPath /path/to/profiler.bash -P profilerConfig ignored -P nOCR 2 -P model /path/to/model/model.zip              The profilerConfig parameters can be specified in a JSON file. If you do not want to use a profiler, you can set the value for profilerConfig to ignored.      In this case, your profiler.bash should look like this:#!/bin/bashcat &gt; /dev/nullecho &#39;{}&#39;      For model you need to pass the local path on your hard drive as parameter value.      There is no default model.            ocrd-cis-postcorrect -I OCR-D-ALIGN -O OCR-D-CORRECT -p postcorrect.json      ## Evaluation (Optional)If Ground Truth data is available, the OCR can be evaluated.### Step 17: OCR EvaluationIn this processing step, the text output of the OCR or post-correction can be evaluated by aligning with ground truth text and measuring the error rates.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-dinglehopper            For page-wise visual comparison (2 file groups). First input group should point to the ground truth.      ocrd-dinglehopper -I OCR-D-GT,OCR-D-OCR -O OCR-D-EVAL              ocrd-cor-asv-ann-evaluate            -P metric historic-latin      -P confusion 20            For document-wide aggregation (N file groups). First input group should point to the ground truth.       There is no output file group, it only uses logging. If you want to save the evaluation findings in a file, you could e.g. add 2&gt; eval.txt at the end of your command (or use ocrd-make).      ocrd-cor-asv-ann-evaluate -I OCR-D-GT,OCR-D-OCR      ## Generic Data Management (Optional)OCR-D produces PAGE XML files which contain the recognized text as well as detailedinformation on the structure of the processed pages, the coordinates of the recognizedelements etc. Optionally, the output can be converted to other formats, or copied verbatim (re-generating PAGE-XML)### Step 18: Adaptation of CoordinatesAll OCR-D processors are required to relate coordinates to the original image for each page, and to keep the original image reference (`Page/@imageFilename`). However, sometimes it may be necessary to deviate from that strict requirement in order to get the overall workflow to function properly.For example, if you have a page-level dewarping step, it is currently impossible to correctly relate to the original image&#39;s coordinates for any segments annotated after that, because there is no descriptive annotation of the underlying coordinate transform in PAGE-XML. Therefore, it is better to _replace the original image_ of the output PAGE-XML by the dewarped image before proceeding with the workflow. (If the dewarped image has also been cropped or deskewed, then of course all existing coordinates are re-calculated accordingly as well.)Another use case is exporting PAGE-XML for tools that cannot apply cropping or deskewing, like [LAREX](https://github.com/OCR4all/LAREX) or Transkribus.Conversely, you might want to align two PAGE-XML files for the same page that have different original image references, projecting all segments below the page level from the one to the other (transforming all coordinates according to the page-level annotation, or keeping them unchanged).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-segment-replace-original      &amp;nbsp;      &amp;nbsp;    ocrd-segment-replace-original -I OCR-D-CROP-DESK -O OCR-D-CROP-DESK-SUBST              ocrd-segment-replace-page      &amp;nbsp;      &amp;nbsp;    ocrd-segment-replace-page -I OCR-D-CROP-DESK,OCR-D-CROP-DESK-SUBST-SEG -O OCR-D-CROP-DESK-SEG -P transform_coordinates true      ### Step 19: Format ConversionIn this processing step the produced PAGE XML files can be converted to ALTO,PDF, hOCR or text files. Note that ALTO and hOCR can also be converted intodifferent formats whereas the PDF version of PAGE XML OCR results is a widelyaccessible format that can be used as-is by expert and layman alike.#### Available processors          Processor      Parameter      Remarks      Call              ocrd-fileformat-transform      -P from-to &quot;alto2.0 alto3.0&quot;      # or &quot;alto2.0 alto3.1&quot;      # or &quot;alto2.0 hocr&quot;      # or &quot;alto2.1 alto3.0&quot;      # or &quot;alto2.1 alto3.1&quot;      # or &quot;alto2.1 hocr&quot;      # or &quot;alto page&quot;      # or &quot;alto text&quot;      # or &quot;gcv hocr&quot;      # or &quot;hocr alto2.0&quot;      # or &quot;hocr alto2.1&quot;      # or &quot;hocr text&quot;      # or &quot;page alto&quot;      # or &quot;page hocr&quot;      # or &quot;page text&quot;                  As the value consists of two words, when using -P form it has to be enclosed in quotation marks.      If you want to save all OCR results in one file, you can use the following command: `cat OCR* &gt; full.txt`          ocrd-fileformat-transform -I OCR-D-OCR -O OCR-D-ALTO              ocrd-pagetopdf      {  # font file name to use for rendering text  &quot;font&quot;: &quot;AletheiaSans.ttf&quot;,  # fix (invalid) negative coordinates  &quot;negative2zero&quot;: true,  # concatenate to multi-page PDF (empty for none)  &quot;multipage&quot;: &quot;name_of_pdf&quot;,  # multi-page PDF page labels  &quot;pagelabel&quot;: &quot;pageId&quot;,  # render text on this hierarchy level  &quot;textequiv_level&quot;: &quot;word&quot;,  # draw polygon outlines in the PDF (empty for none)  &quot;outlines&quot;: &quot;line&quot;}                  ocrd-pagetopdf -I OCR-D-OCR -O OCR-D-PDF -P textequiv_level word              ocrd-export-larex            Create a file group with PAGE alongside image files (differing only in file name suffix) to accommodate LAREX&#39; bookpath directory assumptions.      ocrd-export-larex -I OCR-D-OCR -O OCR-D-LAREX              ocrd-segment-extract-pages      -P mimetype image/png -P transparency true      Get page images (cropped and deskewed as annotated; raw and binarized) and mask images (color-coded for regions) along with JSON files for region annotations (custom and COCO format).      ocrd-segment-extract-pages -I OCR-D-SEG-REGION -O OCR-D-IMG-PAGE,OCR-D-IMG-PAGE-BIN,OCR-D-IMG-PAGE-MASK              ocrd-segment-extract-regions      -P mimetype image/png -P transparency true      Get region images (cropped, masked and deskewed as annotated) along with JSON files for region annotations (custom format).      ocrd-segment-extract-regions -I OCR-D-SEG-REGION -O OCR-D-IMG-REGION              ocrd-segment-extract-lines      -P mimetype image/png -P transparency true      Get text line images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for line annotations (custom format).      ocrd-segment-extract-lines -I OCR-D-SEG-LINE -O OCR-D-IMG-LINE              ocrd-segment-extract-words      -P mimetype image/png -P transparency true      Get word images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for word annotations (custom format).      ocrd-segment-extract-words -I OCR-D-SEG-WORD -O OCR-D-IMG-WORD              ocrd-segment-extract-glyphs      -P mimetype image/png -P transparency true      Get glyph images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for glyph annotations (custom format).      ocrd-segment-extract-glyphs -I OCR-D-SEG-GLYPH -O OCR-D-IMG-GLYPH              ocrd-segment-from-masks      -P colordict &#39;{  &quot;#969696&quot;: &quot;TableRegion&quot;,   &quot;#00FF00&quot;: &quot;TextRegion:page-number&quot;,   &quot;#FFFF00&quot;: &quot;TextRegion:heading&quot;,   &quot;#00FFFF&quot;: &quot;GraphicRegion:logo&quot;,   &quot;#0000FF&quot;: &quot;TextRegion:subject&quot;,   &quot;#FF0000&quot;: &quot;TextRegion:catch-word&quot;,   &quot;#FF00FF&quot;: &quot;TextRegion:footnote&quot;,   &quot;#646464&quot;: &quot;TextRegion:paragraph&quot; }&#39;      Import mask images as region segmentation. If colordict is empty, defaults to PageViewer color scheme (also written by ocrd-segment-extract-pages).      ocrd-segment-from-masks -I OCR-D-SEG-PAGE,OCR-D-IMG-PAGE-MASK -O OCR-D-SEG-REGION              ocrd-segment-from-coco            Import COCO format region segmentation (also written by ocrd-segment-extract-pages).      ocrd-segment-from-coco -I OCR-D-SEG-PAGE,OCR-D-SEG-COCO -O OCR-D-SEG-REGION    ### Step 20: ArchivingAfter you have successfully processed your images, the results should be saved and archived. OLA-HD isa longterm archive system which works as a mixture between an archive system and a repository. For furtherdetails on OLA-HD see the extensive [concept paper](https://github.com/subugoe/OLA-HD-IMPL/blob/master/docs/OLA-HD_Konzept.pdf).You can also check out the [prototype](http://141.5.98.232/) to make sure, OLA-HD meets your needs and requirements.To use the prototype, specify http://141.5.98.232/api as the endpoint parameter in your call.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-olahd-client      {  &quot;endpoint&quot;: &quot;URL of your OLA-HD instance&quot;,  &quot;username&quot;: &quot;X&quot;,  &quot;password&quot;: &quot;*&quot;}      the parameters should be written to a json file:    echo &#39;{  &quot;endpoint&quot;: &quot;URL of your OLA-HD instance&quot;,  &quot;username&quot;: &quot;X&quot;,  &quot;password&quot;: &quot;*&quot;}&#39; &gt; olahd.json        ocrd-olahd-client -I OCR-D-OCR -p olahd.json      ### Step 21: Dummy ProcessingSometimes it can be useful to have a dummy processor, which takes the files in an Input fileGrp andcopies them the a new Output fileGrp, re-generating the PAGE XML from the current namespace schema/model.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-dummy      &amp;nbsp;      &amp;nbsp;    ocrd-dummy -I OCR-D-FILEGRP -O OCR-D-DUMMY      # RecommendationsIn order to facilitate the usage of OCR-D and the configuration of workflows, we provide two workflowswhich can be used as a start for your OCR-D-tests. They were determined by testing the processors listedabove on selected pages of some prints from the 17th and 18th century.The results vary quite a lot from page to page. In most cases, segmentation is a problem.Note that for our test pages, not all steps described above werde needed to obtain the best results.Depending on your particular images, you might want to include those processors again for better results.We are currently working on regression tests with the help of which we will be able to provide more profoundworkflows soon, which will replace those interim solutions. ## Minimal workflowSince `ocrd-tesserocr-recognize` can do binarization (Otsu), regionsegmentation, table recognition, line segmentation and text recognition at once, just like theupstream `tesseract` command line tool, it&#39;s a good single-step workflow to geta baseline result to compare to granular workflows.**Note:** Be aware that you will most likely obtain significantly betterresults by configuring a more granular workflow like e.g. the[workflows](#best-results-for-selected-pages)[below](#good-results-for-slower-processors).            Step      Processor      Parameter                  1      ocrd-tesserocr-recognize      -P segmentation_level region -P textequiv_level word -P find_tables true -P model Fraktur_GT4HistOCR      ### Example with ocrd-process```shocrd process &quot;tesserocr-recognize -P segmentation_level region -P textequiv_level word -P find_tables true -P model GT4HistOCR_50000000.997_191951&quot;```## Best results for selected pagesThe following workflow has produced best results for &#39;simple&#39; pages (e.g. [thispage](https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/dda89351-7596-46eb-9736-593a5e9593d3/data/bagit/data/OCR-D-IMG/OCR-D-IMG_0004.tif))  (CER ~1%).            Step      Processor      Parameter                  1      ocrd-cis-ocropy-binarize                    2      ocrd-anybaseocr-crop                    3      ocrd-skimage-binarize      -P method li              4      ocrd-skimage-denoise      P level-of-operation page              5      ocrd-tesserocr-deskew      -P level-of-operation page              7      ocrd-cis-ocropy-segment      -P level-of-operation page              13      ocrd-cis-ocropy-dewarp                    14      ocrd-calamari-recognize      -P checkpoint_dir qurator-gt4histocr-1.0      ### Example with ocrd-process```shocrd process   &quot;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN&quot;   &quot;anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP&quot;   &quot;skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2 -P method li&quot;   &quot;skimage-denoise -I OCR-D-BIN2 -O OCR-D-BIN-DENOISE -P level-of-operation page&quot;   &quot;tesserocr-deskew -I OCR-D-BIN-DENOISE -O OCR-D-BIN-DENOISE-DESKEW -P operation_level page&quot;   &quot;cis-ocropy-segment -I OCR-D-BIN-DENOISE-DESKEW -O OCR-D-SEG -P level-of-operation page&quot;   &quot;cis-ocropy-dewarp -I OCR-D-SEG -O OCR-D-SEG-LINE-RESEG-DEWARP&quot;   &quot;calamari-recognize -I OCR-D-SEG-LINE-RESEG-DEWARP -O OCR-D-OCR -P checkpoint_dir qurator-gt4histocr-1.0&quot;```**Note:**(1) This workflow expects your images to be stored in a folder called `OCR-D-IMG`. If your images are saved in a different folder,you need to adjust `-I OCR-D-IMG` in the second line of the call above with the name of your folder, e.g. `-I MAX`(2) For the last processor in this workflow, `ocrd-calamari-recognize`, you need to specify the model which is to be used. If you didn&#39;t download it via the [OCR-D resource manager](https://ocr-d.de/en/models), you have to use the `checkpoint` parameterand pass your local path to the model on your hard drive as parameter value! In this case, the last line of the `ocrd-process` call above could e.g. look like this:```sh  &quot;calamari-recognize -I OCR-D-SEG-LINE-RESEG-DEWARP -O OCR-D-OCR -P checkpoint /test/data/calamari_models/*.ckpt.json&quot;```All the other lines can just be copied and pasted.## Good results for slower processorsIf your computer is not that powerful you may try this workflow. It works fine for simple pages and produces also good results in shorter time.            Step      Processor      Parameter                  1      ocrd-cis-ocropy-binarize                    2      ocrd-anybaseocr-crop                    3      ocrd-skimage-denoise      -P level-of-operation page              5      ocrd-tesserocr-deskew      -P level-of-operation page              7      ocrd-tesserocr-segment      -P shrink_polygons true              13      ocrd-cis-ocropy-dewarp                    14      ocrd-tesserocr-recognize      -P textequiv_level glyph -P overwrite_segments true -P model GT4HistOCR_50000000.997_191951      ### Example with ocrd-process```shocrd process   &quot;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN&quot;   &quot;anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP&quot;   &quot;skimage-denoise -I OCR-D-CROP -O OCR-D-BIN-DENOISE -P level-of-operation page&quot;   &quot;tesserocr-deskew -I OCR-D-BIN-DENOISE -O OCR-D-BIN-DENOISE-DESKEW -P operation_level page&quot;   &quot;tesserocr-segment -I OCR-D-BIN-DENOISE-DESKEW -O OCR-D-SEG -P shrink_polygons true&quot;   &quot;cis-ocropy-dewarp -I OCR-D-SEG -O OCR-D-SEG-DEWARP&quot;   &quot;tesserocr-recognize -I OCR-D-SEG-DEWARP -O OCR-D-OCR -P textequiv_level glyph -P overwrite_segments true -P model GT4HistOCR_50000000.997_191951&quot;```**Note:**(1) This workflow expects your images to be stored in a folder called `OCR-D-IMG`. If your images are saved in a different folder,you need to adjust `-I OCR-D-IMG` in the second line of the call above with the name of your folder, e.g. `-I my_images`(2) For the last processor in this workflow, `ocrd-tesserocr-recognize`, the environment variable TESSDATA_PREFIX has to beset to point to the directory where the used models are stored if they are not in the default location. If you downloaded your modelswith the [OCR-D resource manager](https://ocr-d.de/en/models), this is already taken care of.",
      "url": " /en/workflows.html"
    },
  

    {
      "slug": "en-workflows-src-html",
      "title": "OCR-D Workflow Guide",
      "content"	 : "# WorkflowsThere are several steps necessary to get the fulltext of a scanned print. The whole OCR process is shown in the following figure:![](https://ocr-d.de/assets/Funktionsmodell.png)The following instructions describe all steps of an OCR workflow. Depending on your particular print (or rather images), not all of thosesteps might be necessary to obtain good results. Whether a step is required or optional is indicated in the description of each step.This guide provides an overview of the available OCR-D processors and their required parameters. For more complex workflows and recommendationssee the [OCR-D-Website-Wiki](https://github.com/OCR-D/ocrd-website/wiki). Feel free to add your own experiences and recommendations in the Wiki!We will regularly amend this guide with valuable contributions from the Wiki.**Note:** In order to be able to run the workflows described in this guide, you need to have prepared your images in an [OCR-D-workspace](https://ocr-d.de/en/user_guide#preparing-a-workspace).We expect that you are familiar with the [OCR-D-user guide](https://ocr-d.de/en/user_guide) which explains all preparatory steps, syntax and differentsolutions for executing whole workflows.## Image Optimization (Page Level)At first, the image should be prepared for OCR.### Step 0.1: Image Enhancement (Page Level, optional)### Step 0.2: Font detection### Step 1: Binarization (Page Level)### Step 2: Cropping (Page Level)### Step 3: Binarization (Page Level)For better results, the cropped images can be binarized again at this point or later on (on region level).#### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            Recommended      ocrd-olena-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-sbb-binarize      -P model      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)            ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname            ocrd-skimage-binarize                  ocrd-skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-cis-ocropy-binarize                  ocrd-cis-ocropy-binarize -I OCR-D-CROP -O OCR-D-BIN2      ### Step 4: Denoising (Page Level)### Step 5: Deskewing (Page Level)### Step 6: Dewarping (Page Level)## Layout AnalysisBy now the image should be well prepared for segmentation.### Step 7: Region segmentation## Image Optimization (Region Level)In the following steps, the text regions should be optimized for OCR.### Step 8:  Binarization (Region Level)In this processing step, a scanned colored /gray scale document image is taken as input and a blackand white binarized image is produced. This step should separate the background from the foreground.The binarization should be at least executed once (on page or region level). If you already binarizedyour image twice on page level, and have no large images, you can probably skip this step.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-skimage-binarize      -P level-of-operation region            ocrd-skimage-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region              ocrd-sbb-binarize      -P model -P operation_level region      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or with the [OCR-D resource manager](https://ocr-d.de/en/models)      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname -P operation_level region          ocrd-preprocess-image              -P level-of-operation region        -P &quot;output_feature_added&quot; binarized        -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            &amp;nbsp;          ocrd-preprocess-image -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                    ocrd-cis-ocropy-binarize      -P level-of-operation region-P &quot;noise_maxsize&quot;: float            ocrd-cis-ocropy-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region      ### Step 9:  Clipping (Region Level)### Step 10:  Deskewing (Region Level)In this processing step, text region images are taken as input and their skew is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-deskew      -P level-of-operation region            ocrd-cis-ocropy-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG -P level-of-operation region              ocrd-tesserocr-deskew            Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG      ### Step 11: Line segmentation### Step 12: Resegmentation (Line Level)### Step 13: Dewarping (Line Level)In this processing step, the text line images get vertically aligned if they are curved.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-dewarp      &amp;nbsp;      &amp;nbsp;      ocrd-cis-ocropy-dewarp -I OCR-D-CLIP-LINE -O OCR-D-DEWARP-LINE      ## Text Recognition### Step 14: Text recognition### Step 14.1: Font style annotation## Post Correction (Optional)### Step 15: Text alignment### Step 16: Post-correction## Evaluation (Optional)If Ground Truth data is available, the OCR can be evaluated.### Step 17: OCR Evaluation## Generic Data Management (Optional)OCR-D produces PAGE XML files which contain the recognized text as well as detailedinformation on the structure of the processed pages, the coordinates of the recognizedelements etc. Optionally, the output can be converted to other formats, or copied verbatim (re-generating PAGE-XML)### Step 18: Adaptation of Coordinates### Step 19: Format Conversion### Step 20: Archiving### Step 21: Dummy ProcessingSometimes it can be useful to have a dummy processor, which takes the files in an Input fileGrp andcopies them the a new Output fileGrp, re-generating the PAGE XML from the current namespace schema/model.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-dummy      &amp;nbsp;      &amp;nbsp;    ocrd-dummy -I OCR-D-FILEGRP -O OCR-D-DUMMY      # Recommendations",
      "url": " /en/workflows.src.html"
    },
  

    {
      "slug": "assets-main-css",
      "title": "",
      "content"	 : "@import &quot;minima&quot;;",
      "url": " /assets/main.css"
    },
  

    {
      "slug": "feed-xml",
      "title": "",
      "content"	 : "{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != &quot;posts&quot; %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: &quot; | &quot; | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: &quot; | &quot; | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% assign posts = site[page.collection] | where_exp: &quot;post&quot;, &quot;post.draft != true&quot; | sort: &quot;date&quot; | reverse %}{% if page.category %}{% assign posts = posts | where: &quot;category&quot;,page.category %}{% endif %}{% for post in posts limit: 10 %}{{ post.title | smartify | strip_html | normalize_whitespace | xml_escape }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{{ post.content | strip | xml_escape }}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: &quot;&quot; | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% endif %}{% for tag in post.tags %}{% endfor %}{% if post.excerpt and post.excerpt != empty %}{{ post.excerpt | strip_html | normalize_whitespace | xml_escape }}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains &quot;://&quot; %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}",
      "url": " /feed.xml"
    },
  

    {
      "slug": "sitemap-xml",
      "title": "",
      "content"	 : "{% if page.xsl %}{% endif %}{% assign collections = site.collections | where_exp:&#39;collection&#39;,&#39;collection.output != false&#39; %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:&#39;doc&#39;,&#39;doc.sitemap != false&#39; %}{% for doc in docs %}{{ doc.url | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }}{% endif %}{% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:&#39;doc&#39;,&#39;doc.sitemap != false&#39; | where_exp:&#39;doc&#39;,&#39;doc.url != &quot;/404.html&quot;&#39; %}{% for page in pages %}{{ page.url | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }}{% endif %}{% endfor %}{% assign static_files = page.static_files | where_exp:&#39;page&#39;,&#39;page.sitemap != false&#39; | where_exp:&#39;page&#39;,&#39;page.name != &quot;404.html&quot;&#39; %}{% for file in static_files %}{{ file.path | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{{ file.modified_time | date_to_xmlschema }}{% endfor %}",
      "url": " /sitemap.xml"
    },
  

    {
      "slug": "robots-txt",
      "title": "",
      "content"	 : "Sitemap: {{ &quot;sitemap.xml&quot; | absolute_url }}",
      "url": " /robots.txt"
    },
  
  
  {
      "slug": "en-2021-08-06-kick-off-phase3-html",
      "title": "OCR-D Phase III started",
      "content"	 : "On 30 July, our kick-off workshop took place, heralding phase III of OCR-D.The day before, the project participants met internally to get to know each other and coordinate their work. On the public workshop day, the team of the Coordination Project gave an introduction into the objectives in phase III and public communication channels of OCR-D, the current status and plans of the OCR-D software, the Web API and the handling of Ground Truth Data in OCR-D. Also, the Coordination Project gave an insight into Best Practices of Software Developing in the past phase of OCR-D, as well as ideas for the community, how to contribute.In addition, the implementation and module projects presented themselves in short presentations to the interested community and our cooperation partnersUB Braunschweig, SLUB Dresden UB Mannheim are extending both OCR-D and Kitodo for productive mass digitisation; SUB Göttingen and GWDG are working on Performance Optimisation and Integration, deploying OCR-D on a High Performance Cluster; GEI Braunschweig, HCI and ZPD of the University of Würzburg will implement OCR-D features in OCR4all, making OCR-D available via their software; the ULB Sachsen-Anhalt will implement OCR-D in their Open Source mass digitization infrastructure .While these project partners will work on four implementation scenarios, we have three module projects, improving OCR-D processors: UB Mannheim enabling work-specific training with Tesseract and Calamari; JGU Mainz and FAU Erlangen-Nürnberg improving font group recognition for better fitting OCR-models; and OLA-HD by SUB Göttingen and GWDG, optimising reliability, searchability and fine-grained referencing of the OLA-HD long-term archiving repository.In our chat channel, the gitter lobby, we always keep you informed about public OCR-D events. Further information about how to stay in touch and contribute to OCR-D can be found in our overview of platforms.",
      "url": " /en/2021/08/06/kick-off-phase3.html"
    }
    ,
  
  {
      "slug": "de-2021-08-06-kick-off-phase3-html",
      "title": "OCR-D Phase III gestartet",
      "content"	 : "Am 30. Juli fand unser Kick-off-Workshop statt, der die Phase III von OCR-D einläutete.Das Team gab eine Einführung in die Ziele und öffentlichen Kommunikationskanäle von OCR-D in Phase III, in Status und Pläne der OCR-Software und der Web-API und in den Umgang mit Ground Truth Daten in OCR-D. Zudem gab das Koordinierungsprojekt einen Einblick in die bisherige Praxis der Softwareentwicklung in OCR-D mit Möglichkeiten, mitzuwirken.Darüber hinaus stellten sich die Implementierungs- und Modulprojekte der interessierten Community und unseren Kooperationspartnern in kurzen Vorträgen vor.Die UB Braunschweig, die SLUB Dresden und die UB Mannheim erweitern OCR-D und Kitodo für die produktive Massendigitalisierung; die SUB Göttingen und die GWDG arbeiten an der Performance-Optimierung und Integration, indem sie OCR-D auf einem Hochleistungscluster einsetzen; das GEI Braunschweig, das HCI und das ZPD der Universität Würzburg werden OCR-D-Funktionen in OCR4all implementieren und damit OCR-D über ihre Software verfügbar machen; die ULB Sachsen-Anhalt wird OCR-D in ihre Open-Source-Massendigitalisierungsinfrastruktur implementieren.Während diese Projektpartner an vier Implementierungsszenarien arbeiten werden, haben wir drei Modulprojekte, die OCR-D-Prozessoren verbessern: die UB Mannheim, die ein werkspezifisches Training mit Tesseract und Calamari ermöglicht; JGU Mainz und FAU Erlangen-Nürnberg, die die Erkennung von Schriftgruppen für besser passende OCR-Modelle vorantreiben; und das Projekt der SUB Göttingen und der GWDG, das die Zuverlässigkeit, Durchsuchbarkeit und feinkörnige Referenzierung des Langzeitarchivs OLA-HD optimiert.In unserem Chat-Kanal, der Gitter-Lobby, halten wir Sie stets über öffentliche OCR-D-Veranstaltungen auf dem Laufenden. Weitere Informationen darüber, wie Sie mit OCR-D in Kontakt treten und zu OCR-D beitragen können, finden Sie in unserer Seite über Plattformen.",
      "url": " /de/2021/08/06/kick-off-phase3.html"
    }
    ,
  
  {
      "slug": "en-2021-06-11-bibtag-html",
      "title": "OCR-D at the Bibliothekartag 2021",
      "content"	 : "OCR-D will also be present at this year’s Bibliothekartag, which will take place virtually from 16-18 June 2021 and on twodays in Bremen. The OCR-D project is participating with two presentations on the current status of the funding initiativeand on the collaborative creation of training materials.Elisabeth Engl describes the current status ofthe OCR-D software and gives an outlook on the many planned application scenarios, which are at the centre of the third project phase. Kay-Michael Würzner and Robert Sachunskyreport on their experiences with setting up and conducting a collaborative transcription initiative at SLUB Dresden to create OCR training material and models by means of OCR-D and low-threshold annotation tools.",
      "url": " /en/2021/06/11/bibtag.html"
    }
    ,
  
  {
      "slug": "de-2021-06-11-bibtag-html",
      "title": "OCR-D auf dem Bibliothekartag 2021",
      "content"	 : "OCR-D ist auch auf dem diesjährigen Bibliothekartag vertreten, der vom 16.-18. Juni 2021 virtuell sowie an zwei Tagen auch vor Ort in Bremen stattfindet. Das OCR-D-Projekt beteiligt sich mit zwei Vorträgen zum aktuellen Stand in der Förderinitiative sowie zur kollaborativenErstellung von Trainingsmaterialien.Elisabeth Engl beschreibt den derzeitigen Stand der OCR-D-Softwareund gibt einen Ausblick auf die vielfältigen geplanten Anwendungsszenarien in bestandshaltenden und -verarbeitenden Einrichtungen,die im Zentrum der dritten Projektphase stehen. Kay-Michael Würzner und Robert Sachunskyberichten von ihren Erfahrungen bei Aufbau und Begleitung einer kollaborativen Transkriptionsinitiative an der SLUB Dresden zur Erstellung von OCR-Trainingsmaterial und -Modellen mithilfe von OCR-D und niedrigschwelliger Annotationswerkzeuge.",
      "url": " /de/2021/06/11/bibtag.html"
    }
    ,
  
  {
      "slug": "en-2021-06-10-projects-html",
      "title": "Implementation and module projects granted",
      "content"	 : "In addition to the coordination project, the DFG also approved seven implementation and module projects that will begin their work in the coming months.Of the eleven proposals submitted, which were developed in the course of extensive piloting of the OCR-D software in the summer of 2020, four implementation and three module projects will be funded by the DFG.The module projects will further develop selected OCR-D tools.  Workflow for work-specific training based on generic models with OCR-D as well as ground truth enhancement (UB Mannheim).  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen)  OLA-HD Service - A Generic Service for Long-Term Archiving of Historical Prints (SUB Göttingen, GWDG)Together with the ca. 65 other OCR-D tools,  these form the basis for the work of the four implementation projects, which will prepare the existing OCR-D software for its productive use in mass digitisation.  Integration of Kitodo and OCR-D for productive mass digitisation (UB Braunschweig, SLUB Dresden, UB Mannheim).  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR-D Software in Modular Mass Digitisation Workflows (ULB Halle)  OCR4all libraries full text recognition of historical collections (GEI Braunschweig, HCI and ZPD of the University of Würzburg)We are very much looking forward to this next project phase and the (further) cooperation with the implementation and module projects, which will prepare the use of the OCR-D software in different usage scenarios.",
      "url": " /en/2021/06/10/projects.html"
    }
    ,
  
  {
      "slug": "de-2021-06-10-projects-html",
      "title": "Implementierungs- und Modulprojekte bewilligt",
      "content"	 : "Neben dem Antrag des Koordinierungsprojekts wurden von der DFG sieben Implementierungs- und Modulprojekte bewilligt, die in den kommenden Monaten ihre Arbeit aufnehmen werden.Von den elf eingereichten Anträgen, die im Zuge einer umfangreichen Pilotierung der OCR-D-Software im Sommer 2020 erarbeitet wurden, werden vier Implementierungs- und drei Modulprojekte von der DFG gefördert.Die Modulprojekte werden ausgewählte OCR-D-Werkzeuge weiterentwickeln.  Workflow für werkspezifisches Training auf Basis generischer Modelle mit OCR-D sowie Ground Truth Aufwertung (UB Mannheim)  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen)  OLA-HD Service - Ein generischer Dienst für die Langzeitarchivierung historischer Drucke (SUB Göttingen, GWDG)Zusammen mit den übrigen derzeit ca. 65 OCR-D-Werkzeugen bilden diese die Basis für dieArbeiten der vier Implementierungsprojekte, die die OCR-D-Software fürden produktiven Einsatz in der Massendigitalisierung vorbereiten werden.  Integration von Kitodo und OCR-D zur produktiven Massendigitalisierung (UB Braunschweig, SLUB Dresden, UB Mannheim)  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR-D Software in modularen Massendigitalisierungs Workflows (ULB Halle)  OCR4all libraries Volltexterkennung historischer Sammlungen (GEI Braunschweig, HCI und ZPD der Universität Würzburg)Wir freuen uns sehr auf diese nächste Projektphase und die (weitere) Zusammenarbeit mit den Implementierungs- und Modulprojekten, die den Einsatz der OCR-D-Software in verschiedenen Nutzungsszenarienvorbereiten werden.",
      "url": " /de/2021/06/10/projects.html"
    }
    ,
  
  {
      "slug": "en-2021-04-26-barcamp-html",
      "title": "OCR(-D) &amp; Co starting in May",
      "content"	 : "On 7 May 2021 will be the inaugural session of our new barcamp-like monthly event OCR(-D) &amp;amp; Co. barcamp format, developers, users and all other interested persons will be will be given the opportunity to talkabout OCR(-D).OCR(-D) &amp;amp; Co will take place every first Friday of the month from May 7 onwards at 10-11 am CET in a BBB room.At the beginning of each meeting, participants have the opportunity to suggest topics of interest, which can thenbe discussed by small groups in breakout rooms. In addition to the open TechCall,where the OCR-D community discusses technical topics every second Wednesday, OCR(-D) &amp;amp; Co also offers participants without in-depthOCR-D knowledge the opportunity to contribute their own questions and ideas to the discussion and to openly exchange ideaswith other OCR-interested people. We look forward to your participation!",
      "url": " /en/2021/04/26/barcamp.html"
    }
    ,
  
  {
      "slug": "de-2021-04-26-barcamp-html",
      "title": "OCR(-D) &amp; Co startet im Mai",
      "content"	 : "Am 7. Mai 2021 startet OCR(-D) &amp;amp; Co, zu dem OCR-D alle Interessierten einlädt. Bei dem offenen Treffen im Barcamp-Format können sich Entwickler, Nutzer und alle weiteren Interessenten niedrigschwellig überOCR(-D) austauschen.OCR(-D) &amp;amp; Co wird ab Mai jeden ersten Freitag im Monat 10-11 Uhr in einem BBB-Raum stattfinden.Zu Beginn jedes Treffens haben die TeilnehmerInnen die Möglichkeit, für sie interessante Themen vorzuschlagen, die dannin Kleingruppen diskutiert und besprochen werden können. In Ergänzung zum offenen TechCall,bei dem sich die OCR-D-Community jeden zweiten Mittwoch über technische Themen austauscht, bietet OCR(-D) &amp;amp; Co auch TeilnehmerInnenohne tiefergehende OCR-D-Kenntnisse die Möglichkeit, ihre eigenen Fragen und Ideen in die Diskussion einzubringen und sichoffen mit anderen OCR-Interessierten auszutauschen. Wir freuen uns auf Ihre Teilnahme!",
      "url": " /de/2021/04/26/barcamp.html"
    }
    ,
  
  {
      "slug": "en-2021-01-19-phase3-html",
      "title": "Phase III of the OCR-D-coordination project granted",
      "content"	 : "The coordination project’s application for the third phase of the OCR-D funding initiative was approved by the DFG in January 2021.In phase III, we will optimise the results of the previous module project phase and we will initiate the productive use of theOCR-D software in mass digitisation both technically and organisationally.The previous project partners from BBAW, HAB and SBB will now be joined by SUB Göttingen and GWDG, whereas the KIT has left the project. Together, the partners can continue to support and coordinate the work of the other OCR-D projects in Phase III.In addition, the OCR-D software will be optimised for its use in mass digitisation and the functioning of theoverall OCR-D workflow will be ensured. Great importance will be put on ensuring the permanent support andfurther development of the OCR-D software and on communicating the results of the implementation work to abroad circle of users who will use it for the efficient full-text digitisation of VD materials.The coordination project is delighted to continue the work of the two previous projectphases and looks forward to working with the other OCR-D projects.",
      "url": " /en/2021/01/19/phase3.html"
    }
    ,
  
  {
      "slug": "de-2021-01-19-phase3-html",
      "title": "Phase III der OCR-D-Koordinierung bewilligt",
      "content"	 : "Der Antrag des Koordinierungsprojekts für die dritte Phase der OCR-D-Förderinitiative wurde im Januar 2021 von der DFG bewilligt.In Phase III werden die Ergebnisse der vorangegangenen Modulprojekt-Phase optimiert und der produktive Einsatz derOCR-D-Software in der Massendigitalisierung technisch und organisatorisch eingeleitet.Die bisherigen Projektpartner aus BBAW, HAB und SBB werden nach dem Ausscheiden des KIT durch SUB Göttingen und GWDG verstärkt. Gemeinschaftlich können die Partner auch in Phase III die Arbeiten der weiteren OCR-D-Projekte unterstützen undkoordinierend begleiten. Zudem soll die OCR-D-Software für ihren Einsatz in der Massendigitalisierung optimiertund die Funktionsweise des OCR-D-Gesamtworkflows sichergestellt werden. Großer Wert wird darauf gelegt werden, die dauerhafte Betreuung und Weiterentwicklung der OCR-D-Software sicherzustellen und die Ergebnisse derImplementierungsarbeiten an einen breiten Kreis an NutzerInnen zu vermitteln, die diese zur effizientenVolltextdigitalisierung der VD-Materialien einsetzen.Das Koordinierungsprojekt freut sich auf die gemeinsame Fortsetzung der Arbeiten aus den beiden vorangegangenenProjektphasen mit den weiteren OCR-D-Projekten.",
      "url": " /de/2021/01/19/phase3.html"
    }
    ,
  
  {
      "slug": "en-2020-10-02-elag-html",
      "title": "OCR-D at the Mini-ELAG",
      "content"	 : "On October 20, 2020 the Mini-ELAG (European Library Automation Group)takes place, where librarians and IT professionals discuss new information technologiesand their application in libraries and documentation centers. OCR-D will be represented at virtual conference with a lecture by Clemens Neudecker (SBB) onOCR-D: An open ecosystem for improving OCR on historical documents.The talk will present the OCR-D software and its functions and disckuss the open, participativedevelopment strategy of the DFG project.",
      "url": " /en/2020/10/02/elag.html"
    }
    ,
  
  {
      "slug": "de-2020-10-02-elag-html",
      "title": "OCR-D bei der Mini-ELAG",
      "content"	 : "Am 20. Oktober 2020 findet die Mini-ELAG (European Library Automation Group)statt, bei der der Bibliothekare und IT-Fachleute neue Informationstechnologien und ihre Anwendung in Bibliotheken und Dokumentationsszentren diskutieren. OCR-D ist bei der virtuellen Konferenz mit einem Vortrag von Clemens Neudecker (SBB) zuOCR-D: An open ecosystem for improving OCR on historical documents vertreten.Die OCR-D-Software wird mit ihren Funktionen vorgestellt und die offene, partizipativeEntwicklungsstrategie des DFG-Projekts diskutiert.",
      "url": " /de/2020/10/02/elag.html"
    }
    ,
  
  {
      "slug": "en-2020-09-22-fair-html",
      "title": "OCR-D at the virtual workshop FAIR &amp; Co",
      "content"	 : "From October 7 to 8, the eHumanities working group of the Union of German Academies of Sciences and Humanities,in cooperation with the Göttingen Academy of Sciences and Humanities,is organizing the workshop FAIR &amp;amp; Co: Visibility and Availability of Digital Academy Research in a Networked Scientific Landscape.The OCR-D project will be represented with a lecture on the topic Digital Transformation: OCR-D, Offer and Vision by Matthias Boenig (BBAW).Using the example of the German Text Archive, it will be shown how the application spectrum of this referencecorpus can be extended by the area of machine learning to improve character and structurerecognition. For the whole program of the workshop see the website of this workshop.",
      "url": " /en/2020/09/22/fair.html"
    }
    ,
  
  {
      "slug": "de-2020-09-22-fair-html",
      "title": "OCR-D beim virtuellen Workshop FAIR &amp; Co",
      "content"	 : "Vom 7. bis 8. Oktober veranstaltet die AG eHumanities der Union der deutschen Akademien der Wissenschaftenin Zusammenarbeit mit der Akademie der Wissenschaften zu Göttingenden Workshop FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft.Das OCR-D-Projekt ist mit einem Vortrag von Matthias Boenig (BBAW) zum Thema: Digitale Transformation: OCR-D, Angebot und Vision vertreten. Am Beispiel desDeutschen Textarchivs wird dargestellt, wie das Anwendungsspektrum dieses Referenzkorpus um den Bereichdes maschinellen Lernens zur Verbesserung der Zeichen- und Strukturerkennung erweitertwerden kann. Das ganze Programm kann auf der Website des Workshopeingesehen werden.",
      "url": " /de/2020/09/22/fair.html"
    }
    ,
  
  {
      "slug": "en-2020-08-01-implementation-workshop-html",
      "title": "Workshop for the implementation plans",
      "content"	 : "Following the successful first (virtual) meeting of those interested in the DFG-call for theimplementation of the OCR-D-Software, a further workshop will be held on 7 August, 9-13 p.m.to prepare the OCR-D grant proposals.At this second meeting the applicants can inform each other about their previous tests ofthe OCR-D software and exchange experiences. In addition, the project plans,which have been further developed in the meantime, will be discussed in order to identifypossible synergies between the grant proposals.We are looking forward to this new exchange and a continuing successful pilot phase!",
      "url": " /en/2020/08/01/implementation-workshop.html"
    }
    ,
  
  {
      "slug": "de-2020-08-01-antragsworkshop-html",
      "title": "Workshop der Implementierungsvorhaben",
      "content"	 : "Im Anschluss an das erfolgreiche erste (virtuelle) Treffen der Implementierungsinteressentenfindet am 7. August, 9-13 Uhr ein weiterer Workshop zur Vorbereitung der OCR-D-Anträge statt.Bei dem erneuten Treffen können sich die Antragsteller gegenseitig über ihre bisherigenTests der OCR-D-Software informieren und Erfahrungen dazu austauschen. Außerdem werden die inzwischen weiterentwickelten Anträgspläne diskutiert, um mögliche Synergien zwischenden Anträgen erkennen zu können.Wir freuen uns auf diesen neuen Austausch und eine weiterhin erfolgreiche Pilotierungsphase!",
      "url": " /de/2020/08/01/antragsworkshop.html"
    }
    ,
  
  {
      "slug": "en-2020-06-04-pilot-html",
      "title": "Kick-off pilot phase",
      "content"	 : "We are very happy about the great interest in the DFG call for proposals for the implementation of the OCR-D software. As OCR-D coordination projectwe will support the planned projects from the pilot phase onwards and promote the exchange of information among interested parties as desired by the DFG.To kick off the pilot phase, we are organising a large video conference on 19 June, 9-13 o’clock, at which all interested parties can get to know each other and the pilot tests can be coordinated.Interested parties who have not submitted a letter of intent themselves and who are still looking for a suitable partner with whom they could engage in the third phase of OCR-D are also welcome to the video conference.If you are interested, please register for the video conference by 12 June at engl@hab.de.We look forward to working with you and to a successful pilot phase!",
      "url": " /en/2020/06/04/pilot.html"
    }
    ,
  
  {
      "slug": "de-2020-06-04-pilotphase-html",
      "title": "Auftakt der Pilotierungsphase",
      "content"	 : "Wir freuen uns sehr über das große Interesse an der DFG-Ausschreibung zur Implementierung der OCR-D-Software. Als OCR-D-Koordinierungsprojektwerden wir die geplanten Vorhaben von der Pilotierung an begleiten und den von der DFG gewünschten Austausch unter den Interessenten befördern.Zum Auftakt der Pilotierungsphase veranstalten wir am 19. Juni, 9-13 Uhr eine große Videokonferenz, bei der sich alle Interessentenkennenlernen und die Pilotierungsarbeiten abgestimmt werden können.Zu der Videokonferenz sind auch Interessenten willkommen, die selbst keine Absichtserklärung eingereicht haben und derzeit noch auf der Suchenach einem geeigneten Partner sind, mit dem sie sich in die dritte OCR-D-Phase einbringen könnten.Bei Interesse melden Sie sich bitte bis zum 12. Juni unter engl@hab.de zur Videokonferenz an.Wir freuen uns auf die Zusammenarbeit und eine erfolgreiche Pilotierungsphase!",
      "url": " /de/2020/06/04/pilotphase.html"
    }
    ,
  
  {
      "slug": "en-2020-02-25-dfg-call-html",
      "title": "Call for OCR-D Implementation online!",
      "content"	 : "The call for the implementation of the OCR-D software for the full text digitisation of historical printsis now available on the website of the German Research Association (DFG).The aim of the OCR-D coordination project, which was launched in autumn 2015,is to describe procedures and develop guidelines in order to achieve an optimalworkflow and the greatest possible standardisation of OCR-related processes andmetadata. Furthermore, the complete transformation of the written Germancultural heritage into a machine-readable form (structured full text) is to beprepared conceptually. Primarily, works from the Union Catalogue of BooksPrinted in German Speaking Countries in the 16th-18th century (VD) as well asbooks published in the 19th century in the German language area will beconsidered. The VD projects comprise about 1 million titles that are currentlybeing digitized and are to be processed by means of OCR in the future.The work done so far in the OCR-D initiative has led to significantimprovements in the optical character and layout recognition of historicalprints.  The Software prototype promises flexible integration into existing(librarian) workflow and digitisation systems due to its modular design. Theimplementation of the OCR-D software in libraries, archives and othercollection holding institutions is the next necessary step to ensure thathigh-quality full texts can be produced.The central goal of the new call is the development of (generic) implementationpackages with acceptable performance for different requirements. It isprimarily aimed at collection holding institutions, the application is due onOctober 7, 2020. The DFG expects applicants to hand in a letter of intent by May ~~5~~ 22. If you are interested in participating, please don’t hesitate tocontact us.",
      "url": " /en/2020/02/25/dfg-call.html"
    }
    ,
  
  {
      "slug": "de-2020-02-25-dfg-ausschreibung-html",
      "title": "Ausschreibung zur OCR-D-Implementierung online!",
      "content"	 : "Die Ausschreibung für die Implementierung der OCR-D-Software zur Volltextdigitalisierung historischer Drucke ist ab sofort auf den Seiten der Deutschen Forschungsgemeinschaft (DFG) zugänglich.Das im Herbst 2015 gestartete Koordinierungsprojekt OCR-D hat zum Ziel, zumeinen Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einenoptimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCRbezogenen Prozessen und Metadaten zu erzielen. Zum anderen soll dievollständige Transformation des schriftlichen deutschen Kulturerbes in einemaschinenlesbare Form (strukturierter Volltext) konzeptionell vorbereitetwerden. Vornehmlich betrachtet werden Werke aus den Verzeichnissen der imdeutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD) sowiedes 19. Jhs. Die VD-Projekte umfassen ca. 1 Mio. Titel, die derzeitdigitalisiert und zukünftig mittels einer OCR prozessiert werden sollen.Die bisherigen Arbeiten in der OCR-D-Initiative haben zu wesentlichenVerbesserungen der Verfahren zur automatischen Text- und Strukturerfassunghistorischer Drucke geführt. Der Software-Prototyp verspricht durch seinenmodularen Aufbau eine flexible Integration in bestehende (bibliothekarische)Workflow- und Digitalisierungssysteme. Die Implementierung der OCR-D-Softwarein Bibliotheken, Archiven sowie anderen bestandshaltenden bzw.bestandsverarbeitenden Einrichtungen ist folglich der nächste notwendigeSchritt, damit die Erzeugung hochqualitativer Volltexte stattfinden kann.Zentrales Ziel der neuen Ausschreibung ist die Entwicklung (generischer)Implementierungspakete mit akzeptabler Performanz für unterschiedlicheAnforderungen. Sie richtet sich vorwiegend an bestandshaltende Einrichtungen,Anträge können bis zum 7. Oktober 2020 bei der DFG eingereicht werden. Die DFG erwartet von Antragstellern vorab eine Absichtserklärung, die bis zum 5. 22. Mai abzugeben ist. BeiInteresse an der Ausschreibung nehmen Sie gerne mit uns Kontakt auf.",
      "url": " /de/2020/02/25/dfg-ausschreibung.html"
    }
    ,
  
  {
      "slug": "en-2020-02-19-dhd-html",
      "title": "OCR-D at the DHd in Paderborn",
      "content"	 : "The OCR-D funding initiative will be represented with several presentations at the DHd 2020, which will take place in Paderborn from 2-6 March.In V15, on March 5, the OCR-D coordination project will discuss the results and perspectives of the funding initiative on the basis of four theses on full text transformation. In the same section, the colleagues of the module project for the development of a model repository and automatic font recognition for OCR-D will report on their findings on the use of fracture in German-language books. For further information on the presentations see the DHd program.",
      "url": " /en/2020/02/19/dhd.html"
    }
    ,
  
  {
      "slug": "de-2020-02-19-dhd-html",
      "title": "OCR-D bei der DHd in Paderborn",
      "content"	 : "Die OCR-D-Förderinitiative ist mit mehreren Vorträgen auf der DHd 2020 vertreten, die vom 2.-6. März in Paderborn stattfindet.In V15, am 5. März, stellt das OCR-D-Koordinierungsprojekt anhand von vier Thesen zur Volltexttransformation Ergebnisse und Perspektiven der Förderinitiative zur Diskussion. In der gleichen Sektion berichten die Kollegen des Modulprojekts zur Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D über ihre Erkenntnisse zur Verwendung von Fraktur in deutschsprachigen Büchern. Weitere Informationen zu den Vorträgen finden Sie im Programm der DHd.",
      "url": " /de/2020/02/19/dhd.html"
    }
    ,
  
  {
      "slug": "en-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Full texts - the future of old prints: OCR-D-Workshop in Bonn",
      "content"	 : "The OCR-D workshop “Full texts - the future of old prints” will take place inBonn on 12 February. The findings and desiderata of the DFG project will bepresented and discussed there with a broad audience of developers, OCR experts,users and funding agencies.The first morning lectures will focus on the OCR-D software itself. It will bedemonstrated with its range of functions and technical possibilities,specifications and documentation will be described as a basis for its creationand use. The experiences and insights that the software developers have gainedon the creation of OCR in libraries are of special interest with regard tofuture OCR projects. The morning section will be concluded in a paneldiscussion with the developers of the OCR-D module projects.The afternoon section is reserved for the discussion about the future of theOCR-D software. The workshop participants will discuss various ways andpossibilities for implementing the software at libraries and transferring it topractical use. Subsequently, possible further funding measures of the DFGproject will be discussed, with the help of which the last steps can be takentowards the mass processing of the VD titles, which is ultimately targeted.",
      "url": " /en/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "de-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Volltexte – die Zukunft alter Drucke: OCR-D-Workshop in Bonn",
      "content"	 : "Am 12. Februar findet in Bonn der OCR-D-Workshop “Volltexte – die Zukunft alterDrucke” statt. Die Erkenntnisse und Desiderate des DFG-Projekts werden dorteinem bereiten Publikum aus Entwicklern, OCR-Experten, Anwendern undFördergebern vorgestellt und diskutiert.Die ersten Vorträge am Vormittag stellen die OCR-D-Software selbst in denMittelpunkt. Diese wird mit ihrem Funktionsumfang und ihren technischenMöglichkeiten vorgeführt sowie Spezifikationen und Dokumentationen als Basisfür deren Erstellung und Verwendung beschrieben. Die Erfahrungen undErkenntnisse, die die Software- Entwickler zur Praxis der OCR-Erstellung inbestandshaltenden Einrichtungen gewonnen haben, sind mit Blick auf künftigeOCR-Projekte von übergeordnetem Interesse. Beschlossen wird der Vormittagsteilin einer Podiumsdiskussion mit den Entwicklern aus den einzelnenOCR-D-Modulprojekten.Die Nachmittagssektion ist der Diskussion über die Zukunft der OCR-D-Softwarevorbehalten. Mit den Workshop-Teilnehmern sollen verschiedene Wege undMöglichkeiten zur Implementierung der Software an bestandshaltendenEinrichtungen und deren Überführung in den Praxiseinsatz erörtert werden. Darananschließend werden mögliche weitere Fördermaßnahmen des DFG-Projektsbesprochen, mit deren Hilfe die letzten Schritte zur letztlich anvisiertenmassenhaften Prozessierung der VD-Titel gegangen werden können.",
      "url": " /de/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "en-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Cooperation with Kitodo signed",
      "content"	 : "Kitodo and OCR-D have signed a Letter of Intent to cooperate on the coordinated and sustainable development and provision of OCR software solutions for mass full text digitization. Kitodo is an open source software suite for the digitisation of cultural property and widely used in the library sector.The basic aim of the cooperation is to make use of overlaps between Kitodo and OCR-D to achieve synergies in technical and organizational aspects. It will be perspectively aspired, to integrate OCR-D as a separate functional area in Kitodo.Production.. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and upcoming developments. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on Interfaces to Kitodo.Production and current and upcoming developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. willcoordinating institutions of the OCR-D project and the In the coming years, OCR-D intends to expand the resulting software solution into a stable, widely used and extensively documented software solution. In mutual agreement and against the background of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and future developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. will support the coordination project and the implementation partners with its extensive experience.",
      "url": " /en/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "de-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Kooperation mit Kitodo vereinbart",
      "content"	 : "Kitodo und OCR-D haben einen Letter of Intent zur Zusammenarbeit bei derkoordinierten und nachhaltigen Entwicklung und Bereitstellung vonOCR-Softwarelösungen zur Massenvolltextdigitalisierung unterzeichnet. Kitodoist eine quelloffene Softwaresuite für die Digitalisierung von Kulturgut inbestandshaltenden Einrichtungen und im Bibliotheksbereich weit verbreitet.Grundlegendes Ziel der Kooperation ist es, inhaltliche und konzeptionelleÜberschneidungen zwischen Kitodo und OCR-D zum Erzielen von Synergien intechnischer und organisatorischer Hinsicht zu nutzen. Es wird perspektivischangestrebt, OCR-D als eigenen Funktionsbereich in Kitodo.Productionaufzunehmen. OCR-D beabsichtigt in den kommenden Jahren, die entstandeneSoftwarelösung zu einer stabil lauffähigen, breit eingesetzten sowieausführlich dokumentierten Softwarelösung auszubauen. Im gegenseitigenEinverständnis und vor dem Hintergrund der anstehenden Implementierungsphasevon OCR-D tauschen sich die Kooperationspartner dazu vor allem überSchnittstellen zu Kitodo.Production und aktuelle sowie kommende Entwicklungender OCR-Massenvolltextdigitalisierung aus. Mit Blick auf die technischeIntegration von OCR-D in Kitodo.Production, wird Kitodo e.V. diekoordinierenden Einrichtungen des OCR-D-Projektes und dieImplementierungspartner mit seinen Erfahrungen umfassend unterstützen.",
      "url": " /de/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "en-2019-08-23-icdar-html",
      "title": "OCR-D at the ICDAR 2019 in Sydney",
      "content"	 : "The OCR-D paper “okralact - a multi-engine Open Source OCR training system (Konstantin Baierer, Rui Dong, Clemens Neudecker) was accepted for the 5th International Workshop on Historical Document Imaging and Processing HIP 2019 (https://www.primaresearch.org/hip2019/) in the context of the ICDAR Conference 2019 in Sydney (https://icdar2019.org/).The source code for the prototype for a multi-engine OCR Training infrastructure presented there is available at https://github.com/Doreenruirui/okralact. Training infrastructure is available at https://github.com/Doreenruirui/okralact for find. With “Dataset of Pages from Early Printed Books with Multiple Font Groups” the module project for the development of a model repository and an automatic font recognition for OCR-D is also at the same workshop with a paper of Mathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier and Vincent Christlein.",
      "url": " /en/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "de-2019-08-23-icdar-html",
      "title": "OCR-D bei der ICDAR 2019 in Sydney",
      "content"	 : "Das OCR-D Paper „okralact – a multi-engine Open Source OCR training system“(Konstantin Baierer, Rui Dong, Clemens Neudecker) wurde für den 5thInternational Workshop on Historical Document Imaging and Processing HIP 2019(https://www.primaresearch.org/hip2019/) im Rahmen der ICDAR Konferenz 2019 inSydney (https://icdar2019.org/) angenommen.Der Quellcode zum dort vorgestellten Prototypen für eine multi-Enginge OCRTrainingsinfrastruktur ist unter https://github.com/Doreenruirui/okralact zufinden. Mit „Dataset of Pages from Early Printed Books with Multiple FontGroups” ist zudem das Modulprojekt zur Entwicklung eines Modellrepositoriumsund einer automatischen Schriftarterkennung für OCR-D mit einem paper vonMathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier andVincent Christlein auf dem gleichen Workshop vertreten.",
      "url": " /de/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "en-2019-08-09-europeanatech-html",
      "title": "OCR-D in EuropeanaTech Insights",
      "content"	 : "On the occasion of DATeCH 2019, the online journal “EuropeanaTech Insights” dedicated its last issue to the topic of optical character recognition.In addition to articles on OCR in Bengali and Arabic texts from the British Library and on automatic article recognition in Finnish journals, the OCR-D project and its first results are presented in detail under the title “OCR-D: An End-to-end open source OCR framework for historical documents”. Authors of the paper are C. Neudecker, K. Baierer, M. Federbusch (Berlin State Library), K.M. Würzner, M. Boenig (Berlin-Brandenburg Academy of Sciences and Humanities), E. Herrmann (Duke-August Library Wolfenbüttel), V. Hartmann (Karlsruhe Institute of Technology).",
      "url": " /en/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "de-2019-08-09-europeanatech-html",
      "title": "OCR-D auf EuropeanaTech Insights",
      "content"	 : "Das Online Journal „EuropeanaTech Insights“ widmete aus Anlass der DATeCH 2019 sein letztes Heft dem Thema Optical Character Recognition.Neben Artikeln über OCR an bengalischen und arabischen Texten der British Library und über automatische Artikelerkennung in finnischen Zeitschriften werden das OCR-D Projekt und erste Ergebnisse daraus ausführlich unter dem Titel „OCR-D: An End-to-end open source OCR framework for historical documents“ vorgestellt. Verfasser des Papers sind C. Neudecker, K. Baierer, M. Federbusch (Staatsbibliothek Berlin), K.M. Würzner, M. Boenig (Berlin-Brandenburgische Akademie der Wissenschaften), E. Herrmann (Herzog-August Bibliothek Wolfenbüttel), V. Hartmann (Karlsruher Institut für Technologie).",
      "url": " /de/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "en-2019-05-13-datech-best-paper-html",
      "title": "Best Paper Award for OCR-D at the DATeCH 2019",
      "content"	 : "At the international conference “Digital Access to Textual Cultural Heritage 2019” (DATeCH2019) held in Brussels, the paper on OCR-D entitled “OCR-D: An end-to-end open-source OCR framework for historical documents” was awarded the Best Paper Award. The authors are Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-Michael Würzner, Matthias Boenig, Elisa Herrmann and Volker Hartmann.",
      "url": " /en/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "de-2019-05-13-datech-best-paper-html",
      "title": "Best Paper Award bei der DATeCH 2019 für OCR-D",
      "content"	 : "Bei der in Brüssel veranstalteten internationalen Konferenz “Digital Access toTextual Cultural Heritage 2019” (DATeCH2019) wurde der Beitrag über OCR-D mitdem Titel “OCR-D: An end-to-end open-source OCR framework for historicaldocuments” mit dem Best Paper Award ausgezeichnet. Als Autorinnen und Autorensind Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-MichaelWürzner, Matthias Boenig, Elisa Herrmann und Volker Hartmann verantwortlich.",
      "url": " /de/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "en-2018-08-31-ocrd-verlaengert-html",
      "title": "OCR-D extended",
      "content"	 : "The German Research Foundation (DFG) has approved an extension of the OCR-D project for another 18 months. The new funding phase will start in October 2018 and will therefore end in March 2020. This good news allows us to continue supporting the module projects and to consolidate the results. On the other hand, it will also allow the coordination committee’s own work packages to be continued and deepened.The current funding phase would end in the 3rd quarter of 2018 and project-immanent tasks such as combining the individual module project results into a software package or updating the DFG’s “Digitisation” rules of practice based on the findings of this project would not have been possible. During the extension period, the Coordinating Committee will work particularly on the OCR-D framework, the continuous development of which you can follow via the GitHub site (http://www.github.de/ocr-d).In addition, we are working on the later distribution of the software, be it by evaluating the modules on the basis of application examples, by contacts to OCR service providers or a pilot program in spring 2019. Within the pilot programme, the requirements already identified should be even better tailored to the needs of the future users.The third focus is on the extension of the ground truth data and the format maintenance and, of course, the coordination and management of the Communication with the module projects.  Besides the internal workshops the coordinating body will hold a final workshop in summer 2019, for which we will invite the professional public.",
      "url": " /en/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "de-2018-08-31-ocrd-verlaengert-html",
      "title": "OCR-D bis 2020 verlängert",
      "content"	 : "Die Deutsche Forschungsgemeinschaft (DFG) hat eine Verlängerung des ProjektsOCR-D für weitere 18 Monate bewilligt. Die neue Förderphase beginnt im Oktober2018 und endet demnach im März 2020. Diese erfreuliche Nachricht ermöglicht unszum einen die weitere Unterstützung der Modulprojekte sowie die Zusammenführungder Ergebnisse. Zum anderen können dadurch auch die eigenen Arbeitspakete desKoordinierungsgremiums weitergeführt und vertieft werden.Die aktuelle Förderphase wäre im 3. Quartal 2018 geendet und projektimmanenteAufgaben, wie die Zusammenführung der einzelnen Modulprojektergebnisse zu einemSoftwarepaket oder die Aktualisierung der DFG-Praxisregeln “Digitalisierung”auf Grundlage der Erkenntnisse aus diesem Vorhaben, wären nicht möglichgewesen. Das Koordinierungsgremium wird in der Verlängerung besonders amOCR-D-Framework arbeiten, dessen stetige Entwicklung Sie über die GitHub-Seite(http://www.github.de/ocr-d) verfolgen können.Daneben arbeiten wir an der späteren Verbreitung der Software, sei es durch dieEvaluierung der Module anhand von Anwendungsbeispielen, durch Kontakte zuOCR-Dienstleistern oder ein Pilotprogramm im Frühjahr 2019. Im Rahmen diesesPilotprogramms sollen die schon ermittelten Anforderungen noch besser auf dieBedürfnisse der späteren Anwender abgestimmt werden.Der dritte Schwerpunkt liegt auf der Erweiterung der Ground-Truth-Daten und derdazugehörigen Formatpflege sowie naturgemäß auf der Koordinierung undKommunikation mit den Modulprojekten.  Neben den internen Workshops organisiertdas Koordinierungsgremium einen Abschlussworkshop im Sommer 2019, zu dem wirdie Fachöffentlichkeit einladen werden.",
      "url": " /de/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "en-2018-03-28-start-der-modulprojekte-html",
      "title": "OCR-D Kick-Off Meeting",
      "content"	 : "From March 5th to 6th the big kick-off meeting of the module projects took place in the Herzog August Bibliothek in Wolfenbüttel, which officially heralds the second phase of OCR-D.The coordination committee of OCR-D met for the first time with the ten module project participants to get to know each other and to present the work done and planned on both sides.In short presentations, the project application contents were presented and connections between the module projects and the coordination committee were sought. During the personal exchange in the World Café as well as during the joint dinner, first project-strategic discussions could be initiated, which will accompany us in the near future.Of the more than 20 project proposals received in response to the DFG call for proposals in March 2017, eight module projects were approved at the end of the year:Scalable methods of text and structure recognition for the full text digitisation of historical prints: Image optimisationGerman Research Center for Artificial Intelligence (DFKI)Scalable methods of text and structure recognition for the full text digitisation of historical prints: layout recognitionDFKIFurther development of a semi-automatic open-source tool for layout analysis and region extraction and classification (LAREX) of early printed booksUniversity of WürzburgNN/FST - Unsupervised OCR Postcorrection based on Neural Networks and Finite-state TransducersLeipzig UniversityOptimized use of OCR processes - Tesseract as a component in the OCR-D workflowUniversity of MannheimAutomatic and semi-automatic post-correction of OCR-recorded historical printsMunich UniversityDevelopment of a model repository and automatic font recognition for OCR-DLeipzig University, Erlangen University, Mainz UniversityOLA-HD - An OCR-D long-term archive for historical printsSUB Göttingen, GWDG GöttingenThe module projects usually have a duration of 18 months, a public final workshop is planned for June 2019. Until then, the coordination committee will organise two project-internal developer meetings to discuss the previous versions of the work. An insight into the development can also be found on the GitHub page of OCR-D: https://github.com//ocr-dIn addition, the coordinating committee and the module projects are in regular exchange via video conferences during this intensive work phase.Further details on the individual module projects and later on the developer meetings will follow on the OCR-D website.We are looking forward to working together!For questions and suggestions: contact",
      "url": " /en/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "de-2018-03-28-start-der-modulprojekte-html",
      "title": "Start der 8 Modulprojekte - OCR-D geht in die zweite Projektphase",
      "content"	 : "Vom 05.-06. März fand in der Herzog August Bibliothek in Wolfenbüttel das große Kick-Off-Treffen der Modulprojekte statt, welches offiziell die zweite Phase von OCR-D einläutet.Das Koordinierunsggremium von OCR-D traf sich zum ersten Mal mit den zehn Modulprojektnehmern zum gegenseitigen Kennenlernen und zum Vorstellen der geleisteten und geplanten Arbeiten auf beiden Seiten.In kurzen Präsentationen wurden die Projektantragsinhalte dargestellt und nach Anknüpfungspunkten zwischen den Modulprojekten sowie zum Koordinierungsgremium gesucht. Beim persönlichen Austausch im World Café sowie beim gemeinsamen Abendessen konnten zudem erste projektstrategische Diskussionen angestoßen werden, die uns in naher Zukunft noch begleiten werden.Von den über 20 eingegangenen Projektanträgen auf die DFG-Ausschreibung im März 2017 wurden Ende des Jahres acht Modulprojekte bewilligt:Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: BildoptimierungDeutsches Forschungszentrum für Künstliche Intelligenz (DFKI)Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: LayouterkennungDFKIWeiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken,Universität WürzburgNN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state TransducersUniversität LeipzigOptimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-WorkflowUniversität MannheimAutomatische und semi-automatische Nachkorrektur OCR-erfasster historischer DruckeUniversität MünchenEntwicklung eines Modellrepositoriums und einer automatischen Schriftarterkennung für OCR-DUniversität Leipzig, Universität Erlangen, Universität MainzOLA-HD – Ein OCR-D-Langzeitarchiv für historische DruckeSUB Göttingen, GWDG GöttingenDie Modulprojekte haben in der Regel eine Laufzeit von 18 Monaten, ein öffentlicher Abschlussworkshop ist für Juni 2019 geplant. Bis dahin organisiert das Koordinierungsgremium zwei projektinterne Entwicklertreffen, auf denen die bisherigen Arbeitsversionen besprochen werden. Einen Einblick in die Entwicklung bietet auch die GitHub-Seite von OCR-D: https://github.com//ocr-dZudem stehen das Koordinierungsgremium und die Modulprojekte in dieser intensiven Arbeitsphase per Videokonferenzen im regelmäßigen Austausch.Weitere Details zu den einzelnen Modulprojekten und später zu den Entwicklertreffen folgen auf der Seite von OCR-D.Wir freuen uns auf die gemeinsame Zusammenarbeit!Für Fragen und Anregungen: Kontakt",
      "url": " /de/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "en-2017-03-06-modulprojektausschreibung-html",
      "title": "Call for OCR-D module project proposals",
      "content"	 : "The call for module projects within the framework of OCR-D can now be found online on the website of the German Research Foundation (DFG) (link to the call)The aim of the OCR-D coordination project, which was launched in autumn 2015, is to describe procedures and develop guidelines in order to achieve an optimal workflow and the greatest possible standardisation of OCR-related processes and metadata. Furthermore, the complete transformation of the written German cultural heritage into a machine-readable form (structured full text) is to be prepared conceptually. Primarily, works from the Union Catalogue of Books Printed in German Speaking Countries in the 16th-18th century (VD) as well as books published in the 19th century in the German language area will be considered. The VD projects comprise about 1 million titles that are currently being digitized and are to be processed by means of OCR in the future.In the first project phase of OCR-D, development needs for automatic text recognition processes were identified. Based on this, the DFG is now issuing calls for proposals for six module project topics, which will be managed in terms of content and technology by the OCR-D coordination project. The following topics are announced:Image presortingLayout recognitionText optimizationModel trainingLong-term archiving and persistenceQuality assuranceIn order to get an impression of the material to be treated, we provide Ground-Truth data (link to the data).",
      "url": " /en/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "de-2017-03-06-modulprojektausschreibung-html",
      "title": "Modulprojektausschreibungen online!",
      "content"	 : "Die Ausschreibung für Modulprojekte im Rahmen von OCR-D ist ab sofort online auf der Seite der Deutschen Forschungsgemeinschaft (DFG) zu finden (Link zur Ausschreibung)Das im Herbst 2015 gestartete Koordinierungsprojekt OCR-D hat zum Ziel, zum einen Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR bezogenen Prozessen und Metadaten zu erzielen. Zum anderen soll die vollständige Transformation des schriftlichen deutschen Kulturerbes in eine maschinenlesbare Form (strukturierter Volltext) konzeptionell vorbereitet werden. Vornehmlich betrachtet werden Werke aus den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD) sowie des 19. Jhs. Die VD-Projekte umfassen ca. 1 Mio. Titel die derzeit digitalisiert und zukünftig mittels einer OCR prozessiert werden sollen.In der ersten Projektphase von OCR-D wurden Entwicklungsbedarfe für Verfahren der automatischen Texterkennung ermittelt. Darauf aufbauend erfolgt nun die Ausschreibungen der DFG zu sechs Modulprojektthemen, die inhaltlich und technisch durch das Koordinierungsgremium von OCR-D betreut. Folgende Themen sind ausgeschrieben:BildvorsortierungLayouterkennungTextoptimierungModelltrainingLangzeitarchivierung und PersistenzQualitätssicherungUm einen Eindruck des zu behandelnden Materials zu bekommen stellen wir Ground-Truth-Daten zur Verfügung (Link zu den Daten).",
      "url": " /de/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "en-2016-12-06-staatsbibliothek-html",
      "title": "State Library in Berlin is New Project Partner of OCR-D",
      "content"	 : "The coordinating body of OCR-D consists of the Herzog August Bibliothek Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities Berlin, in particular the German Text Archive, and now the Staatsbibliothek zu Berlin. In future, the SBB will take over the work of the Bayerische Staatsbibliothek, which withdrew from the project on 31 August 2016.The work packages include long-term archiving and persistence, the Conception of workflows and use cases as well as the composition of training corpora. OCR-D investigates further development possibilities for Optical Character Recognition (OCR) process. The project sees itself as a coordinating body and network at the same time, bringing together developers, researchers and users to combine current research findings with practical requirements in a practicable solution. During the first project phase, development needs were identified on the basis of which module project calls will follow. In these, solutions for the development needs will be worked out and thus the current state of research on OCR will be brought together with the requirements from practice. The calls for proposals are planned for the first half of 2017. The results from OCR-D will have far-reaching changes for digitisation projects. In addition to the goal of preparing the transformation of the titles from the VD projects into machine-readable form, proposals for the DFG’s “Digitisation” practice rules will also be drawn up in response to the new findings in order to complete the media conversion of the entire written cultural heritage published in the German-speaking world in the medium to long term in the spirit of European and national agendas.",
      "url": " /en/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "de-2016-12-06-staatsbibliothek-html",
      "title": "Staatsbibliothek zu Berlin ist neuer Partner von OCR-D",
      "content"	 : "Das Koordinierungsgremium von OCR-D setzt sich aus der Herzog August BibliothekWolfenbüttel, der Berlin-Brandenburgischen Akademie der Wissenschaften Berlin,dort insbesondere dem Deutschen Textarchiv, sowie nun der Staatsbibliothek zuBerlin zusammen. Die SBB übernimmt dabei zukünftig die Arbeiten der BayerischenStaatsbibliothek, die zum 31.08.2016 aus dem Projekt ausgeschieden ist.DieArbeitspakete umfassen u.a. die Langzeitarchivierung und Persistenz, dieKonzeption von Workflows und Use Cases sowie die Zusammenstellung vonTrainingskorpora. OCR-D untersucht Weiterentwicklungsmöglichkeiten fürVerfahren der Optical Character Recognition (OCR). Das Projekt versteht sichdabei als Koordinierungsgremium und Netzwerk zugleich, bringt Entwickler,Forscher und Anwender zusammen um aktuelle Erkenntnisse aus der Forschung mitden Anforderungen aus der Praxis in einer praktikablen Lösung zu vereinen. Inder ersten Projektphase wurden Entwicklungsbedarfe aufgedeckt auf Basis dererModulprojektausschreibungen folgen. In diesen werden für dieEntwicklungsbedarfe Lösungen erarbeitet und so der aktuelle Forschungsstand zurOCR mit den Anforderungen aus der Praxis zusammen gebracht. Die Ausschreibungensind für das erste Halbjahr 2017 geplant. Die Ergebnisse aus OCR-D werdenweitreichende Veränderungen für Digitalisierungsprojekte haben. Neben dem Ziel,die Transformation der Titel aus den VD-Projekten in maschinenlesbare Formvorzubereiten, werden auch Vorschläge für die DFG-Praxisregeln„Digitalisierung“ an die neuen Erkenntnisse erarbeitet, um im Geisteeuropäischer und nationaler Agenden die Medienkonversion des gesamten imdeutschen Sprachraum erschienenen schriftlichen kulturellen Erbes mittel- bislangfristig zu vollenden.",
      "url": " /de/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "en-2016-06-01-ocrd-html",
      "title": "The OCR-D project",
      "content"	 : "OCR-D is a coordination project that is aimed at the further development of Optical Character Recognition (OCR) processes for historical prints.Workflow and methods of automatic text recognition are investigated, described and, if necessary, optimized. A major goal is to conceptually prepare the transformation of prints of the German-speaking countries from the 16th to 19th century into electronic full text.The Herzog August Bibliothek Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the Staatsbibliothek zu Berlin Preußischer Kulturbesitz and the Karlsruhe Institute of Technology are participating in this project. The Bayerische Staatsbibliothek was also involved until 31 August 2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have digitised extensive collections of images. Searchable full texts can be automatically generated from these image data using OCR procedures. The added value provided by the use of digital full texts is indispensable in many scientific disciplines today, especially in the field of humanities research.So far, however, access to the electronic full text is often not possible or only possible in an insufficient form. Many historical holdings are available in digitalised form through the “Verzeichnisse der im deutschen Sprachbereich erschienenen Drucke” (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially fracture, are hardly recognized.There is a need for development here, which we are uncovering in OCR-D. We build on the already existing tools and investigations. By a new combination, in rare cases also by new development, the OCR process for VD prints shall be specialized. Thereby we are looking for answers to current technical, information scientific and organisational problems.The project is funded by the German Research Foundation (DFG) and will run for three years until September 2018. In the first phase, needs will be identified and concepts for the further development will be developed. The cooperation structure will be consolidated and continued in the second phase.  In this phase, calls for proposals for pilot projects will be issued, which will enable other institutions to participate. In all steps we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digitised material from the printed German cultural heritage of the 16th to 19th centuries will be developed.",
      "url": " /en/2016/06/01/ocrd.html"
    }
    ,
  
  {
      "slug": "de-2016-06-01-ocrd-html",
      "title": "Das OCR-D-Projekt",
      "content"	 : "OCR-D ist ein Koordinierungsprojekt, welches auf die Weiterentwicklung vonVerfahren der Optical Character Recognition (OCR) für historische Druckeausgerichtet ist.Dabei werden Workflow und Verfahren der automatischen Texterkennung untersucht,beschrieben und ggf. optimiert. Ein wesentliches Ziel ist es, dieTransformation von Drucke des deutschsprachigen Raums aus dem 16.-19.Jahrhundert in elektronischen Volltext konzeptuell vorzubereiten.An diesem Vorhaben beteiligen sich die Herzog August Bibliothek Wolfenbüttel,die Berlin-Brandenburgische Akademie der Wissenschaften in Berlin sowie dieStaatsbibliothek zu Berlin Preußischer Kulturbesitz und dem Karlsruher Institutfür Technologie. Ebenfalls beteiligt war bis zum 31.08.2016 die BayerischeStaatsbibliothek. Unterstütz wird das Projekt durch Experten, Wissenschaftlerund Bibliotheken.In den letzten Jahren haben vor allem wissenschaftliche Bibliothekenumfangreiche Bestände bilddigitalisiert. Mit Hilfe von OCR-Verfahren können ausdiesen Bilddaten durchsuchbare Volltexte automatisch generiert werden. DerMehrwert durch die Nutzung von digitalen Volltexten ist in vielenWissenschaftsdisziplinen, insbesondere im Bereich der geisteswissenschaftlichenForschung heute unverzichtbar.Bislang ist der  Zugriff auf den elektronischen Volltext jedoch oft nicht odernur in unzureichender Form möglich. Viele historische Bestände liegen in digitalisierter Form durch die „Verzeichnisse der im deutschen Sprachbereich erschienenen Drucke“ (kurz VD) vor. Resultate aus gängigen OCR-Verfahren waren bislang ungenügend. Insbesondere werden alte Drucktypen, vor allem Fraktur, nur schwerlich erkannt.Hier besteht Entwicklungsbedarf, den wir in OCR-D  aufdecken. Wir bauen dabei auf die bereits bestehende Tools und Untersuchungen auf. Durch eine Neu-Kombination, in seltenen Fällen auch durch Neuentwicklung, soll der OCR-Prozess für die VD-Drucke spezialisiert werden. Dabei wird nach Antworten auf aktuelle technische, informationswissenschaftliche und organisatorische Probleme gesucht.Das Projekt wird durch die Deutsche Forschungsgemeinschaft (DFG) gefördert und hat eine Laufzeit von drei Jahren bis September 2018. In der ersten Phase  werden Bedarfe aufgedeckt und Konzepte für den weiteren Verlauf erarbeitet. Die Kooperationsstruktur wird gefestigt und in der zweiten Phase fortgeführt.  In dieser werden Ausschreibungen für Pilotprojekte erfolgen, die eine Beteiligung weiterer Einrichtungen ermöglicht. In allen Schritten begrüßen wir einen regen Austausch mit Kolleginnen und Kollegen aus  artverwandten Projekten und Einrichtungen sowie Dienstleistern.Am Ende des Gesamtvorhabens soll ein konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des gedruckten deutschen Kulturerbes des 16. bis 19. Jh. erarbeitet sein.",
      "url": " /de/2016/06/01/ocrd.html"
    }
    
  
]
