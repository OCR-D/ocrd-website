[
  

    {
      "slug": "404-html",
      "title": "",
      "content"	 : "  404 - Page not found",
      "url": " /404.html"
    },
  

    {
      "slug": "en-spec-changelog-html",
      "title": "Change Log",
      "content"	 : "Change LogAll notable changes to the specs will be documented in this file.Versioned according to Semantic Versioning.Unreleased3.24.0 - 2024-01-25Changed:  Rewritten web_api to match the implementation in OCR-D/coreFixed:  typos in ocrd_tool3.23.0 - 2023-03-16Added:  JSON-schema for QUIVER / QA Spec, #2363.22.0 - 2023-03-03Added:  QA Spec: defining metrics for evaluation of OCR against Ground Truth, #225  Web API Spec: Defining the components and interactions of the Web API, #222Fixed:  ocrd-tool.json: Wording on size attribute of resources, #2333.21.0 - 2022-11-30Fixed:  web api: GET /workflow/{workflow-id} should return a workflow, not a workspace, #223Changed:  web api: POST /workflow/{workflow-id} accepts a WorkflowArgs object with workspace ID and workflow parameters, #220  mets: Reorganize structure of the document, add numbered section headings, #155, #207Added:  cli: --page-id/-g option accepts regular expressions as well, #221, OCR-D/core#856  mets: conventions for providing document-wide files (FULLDOWNLOAD_...), #154, #207  cli: clarify the semantics and order of preference for multi-value/regex/range param values, #232, OCR-D/core#9213.20.0 - 2022-08-14Changed:  web api: /workspace: support content-negotiation for either OcrdZip or JSON description, #210  web api: POST/PUT /workspace: return OcrdZip, #209  ocrd-tool.json schema: default for parameters is an empty object {}Removed:  logging: obsolete docs better described in cli, #2193.19.0 - 2022-05-23Fixed:  typos in the web OCR-D Web API, #199  typo in the OCRD-ZIP spec, #203  fix bagit-profile URL, #205Changed:  Drop Ocrd-Manifestation-Depth and disallow fetch.txt bagit mechanism, #182  Drop unclear has_docker attribute in discovery response in OCR-D Web API, #201Added:  Parameters can now be described with most JSON-Schema constructs, #206, OCR-D/core#8483.18.0 - 2022-04-06Added:  Initial version of the OCR-D Web API, #1733.17.0 - 2022-02-14Added:  ocrd-tool.json: Support processors listing their own resources and restrict resource_locations, OCR-D/spec#181, OCR-D/spec#1903.16.0 - 2022-01-30Changed:  --page-id can accept the .. numerical range operator, #172, OCR-D/core#672  ocrd-tool.json: Parameters that accept a directory must have content-type == &quot;text/dirctory&quot;, #189, OCR-D/core#750, OCR-D/core#691Added:  German translation of the glossary, OCR-D/ocrd-website#2903.15.0 - 2021-12-07Changed:  mets:fileGrp/@USE must be valid xs:ID, #1853.14.0 - 2021-11-03Changed:  Resource lookup: for --location cwd look directly in &amp;lt;cwd&amp;gt;, no subdirectory, OCR-D/core#7273.13.0 - 2021-09-20Changed:  CLI: Logging should go to STDERR, parseable output to STDOUT, #183, OCR-D/core#7133.12.0 - 2021-01-26Changed:  Resource lookup: Remove XDG_CONFIG_HOME and XDG_CACHE_HOME  Resource lookup: Add /usr/local/share/ocrd-resources3.11.0 - 2021-01-20Changed:  Resource lookup in an intermediary ocrd-resources directory  Drop python-specific resource locations  Drop /usr/local/share resource location3.10.0 - 2020-12-02Changed:  Revise glossary, mostly by @bertsky3.9.1 - 2020-10-12Changed:  processor parameter values can be arrays, #1743.9.0 - 2020-07-21Changed:  CLI: Processors being called without valid METS file -&amp;gt; show help, #1563.8.0 - 2020-07-13Added:  Parameter JSON files may contain #-prefixed comments, #161  Processor resources, encompassing bundled/user-provided parameter JSON files and file parameter values like models, #158, #162  Mechanism for resolving file parameter values to actual filenames, #163  CLI: -P/--parameter-override to override single key-value pairs of parameter JSON, #166Changed:  mets:file representing page:AlternativeImage should not be added to separate mets:fileGrp but rather to the PAGE-XML whence they originate, #164  Recommendation how file IDs should be derived from existing mets:file, #164  CLI: -p/--parameter option repeatable, results are merged right to left, #161  METS: Simplify the convention for mets:file/@ID for derived images, #164  mets:fileGrp for prerprocessing steps should use the qualifier PRE instead of IMG, #164Removed:  Recommendations on fileGrp/@USE for images, #1643.7.0 - 2020-06-07Added:  ocrd-tool.json: Parameter values may be objects, #143  glossary: definitions of “print space” and “border”, #1143.6.0 - 2020-04-30Added:  CLI: --overwrite flag to delete existing output files before processing, #1513.5.0 - 2020-04-20Changed:  CLI: clarify requirements on processors, ht @bertsky, #148  Use region instead of block for areas on the page, #135  PAGE: imageFilename must NOT be a URL but a relative filename, #140  Updated URLs to point to https://ocr-d.de instead of https://ocr-d.github.io, #149Added:  docker: instructions on naming and labelling images, #139  CLI tools must implement -h/--help, #1153.4.2 - 2020-01-08Changed:  bagit-profile accepts metadata as non-payload dir, #133  Relaxed the requirement for the mets:fileGrp/@USE syntax, #1383.4.1 - 2020-01-03Added:  No multi-page TIFF, #1323.4.0 - 2019-11-05Fixed  Various typos, #128Changed:  Dockerfile: no CMD, no ENTRYPOINT, #130  Processors should assume 300 dpi if image metadata cannot be trusted, #129Added:  Spec for provenance, #1263.3.0 - 2019-10-23Added:  Draft spec for logging  Draft spec for provenanceChanged:  ocrd-tool: Additional additional category layout/segmentation/text-image  ocrd-tool: Remove syntactical restriction for content-type  ocrd-tool: output_file_grp no longer required  CLI: --mets and --working-dir are optional not required  CLI: --output-file-grp is optional, OCR-D/core#2963.2.1 - 2019-06-25Added:  glossary: “MP”, #112  glossary: “font family”, #100 #109  cli: allow JSON strings for -p, OCR-D/core#239 #110Fixed:  bagit: path of OcrdMets must be relative to /data, fix #107, #1133.2.0 - 2019-02-27Added:Convention for columnsFixed:PAGE: link to the page xml docs3.1.0 - 2018-12-20Added:  Consistency check level ‘lax’Fixed:  Example in ocrd_tool.md is from ocrd_kraken, not ocrd_tesserocr3.0.0 - 2018-12-13Added:  PAGE text result and consistency checks, #82, OCR-D/assets#16Changed:  :fire: Drop recommendation on reusing source file ID for page grouping  :fire: Drop GROUPID and replace with mets:structMap[@TYPE=”PHYSICAL”] throughout  :fire: CLI: Replace -g/-group-id with -g/--page-id  CLI: Mark possible comma-separated multi-value parameters as such  CLI: Update ocrd process example  OCRD-ZIP: Set BagIt-Profile-Version to 1.22.7.0 - 2018-12-04Added:  Font information, #76, #962.6.3 - 2018-11-23Changed:  OCRD-ZIP: Ocrd-Mets and mets:FLocat URI/paths must be relative to /data, #99  OCRD-ZIP: Ocrd-Mets only relevant for extraction  OCRD-ZIP: Filenames MUST be relative to mets.xml  METS: Filenames MAY/SHOULD be relative to mets.xml  OCRD-ZIP: Allow a limited set of files in the bag basedir (readme, build files), #972.6.2 - 2018-11-22Changed:  OCRD-ZIP bagit profile: Add empty list requirement for Tag-Manifest-Required, Tag-Files-Required  OCRD-ZIP bagit profile: Contact info  OCRD-ZIP allow fetch.txt, #982.6.1 - 2018-11-09Fixed:  OCRD-ZIP: typo in bagit-profile: Bagit- –&amp;gt; BagIt-  OCRD-ZIP: Require BagIt-Profile-Identifier  OCRD-ZIP: Version number must be a string, bagit-profile/bagit-profile#132.6.0 - 2018-11-06Changed:  Base workspace and workspace serialization mechanics on bagit, #702.5.0 - 2018-10-30Added:  Recording processing information in METS, #89  Input and output file groups can be provided in ocrd-tool.json, #91Changed  :fire: METS: grouping pages by physical structMap not GROUPID, #812.4.0 - 2018-10-19Added:  File parameters, #69  Step for post-correction, #642.3.1 - 2018-10-10Fixed  CLI: Example used repeated options2.3.0 - 2018-09-26Changed:  CLI: filtering by log level required, OCR-D/core#173, #74  CLI: log messages must adhere to uniform pattern, #78Added:  CLI: Convention to prefer comma-separated values over repeated flags, #682.2.2 - 2018-08-14Fixed:  Missed description for parameters2.2.1 - 2018-07-25Changed  spell out parameter properties in ocrd-tool.json schem2.2.0 - 2018-07-23Added:  CLI: Conventions for handling URL on the command line2.1.2 - 2018-07-19Added:  Reference PAGE media type in PAGE conventions, #652.1.1 - 2018-06-18Fixed:  ocrd-tool: regex for version had a YAML error2.1.0 - 2018-06-18Added:  ocrd-tool: Must define version  METS: mets:file must have ID  METS: mets:fileGrp must have consistent MIMETYPE  METS: mets:file GROUPID must be unique with a mets:fileGrp2.0.0 - 2018-06-18Removed:  –output-mets CLI option1.3.0 - 2018-06-15Added:  Glossary, #56Removed:  drop OCR-D-GT-PAGE, #61Fixed:  explain GT- prefix for fileGrp@USE of ground truth files, #58  various typos1.2.0 - 2018-05-25Fixed:  Fix example for ocrd_tool  Fix TIFF media typeAdded:  -J/–dump-json, #30Changed  ocrd-tool: tags -&amp;gt; category, #44  ocrd-tool: step -&amp;gt; steps (now an array), #44  ocrd-tool: parameterSchema -&amp;gt; parameters, #48  ocrd-tool: ‘tools’ is an object now, not an array, #431.1.5 - 2018-05-15Added:  ocrd-tool: Steps: preprocessing/optimization/grayscale_normalization and layout/segmentation/word  PAGE conventions1.1.4 - 2018-05-02Added:  PAGE/XML media type, #33  mets:file@GROUPID == pg:pcGtsId, #311.1.3 - 2018-04-28Added:  Add OCR-D-SEG-WORD and OCR-D-SEG-GLYPH as USE attributes1.1.2 - 2018-04-23Changed:  rename repo OCR-D/pyocrd -&amp;gt; OCR-D/core  rename repo OCR-D/ocrd-assets -&amp;gt; OCR-D/assets  renamed docker base image ocrd/pyocrd -&amp;gt; ocrd/coreFixed:  In ocrd_tool example: renamed parameter structure-level -&amp;gt; level-of-operation1.1.1 - 2018-04-19Fixed:  typo: exceutable -&amp;gt; executable  disallow custom properties1.1.0 - 2018-04-19Added  Spec for OCRD-ZIPChanged  Use executable instead of binary to reduce confusionFixed  typos (@stweil)Removed1.0.0 - 2018-04-16Initial Release",
      "url": " /en/spec/CHANGELOG.html"
    },
  

    {
      "slug": "en-spec-readme-html",
      "title": "Specification of the technical architecture, interface definitions and data exchange format(s)",
      "content"	 : "Specification of the technical architecture, interface definitions and data exchange format(s)See https://ocr-d.de/en/spec/.Translating the specThe specification is in English. To add a German translation of a file, replace the .md suffix with .de.md.Building JSON files.json files are built from the easier-to-edit .yml files.To regenerate the .json files after editing .yml files, runmake jsonThis requires python3 with the click and yaml libraries. To install the libraries runmake deps",
      "url": " /en/spec/README.html"
    },
  

    {
      "slug": "en-about-html",
      "title": "The OCR-D project",
      "content"	 : "The OCR-D projectBackgroundWith the Union Catalogue of Books of the 16th–18th century (VD 16, VD 17, VD 18) published in the German-speaking countries, a retrospective national bibliography of early modern writings from the German-speaking countries is being compiled. In order to facilitate research access to these texts, great concerted efforts have been and are being undertaken to make fully digitised copies or key pages for the recorded titles available in digital form.In view of the developments and new possibilities in the field of Optical Character Recognition (OCR), experts at a DFG workshop in March 2014 assessed the full-text transformation of VD as an ambitious but achievable goal. Making full texts available for the purpose of full-text search and further processing, for example with tools of the Digital Humanities, is a major desideratum of research, which is to be addressed by a coordinated funding initiative.OCR is a comprehensive process that typically involves a sequence of several steps in the workflow: Besides the pure recognition of letters and words, techniques such as pre-processing (image optimization and binarization), layout analysis (recognition and classification of structural features such as headings, paragraphs, etc.) and post-processing (error correction) are applied. While most of these steps can also benefit from the use of Deep Neural Networks, so far hardly any free and open standard tools and related best practices have emerged. The full text recognition of historical documents is particularly complicated due to their great variability in font, layout, language and orthography.Goals and structure of the OCR-D projectThis is where the DFG-funded project OCR-D comes in. Its main goal is the conceptual and technical preparation of the full text transformation of the VD. The task of automatic full-text recognition is broken down into its individual process steps, which can be retraced in the open source OCR-D software. This allows to create optimal workflows for the old prints to be processed and thus to generate scientifically usable full texts.For this purpose, a coordination project was formed that identified development needs in the first project phase. These were worked on in the second project phase by a total of eight module projects. In the current third project phase, the focus is on the conceptual preparation for the automatic generation of full texts for VD 16, VD 17 and VD 18. In addition, four implementation projects are working on integrating OCR-D into existing applications and infrastructures, while three module projects are further optimising OCR-D tools.Full-text recognition is understood as a complex process that includes several preprocessing and postprocessing steps in additionto the actual text recognition (see figure). First, a digital image is prepared for text recognition in preprocessing by binarization, cropping,deskewing, dewarping and despeckling. This is followed by layout recognition, which identifies the text areas of a page down to line level. Especially the recognition of the lines respectively the baseline is important for the following actual text recognition, which in all modern approaches is based on neural networks. The individual structures or elements of the full-text recognized document are then classified according to their typographic function and the OCR result is improved in the post-correction process if necessary, before it is transferred to repositories for long-term archiving.In addition to the envisaged full text transformation of VD titles (16th-19th century), which is technically and conceptually prepared within the OCR-D project, OCR-D pursues the following further objectives:  the creation of reference corpora for training and testing  the development of standards in the fields of metadata, documentation and ground truth  the further development of individual processing steps, with a particular focus on Optical Layout Recognition (OLR)  the analysis of existing tools and their further development  the creation of reusable software packages  the establishment of quality assurance proceduresCommunityIn all steps we welcome a lively exchange with colleagues from other projects and institutions as well as service providers, in order to finally be able to realize a consolidated procedure for the OCR processing of digitized material of the printed German cultural heritage of the 16th–19th century.To this end, there is already an active community, which you can join via our chat or our regular open online meetings.Interested parties from academia and practice are just as welcome as private individuals who (would like to) use OCR-D.On our website you will also find a collection of (scientific) publications and lectures on the topic of OCR(-D) by our current and former project participants.Results of past project phasesThe third phase of OCR-D is currently being finalised. Results of past project phases can be found on these pages at any time:  User survey [Phase I]  Module projects from Phase II  Pilot study [Phase II]",
      "url": " /en/about.html"
    },
  

    {
      "slug": "de-about-html",
      "title": "Das OCR-D-Projekt",
      "content"	 : "Das OCR-D-ProjektHintergrundMit den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.–18. Jahrhunderts (VD 16, VD 17, VD 18) wird eine retrospektive Nationalbibliografie des frühneuzeitlichen Schriftguts aus dem deutschsprachigen Raum erstellt. Um der Forschung die Zugänglichkeit zu diesen Texten zu erleichtern, wurden und werden große, konzertierte Anstrengungen unternommen, Volldigitalisate oder Schlüsselseiten zu den einzelnen verzeichneten Titeln digital bereitzustellen.Mit Blick auf die Entwicklungen und neuen Möglichkeiten im Bereich der Optical Character Recognition (OCR) haben Experten im März 2014 im Rahmen eines DFG-Workshops die Volltexttransformation der VD als ambitioniertes, aber erreichbares Ziel eingeschätzt. Die Verfügbarmachung von Volltexten zum Zweck der Volltextsuche und Weiterbearbeitung, bspw. mit Werkzeugen der Digital Humanities, ist ein großes Desiderat der Forschung, das durch eine koordinierte Förderinitiative zu bearbeiten ist.OCR ist ein umfassender Prozess, der typischerweise eine Abfolge von mehreren Schritten im Workflow beinhaltet: Neben der reinen Erkennung von Buchstaben und Wörtern werden Techniken wie die Vorverarbeitung (Bildoptimierung und Binarisierung), die Layoutanalyse (Erkennung und Klassifizierung von Strukturmerkmalen wie Überschriften, Absätzen usw.) und die Nachbearbeitung (Fehlerkorrektur) angewendet. Während die meisten dieser Schritte auch von der Nutzung von Tiefen Neuronalen Netzen profitieren können, sind bisher kaum freie und offene Standardwerkzeuge und damit verbundene Best Practices entstanden. Die Volltexterkennung historischer Dokumente wird insbesondere durch deren große Variabilität in Schriftart, Layout, Sprache und Orthographie erschwert.Ziele und Aufbau des OCR-D-ProjektsHier setzt das DFG-geförderte Projekt OCR-D an, dessen Hauptziel die konzeptionelle und technische Vorbereitung der Volltexttransformation der VD ist. Die Aufgabe der automatischen Volltexterkennung wird in ihre einzelnen Prozessschritte zerlegt, die in der Open Source OCR-D-Software nachvollzogen werden können. Dies ermöglicht es, optimale Workflows für die zu prozessierenden alten Drucke zu erstellen und damit wissenschaftlich verwertbare Volltexte zu generieren.Dazu wurde ein Koordinationsprojekt gebildet, das in der ersten Projektphase Entwicklungsbedarfe identifizierte. Diese wurden in der zweiten Projektphase von insgesamt acht Modulprojekten bearbeitet. In der derzeitigen dritten Projektphase steht die konzeptionelle Vorbereitung für die automatische Generierung von Volltexten für die Verzeichnisse der im deutschen Sprachraum erschienenen Drucke des 16., 17. und 18. Jahrhunderts im Fokus. Außerdem arbeiten vier Implementierungsprojekte daran, OCR-D in bestehende Anwendungen und Infrastrukturen zu integrieren, während drei Modulprojekte OCR-D-Werkzeuge weiter optimieren.Volltexterkennung wird dabei als ein komplexer Prozess aufgefasst, der neben der eigentlichen Texterkennung mehrere vor- und nachgelagerte Schritte einschließt (vgl. Abbildung). Zunächst wird ein Bilddigitalisat im Preprocessing für die Texterkennung aufbereitet, indem es nach Bedarf in ein Schwarz-Weiß-Bild umgewandelt (Binarization), zugeschnitten (Cropping), begradigt (Deskewing), entzerrt (Dewarping) und von Flecken bereinigt (Despeckling) wird. Im Anschluss erfolgt die Layouterkennung, die die Textbereiche einer Seite bis auf Zeilenebene identifiziert. Besonders die Erkennung der Zeilen bzw. der Grundlinie ist wichtig für die anschließende eigentliche Texterkennung, die in allen modernen Ansätzen auf Neuronalen Netzen beruht. Danach werden die einzelnen Strukturen bzw. Elemente des volltexterkannten Dokuments ihrer typografischen Funktion nach klassifiziert und das OCR-Ergebnis ggf. in der Nachkorrektur verbessert, bevor es in Repositorien zur Langzeitarchivierung überführt wird.Neben der anvisierten Volltexttransformation von VD-Titeln (16.–19. Jahrhundert), die im Rahmen des OCR-D-Projekts technisch und konzeptionell vorbereitet wird, verfolgt OCR-D die folgenden weiteren Ziele:  die Erstellung von Referenzkorpora zum Trainieren und Testen  die Erarbeitung von Standards in den Bereichen Metadaten, Dokumentation und Ground Truth  die Weiterentwicklung einzelner Verarbeitungsschritte, wobei der Fokus insbesondere auf der Optical Layout Recognition (OLR) liegt  die Analyse vorhandener Tools und deren Weiterentwicklung  die Erstellung nachnutzbarer Softwarepakete  die Erstellung von Verfahren der QualitätssicherungCommunityIn allen Schritten begrüßen wir einen regen Austausch mit Kolleginnen und Kollegen aus anderen Projekten und Einrichtungen sowie Dienstleistern, um schließlich ein konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des gedruckten deutschen Kulturerbes des 16.–19. Jahrhunderts realisieren zu können. Zu diesem Zweck gibt es bereits eine aktive Community, die unter anderem über unseren Chat oder unsere regelmäßig stattfindenden Onlinemeetings erreicht werden kann. Interessierte aus Wissenschaft und Praxis sind ebenso willkommen wie Privatpersonen, die OCR-D nutzen (möchten).Auf unserer Website finden Sie außerdem eine Sammlung von (wissenschaftlichen) Publikationen und Vorträgen zum Thema OCR(-D) unserer aktuellen und ehemaligen Projektbeteiligten.Ergebnisse vergangener ProjektphasenDie dritte Projektphase befindet sich aktuell in der Nachbereitung. Ergebnisse der vergangenen Projektphasen können Sie auf diesen Seiten jederzeit nachlesen:  Nutzer*innenumfrage [Phase I]  Modulprojekte aus Phase II  Pilotstudie [Phase II]",
      "url": " /de/about.html"
    },
  

    {
      "slug": "de-api-html",
      "title": "Web Api",
      "content"	 : "Web APIWhat the Web API isWhy we need a Web APIThe Web API is an important increment for implementers to be able to base their outward-facing HTTP interfaces on a common API definition, so that the different implementations are interoperable on this level.Basic ArchitectureWorkflowsWhat is NF and why did we choose it?Nextflow is a workflow framework that allows the integration of various scripting languages into a single cohesive pipeline. Nextflow also has its own Domain Specific Language (DSL) that extends Groovy (extension of Java).We choose it due to its rich set of features:  Stream oriented: Promotes programming approach extending Unix pipes model.  Fast Prototyping: Let’s you write a computational pipeline from smaller tasks.  Reproducibility: Supports Docker, Singularity, and 3 other types of containers.  Portable: Can run locally, Slurm, SGE, PBS, and cloud (Google, Kubernetes, and AWS).  Continuous checkpoints: Each process in the workflow is checkpointed. It is possible to retry failed workflows and start from the last checkpoint.  Supports various scripting languages including Bash, Python, Perl, and others.  Enables separation between configuration (how to do) and workflow logic (what to do).  Modularization of tasks possible via workflow, sub-workflows, and processes.  Provides detailed logs and various types of execution reports.How is the NF script structured?The NF script contains the following structures:DSL and ParametersDefinition of processesDefinition of workflowsMain workflowCheck this source code example: seq_ocrd_wf_many.nfWhich features of NF do we use, i.e. what features have to be implemented in potential implementations?The minimally used features for local runs are the parameters, processes, process decorators, and workflows. I will provide further answers to any following questions related to this main question. I am not sure what else to cover here for now.How does parallelization work, both within works and across works?A Nextflow workflow script contains several processes. Processes are executed independently and are isolated from each other (i.e. they do not have a shared memory space). Communication between the processes is possible only through data channels (similar to the pipes model in Unix). These channels are basically asynchronous FIFO queues. Any process can define one or more channels as input and output. The order of interaction between these processes, and ultimately the order of workflow execution depends on the communication channel dependencies between processes. For example, if process A writes data to channel A and process B reads data from channel A, then Nextflow knows that process A must be executed before process B.Check this source code example: seq_ocrd_wf_many.nfTODO: I will provide more parallelization details here based on the example above.How does the NF script interact with the processing server?There is still no running processing server. More details will be announced once there is more to talk about. The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.How does the NF script interact with the METS server?There is still no running METS server. More details will be announced once there is more to talk about. The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.How to convert the existing OCR-D process workflows we reference to NF?I have written an OtoN (OCR-D to Nextflow) converter which converts basic OCR-D process workflows to Nextflow workflow scripts. Check here: OtoNKeep in mind that I have just started working on the converter. It is still very fresh and there are still known problems (related to the produced Nextflow scripts) and I am trying to fix them. It is also not convenient to use (no proper CLI). Neither it has usage instructions. Stay tuned for more updates.I have tested most edge cases for the lexer/parser of the OCR-D process file while implementing. There may be input OCR-D process files that are not handled well enough. Feel free to report any bugs, errors, or lack of errors (when an error is expected).The tool will probably be a part of the OCR-D software in the future when it is stable enough for general use.How should NF scripts be written, tested, deployed, and evaluated?Depends on the use case. Detailed instructions for local executions and example Nextflow workflow scripts can be found here: NextflowI will provide further answers to any following questions related to this main question.What conventions do we encourage, naming, structure, documentation, etc.?Try to stick to the structure provided in point 2 when writing Nextflow scripts. You can also check the Nextflow examples provided in point 8. The naming conventions for variables, function names, process names, and workflow names are encouraged to follow the snake case. I will provide further answers to any following questions related to this main question.",
      "url": " /de/api.html"
    },
  

    {
      "slug": "en-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /en/blog.html"
    },
  

    {
      "slug": "de-blog-html",
      "title": "OCR-D Blog",
      "content"	 : "",
      "url": " /de/blog.html"
    },
  

    {
      "slug": "en-spec-cli-html",
      "title": "Command Line Interface (CLI)",
      "content"	 : "Command Line Interface (CLI)All tools provided by MP must be standalone executables, installable into $PATH.Those tools intended for run-time data processing (but not necessarily tools for training or deployment) are called processors.Processors must adhere to the following uniform interface, including mandatory and optional parameters (i.e. no more or fewer are permissible).NOTE: Command line options cannot be repeated, except those explicitlymarked as REPEATABLE (e.g. -p params.json -p &#39;{&quot;val&quot;: 1}&#39; is allowedbecause -p is repeatable.NOTE: Parameters marked MULTI-VALUE cannot be repeated but can specifymultiple values, formatted as a single string with comma-separated items (e.g.-I group1,group2,group3 instead of -I group1 -I group2 -I group3). Due tocomma being used as a separator (and not allowed in METS identifiers), the values themselves must not contain commas.NOTE: Parameters marked RANGE support the numeric range operator .. togenerate all values between the start and end value by incrementing the numericpart of the string.NOTE: Parameters marked REGEX support the regular expression operator // togenerate all values matching the pattern after that prefix.NOTE: For parameters which allow for a combination of multi-value, range and/or regex,the operator precedence is  MULTI-VALUE &amp;gt; REGEX &amp;gt; RANGE.CLI executable nameEvery CLI executable’s name must begin with ocrd-.Examples:  ocrd-kraken-binarize  ocrd-tesserocr-recognizeNo parametersIf no arguments are passed to a processor, it must show the --helpmessage message and exit with return code 1.Mandatory parameters-I, --input-file-grp GRPMULTI-VALUEMETS file group(s) used as input.Input file groups must not be modified.Optional parameters-O, --output-file-grp GRPMULTI-VALUEMETS file group(s) used as output.Omit to resort to default output file groups of the processor, or for processors that inherently do not produce output files.-g, --page-id IDMULTI-VALUE RANGE REGEXThe mets:div[@TYPE=&#39;page&#39;]/@ID that contains the mets:fptr/@FILEID pointersto files representing a page. Effectively, only those files in the input filegroup that are referenced in thesemets:div[@TYPE=&quot;page&quot;] will be processed.Omit to process all pages.--overwriteDelete files in the output file group(s) before processing.If --overwrite is set, but --page-id is not set, deleteall output file groups set with--output-file-grp, including all files that belong tothose file groups.If --overwrite is set and --page-id is set, delete all files that representany of the page IDs given with --page-id from all outputfile groups set with--output-file-grp“File deletion” in the context of --overwrite means deletion of matchingmets:file elements from the METS document and all local files thesemets:file represent.“Group deletion” in the context of --overwrite means deletion of themets:fileGrp element from METS, and deletion of all files that belong to thismets:fileGrp element.-p, --parameter PARAM_JSONREPEATABLEURL of parameter file in JSON format corresponding to theparameters section of the processor’s ocrd-tool metadata. Ifthat file is not readable and PARAM_JSON begins with { (opening brace), tryto parse PARAM_JSON as JSON. If that also fails, throw an exception.When parsing JSON, all lines matching the regular expression ^s*#.*, i.e.lines whose first non-whitespace character is #, are to be disregarded ascomments.When -p is repeated, the parsed values should be shallowly merged from rightto left.-p can be omitted to use default parameters only, or for processors withoutany parameters.-P, --parameter-override KEY VALREPEATABLECompanion to -p, --parameter PARAM_JSON that allows overriding KEY in the parameter JSON object with VAL. All P key-value-pairs are applied to the parameter JSON in the order they are given on the command line.Syntactically, VAL SHOULD be a valid JSON datatype:  &quot;a string&quot; - a string should be enclosed with double quotes, contained double quotes backslash-escaped  123 - an int  123.45 - a float  true, false - a boolean  [1, &quot;two&quot;, 3.33] - an array  {&quot;foo&quot;: 42} - an objectAs a convenience, if VAL fails to parse as a valid JSON type, it isinterpreted as a string (a string is equivalent to &quot;a string&quot;, but truewill be parsed as the boolean value true, not the string &quot;true&quot;).-m, --mets METS_INInput METS URL. Default: mets.xml-w, --working-dir DIRWorking Directory. Default: current working directory.-l, --log-level LOGLEVELSet the global maximum verbosity level. More verbose log entries will beignored. (One of OFF, ERROR, WARN, INFO (default), DEBUG, TRACE).NOTE: Setting the log level via --log-level parameter should override anyother implementation-specific means of logging configuration. For example, with--log-level TRACE no log messages should be filtered globally, whereas--log-level ERROR, only errors should be output globally.-J, --dump-jsonInstead of processing METS, output the ocrd-tool description forthis executable, in particular its parameters.-C, --show-resource FILENAMEPrint the contents of processor resource FILENAME. Look up the resp. absolute filename according to the file parameter lookup rules.-L, --list-resourcesList the names of processor resources in all of the paths defined by.the file parameter lookup rules, one name per line.-h, --helpPrint a concise description of the tool, the command line options andparameters it accepts as well as the input/output groups. This information shouldbe generated from ocrd-tool.json as much as possible.Return valueSuccessful execution should signal 0. Any non-zero return value is considered a failure.LoggingData printed to STDERR is captured linewise and stored as log data.Processors must adjust logging verbosity according to the --log-level parameter.The log messages must have the format TIME LEVEL LOGGERNAME - MESSAGEn, where  TIME is the current time in the format HH:MM:ss.mmm, e.g. 07:05:31.007  LEVEL is the log level of the message, in uppercase, e.g. INFO  LOGGERNAME is the name of the logging component, such as the class name. Segments of LOGGERNAME should be separated by dot ., e.g. ocrd.fancy_tool.analyze  MESSAGE is the message to log, should not contain new lines.  n is ASCII char 0x0a (newline)Processor resourcesParameters that reference files can be resolved from relative to absolutefilename by following the conventions laid out in the ocrd_toolspec. These files, either bundled by the processordeveloper or put in place by the user, are called processor resources. Theprocessor resources of a processor can be listed with the -L/--list-resourcesoption and individual processor resources can be retrieved with the-C/--show-resource option. Since processor resources use the same mechanismas file parameters, they can be used  as the argument to the -p/--parameter option (i.e. a preset file), and  as the value of a file-type parameter (e.g. a model file)and the processor must resolve them to absolute paths according to the rulesfor file parameters.URL/file conventionWhenever a URL is expected, it should be possible to use a local file pathinstead and have the implementation interpret as a file:// URL on the fly.Implementations should adhere to this algorithm when resolving a URL u:  If u contains the string ://: Do not modify.  If u is an absolute path according to the mechanics of the underlying file system: Prepend file:// to u.  Otherwise: Resolve u as a path relative to the current working directory, prepend file:// to u.NOTE: This convention is limited to the CLI for convenience of users anddevelopers. In METS and PAGE documents, URLs must be strictly valid andresolvable by common software agents as-is.ExampleThis is how the CLI provided by the MP should work:$&amp;gt; ocrd-kraken-binarize     --mets &quot;file:///path/to/file/mets.xml&quot;     --working-dir &quot;file:///path/to/workingDir/&quot;     --parameters &quot;file:///path/to/file/parameters.json&quot;     --page-id PHYS_0001,PHYS_0002,PHYS_0003     --input-file-grp OCR-D-IMG    --output-file-grp OCR-D-IMG-BIN-KRAKENAnd this is how it will be called with the ocrd process CLI:$&amp;gt; ocrd process     &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN-KRAKEN -p /path/to/file/parameters.json&#39;    -m &quot;file:///path/to/file/mets.xml&quot;     -g PHYS_0001,PHYS_0002,PHYS_0003    preprocessing/binarization/kraken-binarizeMETS input&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;Input JSON parameter file{    &quot;threshold&quot;: 0.05,    &quot;zoom&quot;: 2,    &quot;range&quot;: [5, 10],}METS outputThis is the METS file after being run through the MP CLI:&amp;lt;mets:mets&amp;gt;    &amp;lt;!-- ... --&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div DMDID=&quot;DMDPHYS_0000&quot; ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0002&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0002&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;PHYS_0003&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0003&quot;/&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000001.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0002&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000002.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG_0003&quot; MIMETYPE=&quot;image/tiff&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;https://github.com/OCR-D/spec/raw/master/io/example/00000003.tif&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG-BIN-KRAKEN&quot;&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0001&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0001.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0002&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0002.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;      &amp;lt;mets:file ID=&quot;OCR-D-IMG-BIN-KRAKEN_0003&quot; MIMETYPE=&quot;image/png&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;URL&quot; xlink:href=&quot;file:///tmp/ocrd-workspace-ABC123/0003.png&quot; /&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /en/spec/cli.html"
    },
  

    {
      "slug": "en-community-html",
      "title": "OCR-D Community",
      "content"	 : "OCR-D CommunityIn OCR-D, we strive for maximum transparency and communication of our workinternally as well as externally. The project is active on various platforms,which are mostly open to the professional public to read and participate.Events and regular CallsOCR-D TechCall – Take part in our technical discussions about OCR-D tools and issues; every third Wednesday in the month, 2 to 3 PM (Berlin Time):Link&amp;nbsp;to&amp;nbsp;agenda/minutesOCR-D in der Praxis – Discuss the practical deployment of OCR-D; every second Friday, 1 to 2 PM (Berlin Time): Link&amp;nbsp;to&amp;nbsp;agenda/minutesParticipation and communicationPlatformsGitter – Follow up on our discussions and ask your own questions: We use the chat platform Gitter for short-term and low-threshold communication with each other:https://gitter.im/OCR-D/Lobby GitHub – Try it out yourself: On GitHub you can find the latest status of our developmental work on the OCR-D framework and the module projects. In case of problems, open an issue or participate directly with pull requests! https://github.com/OCR-DWiki – Tutorials and more from and for the community:https://github.com/OCR-D/ocrd-website/wikiGitHubMore repositories at https://github.com/OCR-DData and publicationsGround Truth Repository – Make use of our data: We collect our test and reference data in the Ground Truth Repository. You may use the data for your own training: https://ocr‑d.de/en/dataTechnology Watch – In the Zotero group of OCR-D we collect relevant literature about OCR together with interested parties:Zotero list",
      "url": " /en/community.html"
    },
  

    {
      "slug": "de-community-html",
      "title": "OCR-D Community",
      "content"	 : "OCR-D CommunityIn OCR-D bemühen wir uns um maximale Transparenz und Kommunikation unserer Arbeit nach innen und außen. Das Projekt ist auf verschiedenen Plattformen aktiv,die größtenteils für die Fachöffentlichkeit offen sind zum Lesen und Mitmachen.Veranstaltungen und regelmäßge VideokonferenzenOCR-D TechCall – Nehmen Sie an unseren technischen Diskussionen über OCR-D-Werkzeuge und -Themen teil; jeden dritten Mittwoch im Monat, 14 bis 15 Uhr:Link,&amp;nbsp;Agenda&amp;nbsp;und&amp;nbsp;ProtokollOCR-D in der Praxis  – Nehmen Sie an unseren Diskussionen zum praktischen Einsatz von OCR-D teil; jeden zweiten Freitag, 13 bis 14 Uhr:Link,&amp;nbsp;Agenda&amp;nbsp;und&amp;nbsp;ProtokollMitwirken und MitredenPlattformenGitter – Verfolgen Sie unsere Diskussionen und stellen Sie Ihre eigenen Fragen: Für die schnelle und niederschwellige Kommunikation untereinander nutzen wir den Gitter-Chat:  https://gitter.im/OCR-D/Lobby GitHub – Probieren Sie es selbst aus: Auf GitHub finden Sie den aktuellen Stand unserer Entwicklungsarbeit am OCR-D Framework und den Modulprojekten. Öffnen Sie bei Problemen ein Issue oder wirken Sie direkt mit, indem Sie ein Pull Request öffnen! https://github.com/OCR-DWiki – Tutorials und mehr von der und für die Community:https://github.com/OCR-D/ocrd-website/wikiGitHubMehr Repositorien unter https://github.com/OCR-DDaten und PublikationenGround Truth Repository – Nutzen Sie unsere Daten: Wir sammeln unsere Test- und Referenzdaten im Ground Truth Repository. Sie können diese Daten gerne auch für Ihr eigenes Training nutzen: https://ocr‑d.de/de/dataTechnology Watch – In der Zotero-Gruppe von OCR‑D sammeln wir gemeinsam mit Interessierten relevante Literatur über OCR:Zotero-Liste",
      "url": " /de/community.html"
    },
  

    {
      "slug": "en-contact-html",
      "title": "OCR-D is you and me",
      "content"	 : "OCR-D is you and meContactContact person:Leonie EckertProject coordination OCR-DHerzog August Library Wolfenbütteleckert[at]hab.deSubstitution:Sandra SimonDeputy head of departmentNew Media, Digital LibraryHerzog August Library Wolfenbüttelsimon[at]hab.deJohannes MangeiDeputy Director of the Herzog August Library WolfenbüttelHead of Department New Media, Digital LibraryHerzog August Library Wolfenbüttelmangei[at]hab.deManagersJohannes MangeiHerzog August Library Wolfenbüttelmangei[at]hab.deAlexander GeykenBerlin-Brandenburg Academy of Sciences and Humanities in Berlingeyken[at]bbaw.deReinhard AltenhönerState Library of Berlin Prussian Cultural Heritagereinhard.altenhoener[at]sbb.spk-berlin.deMustafa DoganGöttingen State and University Librarydogan[at]sub.uni-goettingen.dePhilipp WiederGesellschaft für wissenschaftliche Datenverarbeitung Göttingenphilipp.wieder[at]gwdg.deStaffLeonie EckertProject coordinationHerzog August Library Wolfenbütteleckert[at]hab.deKonstantin BaiererBerlin State Library - Prussian Cultural Heritagekonstantin.baierer[at]sbb.spk-berlin.deClemens NeudeckerBerlin State Library - Prussian Cultural Heritage clemens.neudecker[at]sbb.spk-berlin.deKristine Schima-VoigtNiedersächsische Staats- und Universitätsbibliothek Göttingenschima-voigt[at]sub.uni-goettingen.deMichelle WeidlingGöttingen State and University Libraryweidling[at]sub.uni-goettingen.dePaul PestovGöttingen State and University Librarypestov[at]sub.uni-goettingen.deTriet DoanGesellschaft für wissenschaftliche Datenverarbeitung Göttingentriet.doan[at]gwdg.deMehmed MustafaGesellschaft für wissenschaftliche Datenverarbeitung Göttingenmehmed.mustafa[at]gwdg.deJonas SchreweGesellschaft für wissenschaftliche Datenverarbeitung Göttingenjonas.schrewe[at]gwdg.deFormer ManagersRainer StotzkaKarlsruhe Institute of TechnologyThomas StäckerDuke August Library WolfenbüttelMarkus BrantlBavarian State Library MunichFormer staffMatthias BoenigBerlin-Brandenburg Academy of Sciences and Humanities in BerlinElisabeth EnglHerzog August Library WolfenbüttelMareen GeestmannGöttingen State and University LibraryVolker HartmannKarlsruhe Institute of Technology/Steinbuch Centre for ComputingElisa HerrmannHerzog August Library WolfenbüttelLena HinrichsenHerzog August Library WolfenbüttelSebastian MangoldBavarian State Library MunichAjinkya PrabhuneKarlsruhe Institute of TechnologyKay-Michael WürznerBerlin-Brandenburg Academy of Sciences and Humanities in BerlinScientific Advisory BoardThe project was counseled by a scientific advisory board constituted by the following:Max KaiserAustrian National LibraryJoachim KöhlerFraunhofer IAISGerhard LauerJGU MainzSebastian MeyerSLUB DresdenGünter MühlbergerUniversity of InnsbruckKlaus SchulzLMU Munich – CISThomas StäckerULB DarmstadtRobert StrötgenTU BraunschweigCooperation PartnersThe project has signed Letters of Intent with several companies, non-profit organizations and related projects,mainly in order to exchange ideas and information on interfaces for and current developments in mass digitization.Furthermore, those cooperations are part of OCR-D’s dissemination strategy, which will gain more and more prominenceas the project continues and the OCR-D-software is developing into an operable, stabilized product.  Content Conversion Specialists (CCS)  Kitodo. Key to digital objects e.V.  OCR4all, GitHub  semantics Kommunikationsmanagement GmbH  Zeutschel GmbH",
      "url": " /en/contact.html"
    },
  

    {
      "slug": "en-data-html",
      "title": "OCR-D data",
      "content"	 : "DataReference DataThe reference data includes a Ground Truth corpus and other special corpora.The Ground Truth corpus contains pages from publications printed between 1500and 1900. The content of the corpus is based on a particular selection from theholdings of the DFG project “German TextArchive”, the Digitized Collections of theStaatsbibliothek zu Berlin and theWolfenbüttel DigitalLibrary ofthe Duke August library. The holdings of projects and digital collections ofother libraries as well as additional Ground Truth data, which are compiledtogether with module projects, can be included in the corpus as specialextensions in concertation with the OCR-D coordination project. If additionalannotations or texts are necessary, these can be created in consultation withthe OCR-D coordination project.Depth of Annotation, Text Accuracy and ArtifactsThe Ground Truth corpus offers three annotation depths:  Structural regions, text lines, word coordinates  Structure regions, text lines  Text linesOverviewThe special corpora contain:  a corpus of data with lower text accuracy (dirty OCR), which can be used for individual comparisons and evaluations  a corpus of artifacts with objects that show disturbancesOverviewCreation of the Ground TruthThe image data were first subjected to a layout analysis (text region and line recognition) using Transkribus and then segmented automatically. The automatically recognized text as well as the lines and text regions were corrected manually. Finally the data in form of PAGE files, digital images and METS files were zipped as a BagIt file.If you are interested in further Ground Truth data (e.g. for binarization) please contact us: eckert[at]hab.deThe data are subject to a CC-BY-SA license, for the use of the image datadifferent licenses may exist. Please contact the project and/or the owning library.OCR-D Research Data RepositoryThe OCR-D research data repository collects all versions of documents and (intermediate) results created during the document analysis. It contains at least the end results of every processed document. During the ingest much metadata about the document will be extracted and made available for search/filter (e.g. identifier(s), title, classification(s), genre(s), semantic label(s), used processor(s), text). In future, there may also be export options for specific tools.OCR-D Ground Truth RepositorySimilarly, there is a publicly available OCR-D repository, which contains all ground truth data provided by OCR-D.  For further information about the metadata format, see https://github.com/OCR-D/gt-labelling  The repository itself is available at https://github.com/OCR-D/gt_structure_text/releases",
      "url": " /en/data.html"
    },
  

    {
      "slug": "de-daten-html",
      "title": "Daten",
      "content"	 : "DatenDie ReferenzdatenDie Referenzdaten umfassen ein Ground-Truth-Korpus und weitere Spezialkorpora.Das Ground-Truth-Korpus umfasst Seiten aus Publikationen aus dem Zeitraum 1500–1900. Der Inhalt des Korpus basiert auf einer gezielten Auswahl aus demBestand des DFG-Projektes „Deutsches Textarchiv“, der DigitalisiertenSammlungen der Staatsbibliothek zu Berlin und der Wolfenbütteler DigitalenBibliothek der Herzog August Bibliothek. Bestände von Projekten und digitalenSammlungen anderer Bibliotheken sowie zusätzliche Ground-Truth-Daten, diezusammen mit Modulprojekten erarbeitet werden, können in Abstimmung mit demOCR-D-Koordinierungsgremium in das Korpus als spezielle Erweiterungenaufgenommen werden. Sollten zusätzliche Annotationen oder Texte notwendig sein,können diese in Abstimmung erstellt werden.Annotationstiefe, Textgenauigkeit und ArtefakteDas Ground-Truth-Korpus bietet drei Annotationstiefen an:  Strukturregionen, Textzeilen, Wortkoordinaten  Strukturregionen, Textzeilen  TextzeilenZur ÜbersichtDie Spezialkorpora umfassen:  Spezialkorpus von Daten geringerer Textgenauigkeit (schmutzige OCR), kann für einzelne Vergleiche und Evaluationen herangezogen werden.  Spezialkorpus Artefakte: Dieses Korpus beinhaltet ausschließlich Objekte die Störungen aufweisen.Zur ÜbersichtErstellung des Ground TruthDie Image-Daten wurden mittels Transkribus und Aletheia zunächst einer Layout-Analyseunterzogen und anschließend automatisiert segmentiert. Der so automatisch segmentierte Text (Wörter) sowie die Zeilen und Textregionen wurden manuell bearbeitet. Abschließend wurde ein Datenpaket aus den Daten im PAGE-Format, den digitalen Bildern und einem METS-Metadatensatz als Bagit-Datei gepackt.Wenn Sie Interesse an weiteren Ground-Truth-Daten haben (bspw. zurBinarisierung) schreiben Sie uns bitte: eckert[at]hab.deDie Daten unterliegen einer CC-BY-SA-Lizenz, für die Verwendung der Bilddatenkönnen abweichende Lizenzen vorliegen. Bitte kontaktieren Sie diesbezüglich dasProjekt und/oder die besitzende Bibliothek.OCR-D ForschungsdatenrepositoriumDer OCR-D-Forschungsdatenspeicher sammelt alle Versionen von Dokumenten und (Zwischen-)Ergebnissen, die während der Dokumentenanalyse erstellt wurden. Es enthält mindestens die Endergebnisse jedes verarbeiteten Dokuments. Während der Aufnahme werden viele Metadaten über das Dokument extrahiert und für die Suche/Filterung zur Verfügung gestellt (z.B. Identifizierer, Titel, Klassifikation(en), Genre(s), semantische Bezeichnung(en), verwendete Prozessor(en), Text). In Zukunft wird es möglicherweise auch Exportoptionen für bestimmte Werkzeuge geben.OCR-D Ground Truth RepositoriumEbenso gibt es ein öffentlich zugängliches OCR-D Repositorium, das alle von OCR-D bereitgestellten Ground Truth Daten enthält.  Weitere Informationen über das Metadatenformat finden Sie unter https://github.com/OCR-D/gt-labelling  Das Repository selbst ist unter https://github.com/OCR-D/gt_structure_text/releases verfügbar.",
      "url": " /de/daten.html"
    },
  

    {
      "slug": "en-spec-decisions-html",
      "title": "Decisions in OCR-D",
      "content"	 : "Decisions in OCR-DIn a software project, especially a highly distributed one like OCR-D,decisions need to be made on the technology used, how interfaces shouldinteroperate and how the software as a whole is designed.In this document, such decisions on key aspects of OCR-D are discussed for thebenefit of all OCR-D stakeholders.Terminology  current refers to August 1, 2024, the last change of this document  Q1-Q4 refers to yearly quarters  target version is the version we mainly test and develop for  supported version means that we test this version and ensure compatibilityGeneral decisions  [Q2 2023] We will update to Ubuntu 22.04 and Python 3.8 as soon as possible. OCR-D/core#956  [Q3 2024] Switch to Slim Containers in ocrd_all  [Q3 2024] Python API changes (Pagewise processing) OCR-D/core#322Workflow format  [Q3 2024] We use ocrd process, a simplified shell syntax.Web API  [Q3 2023] Switch to the new architecture with message queue.          Processing Broker and Processing Server will be provided via OCR-D Core.        [2022] OCR-D Coordination Project provides the Web API spec and implementation.QUIVER  [2022] We will create a web application, QUIVER (for QUalIty oVERview), inwhich some information about OCR-D processors are provided:          a general overview of the projects (i.e. GitHub repositories), e.g. if their ocrd-tool.json is valid, when their last release has been made etc.      a workflow section where we benchmark different workflows for different corpora.      a general overview of the available processors      Benchmarking  [2022] To execute the benchmarking, we will create several corpora with different characteristics (font, creation date, layout, …) and run different workflows with these as input. The result is then displayed in the QUIVER workflow tab.The corpora will be publicly available for better transparency.  [2022] Relevant metrics for the minimum viable product (MVP) will be:          CER      WER      Bag of Words      Reading order      IoU      CPU time      wall time      I/O      Memory Usage      Disc usage        [2022] The benchmarking will be executed automatically in a regular interval to measure if changes in the processors improve the result.This might be done via CI, GitHub Actions or as a CRON job on a separate server.OCR-D/coreMETS server  The current approach to file management requires processors accessing a single METS file on disk, which turns file management into a bottleneck for workflows. To alleviate this, we will develop an HTTP server that provides asynchronous and parallel access to the METS in Q4 2022.Decentralized resource list  We currently maintain a list of processor resources centrally in OCR-D/core.  In Q3 2022, to allow processor developers to maintain their own separate list of resources, we have implemented mechanisms to store resource lists in a processor’s ocrd-tool.json and bundle resources in their own module directory.  By Q1 2023 we should have updated all the processors and whittled down the central list to a mostly empty list.Page-wise processing  Currently, processors iterate through the files of a workspace by looping through all the files in the input file group(s) themselves.  In Q3 2024 we will refactor the processor API, deprecate the currentapproach of processors iterating in a process method and enable processorsto process individual pages in a process_page method.ocrd_all Docker deployment  Our current target container is a fat container, with maximum,medium and minimum versions with decreasing amount of processorscontained.  We will wrap processor projects individually and transition to slim containers in Q3 2024.Supported Python versions  Our current target version for Python is 3.8, 3.9 - 3.10 are fully supported, 3.11 and 3.12 partially (not all libraries of all processors supported, but core is).  Support for 3.6 will end Q4 2022. We will not test and include Python 3.6 after that.  We will start to support 3.11 in Q1 2023.  We will start to support 3.12 in Q2 2023 (:warning: won’t have distutils anymore)  Support for 3.7 will end Q2 2023.  Support for 3.8 will end Q4 2024.  Support for 3.9 will end Q1 2025.Base OS image  Our current base image for deployment is Ubuntu 20.04, we support Ubuntu 18.04, 20.04 and 22.04.  We will change the base image to Ubuntu 20.04 in Q4 2022.  We will change the base image to Ubuntu 22.04 in Q4 2024.  Support for Ubuntu 18.04 will end in Q1 2023.  Support for Ubuntu 20.04 will end in Q1 2025.Software librariescalamari  Our currently supported calamari version is 1.x.      We will switch to 2.x in Q1 2023.    Support for 1.x will end in Q1 2022pillow  We currently support Pillow 5.x to v10.xtensorflow  Our target version is 2.5.0  We currently support 1.15.x, 2.4.0 and 2.5.0. While we stronglyencourage moving away from 1.15.x, due to the logistics of updatingtrained models, we don’t have a fixed cut-off date. However, since tf1 is notsupported by python &amp;lt; 3.8, we will only provide Docker images and not supportnative installation anymore from Q4 2024.torch  Our current target version is 1.10.x.bash  We use bash scripting for development tasks and for the bashlib library in OCR-D.  Our current target version is 4.4.",
      "url": " /en/spec/decisions.html"
    },
  

    {
      "slug": "en-decisions-html",
      "title": "Decisions in OCR-D",
      "content"	 : "Decisions in OCR-DIn a software project, especially a highly distributed one like OCR-D,decisions need to be made on the technology used, how interfaces shouldinteroperate and how the software as a whole is designed.In this document, such decisions on key aspects of OCR-D are discussed for thebenefit of all OCR-D stakeholders.Terminology  current refers to December 13, 2022, the last change of this document  Q1-Q4 refers to yearly quarters  target version is the version we mainly test and develop for  supported version means that we test this version and ensure compatibilityGeneral decisions  [Q1 2023] We will update to Ubuntu 22.04 and Python 3.7 as soon as possible. OCR-D/core#956  [Q1 2023] Switch to Slim Containers in ocrd_all  [Q1 2023] Python API changes (Pagewise processing) OCR-D/core#322Workflow format  [Q3 2022] We use Nextflow. The whole .nf file (Nextflow file) as the workflowformat workflow server and processing server including web API implementation is part of theimplementation projects. Further details can be found in the nextflow spec.Web API  [Q3 2022] Switch to the new architecture with message queue.          Processing Broker and Processing Server will be provided via OCR-D Core.        [2022] OCR-D Coordination Project provides the Web API spec. Onlythe REST API wrapper of a single processor is provided by OCR-D Core.QUIVER  [2022] We will create a web application, QUIVER (for QUalIty oVERview), inwhich some information about OCR-D processors are provided:          a general overview of the projects (i.e. GitHub repositories), e.g. if their ocrd-tool.json is valid, when their last release has been made etc.      a workflow section where we benchmark different workflows for different corpora.      a general overview of the available processors      Benchmarking  [2022] To execute the benchmarking, we will create several corpora with different characteristics (font, creation date, layout, …) and run different workflows with these as input. The result is then displayed in the QUIVER workflow tab.The corpora will be publicly available for better transparency.  [2022] Relevant metrics for the minimum viable product (MVP) will be:          CER      WER      Bag of Words      Reading order      IoU      CPU time      wall time      I/O      Memory Usage      Disc usage        [2022] The benchmarking will be executed automatically in a regular interval to measure if changes in the processors improve the result.This might be done via CI, GitHub Actions or as a CRON job on a separate server.OCR-D/coreMETS serverThe current approach to file management requires processors accessing a singleMETS file on disk, which turns file management into a bottleneck for workflows.To alleviate this, we will develop an HTTP server that provides asynchronous andparallel access to the METS in Q4 2022.Decentralized resource listWe currently maintain a list of processor resources centrally in OCR-D/core.In Q3 2022, to allow processor developers to maintain their own separatelist of resources, we have implemented mechanisms to store resource lists in aprocessor’s ocrd-tool.json and bundle resources in their own module directory.By Q1 2023 we should have updated all the processors and whittled down thecentral list to a mostly empty list.Page-wise processingCurrently, processors iterate through the files of a workspace by looping throughall the files in the input file group(s) themselves.In Q1 2023 we will refactor the processor API, deprecate the currentapproach of processors iterating in a process method and enable processorsto process individual pages in a process_page method.ocrd_all Docker deployment  Our current target container is a fat container, with maximum,medium and minimum versions with decreasing amount of processorscontained.  We will wrap processor projects individually and transition to slim containers in Q1 2023.Supported Python versions  Our current target version for Python is 3.7, we support 3.6 and 3.7 fully, later versions partially.  :warning: We cannot currently upgrade beyond 3.7 because there are no tensorflow v1.15.x prebuilt images available. We need to investigate how to alleviate this until Q4 2022.  We will change the target version for Python to 3.10 in Q1 2023 when we have solved the tensorflow problem.  Support for 3.6 will end Q4 2022. We will not test and include Python 3.6 after that.  We will start to support 3.11 in Q1 2023.  We will start to support 3.12 in Q2 2023 (:warning: won’t have distutils anymore)  Support for 3.7 will end Q2 2023.  Support for 3.8 will end Q3 2023.  Support for 3.9 will end Q4 2023.Base OS image  Our current base image for deployment is Ubuntu 18.04, we support Ubuntu 18.04, 20.04 and 22.04.  We will change the base image to Ubuntu 20.04 in Q4 2022.  We will change the base image to Ubuntu 22.04 in Q1 2023.  Support for Ubuntu 18.04 will end in Q1 2023.  Support for Ubuntu 20.04 will end in Q2 2024.Software librariescalamari  Our currently supported calamari version is 1.x.      We will switch to 2.x in Q1 2023.    Support for 1.x will end in Q1 2022pillow  We currently support Pillow 5.x to v9.xtensorflow  Our target version is 2.5.0  We currently support 1.15.x, 2.4.0 and 2.5.0.While we strongly encourage moving away from 1.15.x, due to thelogistics of updating trained models, we don’t have a fixedcut-off date.torch  Our current target version is 1.10.x.bash  We use bash scripting for development tasks and for the bashlib library in OCR-D.  Our current target version is 4.4.",
      "url": " /en/decisions.html"
    },
  

    {
      "slug": "de-decisions-html",
      "title": "Entscheidungen in OCR-D",
      "content"	 : "Entscheidungen in OCR-DIn einem Softwareprojekt, insbesondere in einem stark verteilten Projekt wie OCR-D,müssen Entscheidungen getroffen werden über die verwendete Technologie, das Zusammenspiel der Schnittstellenund darüber, wie die Software als Ganzes konzipiert ist.In diesem Dokument werden solche Entscheidungen über Schlüsselaspekte von OCR-D zum Nutzen aller OCR-D-Akteure diskutiert.Terminologie  aktuell bezieht sich auf den 13. Dezember 2022, die letzte Änderung dieses Dokuments  Q1-Q4 bezieht sich auf die Jahresquartale  Zielversion ist die Version, für die wir hauptsächlich testen und entwickeln  Unterstützte Version bedeutet, dass wir diese Version testen und Kompatibilität sicherstellenAllgemeine Entscheidungen  [Q1 2023] Wir werden so bald wie möglich auf Ubuntu 22.04 und Python 3.7 aktualisieren. OCR-D/core#956  [Q1 2023] Wechsel zu Slim Containern in ocrd_all.  [Q1 2023] Python API Änderungen (seitenweise Prozessierung) OCR-D/core#322Workflow-Format  [Q3 2022] Wir verwenden Nextflow. Die gesamte .nf-Datei (Nextflow-Datei) als Workflow-Format, Workflow-Server und Verarbeitungs-Server einschließlich Web-API-Implementierung ist Teil der Implementierungsprojekte.  Weitere Details sind in der nextflow spec zu finden.Web-API  [2022] Das OCR-D Koordinierungsprojekt stellt die Web API spec zur Verfügung.Nur der REST API wrapper eines einzelnen Prozessors wird von OCR-D Core bereitgestellt.QUIVER  [2022] Wir werden eine Webanwendung, QUIVER (für QUalIty oVERview), entwickeln, in in der verschiedene Informationen über OCR-D Prozessoren bereitgestellt werden:          eine allgemeine Übersicht über die Projekte (d.h. GitHub Repositories), z.B. ob ihre ocrd-tool.json gültig ist, wann ihre letzte Version gemacht wurde usw.      ein Abschnitt über Workflows, in dem wir verschiedene Workflows für verschiedene Korpora benchmarken.      eine allgemeine Übersicht über die verfügbaren Prozessoren      Benchmarking  [2022] Um das Benchmarking durchzuführen, werden wir mehrere Korpora mit unterschiedlichen Eigenschaften (Schriftart, Erstellungsdatum, Layout, …) erstellen und verschiedene Workflows mit diesen als Input laufen lassen. Das Ergebnis wird dann in der Registerkarte QUIVER-Workflow angezeigt.Die Korpora werden zur besseren Transparenz öffentlich zugänglich sein.  [2022] [Relevante Metriken] (https://github.com/OCR-D/spec/pull/225) für das Minimal Viable Product (MVP) werden sein:          CER      WER      Bag of Words      Lesereihenfolge      IoU      CPU-Zeit      Wandzeit      E/A      Speichernutzung      Festplattennutzung        [2022] Das Benchmarking wird in regelmäßigen Abständen automatisch ausgeführt, um zu messen, ob Änderungen an den Prozessoren das Ergebnis verbessern.Dies kann über CI, GitHub Actions oder als CRON Job auf einem separaten Server erfolgen.OCR-D/coreMETS-ServerDer aktuelle Ansatz zur Dateiverwaltung erfordert Prozessoren, die auf eine einzelneMETS-Datei auf der Festplatte zugreifen, was die Dateiverwaltung zu einem Engpass für Arbeitsabläufe macht.Um dieses Problem zu lösen, werden wir einen HTTP-Server entwickeln, der asynchronen undparallelen Zugriff auf das METS im Q4 2022 ermöglicht.Dezentrale RessourcenlisteDerzeit wird eine Liste der Prozessorressourcen zentral in OCR-D/core geführt.In Q3 2022 haben wir Mechanismen implementiert, die es Entwicklern von Prozessoren ermöglichen,ihre eigene, separate Ressourcenliste zu führen,indem Ressourcenlisten in der ocrd-tool.json des Prozessors gespeichert werden und die Ressourcen in ihrem eigenen Modulverzeichnis gebündelt werden.Bis Q1 2023 sollten wir alle Prozessoren aktualisiert haben und diezentrale Liste auf eine weitgehend leere Liste reduziert haben.Seitenweise ProzessierungDerzeit iterieren die Prozessoren durch die Dateien eines Workspace indem sie eine Schleife durchalle Dateien in der/den Eingabedateigruppe(n) selbst durchführen.In Q1 2023 werden wir die Prozessor-API überarbeiten und den derzeitigen Ansatz derProzessoren, die in einer “process”-Methode iterieren, verwerfen und den Prozessoren ermöglichen,einzelne Seiten in einer process_page-Methode zu verarbeiten.ocrd_all Docker deployment  Unser aktueller Zielcontainer ist ein fat container, mit maximum,medium und minimum Versionen, die eine abnehmende Anzahl von Prozessorenenthalten.  Wir werden Prozessorprojekte einzeln wrappen und in Q1 2023 auf slim container umstellen.Unterstützte Python-Versionen  Unsere aktuelle Zielversion für Python ist 3.7, wir unterstützen 3.6 und 3.7 vollständig, spätere Versionen teilweise.  :Achtung: Wir können derzeit nicht über 3.7 hinaus aktualisieren, da es keine tensorflow v1.15.x vorgefertigten Images gibt. Wir müssen untersuchen, wie wir dieses Problem bis Q4 2022 beheben können.  Wir werden die Zielversion für Python auf 3.10 in Q1 2023 ändern, wenn wir das Tensorflow-Problem gelöst haben.  Die Unterstützung für 3.6 wird in Q4 2022 enden. Wir werden Python 3.6 danach nicht mehr testen und einbinden.  Wir beginnen mit der Unterstützung von 3.11 in Q1 2023.  Wir werden 3.12 ab Q2 2023 unterstützen (:Achtung: wird keine distutils mehr haben)  Die Unterstützung für 3.7 wird in Q2 2023 enden.  Unterstützung für 3.8 wird in Q3 2023 enden.  Unterstützung für 3.9 wird in Q4 2023 enden.Basis-OS-Image  Unser aktuelles Basis-Image für das Deployment ist Ubuntu 18.04, wir unterstützen Ubuntu 18.04, 20.04 und 22.04.  Wir werden das Basis-Image auf Ubuntu 20.04 im Q4 2022 ändern.  Wir werden das Basis-Image auf Ubuntu 22.04 im Q1 2023 ändern.  Der Support für Ubuntu 18.04 wird im Q1 2023 enden.  Der Support für Ubuntu 20.04 wird im Q2 2024 enden.Software-Bibliothekencalamari  Unsere derzeit unterstützte calamari-Version ist 1.x.      Wir werden in Q1 2023 auf 2.x umsteigen.    Die Unterstützung für 1.x wird in Q1 2022 enden.pillow  Wir unterstützen derzeit Pillow 5.x bis v9.x.tensorflow  Unsere Zielversion ist 2.5.0.  Wir unterstützen derzeit 1.15.x, 2.4.0 und 2.5.0.Obwohl wir die Abkehr von 1.15.x dringend empfehlen, haben wir aufgrund derLogistik der Aktualisierung der trainierten Modelle keinen festesStichtag.torch  Unsere aktuelle Zielversion ist 1.10.x.[bash]  Wir verwenden bash-Skripte für Entwicklungsaufgaben und für die bashlib-Bibliothek in OCR-D.  Unsere derzeitige Zielversion ist 4.4.",
      "url": " /de/decisions.html"
    },
  

    {
      "slug": "en-dev-best-practice-html",
      "title": "How we want to develop software in OCR-D",
      "content"	 : "Best Practice for Software Development in OCR-DThis guide is informed by the experience of coordinating distributed development of the OCR-D software and specifications. We strive for a broad consensus on the practicalities and logistics of software development, particular in the current Phase III of OCR-D.You can find a concise summary in the slides to our call on Best Practices, 2020-07-07Communications  Be boldSoftware Development is as much a social endeavor as it is technical. To effectively develop in a distributed setting, transparency and sharing early in the process is vital. We therefore strongly recommend using the following channels over E-Mail.GitHub AccountSince all development shall be openly and transparently carried out on GitHub, it is essential that you create a GitHub account.ChatThe OCR-D Gitter “Lobby” is the general channel for all things OCR-D. We strive to keep the threshold for participation as low as possible. Every question or comment is welcome. It is also the channel we use to announce changes to OCR-D core, the specifications or updates on processors.Gitter accounts can also be used for private/direct conversations, for which the contacts in the Lobby are a good starting point.GitHub IssuesAs with the chat, we aim for low barriers of entry for participation on issues in GitHub. If something is not working within OCR-D projects, you encounter error messages or unexpected behavior: Feel free to open an issue. If you cannot identify which component or project is the cause of the problem, open an issue in OCR-D/core, the moderators can transfer it to the right place later.This openness must go both ways: If you maintain or contribute to a project, it is essential that we treat each other with respect. Many users and even developers are still learning about OCR, software development and distributed version control. A clueless “newbie” can become a helpful contributor if you support them.GitHub ForksA fork is a clone of a GitHub repository hosted on the same site. If you fork a repository, a clone of it will be created under your username. The advantage of a fork over just the original repository is that you have full control of it and can make changes to it to fit your use case, while others also can access it. If you deem your changes a useful contribution to the original project, you can create a pull request to allow discussing and refining them with others, and ultimately merge.Pull RequestsTODO Explain how suggestions workA pull request (“PR”, in Gitlab they are called merge requests) is essentially an issue with an attached Git branch of either the repository or one of the forks. You should create a dedicated branch for your Pull Request. Describe the changes you are proposing in a concise manner and try to find a good title. Try to keep PR as small as possible, ideally not more than a handful of changed files or a few dozen changed lines at most. If a PR is too large to review and integrate effectively, consider splitting it into smaller PRs.If you are maintaining a project and somebody sends a PR: great, because that means your users are not only interested in your software but also willing to spend time on improving it. Check the “Files” tab of the PR to see a visualization of the changes. You can comment directly on code lines, which helps to focus the discussion on specific changes. Once such a discussion is complete, click on “Resolve conversation” which will hide these finished conversations. You can review a PR and either approve, request changes or reject a PR. Nobody likes to feel rejected, so try to help improve a suboptimal PR rather than outright rejecting it.WikiThe wiki of the ocrd-website is the central repository for user-generated documentation, and complements the official OCR-D website. If you have any experiences, configurations, scenarios or documentation in general which are related to your use of OCR-D software, feel free to contribute them to the Wiki. To add a new page, click on “New Page”, decide on a concise title for the page and add your information.It is easy to reorganize and edit pages later, so do not feel pressured to overcome a relevancy threshold. Everybody profits from sharing experiences in the long run. Additionally, the wiki serves as a source for official documentation and the coordination project is very much interested in letting the official documentation be strongly informed by the experience of OCR-D users.WebsiteThe official website of OCR-D is https://ocr-d.de. You can find documentation, tailored towards users and developers, as well as a blog and much more.The website itself is available as repository OCR-D/ocrd-website on GitHub. If you have any comments or questions, don’t understand a section of the documentation or have found typos or broken links: Please open an issue in ocrd-website! We are also happy to merge PR for the website, but issues are just as helpful.Development WorkflowGit basicsBefore doing anything else, configure the user name and e-mail you want to assign to your commits:git config --global user.name &quot;Erika Mustermann&quot;git config --global user.email &quot;erika@mustermann.de&quot;Also make sure that the e-mail address you use is linked to your GitHub account so your commits will be attributed to you, even if you use another e-mail address than the one you registered with.It is worth investing some time in studying quality-of-life features of git, such as aliases or shell integration as these will make working with git substantially more pleasant. Also consider mining $HOME/.gitconfig files on GitHub for your own setup.Branches and Pull RequestsAs mentioned in the section on Pull Requests, you should organize your changes in feature branches that contain a narrow set of commits. When new features depend on others, don’t hesitate to branch from branches.Unit testsUnit tests are code that makes assertions over the behavior of the components of your main code, which can be used for automatic regression testing.Code CoverageThe coverage of your tests is the percentage of lines of code that were executed running all your tests. You should strive for 100% coverage, so that everything your code does is encountered by the tests at least once. Even though a high code coverage is no panacea against bugs, it substantially reduces the amount of regressions, i.e. code changes that unexpectedly break existing behavior.Test dataThe OCR-D coordination project maintains a central repository OCR-D/assets that contains test data for unit tests. The test data consists of unpacked OCRD-ZIP of workspaces, i.e. one or more METS XML files, as well as PAGE-XML, ALTO, images. While they use the same mechanism, these folders are not Ground Truth. Instead, they serve as examples of a wide range of “real-life” data, warts and all. When testing your software, consider using the data from OCR-D/assets. If none of the provided OCRD-ZIP satisfy your needs, please open an issue in OCR-D/assets and describe your test data needs, so we can cooperate on creating test data.Making uniform and well-described test data available to all serves everybody and the coordination project will continue to maintain and update the test data as requirements shift or specifications change. Such a test-specific corpus also improves comparability of processor performance and frees test engineers to focus on testing the actual software, not wrangling with their own test data.Continuous IntegrationContinuous Integration (CI) platforms offer computing resources to software developers that allow building and testing software. These platforms, such as Travis CI, Circle CI, Scrutinizer or GitHub Actions can be integrated into GitHub repositories to automatically build/test when certain actions happen, such as “a PR was opened” or “new commit to master branch”. The big advantage of CI is that you can automate testing of changes to your software in a variety of environments, so you can be confident that a PR did not introduce regressions, or make the module break for an operating system you didn’t anticipate.Semantic VersioningThere is a wide variety of versioning schemes, such as date-based or plain incremental integers. At OCR-D we favor Semantic Versioning. In essence, a version should have the syntax MAJOR.MINOR.PATCH with this convention:  Increase MAJOR by one if a changeset introduces breaking changes.  Increase MINOR by one if a changeset introduces new features.  Increase PATCH by one if a changeset fixes problems with the minor version.You should start versioning at 0.0.1. While MAJOR is 0, the software is to be considered alpha but you should still stick to the scheme for minor and patch revisions. Once you consider the software mature enough to merit a 1.0.0 release, please take extra care to adhere to the convention as much as possible.The advantage of semantic versioning is that it conveys the consequences of an upgrade to the user. There is never a reason not to upgrade to the latest patch release whereas a upgrading to a new major version should be undertaken with care.LicensingOCR-D is firmly committed to the values of F(L)OSS and permissive licensing, as these not only encourage community use and uptake, but also create trust between all stakeholders and transparency of the development process.OCR-D favours the Apache Software License 2.0 (ASL) for it is a permissive license that supports the waiving of liabilities while still granting sufficient freedom for e.g. commercial parties to use and redistribute products that include source code from OCR-D.Second to the ASL, other permissive open source licenses that can be used include (in order of preference): MIT, BSD. If there are strong reasons that prevent using a permissive license for your project, please get in touch with the coordination project to discuss other licensing options. Python project setupSince most processors are implemented in Python, we have been developing certain conventions that have proven effective for interoperability, deployment and robustness. Following these conventions makes it easier to contribute and frees developers from the grunt work of managing a project, to spend more time on development, documentation and support.Have a look at existing projects that follow these cvonventions, e.g. ocrd_tesserocr. Much can be copied, pasted and adapted.Project layoutWhen starting a new project ocrd_foo, create this folder layout:ocrd_foo/ocrd_foo/__init__.pyocrd_foo/ocrd-tool.jsontests/test_foo.pyLICENSEMakefileREADME.mdsetup.pysetup.pyWe use setuptools for python packaging in OCR-D.tests/LICENSEMakefileDeploymentDockerfileocrd_all",
      "url": " /en/dev-best-practice.html"
    },
  

    {
      "slug": "en-dev-html",
      "title": "OCR-D for the technically inclined",
      "content"	 : "Welcome to the OCR-D Developer Section!This section contains all information relevant for the further development of the OCR-D-software, i.e. especially specifications, documentation and information on our GT. You are very welcome to support our development efforts on Github!  Decision Log          Recent decisions on software development        OCR-D Specifications          OCR-D Specifications for CLI, METS, PAGE etc.        OCR-D/core API documentation          Python API documentation of core implementation        OCR-D development best practices          Practical information on distributed development of software and specifications in OCR-D        Ground Truth Guidelines          Guidelines for Ground Truth        PAGE-XML format documentation          Documentation for the PAGE-XML format      ",
      "url": " /en/dev.html"
    },
  

    {
      "slug": "de-dev-html",
      "title": "Willkommen im Entwicklerbereich von OCR-D!",
      "content"	 : "Willkommen im Entwicklerbereich von OCR-D!Dieser Bereich enthält alle Informationen, die für die Entwicklung der OCR-D-Software relevant sind, d.h. Spezifikationen, Dokumentation und Informationen zu Ground Truth. Gerne wollen wir sie auch dazu einladen unsere Entwicklungsarbeiten auf GitHub zu verfolgen und zu unterstützen.  Decision Log          Aktuelle Entscheidungen zur Softwareentwicklung        OCR-D Spezifikationen          Spezifikationen für CLI, METS, PAGE, etc.        OCR-D/core API Dokumentation          Python API Dokumentation für die core Referenzimplementierung        Best practices für Software Entwicklung in OCR-D          Praktische Informationen zur verteilten Entwicklung von Software und Spezifikationen in OCR-D        Ground Truth Richtlinien          Transkriptionsrichtlinien für Ground Truth        PAGE-XML Formatdokumentation          Dokumentation zum PAGE-XML Format      ",
      "url": " /de/dev.html"
    },
  

    {
      "slug": "en-developer-guide-html",
      "title": "OCR-D Developer Guide",
      "content"	 : "OCR-D Developer Guide  A practical guide to the OCR-D frameworkIntroductionThe “OCR-D guide” helps developers writing software and usingtools within the OCR-D ecosystem.Scope and purpose of the OCR-D guideThe OCR-D guide is a collection of concise recipes that provide pragmatic advise on how to  bootstrap a development environment,  work with the ocrd command line tool,  manipulate METS and PAGE documents,  create spec-compliant softwareNotationLines in code examples  starting with #  are comments;  starting with $  are typed shell input (everything after $  is);  are output otherwise.Words in ALL CAPS with a prepended $ are variable names:  $METS_URL: URL or file path to a mets.xml file, e.g.          https://github.com/OCR-D/assets/raw/master/data/kant_aufklaerung_1784/mets.xml        $WORKSPACE_DIR: File path of the workspace created, e.g.          $WORKSPACE_DIR      /data/ocrd-workspaces/kant-aufklaerung-2018-07-11      When referring to a “something command”, it is actually ocrd something onthe command line.Other OCR-D documentation  Specification: Formal specifications  Glossary: A glossary of terms in the OCRdomain as used throughout our documentationBootstrappingUbuntu LinuxOCR-D development is targeted towards Ubuntu Linux &amp;gt;= 18.04 since it is free,widely used and well-documented.Most of the setup will be the same for other Debian-based Linuxes and olderUbuntu versions. You might run into problems with outdated system packagesthough.In particular, it can be tricky at times to install tesseract at the rightversion. Try alex-p’s PPA or buildtesseract from source.Essential system packagessudo apt install   git   build-essential   python python-pip   python3 python3-pip  git: Version control, OCR-D uses git extensively  build-essential: Installs make and C/C++ compiler  python: Python 2.7 for legacy applications like ocropy  python3: Current version of Python on which the OCR-D software core stack is built  pip/pip3: Python package managementPython API and CLIThe OCR-D toolkit is based on a Python API thatyou can reuse if you are developing software in Python.This API is exposed via a command line tool ocrd. This CLI offers much of thesame functionality of the API without the need to write Python code and can be readilyintegrated into shell scripts and external command callouts in your code.So, If you do not intend to code in Python or want to wrapexisting/legacy tools, a major part of the functionality of the API isavailable as a command line tool ocrd.Python setupCreate virtualenvWe strongly recommend using virtualenv (or similar tools if they are morefamiliar to you) over system-wide installation of python packages. It reducesthe amount of pain supporting multiple Python versions and allows you to testyour software in various configurations while you develop it, spinning up andtearing down environments as necessary.sudo apt install   python3-virtualenv   python-virtualenv # If you require Python2 compatCreate a virtualenv in an easy to remember or easy-to-search-shell-history-for location:$ virtualenv -p python3.6 $HOME/ocrd-venv3$ virtualenv -p python2.7 $HOME/ocrd-venv2 # If you require Python2 compatActivate virtualenvYou need to activate this virtual environment whenever you open a new terminal:$ source $HOME/ocrd-venv3/bin/activateIf you tend to forget sourcing the script before working on your code, addsource $HOME/ocrd-venv3 to the end of your .bashrc/.zshrc file and logout and back in.Install ocrd in virtualenv from pypiMake sure, the virtualenv is activated and install ocrd with pip:$ pip install ocrdGeneric setupIn this variant, you still need to install the ocrd Python package. But sinceit’s only used for its CLI (and as a dependency for Python-based OCR-Dsoftware), you can install it system-wide:$ sudo pip install ocrdSetup from sourceIf you want to build the ocrd package fromsource to stay up-to-date on unreleased changesor to contribute code, you can clone the repository and build from source:$ git clone https://github.com/OCR-D/core$ cd coreIf you are using the python setup:$ pip install -r requirements.txt$ pip install -e .If you are using the generic setup:$ sudo pip install -r requirements.txt$ sudo pip install .Verify setupAfter setting up, check that these commands do not throw errors and have theminimum version:$ git --version# Version 1.7 or higher?$ make --version# Version 9.0.1 or higher?$ ocrd --version# ocrd, version 0.4.0Anatomy of an OCR-D module project (MP)MP are git repositories with at least a description of the MP andits provided tools (ocrd-tool.json and aMakefile for installing the MP into a suitable OS.ocrd-tool.jsonThis is a JSON file that describes the software of a particular MP. It servesmainly three purposes:  providing a machine-actionable description of MP and the bundled tools andtheir parameters  concise human-targeted descriptions as the foundation for the applicationdocumentation  ensuring compatible definitions and interfaces, which is essential forsustainable, scalable workflowsThis document is mainly focusing on the first point.The structure and syntax of the ocrd-tool.json is defined by a JSONSchema and expects JSON Schemafor the parameter definitions. In addition to the schema, the ocrd commandline tool can help you validate the ocrd-tool.jsonMechanics of the ocrd-tool.json:fire: TODO :fire:  [kba] Wir brauchen einen besseren Namen, ich kann das schon nicht mehrschreiben dauernd, ocrd-tool.json. Vielleicht einfach manifest.json oderpackage.json oder tool-desc odr irgendwas.:fire: TODO :fire:The ocrd-tool.json has two conceptual levels:  Information about the MP as a whole and thepeople and processes involved  Technical metadata on the level of the individual toolsBeyond the ocrd-tool.json file, it is part of the requirements that the toolscan provide the section of the ocrd-tool.json about ‘themselves’ at runtimewith the -J/--dump-json flags.The reason for this redundancy is to make the tools inspectable at runtime andto prevent “feature drift” where the  software evolves to the point where thedescription/documentation is out-of-date with the actual implementation.From a developer’s perspective, the easiest way to handle this is by bundlingthe ocrd-tool.json into your software, e.g. by the following pattern:  Store the ocrd-tool.json at a location where it is easy to deploy andaccess after installation  Symlink it to the root of the repository: ln -sr src/ocrd-tool.json .  Handle --dump-json by parsing the ocrd-tool.json and sending out therelevant section  Validate input and provide defaults basedon the JSON schema mechanicsMetadata about the module projectRequired properties are bold.  version: Version of the tool, adhering to SemanticVersioning  git_url: URL of the Github  tool: See next section  dockerhub: The project’s DockerHub URL  creators: :rotating_light: TODO  :rotating_light::  institution: :rotating_light: TODO  :rotating_light::  synopsis: :rotating_light: TODO  :rotating_light::Example:{  &quot;version&quot;: &quot;0.0.1&quot;,  &quot;name&quot;: &quot;ocrd-blockissifier&quot;,  &quot;synopsis&quot;: &quot;Tools for reasoning about how these blocks fit on this here page&quot;,  &quot;git_url&quot;: &quot;https://githbub.com/johndoe/ocrd_blocksifier&quot;,  &quot;dockerhub&quot;: &quot;https://hub.docker.com/r/johndoe/ocrd_blocksifier&quot;,  &quot;authors&quot;: [{    &quot;name&quot;: &quot;John Doe&quot;,    &quot;email&quot;: &quot;johndoe@ocr-corp.com&quot;,    &quot;url&quot;: &quot;johndoe.github.io&quot;  }],  &quot;bugs&quot;: {    &quot;url&quot;: &quot;https://github.com/sindresorhus/temp-dir/issues&quot;  },  &quot;tools&quot;: {      /* see next section */    }}Metadata about the toolsThe tools section is an object with the key being the name of the executable described and the value being an object with the following properties (bold means required):  executable: Name of the executable. Must match the key and start with ocrd-  parameters: Description of the parameters this tool accepts  description: Concise description what the tool does  categories: Tools belong to these categories, representing modules within the OCR-D project structure, list is part of the specs  steps: This tool can be used at these steps in the OCR-D functional model, list of values in the specsMetadata about parametersRequired properties are bold.  type: What kind of parameter this is, either a string, a number or a boolean  format: Subtype defining the syntax of the value such as float/integer for numbers or uri for string  required: If true, this parameter must be provided by the user  default: Default value if not required  enum: List of possible values if a fixed listrequired: true and setting default are mutually exclusive.MakefileAll MP should provide a Makefile with at least two targets: deps and install.make deps should install any dependencies, such as required python modules.make install should install the executable(s) into $(PREFIX)/bin.make test should start the unit/regression test suite if provided.Makefile for Python MPmake deps should install dependencies with pip.make install should call python setup.py install.See the makefile of the ocrd_kraken project for an example.Makefile for generic MPmake deps should install dependencies either by compiling from source or using apt-get.make install should  Copy the executables to $(PREFIX)/bin, creating $(PREFIX)/bin if necessary.  Copy any required files to $(PREFIX)/share/&amp;lt;name-of-the-package&amp;gt;, creating the latter if necessarySee the makefile of the ocrd_olena project for an example.ocrd workspace - Working with METSMETS is the container format of choice for OCR-D because it is widely used indigitzation workflows in cultural heritage institutions.A METS file references files in file groups and can contain a variety ofmetadata, the details can be found in the specs.From METS to WorkspaceWithin the OCR-D toolkit, we use the term “workspace”, a folder containing afile mets.xml and any number of the files referenced by the METS.One can think of the mets.xml as the MANIFEST of a JAR or the .git folderof a git repository.The workspace command of the ocrd tool allows various manipulations ofworkspaces and therefore METS files.Git similarity intendedThe workspace command’s syntax and mechanics are strongly inspired bygit so if you know git, this should be familiar.            git      ocrd workspace                  init      init              clone      clone              add      add              ls-files      find              fetch      find --download              archive      pack      Set the workspace to work onFor most commands, workspace assumes the workspace is the current workingdirectory. If you want to use a different directory, use the -d / --directory option# Listing files in the workspace at $PWD$ ocrd workspace find# Listing files in the workspace at $WORKSPACE_DIR$ ocrd workspace -d $WORKSPACE_DIR findUse another name than mets.xmlAccording to convention, the METS of a workspace is named mets.xml.To select a different basename for that file, use the -M / --mets-basename option:# Assume this workspace structure$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets3000.xml# This will fail in a loud and unpleasant manner$ ocrd workspace -d $WORKSPACE_DIR find# This will not$ ocrd workspace -d $WORKSPACE_DIR -m mets3000.xml findCreating an empty workspaceTo create an empty workspace to which you can add files, use the workspace init command$ ocrd workspace -d ws1 init/home/ocr/ws1Load an existing METS as a workspaceTo create a workspace and save a METS file, use the workspace clone command:$ ocrd workspace -d new-workspace clone $METS_URL/home/ocr/new-workspace$ find new-workspacenew-workspacenew-workspace/mets.xmlLoad an existing METS and referenced files as a workspaceTo not only clone the METS but alsodownload the contained files, use workspace clone with the --download flag:$ ocrd workspace -d $WORKSPACE_DIR clone --download $METS_URL$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-GT-ALTO$WORKSPACE_DIR/OCR-D-GT-ALTO/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-GT-PAGE$WORKSPACE_DIR/OCR-D-GT-PAGE/kant_aufklaerung_1784_0020.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifNOTE: This will download all files, which can mean hundreds ofhigh-resolution images. If you want more fine-grained control,clone the bare workspaceand then use the workspace find command with the download flagSearching the files in a METSYou can search the files in a METS file with the workspace find command.  All files: ocrd workspace find  All TIFF files: ocrd workspace find --mimetype image/tiff  All TIFF files in the OCR-D-IMG-BIN group: ocrd workspace find --mimetype image/tiff --file-grp OCR-D-IMG-BINSee ocrd workspace --find for the full range of selection optionsDownloading/Copying files to the workspaceTo download remote or copy local files referenced in the mets.xml to theworkspace, append the --download flag to the workspace findcommand:# Clone Bare workspace:$ ocrd workspace clone $METS_URL$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml# Download all files in the `OCR-D-IMG` file group$ ocrd workspace -d $WORKSPACE_DIR find --file-grp OCR-D-IMG --download[...]$ find $WORKSPACE_DIR$WORKSPACE_DIR$WORKSPACE_DIR/mets.xml$WORKSPACE_DIR/OCR-D-IMG$WORKSPACE_DIR/OCR-D-IMG/kant_aufklaerung_1784_0020.tifThe convention is that files will be downloaded to $WORKSPACE_DIR/$FILE_GROUP/$BASENAME where  $FILE_GROUP is the @USE attribute of the mets:fileGrp  $BASENAME is the last URL segment of the @xlink:href attribute of the mets:FLocatNOTE Downloading a file not only copies the file to the $WORKSPACE_DIRbut also changes the URL of the file from its original to the absolute filepath of the downloaded file.Adding files to the workspaceWhen running a module project, new files are created (PAGE XML, images …). Toregister these new files, they need to be added to the mets.xml as amets:file with a mets:FLocat within a mets:fileGrp, each with the rightattributes. The workspace add command makes this possible:$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$ ocrd workspace -d $WORKSPACE_DIR add   --file-grp OCR-D-IMG-BIN   --file-id PAGE-0013-BIN   --mimetype image/png   --group-id PAGE-0013   page0013binarized.png$ ocrd workspace -d $WORKSPACE_DIR find -k local_filename$WORKSPACE_DIR/OCR-D-IMG/page0013.tif$WORKSPACE_DIR/OCR-D-IMG-BIN/page0013binarized.tifValidating OCR-D compliant METSTo ensure a METS file and the workspace it describes adheres to the OCR-Dspecs, use the workspace validate command:# Create a bare workspaceocrd workspace -d  $WORKSPACE_DIR init# Validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;METS has no unique identifier&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;No files&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;# Oops, let&#39;s set the identifier ...$ ocrd workspace -d $WORKSPACE_DIR set-id &#39;scheme://my/identifier/syntax/kant_aufklaerung_1784&#39;# ... and add a file$ ocrd workspace -d $WORKSPACE_DIR add -G OCR-D-IMG-BIN -i PAGE-0013-BIN -m image/png -g PAGE-0013 page0013binarized.png# Validate again&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;ocrd tool – Working with ocrd-tool.jsonThis command helps you to explore and validate the information in any ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json SUBCOMMANDocrd-tool validateValidate that an ocrd-tool.json is syntactically valid and adheres to the schema.This is useful while developing to make sure there are no typos and all required properties are set.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;false&quot;&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;steps&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[tools.ocrd-wip-xyzzy] &#39;categories&#39; is a required property&amp;lt;/error&amp;gt;  &amp;lt;error&amp;gt;[] &#39;version&#39; is a required property&amp;lt;/error&amp;gt;&amp;lt;/report&amp;gt;This example shows that the ocrd-wip-xyzzy executable is missing the required steps andcategories properties and the root level object is missing the versionproperty.Adding them should result in$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json validate&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;Introspect an ocrd-tool.jsonThese commands are used for enumerating the executables contained in anocrd-tool.json and get root level metadata, such as the version.$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json version0.0.1# Lists all the tools (executables) one per-line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json list-toolsocrd-wip-xyzzyocrd-wip-frobozzIntrospect individual toolsThis set of commands allows introspection of the metadata on individualtools within an ocrd-tool.json.The syntax is ocrd ocrd-tool /path/to/ocrd-tool.json tool EXECUTABLE SUBCOMMAND$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy dump{  &quot;description&quot;: &quot;Nothing happens&quot;,  &quot;categories&quot;: [&quot;Text recognition and optimization&quot;, &quot;Arcane Magic&quot;],  &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],  &quot;executable&quot;: &quot;ocrd-wip-xyzzy&quot;}# Description$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy descriptionNothing happens# List categories one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy categoriesText recognition and optimizationArcane Magic# List steps one per line$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy stepsrecognition/text-recognitionParse parametersThe details of how a tool is configured at run-time are determined byparameters. When a parameter file is passed to atool, it should:  ensure it is valid JSON  validate according to the parameter schema  add default values when no explicit values were providedThe ocrd ocrd-tool tool parse-params command does just that and can outputthe resulting default-enriched parameter as either JSON or as shell scriptassignments to evaluate:# Get JSON$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params --json -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;){  &quot;val1&quot;: 42,  &quot;val2&quot;: false,  &quot;val-with-default&quot;: 23}# Get back shell assignments to an associative array &quot;params&quot;$ ocrd ocrd-tool /path/to/ocrd_wip/ocrd-tool.json tool ocrd-wip-xyzzy parse-params -p &amp;lt;(echo &#39;{&quot;val1&quot;: 42, &quot;val2&quot;: false}&#39;)params[&quot;val1&quot;]=&quot;42&quot;params[&quot;val2&quot;]=&quot;true&quot;params[&quot;val-with-default&quot;]=&quot;23&quot;ocrd process - Run a multi-step workflowOCR requires multiple steps, such as binarization, layout recognition, textrecognition etc. These steps are implemented with command line tools thatadhere to the same command line interface whichmakes it straightforward to chain these calls.For example, to run kraken binarization and tesseract block segmentation, one could execute:ocrd-kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BINocrd-tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.jsonThe disadvantage of individual calls is that it requires the user to check whether runs wereactually successful. To remedy this, users can use the ocrd process CLI which  simplifies the CLI syntax for multiple calls  checks for required and expected-to-be-produced file groups  checks for return value  sets logging levels uniformly across toolsThe same calls mentioned before can be passed to ocrd process as follows:ocrd process -l DEBUG   &quot;kraken-binarize -l DEBUG -I OCR-D-IMG -O OCR-D-IMG-BIN&quot;   &quot;tesserocrd-segment-block -l DEBUG -I OCR-D-IMG-BIN -O OCR-D-SEG-BLOCK -p tesseract-params.json&quot;Wrapping a CLI using bashThis section describes how you can make an existing tool OCR-Dcompliant, i.e. provide a CLI which implements all the specs and callsout to another executable.For this purpose, the ocrd offers a bash library that handles:  command line option parsing  on-line help  parsing and providing defaults for parametersThe shell library is bundled with the ocrd command line tool and can be accessed with theocrd bashlib command.ocrd bashlibTo get the filename of the shell lib, use ocrd bashlib filename, which youcan employ to source the shell code in a wrapper script. After sourcing this scriptyou will have access to a number of shell functions that begin with ocrd__.The only function you definitely need is ocrd__wrap which parses anocrd-tool.json and scaffolds a spec-compliant CLI, parses command linearguments and parameters and lets the developer then react to the inputs.In combination with the ocrd workspace command this allowsyou to write CLI applications without touching any METS or PAGE/XML files by hand.ocrd__wrapocrd__wrap has this signature:ocrd__wrap OCRD_TOOL_JSON EXECUTABLE_NAME ...ARGSwhere  OCRD_TOOL_JSON is the path to the ocrd-tool.json  EXECUTABLE_NAME is the name of an executable within OCRD_TOOL_JSON  ...ARGS are 0..n command line arguments passed on from the userExample:   ocrd__wrap /usr/share/ocrd-wip/ocrd-tool.json ocrd-wip-xyzzy &quot;$@&quot;",
      "url": " /en/developer-guide.html"
    },
  

    {
      "slug": "en-dita-html",
      "title": "",
      "content"	 : "Only available in german",
      "url": " /en/dita.html"
    },
  

    {
      "slug": "de-dita-html",
      "title": "OCR-D: Anforderungsprofil für die Dokumentation der Modulprojekte",
      "content"	 : "OCR-D: Anforderungsprofil für die Dokumentation der Modulprojekte  Allgemein:  Die Dokumentation der Tools und Schnittstellen betrifft sowohl die Anwendung selbst (Anwendungsdokumentation) als auch deren Anwendung von Nutzern (Benutzerdokumentation).Das OCR-D: Anforderungsprofil für die Dokumentation der Modul Projekte stellt nicht eine DITA-Einführung oder DITA-Dokumentation dar, das gleiche betrifft die Markdown Auszeichnungssprache. In dieser Dokumentation werden ergänzende Informationen sowie unmittelbare Anforderungen für eine OCR-D konforme Dokumentation dargelegt.Adressaten der Dokumentation:Es sollen sowohl Techniker, die vor allem Informationen zur Anwendung benötigen (Installation, Wartung sowie Integration im Umfeld der eigenen Werkzeuge), als auch Benutzer, die das Werkzeug nutzen möchten bzw. mit der Anwendung ein bestimmtes Ergebnis erzielen möchten im Rahmen der Anwendungsdokumentation und Benutzerdokumentation informiert werden.Stil: Der Stil der Dokumentation sollte verständlich und in kurzen Sätzen abgefasst sein. Die Dokumentation muss alle Aspekte der Anwendung und Benutzung umfassend beinhalten.Format: Die Dokumentation ist entweder im xmlbasierten DITA-Format oder im Auszeichnungsformat Markdown (Markdown-DITA-Syntax) abzufassen.Software: Zur Erstellung der Dokumentation wird ein Editor  (empfohlen wird ein Editor, mit XML-Unterstützung) sowie das DITA-OT (Open Toolkit) benötigt. Nähere Information finden sich unter: https://www.dita-ot.org/Die Erstellung der DokumentationDie Erstellung der Dokumentation erfolgt stufenweise.Die erste Stufe bildet die Dokumentation des Werkzeuges in Form der ocrd_tool.json (siehe https://ocr-d.github.io/ocrd_tool).In der zweiten Stufe werden manuell u. a. Funktionen, Parameter, Fehlerbehandlungen der Anwendung in Form einzelner Dateien entsprechend den folgenden Formatvorgaben abgefasst.StrukturvorgabenDie Dokumentation sollte wie folgt strukturell aufgebaut sein. Auf Grund der Adressaten der Dokumentation können Schwerpunkte unterschiedlich gesetzt werden.Zum Beispiel wird der Schwerpunkt auf die Benutzerdokumentation gelegt, sollte auf folgende Punkte geachtet werden:  Was kann man mit dem Tool machen? Welches Ergebnis ist von der Anwendung zu erwarten.  Wie wird die Anwendung bedient?  Welche Probleme und Fehlermeldungen können auftreten.Bei der Abfassung ist folgendem allgemeinem Aufbau zu folgen.StrukturvorgabenErstellungVorlagen1. Tool nameInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert2. Release notesmanuell erstellenreleaseNote.md3. Installationmanuell erstelleninstallation.md4. Simple tool descriptionInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert5. Descriptionmanuell erstellenDescription.md6. Optionmanuell erstellenOption.md7. Input format descriptionInhalt der ocrd_tool.jsonInputFormatDescription.md8. ParametersInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert9. Output format descriptionmanuell erstellenOutputFormatDescription.md10. Troubleshootingmanuell erstellenTroubleshooting.dita11. Resourcesmanuell erstellenResources.md12. Glossarmanuell erstellenGlossar.dita13. AuthorsInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert14. ReportingInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiert15. CopyrightInhalt der ocrd_tool.jsonMarkdown-Datei aus ocrd_tool.json generiertDie Dokumentation schreibenDas unmittelbare Schreiben stellt die zweite Stufe der Dokumentation dar. Anhand der Strukturvorgaben ist zu sehen, dass die Dokumentation nicht aus einer homogenen in sich geschlossenen Beschreibung besteht. Sondern einzelne Aspekte u. a. der Name des Werkzeuges, der Installations- und Bedienungsteil, Fehlerbetrachtungen und eventuell weiterführende Hinweise Bestandteile oder Themenbereiche (Topics) der Dokumentation sind. Sowohl zur Schreibunterstützung als auch zum Lesen, der Veröffentlichung sowie der späteren Pflege werden vom OCR-D Projekt diese spezifischen Formatvorgaben vorgenommen.DITA“Die Darwin Information Typing Architecture (DITA) ist ein topic- und xmlbasiertes Dateiformat.”1 DITA ist ein OASIS-Standard (Organization for the Advancement of Structured Information Standards).2 DITA ist ein Format, das die Dokumentation bei der Erstellung, Verbreitung und (Wieder)verwendung unterstützen soll.TopicsMit Hilfe von Topics werden in sich inhaltlich geschlossene spezifische Bestandteile der Dokumentation gegliedert und typisiert. Allgemein beinhaltet ein Topic immer die Angabe eines Titels (&amp;lt;title&amp;gt;), den sogenannten Textkörper (u. a. &amp;lt;body&amp;gt;) sowie beispielsweise einzelne Absätze (&amp;lt;p&amp;gt;) oder Listen (&amp;lt;ul&amp;gt;,  &amp;lt;ol&amp;gt;). Das Topic wird in der Regel in einer Datei gespeichert.Folgende Topic-Typen stehen für die OCR-D Dokumentation zur Verfügung. Die einzelnen Topic-Typen basieren auf eigenen formalen Dokumentspezifikationen. Die kurzen Beschreibungen in der Tabelle basieren auf der DITA-Spezifikation 1.3.[^3][^3]: siehe http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/dita-v1.3-errata01-os-part3-all-inclusive-complete.html#dita_ref_topic            Topic-Typ      Beschreibung      Konkordanz zur OCR-D Strukturvorgaben      Verweis                  General task      Das general task-Topic beinhaltet allgemein abgefasste Handlungsanweisungen. Diese können in einzelnen Abschnitten  angeordnet werden. Im Unterschied zum *strict task-Topic* können in diesem  verwendet werden. Die  beschreiben in einem umfangreichen Absatz den einzelnen Schritt mit dem jeweiligen Kontext.      3. Installation (alternative Möglichkeit)      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-generic-task-topic.html#dita_generic_task_topic              Task topic (strict task)      Das task topic (strict task) beinhaltet die Handlungsanweisungen die notwendig sind zur Bedienung des jeweiligen Werkzeuges. Dabei werden die einzelnen notwendigen Schritte klar in einzelnen  dokumentiert. Ein Schritt-Element  beinhaltet immer ein Komanndozeilen-Element .      3. Installation      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-task-topic.html#dita_task_topic              Concept      Das concept-Topic beinhaltet maßgebende Informationen, die zur Bedienung des Werkzeuges notwendig sind. Das Topic kann dabei notwendiges Hintergrundwissen für die Bedienung und den Umgang mit dem Werkzeug bieten sowie Definitionen oder Erklärungen enthalten.      4. Simple tool description      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-concept-topic.html#dita_concept_topic              Reference      Das reference-Topic konzentriert sich auf die unmittelbaren Informationen des Werkzeuges oder einer spezifischen Schnittstelle. Mit dem Reference-Topic soll der Nutzer schnell und präzise informiert werden.      5. Input format description, 6. Input Parameters, 7. Output format description, 8. Setting Parameters      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-reference-topic.html#dita_ref_topic              Troubleshooting      Das troubleshooting-Topic beinhalt Anweisungen zur Fehlerbehandlung. Dabei wird zuerst der Fehler oder die Symtome  beschrieben und im darauf folgenden Lösungsteil der Grund  für den Fehler benannt und abschließend die Lösung  des Fehlers dokumentiert.      8. Troubleshooting      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-troubleshooting-topic.html#dita-troubleshooting-topic              Glossary entry      Im glossary entry-Topic wird die Bedeutung eines Begriffes oder Vorgehens definiert. Im  kann der zu definierende Term näher beschrieben werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossary-topic.html#glossaryArch              Glossary group      Im glossary group-Topic können die einzelnen Glossary entry-Topic zusammengefasst werden.      11. Glossar      http://docs.oasis-open.org/dita/dita/v1.3/errata01/os/complete/part3-all-inclusive/archSpec/technicalContent/dita-glossarygroup-topic.html      Markdown DITA syntaxAlternativ zum DITA-XML-Markup kann Markdown zur Abfassung folgender Topic-Typen für das Schreiben der Dokumentation genutzt werden. Die einfache Auszeichnungssprache Markdown im Besonderen Markdown-DITA-Syntax ist entsprechend der Dokumentation des DITA-Open Toolkit http://www.dita-ot.org/3.0/topics/markdown-dita-syntax-reference.html zu verwenden.Folgende Topics werden in Markdown unterstützt:  concept  task (im Besonderen das Task topic: strict task)  referenceDie Topic-Typen:  troubleshooting  glossary group  glossary entrysind ausschließlich in DITA zu schreiben.Beispiele für Topics in der jeweiligen spezifischen Syntax in Markdown-DITA-Syntax oder DITABeispiel: für ein Topic concept in Markdown-DITA-Syntax# Simple tool description {#toolDescription .concept}&quot;A command-line interface or command languageinterpreter (CLI), also known as command-line user interface,console user interface and character user interface (CUI), isa means of interacting with a computer program where the user(or client) issues commands to the program in the form ofsuccessive lines of text (command lines).&quot; Source: Wikipediacontributors. (2018, June 5). Command-line interface. InWikipedia, The Free Encyclopedia. Retrieved 12:45, June 6,2018, from [Wikipeadia](https://en.wikipedia.org/w/index.php?title=Command-line_interface&amp;amp;oldid=844566807)Beispiel: für ein Topic task in Markdown-DITA-Syntax# Installation {#installation .task}1.    erster Schritt2.    zweiter SchrittBeispiel: für ein Topic reference in Markdown-DITA-Syntax# Release Note {#releaseNote .reference}The Command Line Interface (CLI) is a maintenancerelease that fixes issues reported in OCR-D.## RequirementsThe CL can be used with all operating systems.Beispiel: für ein Topic troubleshooting in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE troubleshooting PUBLIC &quot;-//OASIS//DTD DITA 1.3 Troubleshooting//EN&quot; &quot;troubleshooting.dtd&quot;&amp;gt;&amp;lt;troubleshooting id=&quot;Troubleshooting&quot;&amp;gt;    &amp;lt;title&amp;gt;Troubleshooting&amp;lt;/title&amp;gt;    &amp;lt;troublebody&amp;gt;        &amp;lt;condition&amp;gt;            &amp;lt;title&amp;gt;Condition&amp;lt;/title&amp;gt;            &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;        &amp;lt;/condition&amp;gt;        &amp;lt;troubleSolution&amp;gt;            &amp;lt;cause&amp;gt;                &amp;lt;title&amp;gt;Cause&amp;lt;/title&amp;gt;                &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;            &amp;lt;/cause&amp;gt;            &amp;lt;remedy&amp;gt;                &amp;lt;title&amp;gt;Remedy&amp;lt;/title&amp;gt;                &amp;lt;responsibleParty&amp;gt;&amp;lt;/responsibleParty&amp;gt;                &amp;lt;steps&amp;gt;                    &amp;lt;step&amp;gt;                        &amp;lt;cmd&amp;gt;&amp;lt;/cmd&amp;gt;                    &amp;lt;/step&amp;gt;                &amp;lt;/steps&amp;gt;            &amp;lt;/remedy&amp;gt;        &amp;lt;/troubleSolution&amp;gt;    &amp;lt;/troublebody&amp;gt;&amp;lt;/troubleshooting&amp;gt;Beispiel: für ein Topic glossary group in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossgroup PUBLIC &quot;-//OASIS//DTD DITA Glossary Group//EN&quot; &quot;glossgroup.dtd&quot;&amp;gt;&amp;lt;glossgroup id=&quot;Glossar&quot;&amp;gt;    &amp;lt;title&amp;gt;Glossar&amp;lt;/title&amp;gt;    &amp;lt;glossentry id=&quot;txtline&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Textline&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;A TextLine is a block of text without line break.        &amp;lt;/glossdef&amp;gt;    &amp;lt;/glossentry&amp;gt;    &amp;lt;glossentry id=&quot;gt&quot;&amp;gt;        &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;        &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are        transcriptions, specific structure descriptions and word lists.        These are essentially available in PAGE XML format in        combination with the original image. Essential parts of         the GT were created manually.        &amp;lt;/glossdef&amp;gt;&amp;lt;/glossgroup&amp;gt;Beispiel: für ein Topic glossary entry in DITA&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;!DOCTYPE glossentry PUBLIC &quot;-//OASIS//DTD DITA Glossary//EN&quot; &quot;glossary.dtd&quot;&amp;gt;&amp;lt;glossentry id=&quot;gt&quot;&amp;gt;    &amp;lt;glossterm&amp;gt;Ground Truth&amp;lt;/glossterm&amp;gt;    &amp;lt;glossdef&amp;gt;Ground truth (GT) in the context of OCR-D are    transcriptions, specific structure descriptions and word lists.    These are essentially available in PAGE XML format in combination    with the original image. Essential parts of the GT were created    manually.    &amp;lt;/glossdef&amp;gt;&amp;lt;/glossentry&amp;gt;Technische Organisation der DokumenationTechnisch organisiert und zusammengefasst wird die DITA-Dokumentation mit einer sogenannten DITA-Map. Die DITA-Map ähnelt einem Inhaltsverzeichnis, die die Topics auflistet. Die Topics sind in einzelnen Dateien gespeichert.Beispiel DITA-Map ocr-d.ditamap&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;!DOCTYPE map PUBLIC &quot;-//OASIS//DTD DITA Map//EN&quot; &quot;map.dtd&quot;&amp;gt;&amp;lt;map&amp;gt;&amp;lt;title&amp;gt;Titel der Dokumentation&amp;lt;/title&amp;gt;    &amp;lt;topicref href=&quot;releaseNote.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;installation.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;simpletoolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;toolDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Option.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;InputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Parameters.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;OutputFormatDescription.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Troubleshooting.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Glossar.dita&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Authors.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Reporting.md&quot;/&amp;gt;    &amp;lt;topicref href=&quot;Copyright.md&quot;/&amp;gt;&amp;lt;/map&amp;gt;Die Verwendung des DITA-Open Toolkits zur Publikation der DokumentationFür die Generierung der Dokumentation ist die Kommandozeilen-Anwendung des DITA-Open Toolkits (http://www.dita-ot.org/3.0/topics/build-using-dita-command.html) zu verwenden.Mit dieser Anwendung können verschiedene Formate der Dokumentation erstellt werden. Für die finale Dokumentation (Publikation) des OCR-D Moduls ist nur das Format DITA gefordert. Wird die Dokumentation in DITA geschrieben ist die Nutzung der Kommandozeilen-Anwendung nicht notwendig. Bei der Verwendung mit Markdown ist eine Konvertierung mit der Kommandozeilen-Anwendung  notwendig.Aber auch zur Korrektur oder zur Vollständigkeitskontrolle ist eine Konvertierung in ein Präsentationsformat von Nutzem. Es können u. a. folgende Präsentionsformate erstellt werden:  html5  pdf  troff  xhtmlBeispiel Kommandoaufruf für DITA-OT  Für die Erstellung einer DITA-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=dita --output=output/dita       Für die Erstellung einer html5-Ausgabe aus der ocr-d.ditamap Dateidita --input=ocr-d.ditamap --format=html5 --output=output/html5Impressum und und DatenschutzerklärungFolgendes Impressum ist der Dokumentation anzufügen:Impressum und DatenschutzerklärungNachstehend finden Sie die gesetzlich geregelten Pflichtangaben zur Anbieterkennzeichnung sowie rechtliche Hinweise zur Dokumentation des Modulprojektes: XXX des OCR-D Projektes.AnbieterAnbieter dieser Internetpräsenz ist im Rechtssinne XXX[es folgt die Adresse][es folgt der Vertreter]Das Modul-Projekt wird vertreten durch XXX.[es folgt der Redaktionsverantwortliche mit Angabe der Persion und Adresse]Lizenz der DokumentationDie Dokumentation liegt unter dem xmlbasierten Format DITA [http://docs.oasis-open.org/dita/dita/v1.3/dita-v1.3-part3-all-inclusive.html] vor und kann unter der Creative Commons-Lizenz CC BY-SA 4.0 DE (https://creativecommons.org/licenses/by-sa/4.0/de/) genutzt werden.            siehe: Seite „Darwin Information Typing Architecture“. In: Wikipedia, Die freie Enzyklopädie. Bearbeitungsstand: 5. April 2018, 15:34 UTC. URL: https://de.wikipedia.org/w/index.php?title=Darwin_Information_Typing_Architecture&amp;amp;oldid=175806494 (Abgerufen: 23. Mai 2018, 10:40 UTC) &amp;#8617;              siehe https://de.wikipedia.org/wiki/Organization_for_the_Advancement_of_Structured_Information_Standards &amp;#8617;      ",
      "url": " /de/dita.html"
    },
  

    {
      "slug": "en-spec-docker-html",
      "title": "Dockerfile provided by MP",
      "content"	 : "Dockerfile provided by MPMP should provide aDockerfile that shouldresult in a container which bundles the tools developed by the MP alongwith all requirements.Based on ocrd:coreDocker containers should be based on the ocrd baseimage which itself is based on Ubuntu18.04. For one, this allows MP to use the ocrd tool to handle recurrent tasksin a spec-conformant way. Besides, it locally installed and containerizedCLI interchangeable.Naming imagesImage tags MUST be the same as the project name but with underscore (_)replaced with forward slash (/).Examples:            project name      docker tag                  ocrd_tesserocr      ocrd/tesserocr              ocrd_calamari      ocrd/calamari              ocrd_olena      ocrd/olena      Labelling imagesThe Dockerfile MUST accept build args VCS_REF and BUILD_DATE.VCS_REF contains the short id of the latest commit this image was built upon.BUILD_DATE contains an ISO-8601 date.From these build args, the image shall be labelled with this command:LABEL     maintainer=&quot;https://github.com/YOUR/PROJECT/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/YOUR/PROJECT&quot;     org.label-schema.build-date=$BUILD_DATEmaintainer and org.label-schema.cvs-url shall point to the issues andlanding page of the GitHub project resp.Shell entrypointNo CMD should be provided.No ENTRYPOINT should be provided.If CMD or ENTRYPOINT are provided, they should be empty arrays./data as volumeThe directory /data in the the container should be marked as a volume toallow processing host data in the container in a uniform way.ExampleDockerfileFROM ocrd:coreVOLUME [&quot;/data&quot;]ARG VCS_REFARG BUILD_DATELABEL     maintainer=&quot;https://github.com/bar/ocrd_foo/issues&quot;     org.label-schema.vcs-ref=$VCS_REF     org.label-schema.vcs-url=&quot;https://github.com/bar/ocrd_foo&quot;     org.label-schema.build-date=$BUILD_DATE# RUN-commands to install requirements, build and install# e.g.# apt-get install -y curlENTRYPOINT []Command to build docker imagedocker build   -t &#39;ocrd/foo&#39; --build-arg VCS_REF=$(git rev-parse --short HEAD) --build-arg BUILD_DATE=$(date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;)",
      "url": " /en/spec/docker.html"
    },
  

    {
      "slug": "en-example-mets-html",
      "title": "Example METS",
      "content"	 : "Example METS&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;mets:mets xmlns:mets=&quot;http://www.loc.gov/METS/&quot;           xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;           xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;           xsi:schemaLocation=&quot;info:lc/xmlns/premis-v2 http://www.loc.gov/standards/premis/v2/premis-v2-0.xsd                               http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd                                http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/version17/mets.v1-7.xsd                                http://www.loc.gov/mix/v10 http://www.loc.gov/standards/mix/mix10/mix10.xsd&quot;&amp;gt;  &amp;lt;mets:metsHdr CREATEDATE=&quot;2020-02-28T07:52:41.141812&quot;&amp;gt;    &amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;CREATOR&quot;&amp;gt;      &amp;lt;mets:name&amp;gt;ocrd/core v2.3.1&amp;lt;/mets:name&amp;gt;    &amp;lt;/mets:agent&amp;gt;  &amp;lt;/mets:metsHdr&amp;gt;  &amp;lt;mets:dmdSec ID=&quot;DMDLOG_0001&quot;&amp;gt;    &amp;lt;mets:mdWrap MDTYPE=&quot;MODS&quot;&amp;gt;      &amp;lt;mets:xmlData&amp;gt;        &amp;lt;mods:mods xmlns:mods=&quot;http://www.loc.gov/mods/v3&quot;&amp;gt;          &amp;lt;mods:identifier type=&quot;purl&quot;&amp;gt;uniqueID&amp;lt;/mods:identifier&amp;gt;        &amp;lt;/mods:mods&amp;gt;      &amp;lt;/mets:xmlData&amp;gt;    &amp;lt;/mets:mdWrap&amp;gt;  &amp;lt;/mets:dmdSec&amp;gt;  &amp;lt;mets:amdSec ID=&quot;AMD&quot;&amp;gt;    &amp;lt;/mets:amdSec&amp;gt;  &amp;lt;mets:fileSec&amp;gt;    &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      &amp;lt;mets:file MIMETYPE=&quot;image/jpg&quot; ID=&quot;OCR-D-IMG_00001&quot;&amp;gt;        &amp;lt;mets:FLocat LOCTYPE=&quot;OTHER&quot; OTHERLOCTYPE=&quot;FILE&quot; xlink:href=&quot;OCR-D-IMG/OCR-D-IMG_0001.jpg&quot;/&amp;gt;      &amp;lt;/mets:file&amp;gt;    &amp;lt;/mets:fileGrp&amp;gt;  &amp;lt;/mets:fileSec&amp;gt;  &amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;    &amp;lt;mets:div TYPE=&quot;physSequence&quot;&amp;gt;      &amp;lt;mets:div TYPE=&quot;page&quot; ID=&quot;P_00001&quot;&amp;gt;        &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_00001&quot;/&amp;gt;      &amp;lt;/mets:div&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:structMap&amp;gt;&amp;lt;/mets:mets&amp;gt;",
      "url": " /en/example_mets.html"
    },
  

    {
      "slug": "en-faq-html",
      "title": "FAQ",
      "content"	 : "FAQGeneralWhere can I start my journey into the OCR-D ecosphere?Who is the target audience of OCR-D?OCR-D’s primary target audience are libraries and archives, digitizinghistorical prints at scale.Where can I get support on OCR-D?  Open an issue at the OCR-D/core repository  Chat with OCR-D project members and other OCR-D users in OCR-D’s chat room.  Join us at our regular calls.  Send an email to eckert[at]hab.deWhat is the difference between OCR-D and ABBYY?ABBYY is a software developer producing the ABBYY Recognition Server whichoffers layout detection and text recognition with a pay-per-page pricing model.OCR-D is a project that integrates a wide variety of solutions for the fullgamut of possible OCR workflow steps. ABBYY is simple to use but offers fewoptions for customization whereas OCR-D workflows can be fine-tuned for bestrecognition of specific corpora. OCR-D has a strong focus on historical prints,trainable layout detection and text recognition and open interfaces toaccommodate future developments, whereas ABBYY performs more strongly for modernprint. Finally, OCR-D is a community effort with a strong focus on transparencyand Free Software.What is the difference between OCR-D and Tesseract?Tesseract is the leading FreeSoftware OCR solution and tightly integrated into OCR-D in both a technical andorganizational sense. Technically, Tesseract has been wrapped asocrd_tesserocr, an OCR-D-compliantprocessor that is more powerful than the command line tool bundled withTesseract. Organizationally, Tesseract maintainers and contributors have beenpart of the OCR-D project from the beginning and the originally OCR-D-developedTesseract training tooltesstrain has been adopted bythe wider Tesseract community.What is the difference between OCR-D and TRANSKRIBUS?TRANSKRIBUS is a software platform and serverinfrastructure to make it easier for Digital Humanities practitioners tocollaborate on Handwriting Text Recognition. Apart from the different use cases  historical prints for OCR-D, historical manuscripts for TRANSKRIBUS - thereare differences in philosophy: All components of OCR-D are freely available asApache-licensed Free Software whereas some core components of TRANSKRIBUS,particularly the recognition engine, are proprietary. TRANSKRIBUS is aserver-client architecture with an Eclipse-based graphical user interface atits core whereas OCR-D’s focus is on mass digitization and command lineinteraction.Is OCR-D production-ready?Yes! Several libraries in Germany (e.g. Staatsbibliothek Berlin, ULB Göttingen, ULB Sachsen-Anhalt) are already using OCR-D at a large scale, with over 10 million pages digitized already.Which formats are supported by OCR-D?OCR-D is primarily based around METS as a container format and PAGE-XML forlayout detection and text recognition results. Other OCR formats such as ALTO,hOCR or ABBYY FineReader XML are supported through conversion withocrd_fileformat.The preferred image format within OCR-D is TIFF but PNG and JPEG are alsosupported. JPEG2000 is not currently supported but can be added in the futureif there is demand for it.Why does OCR-D need METS files? How can I process images without METS?The processes within OCR-D are designed around METS for the simple reason that it issuch an ubiquitous and well-defined format used in libraries and archivesaround the world. By relying on a container format instead of just images,processors can make use of more information and can store detailed results in awell-defined fashion.If the data to be processed isn’t already described by a METS file, the ocrd command linetool offers simple ways to create new METS files or augment existing ones.How much does it cost to deploy OCR-D?OCR-D is Free Software, licensed under the terms of the Apache 2.0 license andwill be free to use and adapt in perpetuity.What are the system requirements for OCR-D-software?The OCR-D/core framework is fairly lightcompared with other interoperability platforms. System requirements thereforedepend on the actual processors to be used and the scale of the operation. Itis possible to use OCR-D on commodity hardware such as desktop PCs and laptopsbut can also be deployed to massive servers or even single-board computers.However, OCR workflows can be very memory-intensive, in particular when workingwith large neural network models that have to be loaded into memory. We recommendat least 16 GB of RAM to support even the most demanding workflow steps.Another bottleneck for OCR workflows is input/output. We recommend storing dataon SSD instead of HDD.CLIHow can I find out the version of OCR-D software?To find the version of the OCR-D/core framework installed, run the ocrd CLIwith the --version flag:$ ocrd --versionocrd, version 2.2.2All OCR-D processors also support the --version flag, e.g.:ocrd-tesserocr-recognize --versionVersion 0.7.0, ocrd/core 2.2.2How do I get help on ocrd CLI commands?Every command and subcommand of the ocrd CLI tool supports the --helpoption to print a description, arguments and options:ocrd --helpocrd workspace --helpocrd workspace add --helpHow do I get help on OCR-D processors?All OCR-D-compliant processors support the -h/--help flag as well:$ ocrd-tesserocr-recognize --helpHow can I specify parameters on the command line?Parameters to an OCR-D-compliant processor must be specified in the JSON syntax. The JSON datacan be passed to a processor with the -p CLI option, which can be either the filename of a file containing the JSON data or the JSON data itself:ocrd-tesseract-recognize -I IN -O OUT -p &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39;# same effect:echo  &#39;{&quot;model&quot;: &quot;Fraktur&quot;}&#39; &amp;gt; /tmp/params.jsonocrd-tesseract-recognize -I IN -O OUT -p /tmp/params.jsonHow do I specify multiple input/output file groups?You can specify multiple file group names for both input and output by joiningthe names with a comma (,).ocrd-tesserocr-recognize -I DEFAULT,REGIONS -O OCR-TESSSERACTThis would instruct ocrd-tesserocr-recognize to take images from theDEFAULT group and region-segmented layout information from the REGIONSgroup.How to stop tensorflow logging spam  @bertsky  Another thing that needs to be added to tame TF isos.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; – before the tensorflow modulegets imported.To achieve the same, run this before executing a TF-based processor in theshell (or even add it to your $HOME/.bashrc to set this permanently):export TF_CPP_MIN_LOG_LEVEL=3",
      "url": " /en/faq.html"
    },
  

    {
      "slug": "de-faq-html",
      "title": "FAQ",
      "content"	 : "Bitte lesen sie die englische Version der Häufig Beantworteten Fragen.",
      "url": " /de/faq.html"
    },
  

    {
      "slug": "en-spec-glossary-html",
      "title": "OCR-D Glossary",
      "content"	 : "OCR-D Glossary  Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D frameworkThis section is non-normative.Layout and TypographyBlockSee RegionBorderFrom the PAGE-XML content schema documentation  Border of the actual page (if the scanned image contains parts not belonging to the page).Font familyWithin OCR-D, font family refers to grouping elements by font similarity. Thesemantics of a font family are up to the data producer.GlyphWithin OCR-D, a glyph is the atomic unit within a word.Grapheme ClusterSee GlyphLineSee TextLineReading OrderReading order describes the logical sequence of regions within a document.RegionA region is described by a polygon inside a page.Region typeThe semantics or function of a region such as heading, page number, column, table…SymbolSee GlyphTextLineA text line is a single row of words within a text region. (Depending on the region’s or page’s orientation, and the script’s writing direction, it can be horizontal or vertical.)Print spaceFrom the PAGE-XML content schema documentation  Determines the effective area on the paper of a printed page. Its size is equal for all pages of a book (exceptions: titlepage, multipage pictures).  It contains all living elements (except marginalia) like paragraphs and headings, as well as footnotes, headings, running titles.  It does not contain pagenumber (if not part of running title), marginalia, signature mark, preview words.WordA word is a sequence of glyphs within a line which does not contain any word-bounding whitespace. (That is, it includes punctuation and is synonym to token in NLP.)DataGround TruthGround truth (GT) in the context of OCR-D aretranscriptions, specific structure descriptions and word lists. These areessentially available in PAGE XML format in combination with the originalimage. Essential parts of the GT were created manually.We distinguish different usage scenarios for GT:Reference dataWith the term reference data, we refer to data that illustratesdifferent stages of an OCR/OLR process on representative materials. They aresupposed to support the assessment of commonly encountered difficulties and challenges whenrunning certain analysis operations and are therefore manually annotatedat all levels.Evaluation dataEvaluation data are used to quantitatively evaluate the performance of OCR toolsand/or algorithms. Parts of these data which correspond to the tool(s) under considerationare guaranteed to be recorded manually.Training dataMany OCR-related tools need to be adapted to the specific domain of the works which are tobe processed. This domain adaptation is called training. Data used to guide this processare called training data. It is essential that those parts of these data which are fedto the training algorithm are captured manually.ActivitiesBinarizationBinarization means converting all color or grayscale pixels in an image to either black or white.Controlled term: binarized (comments of a mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)See Felix’ Niklas interactive demoDewarpingManipulate an image in such a way that all text lines arestraightened and any geometrical distortions have been corrected.Controlled term: preprocessing/optimization/dewarpingSee Matt Zucker’s entry on Dewarping.DespecklingRemove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to remove “salt-and-pepper” noise resulting from Binarization.Controlled term: preprocessing/optimization/despecklingDeskewingRotate an image so that all text lines are horizontal.Controlled term: preprocessing/optimization/deskewingFont identificationDetect the font type(s) used in the document, either before or after an OCR run.Controlled term: recognition/font-identificationGrayscale normalization  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled term:  gray_normalized (comments in file)  preprocessing/optimization/cropping (step)Gray normalization is similar to binarization but instead of a purely bitonalimage, the output can also contain shades of gray to avoid inadvertentlycombining glyphs when they are very close together.Document analysisDocument analysis is the detection of structure on the document level to e.g. create a table of contents.Reading order detectionDetect the reading order of regions.CroppingDetecting the print space in a page, as opposed to the margins. It is a form ofregion segmentation.Controlled term: preprocessing/optimization/cropping.Border removal–&amp;gt; CroppingSegmentationSegmentation means detecting areas within an image.Specific segmentation algorithms are labelled by the semantics of the regionsthey detect not the semantics of the input, i.e. an algorithm that detectsregions is called region segmentation.Region segmentationSegment an image into regions. Also determines whether this is a textor non-text region (e.g. images).Controlled term:  SEG-REGION (USE)  layout/segmentation/region (step)Region classificationDetermine the type of a detected region.Line segmentationSegment text regions into textlines.Controlled term:  SEG-LINE (USE)  layout/segmentation/line (step)Line recognitionSee OCR.OCRMap pixel areas to glyphs and words.Word segmentationSegment a textline into wordsControlled term:  SEG-LINE (USE)  layout/segmentation/word (step)Glyph segmentationSegment a textline into glyphsControlled term: SEG-GLYPHText recognitionSee OCR.Text optimizationText optimization encompasses the manipulations to the text based on the stepsup to and including text recognition. This includes (semi-)automatically correctingrecognition errors, orthographical harmonization, fixing segmentation errors etc.Data PersistenceSoftware repositoryThe software repository contains all OCR-D algorithms and tools developedduring the project including tests. It will also contain the documentation andinstallation instructions for deploying a document analysis workflow.Ground Truth repositoryContains all the ground truth data.Research data repositoryThe research data repository may contain the results of allactivities during document analysis. At least it contains theend results of every processed document and its full provenance. The researchdata repository must be available locally.Model repositoryContains all trained (OCR) models for text recognition. The model repositoryhas to be available at least locally. Ideally, a publicly available model repository willbe developed.WorkspaceA workspace is a representation for some document in the local file system. Minimally it consists of a directory with a copy of the METS file. Additionally, that directory may contain physical data files and sub-directories belonging to the document (required or generated by run-time OCR-D processing), as referenced by the METS via mets:file/mets:FLocat/@href and mets:fileGrp/@USE. Files and sub-directories without reference (like log or config files) are not part of the workspace, as are references to remote locations. They can be added to the workspace by referencing them in the METS via their relative local path names.Workflow modulesThe OCR-D project divided the various elements of an OCRworkflow into six abstract modules.Image preprocessingManipulating the input images for subsequent layout analysis and text recognition.Layout analysisDetection of structure within the page.Text recognition and optimizationRecognition of text and post-correction of recognition errors.Model trainingGenerating data files from aligned ground truth text and images to configurethe prediction of text and layout recognition engines.Long-term preservation and persistenceStoring results of OCR and OLR indefinitely, taking into account versioning,multiple runs, provenance/parametrization and providing access to these savedsnapshots in a granular fashion.Quality assuranceProviding measures, algorithms and software to estimate the quality of the individual processes within the OCR-D domain.Component architecture(OCR-D) ApplicationApplication composed of various servers that can execute processors; can be a desktop computer or workstation, a distributed system comprising a controller and multiple processing servers, or an HPC cluster.OCR-D Web APIAs proposed in OCR-D/spec#173, the OCR-D Web API defines uniform and interdependent services that can be distributed across network components, depending on the use case.(OCR-D) ServiceGroup of endpoints of the OCR-D Web API; discovery/workspace/processing/workflow/…(OCR-D) ServerConcrete implementation of a subset of OCR-D services, or the network host providing it.(OCR-D) ControllerOCR-D Server (implementing at least discovery, workspace and workflow services) executing workflows (a single workflow or multiple workflows simultaneously), distributing tasks to configured processing servers, managing workspace data management. Should also manage load balancing.(OCR-D) Processing ServerOCR-D server (implementing at least discovery and processing services) that can execute one or more (locally installed) processors or evaluators, manages workspace data; implementer should consider whether a single OCR-D processing server (with page-parallel processing) best fits the use case, or multiple OCR-D processing servers (with document-parallel processing), or even dedicated OCR-D processing servers with GPU/CUDA support.(OCR-D) BackendSoftware component of a server concerned with network operations; e.g. Python library with request handlers, implementing service discovery and network-capable workspace data management.(OCR-D) Workflow Runtime LibrarySoftware component of a server or processor concerned with OCR systems modelling; e.g. Python library in OCR-D/core providing classes for all essential functional components (OcrdPage, OcrdMets, Workspace, Resolver, Processor, ProcessorTask, Workflow, WorkflowTask …), including mechanisms for signalling and orchestration of workflows, on top of which components (from processor to controller) can be implemented.(OCR-D) Workflow EngineCentral software component of the controller, executing workflows, including control structures (in a linear/parallel/incremental way). Also needed in single-host CLI deployments (where it can be based on inter-process communication and file system I/O alone), like ocrd process.(OCR-D) ProcessorA processor is a tool that implements the uniform OCR-D command-line-interface for run-time data processing. That is, it executes a single workflow step, or a combination of multiple workflow steps, on the workspace (represented by local METS), reading input files for all or requested physical pages of the input fileGrp(s), and writing output files for them into the output fileGrp(s). It may take a number of optional or mandatory parameters.→ OCR-D Workflow Guide(OCR-D) EvaluatorAn evaluator is a tool that implements the uniform OCR-D CLI for run-time quality estimation, assessing an activity’s annotation (i.e. a processor’s output) with some quality metric to yield a score and applying a given threshold against it to signal full or partial success/failure.(OCR-D) ModuleSoftware package/repository providing one or more processors or evaluators, possibly encompassing additional areas of functionality (training, format conversion, creation of GT, visualization)Modules can comprise multiple methods/activities that are called processorsfor OCR-D. There were eight MP in thesecond phase of OCR-D (2018-2020).MessagingMessaging service on the basis of Publish/Subscribe architecture (or similar) to coordinate network components, in particular for the distribution of tasks and load balancing, as well as signalling processor/evaluator results.OCR-D WorkflowCombination of activities via concrete processors and evaluators and their parameterization configured as a sequence or lattice, depending on their success or failure. Implemented in the OCR-D Workflow Runtime Library and serializable in a yet-to-specifcy format (as of 2020/10).The term Workflow is understood to encompass more features in other contexts, such as manual intervention by the user. In contrast to the terminology in workflow engines like Taverna or digitization frameworks like Kitodo, an OCR-D workflow is a fully automatic process.",
      "url": " /en/spec/glossary.html"
    },
  

    {
      "slug": "de-spec-glossary-html",
      "title": "OCR-D Glossar",
      "content"	 : "OCR-D Glossar  Glossar von Begriffen aus dem Bereich der Bildverarbeitung/OCR und deren Verwendung im Rahmen von OCR-DLayout und TypografieBorderAus der PAGE XML-Dokumentation  Rand der eigentlichen Seite (wenn das gescannte Bild Teile enthält, die nicht zur Seite gehören)GlypheIn OCR-D ist eine Glyphe die atomare Einheit innerhalb eines Wortes.Grapheme ClusterSiehe GlypheLesereihenfolge (Reading Order)Die Lesereihenfolge beschreibt die logische Abfolge von Regionen innerhalb eines Dokuments, wie sie von einem Menschen gelesen wird. Marginalien, Tabellen, Fußnoten und andere Elemente, die nicht in einer bestimmten Reihenfolge gelesen werden, sind nicht zwingend in der Beschreibung der Lesereihenfolge enthalten.LineSiehe TextLinePrintSpaceAus der PAGE XML-Dokumentation  Bestimmt die effektive Fläche einer gedruckten Seite auf dem Papier. [,,,]  Sie enthält alle lebenden Elemente (außer Marginalien) wie Absätze und Überschriften sowie Fußnoten, Überschriften, laufende Titel.  Sie enthält keine Seitenzahlen (wenn sie nicht Teil des laufenden Titels sind), Marginalien, Bogensignatur, Kustoden.RegionEine Region wird durch ein Polygon innerhalb einer Seite beschrieben.Region typeDie Semantik oder Funktion einer Region wie Überschrift, Seitenzahl, Spalte, Tabelle…SatzspiegelSiehe PrintSpaceSchriftfamilieInnerhalb von OCR-D bezieht sich Schriftfamilie auf die Gruppierung von Elementen nach Schriftähnlichkeit. Die Semantik einer Schriftfamilie bleibt denen überlassen, von denen die Daten erstellt werden.SymbolSiehe GlypheTextzeileEine Textzeile ist eine Reihe von Wörtern innerhalb einer Text-Region. (Je nach Ausrichtung der Region oder der Seite und je nach Schreibrichtung der Schrift kann sie horizontal oder vertikal sein).WortEin Wort ist eine Folge von Glyphen innerhalb einer Zeile, die keine wortbegrenzenden Leerzeichen enthält. (Das heißt, es schließt Interpunktion ein und ist ein Synonym für token im NLP.)ZeileSiehe TextzeileDatenGround TruthGround Truth (GT) im Kontext von OCR-D sindTranskriptionen, spezifische Strukturbeschreibungen und Wortlisten. Diese sindsind im PAGE XML-Format in Kombination mit dem Originalbild verfügbar.Wesentliche Teile der GT wurden manuell erstellt.Wir unterscheiden verschiedene Nutzungsszenarien für GT:EvaluierungsdatenEvaluierungsdaten dienen der quantitativen und qualitativen Bewertung der Leistung von OCR-Werkzeugen und/oder -Algorithmen. Da diese Daten zur Bewertung genutzt werden, können sie nicht mit dem selben OCR-Werkzeug erstellt werden wie die zu evaluierenden Daten, sondern müssen in einem kontrollierten und nachvollziehbaren Verfahren erstellt werden. Der manuelle Anteil an der Erstellung kann dabei bis zu 100% betragen. Je nach Zweck der Evaluation braucht man entsprechend annotierte Evaluierungsdaten. Wenn man beispielsweise einen Algorithmus zur Segmentierung von Regionen evaluieren möchte, benötigt man Evaluierungsdaten mit annotierten Regionen.ReferenzdatenMit dem Begriff Referenzdaten bezeichnen wir Daten, die verschiedene Stadien eines OCR/OLR-Prozesses an repräsentativen Materialien veranschaulichen. Sie sollen die Bewertung von häufig auftretenden Schwierigkeiten und Herausforderungen bei der Durchführung bestimmter Analysevorgänge unterstützen und werden daher auf allen Ebenen manuell kommentiert.TrainingsdatenViele OCR-Tools müssen an den spezifischen Bereich der zu bearbeitenden Werke angepasst werden. Diese Anpassung an den Bereich wird als Training bezeichnet. Die Daten, die zur Steuerung dieses Prozesses verwendet werden, nennt man Trainingsdaten. Es ist wichtig, dass die Teile dieser Daten, die dem Trainingsalgorithmus zugeführt werden, manuell erfasst werden und möglichst fehlerfrei sind.VerarbeitungsschritteBinarisierungBinarisierung bedeutet die Umwandlung aller Farb- oder Graustufenpixel eines Bildes in Schwarz oder Weiß.Controlled Term: binarized (comments einer mets:file), preprocessing/optimization/binarization (step in ocrd-tool.json)Siehe Felix Niklas’ interaktive DemoBorder removalsiehe CroppingCroppingErkennung des Satzspiegels auf einer Seite, im Gegensatz zu den Rändern. Dies ist eine Form derRegionensegmentierung.Controlled Term: preprocessing/optimization/cropping.DeskewingEin Bild so drehen, dass die meisten Textregionen aufrecht (d.h. von links nach rechts, von oben nach unten lesbar) und gerade (d.h. nicht schief) liegen.Controlled Term: preprocessing/optimization/deskewingDespecklingArtefakte wie Flecken, Tintenkleckse, Unterstreichungen usw. aus einem Bild entfernen. Wird üblicherweise angewendet, um “Salz-und-Pfeffer”-Rauschen zu entfernen, das durch Binarisierung entstanden ist.Controlled Term: preprocessing/optimization/despecklingDewarpingEin Bild so bearbeiten, dass alle Textzeilen begradigt und alle geometrischen Verzerrungen korrigiert sind.Controlled Term: preprocessing/optimization/dewarpingSiehe Matt Zuckers Eintrag zu Dewarping.DokumentenanalyseDie Dokumentenanalyse ist die Erkennung von Strukturen auf Dokumentenebene, um z.B. ein Inhaltsverzeichnis zu erstellen.Font-IdentifizierungErkennung der im Dokument verwendete(n) Schrift(en), entweder vor oder nach einem OCR-Prozess.Controlled Term: recognition/font-identificationGlyph segmentationEine Textzeile in Glyphen unterteilen.Controlled Term: SEG-GLYPHGraustufen-Normalisierung  ISSUE: https://github.com/OCR-D/spec/issues/41Controlled Term:  gray_normalized (comments in der Datei)  preprocessing/optimization/cropping (step in ocrd-tool.json)Die Graustufen-Normalisierung ist ähnlich wie die Binarisierung, aber statt eines rein bitonalenBildes kann die Ausgabe auch Graustufen enthalten, um zu verhindern, dass versehentlichGlyphen kombiniert werden, wenn sie sehr nahe beieinander liegen.Line recognitionSiehe OCR.OCRInterpretation von Pixelbereichen als Textregionen, Zeilen, Wörter und Zeichen. Meint im engeren Sinne die Elementaroperation des Mustererkenners (welche früher auf Zeichen, heute auf ganzen Wörtern oder Zeilen angewandt wird), im weiteren Sinne alle dazu vorab nötigen Verarbeitungsschritte, also auch die Segmentierung in Satzspiegel (d.h. Cropping), Regionen (d.h. Regionensegmentierung) und Zeilen (d.h. Zeilensegmentierung).RegionenklassifikationBestimmung des Typs einer erkannten Region.RegionensegmentierungSegmentiert ein Bild in Regionen. Bestimmt auch, ob es sich um eine Text oder Nicht-Text-Region (z.B. Bilder) handelt.Controlled Term:  SEG-REGION (USE)  layout/segmentation/region (step in ocrd-tool.json)SegmentierungSegmentierung bedeutet die Erkennung von Bereichen innerhalb eines Bildes.Spezifische Segmentierungsalgorithmen werden durch die Semantik der Regionen gekennzeichnet die sie erkennen, und nicht nach der Semantik der Eingabe, d. h. ein Algorithmus, der Regionen erkennt, wird Regionensegmentierung genannt.TextoptimierungDie Textoptimierung umfasst die Manipulationen am Text anhand der Schrittebis hin zur Texterkennung. Dazu gehören die (halb-)automatische Korrektur vonErkennungsfehlern, orthografische Vereinheitlichung, Korrektur von Segmentierungsfehlern usw.TexterkennungSiehe OCR.WortsegmentierungSegmentierung einer Textzeile in Wörter.Controlled Term:  SEG-LINE (USE)  layout/segmentation/word (step in ocrd-tool.json)ZeilensegmentierungSegmentiert Textregionen in Textzeilen.Controlled Term:  SEG-LINE (USE)  layout/segmentation/line (step in ocrd-tool.json)DatenpersistenzForschungsdaten-RepositoryDas Forschungsdaten-Repository kann die Ergebnisse aller Verarbeitungsschritte während der Dokumentenanalyse enthalten. Zumindest enthält es die Endergebnisse jedes verarbeiteten Dokuments und seine vollständige Provenienz. Das Forschungsdaten-Repository muss lokal verfügbar sein.Ground-Truth-RepositoryEnthält alle Ground-Truth-Daten.Modell-RepositoryEnthält alle trainierten (OCR-)Modelle für die Texterkennung. Das Modell-Repository muss zumindest lokal verfügbar sein. Idealerweise wird ein öffentlich zugänglicher Modellspeicherentwickelt werden.Software-RepositoryDas Software-Repository enthält alle OCR-D-Algorithmen und -Tools, diewährend des Projekts entwickelt wurden, einschließlich Tests. Es enthält auch die Dokumentation undInstallationsanweisungen für den Einsatz eines Dokumentenanalyse-Workflows.WorkspaceEin Workspace ist eine Repräsentation für ein Dokument im lokalen Dateisystem. Er besteht im Wesentlichen aus einem Verzeichnis mit einer Kopie der METS-Datei. Zusätzlich kann dieses Verzeichnis physische Datendateien und Unterverzeichnisse enthalten, die zu dem Dokument gehören (erforderlich oder durch die OCR-D-Verarbeitung zur Laufzeit erzeugt), wie sie von METS über mets:file/mets:FLocat/@href und mets:fileGrp/@USE referenziert werden. Dateien und Unterverzeichnisse ohne Verweis (wie Log- oder Konfigurationsdateien) sind nicht Teil des Workspaces, ebenso wenig wie Verweise auf entfernte Speicherorte. Sie können dem Arbeitsbereich hinzugefügt werden, indem sie in der METS-Datei über ihre relativen lokalen Pfadnamen referenziert werden.WorkflowmoduleDas OCR-D-Projekt hat die verschiedenen Elemente eines OCR-Workflows in sechs abstrakte Module aufgeteilt:  Bildvorverarbeitung  Layoutanalyse  Texterkennung und -optimierung  Modelltraining  Langzeitarchivierung und Persistenz  QualitätssicherungBildvorverarbeitungManipulation der Eingabebilder für die anschließende Layoutanalyse und Texterkennung.Langzeitarchivierung und PersistenzSpeicherung der Ergebnisse von OCR und OLR auf unbestimmte Zeit unter Berücksichtigung der Versionierung, mehrerer Durchläufe, Provenienz/Parametrisierung und Bereitstellung des granularen Zugriffs auf diese gespeicherten Snapshots.LayoutanalyseErkennung von Strukturen innerhalb der Seite.ModelltrainingGenerierung von Datendateien aus abgeglichenen Ground-Truth-Texten und -bildern zur Konfiguration der Vorhersage von Text- und Layout-Erkennungsprogrammen.Texterkennung und -optimierungErkennung von Text und Nachkorrektur von Erkennungsfehlern.QualitätssicherungBereitstellung von Messgrößen, Algorithmen und Software zur Bewertung der Qualität der einzelnen Prozesse innerhalb von OCR-D.Komponenten-ArchitekturMessagingMessagingdienst auf der Grundlage der Publish/Subscribe-Architektur (oder einer ähnlichen Architektur) zur Koordinierung der Netzkomponenten, insbesondere für die Verteilung von Aufgaben und den Lastausgleich sowie für die Übermittlung von Prozessor-/Evaluatorergebnissen.(OCR-D) ApplikationAnwendung, die aus verschiedenen Servern besteht, die Prozessoren ausführen können; kann ein Desktop-Computer oder eine Workstation sein, ein verteiltes System, das einen Controller und mehrere Verarbeitungsserver umfasst, oder ein HPC-Cluster.(OCR-D) BackendSoftwarekomponente eines Servers, die sich mit dem Netzbetrieb befasst; z. B. Python-Bibliothek mit Request-Handlern, die eine Dienstsuche und eine netzfähige Arbeitsbereichsdatenverwaltung implementieren.(OCR-D) ControllerOCR-D Server (Implementierung von mindestens Discovery-, Workspace- und Workflow-Diensten), Ausführung von Workflows (ein einzelner Workflow oder mehrere Workflows gleichzeitig), Verteilung von Aufgaben an konfigurierte Verarbeitungsserver, Verwaltung von Workspace-Daten. Sollte auch den Lastausgleich verwalten.(OCR-D) EvaluatorEin Evaluator ist ein Werkzeug, das die einheitliche OCR-D CLI für die Qualitätsbewertung zur Laufzeit implementiert, indem es die Anmerkung eines Verarbeitungsschrittes (d. h. die Ausgabe eines Prozessors) mit einer Qualitätsmetrik bewertet, um eine Metrik zu erhalten, und einen bestimmten Schwellenwert anwendet, um einen vollständigen oder teilweisen Erfolg/Fehlschlag zu signalisieren.(OCR-D) ModuleSoftwarepaket/Repository, das einen oder mehrere Prozessoren oder Evaluator bereitstellt und möglicherweise zusätzliche Funktionsbereiche umfasst (Training, Formatkonvertierung, Erstellung von GT, Visualisierung)Module können aus mehreren Methoden/Aktivitäten bestehen, die in OCR-D als Prozessoren bezeichnet werden. Es gab acht Modulprojekte in der zweiten Phase von OCR-D (2018–2020). In der aktuellen dritten Phase (2021–2024) gibt es drei Modulprojekte.(OCR-D) Processing-ServerOCR-D-Server (der mindestens Discovery- und Processing-Dienste implementiert), der einen oder mehrere (lokal installierte) Prozessoren oder Evaluator ausführen kann und Workspacedaten verwaltet; die Implementierer sollten abwägen, ob ein einzelner OCR-D-Processing-Server (mit seitenparalleler Verarbeitung) oder mehrere OCR-D-Processing-Server (mit dokumentenparalleler Verarbeitung) oder sogar dedizierte OCR-D-Processing-Server mit GPU/CUDA-Unterstützung am besten für den Anwendungsfall geeignet ist.(OCR-D) ProzessorEin Prozessor ist ein Werkzeug, das die einheitliche OCR-D-Befehlszeilenschnittstelle für die Datenverarbeitung zur Laufzeit implementiert. Das heißt, er führt einen einzelnen Workflowschritt oder eine Kombination mehrerer Workflowschritte auf dem Workspace (dargestellt durch lokale METS) aus, wobei er Eingabedateien für alle oder angeforderte physische Seiten der fileGrp(s) liest und Ausgabedateien für sie in die Output-fileGrp(s) schreibt. Er kann eine Reihe von optionalen oder obligatorischen Parametern erwarten.→ OCR-D Workflow Guide(OCR-D) ServerKonkrete Implementierung einer Teilmenge von OCR-D-Diensten oder der Netzwerk-Host, der sie bereitstellt.(OCR-D) ServiceGruppe von Endpunkten der OCR-D Web-API; discovery/workspace/processing/workflow/…OCR-D Web APIWie in OCR-D/spec#173 vorgeschlagen, definiert die OCR-D-Web-API einheitliche und voneinander abhängige Dienste, die je nach Anwendungsfall auf Netzkomponenten verteilt werden können.OCR-D WorkflowKombination von Verarbeitungsschritten über konkrete Prozessoren und Evaluatoren und deren Parametrisierung als Sequenz oder Verband konfiguriert, abhängig von deren Erfolg oder Misserfolg. Implementiert in der OCR-D Workflow Runtime Library und serialisierbar in einem noch zu spezifizierenden Format.Der Begriff Workflow wird in anderen Kontexten so verstanden, dass er mehr Funktionen umfasst, wie z.B. manuelle Eingriffe durch den Benutzer. Im Gegensatz zur Terminologie in Workflow-Engines wie Taverna oder Digitalisierungs-Frameworks wie Kitodo ist ein OCR-D-Workflow ein vollautomatischer Prozess.(OCR-D) Workflow EngineZentrale Softwarekomponente des Controllers, die Arbeitsabläufe, einschließlich Kontrollstrukturen (linear/parallel/inkrementell), ausführt. Wird auch bei CLI-Einsätzen auf einem einzigen Host benötigt (wo es allein auf Interprozesskommunikation und Dateisystemein- und -ausgabe beruhen kann), z. B. ocrd process.(OCR-D) Workflow Runtime LibrarySoftwarekomponente eines Servers oder Prozessors, die sich mit der Modellierung von OCR-Systemen befasst; z.B. Python-Bibliothek in OCR-D/core, die Klassen für alle wesentlichen funktionalen Komponenten (OcrdPage, OcrdMets, Workspace, Resolver, Processor, ProcessorTask, Workflow, WorkflowTask …) bereitstellt, einschließlich Mechanismen zur Signalisierung und Orchestrierung von Workflows, auf denen Komponenten (vom Prozessor bis zum Controller) implementiert werden können.",
      "url": " /de/spec/glossary.html"
    },
  

    {
      "slug": "de-impressum-html",
      "title": "Impressum",
      "content"	 : "ImpressumNachstehend finden Sie die gesetzlich geregelten Pflichtangaben zur Anbieterkennzeichnung sowie rechtliche Hinweise zur Internetpräsenz des OCR-D-Projekts.Angaben gemäß § 5 TMGAnbieterAnbieter dieser Internetpräsenz ist im Rechtssinne die Herzog August Bibliothek Wolfenbüttel.Herzog August Bibliothek,Lessingplatz 1,D-38304 WolfenbüttelTel.: +49(0)5331/808-0,Fax: +49(0)5331/808-302,E-Mail: auskunft@hab.dehttp://www.hab.deVertreterDie HAB Wolfenbüttel wird vertreten durch ihren Direktor, Herrn Prof. Dr. Peter BurschelUmsatzsteueridentifikationsnummer: DE811255517Verantwortlich für den Inhalt nach § 55 Abs. 2 RStV:Johannes Mangei, Herzog August Bibliothek,Lessingplatz 1,D-38304 WolfenbüttelTel.: +49(0)5331/808-3303,E-Mail: mangei@hab.deHaftungsausschluss:Haftung für InhalteDie Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit,Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen.Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nachden allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieterjedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachenoder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungenzur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleibenhiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniseiner konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungenwerden wir diese Inhalte umgehend entfernen.Haftung für LinksUnser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einflusshaben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für dieInhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seitenverantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf möglicheRechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nichterkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkreteAnhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungenwerden wir derartige Links umgehend entfernen.UrheberrechtDie durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen demdeutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art derVerwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung desjeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten,nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreibererstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritterals solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksamwerden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungenwerden wir derartige Inhalte umgehend entfernen.DatenschutzDie Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich.Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift odereMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis.Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben.Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation perE-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriffdurch Dritte ist nicht möglich.Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Drittezur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wirdhiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklichrechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durchSpam-Mails, vor.Wir verwenden Google Programmable Search Engine als Suchdienst auf der Website.Der eingebundene Suchdienst ermöglicht eine Volltextsuche auf ocr-d.de.Bei der Nutzung des Suchdienstes werden Daten an Google übertragen. Dazu gehören z.B. die von Ihnen eingegebenen Suchbegriffe und die IP-Adresse des von Ihnen benutzten Gerätes. Zudem können bei Nutzung des Dienstes Cookies gespeichert werden, was Sie über Ihre Browsereinstellungen verhindern können.Mit der Benutzung der Volltext-Suche stimmen Sie der Nutzung des Google-Suchdienstes und damit auch der Übertragung von Daten an den Google-Service sowie ggf. der Speicherung von Cookies zu. Sofern Sie gleichzeitig bei Google eingeloggt sind, kann Google zudem die Informationen unmittelbar Ihrem Nutzerprofil zuordnen. Sie können sich ausloggen, um eine Sammlung von Profilinformationen über Sie zu vermeiden.Wir weisen ausdrücklich darauf hin, dass die Verarbeitung, insbesondere die Speicherung, Nutzung und Löschung von möglicherweise übertragenen personenbezogenen Daten dem Anbieter des Suchdienstes obliegt.Die Datenschutzerklärung von Google finden Sie unter: http://www.google.com/intl/de/policies/privacyVerwendete BilderDiese Website nutzt die folgenden Abbildungen aus Wikimedia Commons:Kupferstich “Le cabinet de la Bibliotheque de Sainte Genevieve”, Paris: 1692, von Franz ErtingerTyp 615.92.341, Houghton Library, Harvard UniversityKupferstich “Calculating machine designed by Pascal”. Louvet sculpt. Rue Galande No. 51https://lccn.loc.gov/2006690493, Library of CongressWebsite Impressum erstellt durch impressum-generator.de von der Kanzlei Hasselbach",
      "url": " /de/impressum.html"
    },
  

    {
      "slug": "en-imprint-html",
      "title": "Imprint",
      "content"	 : "ImprintIn the following you will find the legally regulated obligatory information on provideridentification as well as legal information on the Internet presence of the OCR-D project.Information according to § 5 TMGProviderThe provider of this internet presence in the legal sense is the Herzog August Bibliothek Wolfenbüttel.Duke August Library,Lessingplatz 1,D-38304 WolfenbüttelPhone: +49(0)5331/808-0,Fax: +49(0)5331/808-302,E-Mail: auskunft@hab.dehttp://www.hab.deRepresentativeThe HAB Wolfenbüttel is represented by its director, Prof. Dr. Peter BurschelSales tax identification number: DE811255517Responsible for the content according to § 55 Abs. 2 RStV:Johannes Mangei,Duke August Library,Lessingplatz 1,D-38304 WolfenbüttelPhone: +49(0)5331/808-303,E-Mail: mangei@hab.deDisclaimer:Liability for contentsThe contents of our pages were created with the greatest care. However, we cannot guaranteethat the contents are correct, complete and up-to-date. As a service provider, we areresponsible for our own content on these pages according to § 7 para.1 TMG (German TelemediaAct) and general laws. According to §§ 8 to 10 TMG we are not obliged to monitor transmittedor stored information from third parties or to investigate circumstances that indicateillegal activity. Obligations for the removal or blocking of the use of information accordingto the general laws remain unaffected. However, liability in this respect is only possiblefrom the time of knowledge of a concrete infringement. If we become aware of any such legalinfringements, we will remove these contents immediately.Liability for linksOur offer contains links to external websites of third parties, on whose contents we haveno influence. Therefore we cannot assume any liability for these external contents. As theContents of the linked pages are always the responsibility of the respective provider oroperator of the pages.The linked pages were checked for possible legal violations at thetime of linking. Illegal contents were not identified at the time of linking. However, apermanent control of the contents of the linked pages is not reasonable without concreteevidence of a violation of the law.  If infringements of rights become knownwe will remove such links immediately.CopyrightThe contents and works on these pages created by the site operators are subject to theGerman copyright law. The duplication, processing, distribution and any kind ofuse outside the limits of copyright law requires the written consent of thethe respective author or creator.Downloads and copies of these pages are only permittedfor private, non-commercial use. Insofar as the contents on this site were not created bythe operator, the copyrights of third parties are observed. In particular, third-partycontent is identified as such. Should you nevertheless become aware of a copyrightinfringement, please inform us accordingly. If we become aware of any infringements, wewill remove such contents immediately.Privacy policyThe use of our website is usually possible without providing personal data.As far as personal data (e.g. name, address or e-mail addresses) is collected on our website,this is always done on a voluntary basis, as far as possible.This data will not be passed on to third parties without your express consent.We would like to point out that data transmission on the Internet (e.g. communication viaE-Mail) can have security gaps. A complete protection of the data against accessby third parties is not possible.The usage of contact data published within the scope of the imprint obligation by thirdparties for the transmission of not expressly requested advertising and information materialis hereby expressly contradicted. The operators of the pages expressly reserve the right totake legal action in the event of the unsolicited sending of advertising information, forexample through spam mails.We use Google Programmable Search Engine as a search service on the website.The integrated search service enables a full text search on ocr-d.de.When using the search service, data is transferred to Google. This includes, for example, the search terms you enter and the IP address of the device you are using. In addition, cookies may be stored when you use the service, which you can prevent via your browser settings.By using the full-text search, you consent to the use of the Google search service and thus to the transfer of data to Google,and thus also to the transfer of data to the Google service and, if applicable, to the storage of cookies. If you are logged in to Google at the same time, Google can also assign the information directly to your user profile. You can log out to avoid the collection of profile information about you.We expressly point out that the processing, in particular the storage, use and deletion of any personal data that may be transmitted is the responsibility of the provider of the search service.You can find Google’s privacy policy at: http://www.google.com/intl/de/policies/privacyImages usedThis website uses the following images from Wikimedia Commons:Copper engraving “Le cabinet de la Bibliotheque de Sainte Genevieve”, Paris: 1692, byFranz ErtingerType 615.92.341, Houghton Library, Harvard UniversityCopper engraving “Calculating machine designed by Pascal”. Louvet sculpt. Rue Galande No. 51https://lccn.loc.gov/2006690493, Library of CongressWebsite imprint created by impressum-generator.de of the law firm Hasselbach",
      "url": " /en/imprint.html"
    },
  

    {
      "slug": "en",
      "title": "",
      "content"	 : "                                open source    Download, use and adapt our software. Free for any purpose.scalableYour solution for mass digitisation and creating large corpora of full texts.    flexibleDefine your own OCR Workflows. Use OCR-D in you command line or try out various implementations, e.g. OCR-D in Kitodo or OCR&amp;#8209;D in OCR4all.    community drivenBenefit from a helpful community. Join our regular online meetings and use our chat. Connect with service providers and our partners for your individual needs.                                           Get Started     Quick StartInstall OCR&amp;#8209;D in a few stepsSetup Guide  Advanced instructions and on how to install the OCR&amp;#8209;D stackUser Guide  Instructions how to use OCR&amp;#8209;D components Workflows  Steps of an OCR&amp;#8209;D-workflow with sample workflows    Models  Learn how to use different models in OCR&amp;#8209;D  FAQ Frequently Asked QuestionsSearch our WebsiteFor this feature, we implemented Google Custom Search. If you use it, please note that cookies may be stored and Privacy Policy by Google LLC applies: https://policies.google.com/privacy Agree, show me Google Search!                       Technical ResourcesOCR&amp;#8209;D Decision Log Recent decisions on software development              OCR&amp;#8209;D Specifications   OCR&amp;#8209;D Specifications for CLI, METS, PAGE etc.    OCR&amp;#8209;D/core API documentation  Python API documentation of core implementation    OCR&amp;#8209;D development best practices  Practical information on distributed development of software and specifications in OCR&amp;#8209;D    Ground Truth  Ground Truth corpus and special corpora    Ground Truth Guidelines  Guidelines for Ground Truth    PAGE-XML format documentation  Documentation for the PAGE-XML format    QUIVER  Quality Overview for OCR&amp;#8209;D Repositories and Workflows                                        Further Information About OCR&amp;#8209;D  Learn more about OCR&amp;#8209;DCommunity Communication channels and participationProjects  Information about our implementation and module projects in phase III  Reference Data  Ground Truth corpus and special corpora  Glossary  Glossary of technical terms used in OCR&amp;#8209;D  Contact  Contact us!    ",
      "url": " /en/"
    },
  

    {
      "slug": "de",
      "title": "",
      "content"	 : "                                BlogAktuelles vom OCR&amp;#8209;D-ProjektÜber OCR&amp;#8209;DErfahren Sie mehr über das OCR&amp;#8209;D-ProjektCommunityKommunikationskanäle und BeteiligungsmöglichkeitenProjekteInformationen über unsere Implementierungs- und Modulprojekte in&amp;nbsp;Phase&amp;nbsp;IIIReferenzdatenGround-Truth-Korpus und SpezialkorporaKontaktKontaktieren Sie uns!GitHub|Gitter|Twitter                                       Implementierungsprojekte     Integration von Kitodo und OCR&amp;#8209;D zur produktiven MassendigitalisierungOPERANDI – OCR&amp;#8209;D Performanzoptimierung und IntegrationOCR4all libraries – Volltexterkennung historischer SammlungenODEM: OCR&amp;#8209;D Erweiterung für MassendigitalisierungModulprojekteWorkflow für werkspezifisches Training auf Basis generischer Modelle mit OCR&amp;#8209;D sowie Ground-Truth-AufwertungErkennung von Schriftartgruppen zur OCR-Verbesserung  OLA-HD Service - Ein generischer Dienst für die Langzeitarchivierung historischer Drucke                              Technische Dokumentation            OCR&amp;#8209;D Decision Log Aktuelle Entscheidungen zur SoftwareentwicklungOCR&amp;#8209;D Spezifikationen Spezifikationen für CLI, METS, PAGE, etc.OCR&amp;#8209;D/core API DokumentationPython API Dokumentation für die core Referenzimplementierung    Best Practice für Softwareentwicklung in OCR&amp;#8209;D  Praktische Informationen zur verteilten Entwicklung von Software und Spezifikationen in OCR&amp;#8209;D    Ground Truth Richtlinien  Transkriptionsrichtlinien für Ground Truth    PAGE-XML Formatdokumentation  Dokumentation zum PAGE-XML Format  Nutzungsinformationen und AnleitungenSetup AnleitungSchritt-für-Schritt Anleitung zur Installation von OCR&amp;#8209;D (aktuell&amp;nbsp;nur&amp;nbsp;auf&amp;nbsp;Englisch&amp;nbsp;verfügbar)NutzeranleitungInstruktionen zum Arbeiten mit OCR&amp;#8209;D (aktuell&amp;nbsp;nur&amp;nbsp;auf&amp;nbsp;Englisch&amp;nbsp;verfügbar)Workflows Schritte eines OCR&amp;#8209;D-Workflows mit Beispielworkflows (aktuell&amp;nbsp;nur&amp;nbsp;auf&amp;nbsp;Englisch&amp;nbsp;verfügbar)Modelle  Überblick zu Modellen verschiedener OCR-Engines (aktuell&amp;nbsp;nur&amp;nbsp;auf&amp;nbsp;Englisch&amp;nbsp;verfügbar)Glossar Fachbegriffe aus dem Bereich der OCR erklärt            FAQ(aktuell nur auf Englisch verfügbar)SucheFür diese Funktion haben wir Google Programmable Search Engine implementiert. Wenn Sie diese nutzen, beachten Sie bitte, dass Cookies gespeichert werden können und die Datenschutzbestimmungen von Google LLC gelten:https://policies.google.com/privacy  Einverstanden, zeige die Google-Suche! ",
      "url": " /de/"
    },
  

    {
      "slug": "",
      "title": "",
      "content"	 : "                                                      Deutsch                                                                                          English                                                ",
      "url": " /"
    },
  

    {
      "slug": "slides-2019-03-25-dhd",
      "title": "",
      "content"	 : "Workshop DHD 2019Theorie: Einführung in OCR allgemein20’ @kbaSlidesTheorie: OCR-D Projektuebersicht10’ @ehrmnSlidesTheorie: OCR-D Ground Truth10’ @tboenigSlidesTheorie: OCR-D Repository15’ @VolkerHartmannSlides (Google Slides)Slides (PowerPoint)Demo (hackmd)Theorie: OCR-D Spezifikationen und Software15’ @kbaSlidesPraxis: Installation des OCR-D Stack30’ @kba, @VolkerHartmannSetup guidePause (30’)Praxis: Erstellen von Ground Truth30’ @tboenigGuidePraxis: OCR-D auf existierendes METS anwenden30’ @kbaGuidePraxis: OCR-D auf willkürliche Bilder anwenden15’ @kbaGuidePraxis: OCRD-ZIP erzeugen und untersuchenGuide",
      "url": " /slides/2019-03-25-dhd/"
    },
  

    {
      "slug": "en-spec",
      "title": "Specifications",
      "content"	 : "            CLI      Requirements for command line tools                  Web API      Information for developers using the OCR&amp;#8209;D Web API                    Nextflow      Usage of Nextflow in OCR&amp;#8209;D                  METS      OCR&amp;#8209;D METS conventions                  OCRD-ZIP      METS workspace serialized as ZIP                  ocrd-tool.json      OCR&amp;#8209;D tool description                  PAGE      PAGE conventions                  GT Guidelines      OCR&amp;#8209;D Ground Truth transcription guidelines                    Metrics for&amp;nbsp;QA      Definitions for Quality Assurance used in OCR&amp;#8209;D                  Dockerfile      OCR&amp;#8209;D Dockerfile conventions      ",
      "url": " /en/spec/"
    },
  

    {
      "slug": "de-spec",
      "title": "",
      "content"	 : "            CLI      Anforderungen an Kommandozeilentools                  Web API      Informationen zur OCR&amp;#8209;D Web API                  Nextflow      Nextflow in OCR&amp;#8209;D                  METS      OCR&amp;#8209;D METS Konventionen                  OCRD-ZIP      ZIP-Serialisierung METS-basierter Workspaces                  ocrd-tool.json      Die Beschreibungssprache der OCR&amp;#8209;D Werkzeuge                  PAGE      PAGE conventions                  GT Richtlinien      OCR&amp;#8209;D Ground Truth Transkriptionsrichtlinien                   Metriken für&amp;nbsp;QA      Definitionen für Qualitätssicherung in OCR&amp;#8209;D                  Dockerfile      OCR&amp;#8209;D Dockerfile conventions      ",
      "url": " /de/spec/"
    },
  

    {
      "slug": "en-initial-tests-html",
      "title": "Results and findings of the first OCR-D test",
      "content"	 : "Results and findings of the first OCR-D testBackgroundAt the turn of the year 2019/2020, the OCR-D software was tested for the firsttime in nine pilot libraries. This was to ensure the practical acceptance ofthe software by future, potential users. Therefore the focus was on itsfunctionality and usability in practice. In addition to the partners of thecoordination project, two libraries involved in the module projects as well asfour other libraries took part in the testing. The findings of this first testrun will be incorporated into the further development of the OCR-D prototype.All pilot libraries have some know-how and experience with OCR, as they havealready produced full texts at least on project level or via commercial serviceproviders. The extent to which OCR, which is regarded as important, should becarried out in-house in future and firmly anchored in the digitisationworkflow, is currently discussed in the libraries. Concerning the target groupsof the full texts there are different opinions in the testing institutions.While one third name humanities scholars in general, another third would liketo serve a very broad target group (humanities, digital humanities, computerlinguistics and economics). The remaining libraries only see relatively few,specialised users (Digital Humanities or Computational Linguistics) as thetarget group for OCR texts.The pilot libraries demand the following features of an OCR software:  High recognition rate of layout and text  Cost-effective use  Quick adaptability/troubleshooting  Modularity  Output in standard formats  Connection to existing workflows  Well-documented interfaces  Word coordinates  Trainability  Extensive GT corporaHigh quality of text recognition is most important, whereas the othercharacteristics are only named each by some of the pilot libraries and shouldbe considered as desired but subordinate optional features.Evaluation of the software testsIn order to ensure the comparability of the individual tests in the pilotlibraries, a questionnaire distributed to the pilot libraries at the beginningof the test. In this questionnaire, the general conditions of the test run,e.g. the technical equipment used and the tested OCR-D processors, as well asthe documentation of the software, interfaces, functionality and usability ofthe software, its possibilities for being integrated into existing workflowsand the required output formats are recorded. Furthermore, recognition quality,functionality and usability, open requirements and positive features of theOCR-D software as well as the results of the test were to be described.During the test phase, the various options for installing the OCR-D softwarewith and without a Docker container were tried out and the software wassuccessfully installed on a wide range of servers with varying processingpower, some of them virtual. For Non-Intel CPU architectures (ARM, PowerPC64)this was more complicated and time consuming, as individual Python packageswere not packaged for these computers and had to be adapted manually. Thecomplete installation of all available OCR-D processors (ocrd_all), which wasdeveloped during the test phase, was confirmed as the least complex variant andwas therefore the most recommendable. No pilot library integrated the OCR-Dsoftware In a workflow software such as Kitodo, as this would have meant toomuch work for a simple test run.The usage of the numerous OCR-D processors was described as a challenge.Calling the processors was not so much of a problem than understanding theirrespective areas of application and, in particular, selecting and combining theprocessors to form meaningful workflows. For the first test, besides thetechnical documentation of the software, there was no overall documentation onits use available yet, which is also aimed at users inexperienced in OCR. Therequirements and wishes of the testers for such a documentation were taken intoaccount when preparing the manuals, which are now available in the user areaof the OCR-D website and will be improvedcontinuously.The OCR-D software runs very stable, no library reported any crashes. All ofthe required output formats are already provided, whereas requested changes onOCR-D’s interfaces are already planned for the further development of theprototype.The recognition quality was only checked on individual pages by the respectivepilot libraries, as there is no Ground Truth available for the test books.Overall, the results of this first test run are promising. The MannheimUniversity Library has tested OCR-D with a focus on Tesseract processors onfive prints from the 16th to 19th century. On antiqua prints from the 17th and18th centuries and a blackletter (Fraktur) text from the 19th century, asexpected, the best results of - in the case of the antiqua - significantly lessthan 0.1 CER were achieved for the raw data, whereas the blackletter printsfrom the 17th century was slightly above 0.1 CER. The greatest challenge wasthe 16th century blackletter, where only a CER of just under 0.16 was reached.The BBAW provides a comprehensive insight into their testing, by making theirreport and data publiclyavailable.The OCR-D testers formulate some desiderata especially with regard todocumentation, quality and usability of the processors as well as their futurescalability. The requirements for the documentation of the OCR-D software havealready been implemented to a large extent, though documentation as a whole isregarded as a continuous task which must successively include above allpractical experience in the application of the OCR-D software. For theprocessors, some improvements would still be desirable, especially in layoutrecognition and post-correction. The corresponding developments of the OCR-Dmodule projects could only be tested to a limited extent due to theirdevelopment stage or their special technical requirements (GPU). Hopefully, theabove mentioned desiderata can be met with their results or with furthermodels. For the use of the OCR-D software in mass digitisation, the runtime ofseveral processors - as originally planned for the third project phase - stillneeds to be optimised, and the possibilities for parallelisation should also beexpanded.The testers positively emphasized the modular and transparent structure of theOCR-D software, which distinguishes it in particular from other OCR solutionsand allows the its users to configure optimal workflows for particular usecases. Furthermore, all OCR-D source code is freely available and can befurther developed by experts specialized in their respective fields and can beadapted in-house for usage in experiments on the OCR workflow without extensiveprogramming work. In case of questions and problems, the developers quicklyprovide low-threshold support. All in all, it is comparatively easy to initiatethe robustly running OCR-D full-text generation process, which is still in needof further optimization but already delivers promising results.",
      "url": " /en/initial-tests.html"
    },
  

    {
      "slug": "en-spec-intro-html",
      "title": "OCR-D Specs Overview",
      "content"	 : "OCR-D Specs OverviewSince OCR-D focuses on improving access to mass digitization for historicalprints, it is important that its tools are sufficiently uniform in their interfacesand data access patterns to support the widest possible application withinGLAM digitization workflows.This website lays out a set of conventions and interface definitions that mustbe implemented by the OCR-D module projects (MP) to be usable within the OCR-D ecosphere.CLISoftware developed by MP must be executable with acommand line interface (CLI) on a Linux OS. CLI are straightforward to run andtest and can be easily embedded in automated setups. The mechanics of OCR-Dconformant CLI tools are laid out in the CLI specs.METSTo allow processing OCR-related data in a digitization workflow, a uniform dataexchange format is necessary. OCR-D decided to use the widely used METS formatand has developed conventions on how MP must access and manipulateMETS data in order to be interoperable.ocrd-tool.jsonInteroperability needs metadata, both descriptive and technical. OCR-D hasdeveloped a format that allows MP to express general informationabout themselves and detailed information about the tools they develop.RESTOCR-D will offer RESTful access to the MP CLI based on HTTP, usingthe Open API / Swagger set of tools.DockerfileDocker is a widely used system for containerization of software. MPs areencouraged to package the tools they develop as a docker image by providing aDockerfile. OCR-D offers recommendations on how the Dockerfile should bestructured.",
      "url": " /en/spec/intro.html"
    },
  

    {
      "slug": "de-kontakt-html",
      "title": "Das OCR-D-Team",
      "content"	 : "Das OCR-D-TeamKontaktAnsprechpartnerin:Leonie EckertHerzog August Bibliothek Wolfenbütteleckert[at]hab.deStellvertreter*innen:Sandra SimonStellvertreterin der Abteilungsleitung Neuere Medien, Digitale BibliothekHerzog August Bibliothek Wolfenbüttelsimon[at]hab.deJohannes MangeiStellvertretender Direktor der Herzog August BibliothekAbteilungsleitung Neuere Medien, Digitale BibliothekHerzog August Bibliothek Wolfenbüttelmangei[at]hab.deProjektverantwortlicheJohannes MangeiHerzog August Bibliothek Wolfenbüttelmangei[at]hab.deAlexander GeykenBerlin-Brandenburgische Akademie der Wissenschaften in Berlingeyken[at]bbaw.deReinhard AltenhönerStaatsbibliothek zu Berlin Preußischer Kulturbesitzreinhard.altenhoener[at]sbb.spk-berlin.deMustafa DoganNiedersächsische Staats- und Universitätsbibliothek Göttingendogan[at]sub.uni-goettingen.dePhilipp WiederGesellschaft für wissenschaftliche Datenverarbeitung Göttingenphilipp.wieder[at]gwdg.deProjektmitarbeitendeLeonie EckertProjektkoordinatorinHerzog August Bibliothek Wolfenbütteleckert[at]hab.deKonstantin BaiererStaatsbibliothek zu Berlin - Preußischer Kulturbesitzkonstantin.baierer[at]sbb.spk-berlin.deClemens NeudeckerStaatsbibliothek zu Berlin - Preußischer Kulturbesitzclemens.neudecker[at]sbb.spk-berlin.deKristine Schima-VoigtNiedersächsische Staats- und Universitätsbibliothek Göttingenschima-voigt[at]sub.uni-goettingen.deMichelle WeidlingNiedersächsische Staats- und Universitätsbibliothek Göttingenweidling[at]sub.uni-goettingen.dePaul PestovNiedersächsische Staats- und Universitätsbibliothek Göttingenpestov[at]sub.uni-goettingen.deTriet DoanGesellschaft für wissenschaftliche Datenverarbeitung Göttingentriet.doan[at]gwdg.deMehmed MustafaGesellschaft für wissenschaftliche Datenverarbeitung Göttingenmehmed.mustafa[at]gwdg.deJonas SchreweGesellschaft für wissenschaftliche Datenverarbeitung Göttingenjonas.schrewe[at]gwdg.deEhemalige ProjektbeteiligteMarkus BrantlBayerische Staatsbibliothek MünchenRainer StotzkaKarlsruhe Institute of TechnologyThomas StäckerHerzog August Bibliothek WolfenbüttelEhemalige ProjektmitarbeitendeMatthias BoenigBerlin-Brandenburgische Akademie der Wissenschaften in BerlinElisabeth EnglHerzog August Bibliothek WolfenbüttelMareen GeestmannNiedersächsische Staats- und Universitätsbibliothek GöttingenVolker HartmannKarlsruher Institut für Technologie/Steinbuch Centre for ComputingElisa HerrmannHerzog August Bibliothek WolfenbüttelLena HinrichsenHerzog August Bibliothek WolfenbüttelSebastian MangoldBayerische Staatsbibliothek MünchenAjinkya PrabhuneKarlsruher Institut für TechnologieKay-Michael WürznerBerlin-Brandenburgische Akademie der Wissenschaften in BerlinWissenschaftlicher BeiratDas Projekt wurde von einem wissenschaftlichen Beirat aus den folgenden Personen beraten:Max KaiserÖsterreichische NationalbibliothekJoachim KöhlerFraunhofer IAISGerhard LauerJGU MainzSebastian MeyerSLUB DresdenGünter MühlbergerUniversität InnsbruckKlaus SchulzLMU München – CISThomas StäckerULB DarmstadtRobert StrötgenTU BraunschweigKooperationspartnerDas Projekt hat mit mehreren Unternehmen, Non-Profit-Organisationen und verwandten Projekten Absichtserklärungen (Letters of Intent)unterzeichnet, die hauptsächlich darauf abzielen, sich über Schnittstellen für und aktuelle Entwicklungen in der Massenvolltextdigitalisierungauszutauschen. Darüber hinaus sind diese Kooperationen Teil der Disseminationsstrategie von OCR-D, die im weiteren Verlauf des Projektsund mit der Entwicklung der OCR-D-Software zu einem einsatzfähigen, stabilisierten Produkt zunehmend an Bedeutung gewinnen wird.  Content Conversion Specialists (CCS)  Kitodo. Key to digital objects e.V.  OCR4all, GitHub  semantics Kommunikationsmanagement GmbH  Zeutschel GmbH",
      "url": " /de/kontakt.html"
    },
  

    {
      "slug": "en-kwalitee-html",
      "title": "Kwalitee",
      "content"	 : "           GitHub              Last update              Number of contributors                 cor-asv-ann    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-annn    OCR post-correction with encoder-attention-decoder LSTMsnn[![CircleCI](https://circleci.com/gh/ASVLeipzig/cor-asv-ann.svg?style=svg)](https://circleci.com/gh/ASVLeipzig/cor-asv-ann)nn## IntroductionnnThis is a tool for automatic OCR _post-correction_ (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the _character level_ with a model architecture akin to neural machine translation, i.e. a stacked **encoder-decoder** network with attention mechanism. nnThe **attention model** always applies to full lines (in a _global_ configuration), and uses a linear _additive_ alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a _soft alignment_ between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start &quot;forgetting&quot; the input altogether at some point in the line and behave like unconditional LM generators.)nn...FIXME: mention: n- stacked architecture (with bidirectional bottom and attentional top), configurable depth/widthn- weight tyingn- underspecification and gapn- confidence input and alternative inputn- CPU/GPU optionn- incremental training, LM transfer, shallow transfern- evaluation (CER, PPL)nn### Processing PAGE annotationsnnWhen applied on PAGE-XML (as OCR-D workspace processor), this component also allows processing below the `TextLine` hierarchy level, i.e. on `Word` or `Glyph` level. For that it uses the soft alignment scores to calculate an optimal hard alignment path for characters, and thereby distributes the transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary.nn...nn### Architecturenn...FIXME: show!nn### Input with confidence and/or alternativesnn...FIXME: explain!nn### Multi-OCR inputnnnot yet!nn### ModesnnWhile the _encoder_ can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a **beam-search** _decoder_ requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also **greedily** use the best output only for each position (without beam search). And in doing so, another option is to feed back the softmax output directly into the decoder input instead of its argmax unit vector. This effectively passes the full probability distribution from state to state, which (not very surprisingly) can increase correction accuracy quite a lot – it can get as good as a medium-sized beam search results. This latter option also allows to run in parallel again, which is also much faster – consuming up to ten times less CPU time.nnThererfore, the backend function `lib.Sequence2Sequence.correct_lines` can operate the encoder-decoder network in either of the following modes:nn#### _fast_nnDecode greedily, but feeding back the full softmax distribution in batch mode.nn#### _greedy_nnDecode greedily, but feeding back the argmax unit vectors for each line separately.nn#### _default_nnDecode beamed, feeding back the argmax unit vectors for the best history/output hypotheses of each line. More specifically:nn&amp;gt; Start decoder with start-of-sequence, then keep decoding untiln&amp;gt; end-of-sequence is found or output length is way off, repeatedly.n&amp;gt; Decode by using the best predicted output characters and several next-bestn&amp;gt; alternatives (up to some degradation threshold) as next input.n&amp;gt; Follow-up on the N best overall candidates (estimated by accumulatedn&amp;gt; score, normalized by length and prospective cost), i.e. do A*-liken&amp;gt; breadth-first search, with N equal `batch_size`.n&amp;gt; Pass decoder initial/final states from character to character,n&amp;gt; for each candidate respectively.n&amp;gt; Reserve 1 candidate per iteration for running through `source_seq`n&amp;gt; (as a rejection fallback) to ensure that path does not fall off then&amp;gt; beam and at least one solution can be found within the search limits.nn### EvaluationnnText lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (`cor-asv-ann-eval`) and PAGE-XML annotations (`ocrd-cor-asv-ann-evaluate`). nnDistances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.nnThere are a number of distance metrics available (all operating on grapheme clusters, not mere codepoints):n- `Levenshtein`:  n  simple unweighted edit distance (fastest, standard; GT level 3)n- `NFC`:  n  like `Levenshtein`, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)n- `NFKC`:  n  like `Levenshtein`, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for `ſ`, which is already normalized to `s`)n- `historic_latin`:  n  like `Levenshtein`, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter `e` as in `Wuͤſte` instead of  to `Wüſte`, `ſ` vs `s`, or quotation/citation marks; GT level 1)nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-venv`` or ``python3-venv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies:n```shellnmake depsn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtn```nnTo install this module, then do:n```shellnmake installn```nWhich is the equivalent of:n```shellnpip install .n```nn## UsagennThis packages has the following user interfaces:nn### command line interface `cor-asv-ann-train`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-eval`nnTo be used with string arguments and plain-text files.nn...nn### command line interface `cor-asv-ann-repl`nninteractivenn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-process`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). nn...nn```jsonn    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    }n```nn...nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-cor-asv-ann-evaluate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Inputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.nn...nn```jsonn    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;n        }n      }n    }n```nn...nn## Testingnnnot yet!n...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;,n  &quot;version&quot;: &quot;0.1.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-ann-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;n        }n      }n    },n    &quot;ocrd-cor-asv-ann-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-ann-evaluate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/evaluation&quot;n      ],n      &quot;description&quot;: &quot;Align different textline annotations and compute distance&quot;,n      &quot;parameters&quot;: {n        &quot;metric&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;],n          &quot;default&quot;: &quot;Levenshtein&quot;,n          &quot;description&quot;: &quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;n        },n        &quot;confusion&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;minimum&quot;: 0,n          &quot;default&quot;: 0,n          &quot;description&quot;: &quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-ann-trainn    - cor-asv-ann-evaln    - cor-asv-ann-repln    - ocrd-cor-asv-ann-processn    - ocrd-cor-asv-ann-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_ann&#39;,n    version=&#39;0.1.2&#39;,n    description=&#39;sequence-to-sequence translator for noisy channel error correction&#39;,n    long_description=README,n    author=&#39;Robert Sachunsky&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-ann&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-ann-train=ocrd_cor_asv_ann.scripts.train:cli&#39;,n            &#39;cor-asv-ann-eval=ocrd_cor_asv_ann.scripts.eval:cli&#39;,n            &#39;cor-asv-ann-repl=ocrd_cor_asv_ann.scripts.repl:cli&#39;,n            &#39;ocrd-cor-asv-ann-process=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_process&#39;,n            &#39;ocrd-cor-asv-ann-evaluate=ocrd_cor_asv_ann.wrapper.cli:ocrd_cor_asv_ann_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 00:58:56 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;49&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-ann&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-ann-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align different textline annotations and compute distance&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-evaluate&quot;, &quot;parameters&quot;=&amp;gt;{&quot;confusion&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;Count edits and show that number of most frequent confusions (non-identity) in the end.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;minimum&quot;=&amp;gt;0, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;metric&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;Levenshtein&quot;, &quot;description&quot;=&amp;gt;&quot;Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except ſ-s), Levenshtein for GT level 3&quot;, &quot;enum&quot;=&amp;gt;[&quot;Levenshtein&quot;, &quot;NFC&quot;, &quot;NFKC&quot;, &quot;historic_latin&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/evaluation&quot;]}, &quot;ocrd-cor-asv-ann-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by character-level encoder-attention-decoder ANN model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-ann-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with cor-asv-ann-train&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read/write TextEquiv input/output on&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cor-asv-ann-evaluate] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cor-asv-ann-evaluate.parameters.confusion] Additional properties are not allowed (&#39;minimum&#39; was unexpected)n  [tools.ocrd-cor-asv-ann-evaluate.steps.0] &#39;recognition/evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-ann&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_ann&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-ann&quot;}         cor-asv-fst    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# cor-asv-fstn    OCR post-correction with error/lexicon Finite State Transducers andn    chararacter-level LSTM language modelsnn## Introductionnnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnIn addition to the requirements listed in `requirements.txt`, the toolnrequires then[pynini](http://www.opengrm.org/twiki/bin/view/GRM/Pynini)nlibrary, which has to be installed from source.nn## UsagennThe package has two user interfaces:nn### Command Line InterfacennThe package contains a suite of CLI tools to work with plaintext data (prefix:n`cor-asv-fst-*`). The minimal working examples and data formats are describednbelow. Additionally, each tool has further optional parameters - for a detailedndescription, call the tool with the `--help` option.nn#### `cor-asv-fst-train`nnTrain FST models. The basic invocation is as follows:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -t TRAINING_FILEn```nnThis will create two transducers, which will be stored in `LEXICON_FILE` andn`ERROR_MODEL_FILE`, respectively. As the training of the lexicon and the errornmodel is done independently, any of them can be skipped by omitting thenrespective parameter.nn`TRAINING_FILE` is a plain text file in tab-separated, two-column formatncontaining a line of OCR-output and the corresponding ground truth line:nn```n» Bergebt mir, daß ih niht weiß, wiet»Vergebt mir, daß ich nicht weiß, wienaus dem (Geiſte aller Nationen Mahrunqtaus dem Geiſte aller Nationen NahrungnKannſt Du mir die re&amp;lt;hée Bahn niché zeigen ?tKannſt Du mir die rechte Bahn nicht zeigen?nfrag zu bringen. —ttrag zu bringen. —nſie ins irdij&amp;lt;he Leben hinein, Mit leichtem,tſie ins irdiſche Leben hinein. Mit leichtem,n```nnEach line is treated independently. Alternatively to the above, the trainingndata may also be supplied as two files:nn```shellncor-asv-fst-train -l LEXICON_FILE -e ERROR_MODEL_FILE -i INPUT_FILE -g GT_FILEn```nnIn this variant, `INPUT_FILE` and `GT_FILE` are both in tab-separated,ntwo-column format, in which the first column is the line ID and the second thenline:nn```n&amp;gt;=== INPUT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ih denke. Aber was die ſelige Frau Geheimräth1nnalexis_ruhe01_1852_0035_019     „Das fann ich niht, c’esl absolument impos-nalexis_ruhe01_1852_0087_027     rend. In dem Augenbli&amp;gt; war 1hr niht wohl zunalexis_ruhe01_1852_0099_012     ür die fle ſich ſchlugen.“nalexis_ruhe01_1852_0147_009     ſollte. Nur Über die Familien, wo man ſie einführennn&amp;gt;=== GT_FILE ===&amp;lt;nalexis_ruhe01_1852_0018_022     ich denke. Aber was die ſelige Frau Geheimräthinnalexis_ruhe01_1852_0035_019     „Das kann ich nicht, c&#39;est absolument impos—nalexis_ruhe01_1852_0087_027     rend. Jn dem Augenblick war ihr nicht wohl zunalexis_ruhe01_1852_0099_012     für die ſie ſich ſchlugen.“nalexis_ruhe01_1852_0147_009     ſollte. Nur über die Familien, wo man ſie einführenn```nn#### `cor-asv-fst-process`nnThis tool applies a trained model to correct plaintext data on a line basis.nThe basic invocation is:nn```shellncor-asv-fst-process -i INPUT_FILE -o OUTPUT_FILE -l LEXICON_FILE -e ERROR_MODEL_FILE (-m LM_FILE)n```nn`INPUT_FILE` is in the same format as for the training procedure. `OUTPUT_FILE`ncontains the post-correction results in the same format.nn`LM_FILE` is a `ocrd_keraslm` language model - if supplied, it is used fornrescoring.nn#### `cor-asv-fst-evaluate`nnThis tool can be used to evaluate the post-correction results. The minimalnworking invocation is:nn```shellncor-asv-fst-evaluate -i INPUT_FILE -o OUTPUT_FILE -g GT_FILEn```nnAdditionally, the parameter `-M` can be used to select the evaluation measuren(`Levenshtein` by default). The files should be in the same two-column formatnas described above.nn### [OCR-D processor](https://ocr-d.github.io/cli) interface `ocrd-cor-asv-fst-process`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML)ndocuments in an [OCR-D](https://ocr-d.github.io) annotation workflow.nInput files need a textual annotation (`TextEquiv`) on the givenn`textequiv_level` (currently _only_ `word`!).nn...nn```jsonn  &quot;tools&quot;: {n    &quot;cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n```nn...nn## Testingnn...n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;,n  &quot;version&quot;: &quot;0.1.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-cor-asv-fst-process&quot;: {n      &quot;executable&quot;: &quot;ocrd-cor-asv-fst-process&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/post-correction&quot;n      ],n      &quot;description&quot;: &quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;parameters&quot;: {n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;word&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;n        },n        &quot;errorfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for error model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;lexiconfst_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/vnd.openfst&quot;,n          &quot;description&quot;: &quot;path of FST file for lexicon model&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;pruning_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight for pruning the hypotheses in each word window FST&quot;,n          &quot;default&quot;: 5.0n        },n        &quot;rejection_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;transition weight (per character) for unchanged input in each word window FST&quot;,n          &quot;default&quot;: 1.5n        },n        &quot;keraslm_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for language model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during beam search in language modelling&quot;,n          &quot;default&quot;: 100n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the FST output confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - cor-asv-fst-trainn    - cor-asv-fst-processn    - cor-asv-fst-evaluaten    - ocrd-cor-asv-fst-processn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnninstall_requires = open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;)nnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cor_asv_fst&#39;,n    version=&#39;0.2.0&#39;,n    description=&#39;OCR post-correction with error/lexicon Finite State &#39;n                &#39;Transducers and character-level LSTMs&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Maciej Sumalvico, Robert Sachunsky&#39;,n    author_email=&#39;sumalvico@informatik.uni-leipzig.de, &#39;n                 &#39;sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/ASVLeipzig/cor-asv-fst&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    test_suite=&#39;tests&#39;,n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;cor-asv-fst-train=ocrd_cor_asv_fst.scripts.train:main&#39;,n            &#39;cor-asv-fst-process=ocrd_cor_asv_fst.scripts.process:main&#39;,n            &#39;cor-asv-fst-evaluate=ocrd_cor_asv_fst.scripts.evaluate:main&#39;,n            &#39;ocrd-cor-asv-fst-process=ocrd_cor_asv_fst.wrapper.cli:ocrd_cor_asv_fst&#39;,n        ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 17:54:58 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;178&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst.git&quot;}, &quot;name&quot;=&amp;gt;&quot;cor-asv-fst&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cor-asv-fst-process&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Improve text annotation by FST error and lexicon model with character-level LSTM language model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cor-asv-fst-process&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-ASV&quot;], &quot;parameters&quot;=&amp;gt;{&quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;100, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during beam search in language modelling&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;errorfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for error model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;keraslm_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for language model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lexiconfst_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/vnd.openfst&quot;, &quot;description&quot;=&amp;gt;&quot;path of FST file for lexicon model&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the FST output confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pruning_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;transition weight for pruning the hypotheses in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rejection_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;transition weight (per character) for unchanged input in each word window FST&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read TextEquiv input on (output will always be word level)&quot;, &quot;enum&quot;=&amp;gt;[&quot;word&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/post-correction&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ASVLeipzig/cor-asv-fst&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Maciej Sumalvico, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;sumalvico@informatik.uni-leipzig.de, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cor_asv_fst&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ASVLeipzig/cor-asv-fst&quot;}         ocrd_calamari    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:edgenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /buildnCOPY Makefile .nCOPY setup.py .nCOPY ocrd-tool.json .nCOPY requirements.txt .nCOPY ocrd_calamari ocrd_calamarinnRUN make calamari/buildnRUN pip3 install .nnENTRYPOINT [&quot;/usr/local/bin/ocrd-calamari-recognize&quot;]nn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_calamarinn&amp;gt; Recognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn[![image](https://circleci.com/gh/OCR-D/ocrd_calamari.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_calamari)n[![image](https://img.shields.io/pypi/v/ocrd_calamari.svg)](https://pypi.org/project/ocrd_calamari/)n[![image](https://codecov.io/gh/OCR-D/ocrd_calamari/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_calamari)nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Installationnn### From PyPInn```npip install ocrd_calamarin```nn### From Reponn```shnpip install .n```nn## Install modelsnnDownload models trained on GT4HistOCR data:nn```nmake gt4histocr-calamarinls gt4histocr-calamarin```nn## Example Usagenn~~~nocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn~~~nnWith `test-parameters.json`:n~~~n{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n~~~nn## Development &amp;amp; TestingnFor information regarding development and testing, please seen[README-DEV.md](README-DEV.md).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/kba/ocrd_calamari&quot;,n  &quot;version&quot;: &quot;0.0.3&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-calamari-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-calamari-recognize&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Recognize lines with Calamari&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-CALAMARI&quot;n      ],n      &quot;parameters&quot;: {n        &quot;checkpoint&quot;: {n          &quot;description&quot;: &quot;The calamari model files (*.ckpt.json)&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;file&quot;, &quot;cacheable&quot;: truen        },n        &quot;voter&quot;: {n          &quot;description&quot;: &quot;The voting algorithm to use&quot;,n          &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;confidence_voter_default_ctc&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom pathlib import Pathnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_calamari&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;Calamari bindings&#39;,n    long_description=Path(&#39;README.md&#39;).read_text(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Mike Gerber&#39;,n    author_email=&#39;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&#39;,n    url=&#39;https://github.com/kba/ocrd_calamari&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=Path(&#39;requirements.txt&#39;).read_text().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-calamari-recognize=ocrd_calamari.cli:ocrd_calamari_recognize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 16:14:13 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;84&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-calamari-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize lines with Calamari&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-calamari-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-CALAMARI&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;The calamari model files (*.ckpt.json)&quot;, &quot;format&quot;=&amp;gt;&quot;file&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;voter&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;confidence_voter_default_ctc&quot;, &quot;description&quot;=&amp;gt;&quot;The voting algorithm to use&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_calamari&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_calamari&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Mike Gerber&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, mike.gerber@sbb.spk-berlin.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_calamarinnRecognize text using [Calamari OCR](https://github.com/Calamari-OCR/calamari).nn## IntroductionnnThis offers a OCR-D compliant workspace processor for some of the functionality of Calamari OCR.nnThis processor only operates on the text line level and so needs a line segmentation (and by extension a binarized nimage) as its input.nn## Example Usagenn```shnocrd-calamari-recognize -p test-parameters.json -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-CALAMARIn```nnWith `test-parameters.json`:nn```jsonn{n    &quot;checkpoint&quot;: &quot;/path/to/some/trained/models/*.ckpt.json&quot;n}n```nnTODOn----nn* Support Calamari&#39;s &quot;extended prediction data&quot; outputn* Currently, the processor only supports a prediction using confidence voting of multiple models. While this isn  superior, it makes sense to support single model prediction, too.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-calamari&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-calamari/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;numpy&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;calamari-ocr (==0.3.5)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;click&quot;, &quot;ocrd (&amp;gt;=1.0.0b11)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Calamari bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;6229919, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;sha256&quot;=&amp;gt;&quot;cf08ec027390519d465f6be861e5672b48e7b39b3d1f8e13e54cb401034355b6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a247c6638d77f7590453855f8414a97b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9320, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:11&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:11.044376Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/30/62/d8efee35233443d444fc49f7f89792979234c1d735285d599f989e63cee1/ocrd_calamari-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;sha256&quot;=&amp;gt;&quot;51a09088d677799258d8c796dbaba8a1b44a318d06c060314499f708fa37bdd4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1daa1956ba64485b65d9d69a149dcb6a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3884, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T20:18:13&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T20:18:13.643406Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/46/1a/b5f02d113aa7810cb773f0b586d1202c254d22e4bf3c6b829d937da2c1b0/ocrd_calamari-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;sha256&quot;=&amp;gt;&quot;454164c6b1c063b76c5189ae596115499bffd6e944c896dee3b03f08852f5680&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;24e8cab9e429576704a02890f6ebffb2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5247, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:56.460224Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/53/c05186a309284a22d4f1f0399a5fb241d7b11fb0e5b94c33fa8ae229a6fc/ocrd_calamari-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;sha256&quot;=&amp;gt;&quot;39e0f5b334a735fb8fa20e5490dcd07a96a620bc785c8e2b31f64a23fa13a6fe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7a101d8f9626784f9e54af6dad37179d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3952, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T12:22:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T12:22:57.972949Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/9d/cc/de53bfd3c2b666cab5ef199c93902c85bb83ee03d923e9ef7abe87377857/ocrd_calamari-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;sha256&quot;=&amp;gt;&quot;4b6e0be66b0fdd9f64f5f02e8aac952c1e77f78b39fc4ed9c90f8c9f9a117967&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7bb2ae998a57e2301011073fd532445e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9384, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:38.092102Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/23/85/34b1b520bd8ad7688915d5844caf20e89435fd17a3489963ceec14c06f14/ocrd_calamari-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;sha256&quot;=&amp;gt;&quot;e57cea7935340bcf090e62642a38aa41b0bf68d31afe95ba9e42a18be53ca80d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_calamari-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a435811e11f37b47eec5a5f8a433e99&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;3909, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T17:28:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T17:28:39.643369Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/15/e01d70177d89e9d0c0ec07ea8a2a31194f46154758788af781724c5b3354/ocrd_calamari-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/kba/ocrd_calamari&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_calamari&quot;}         ocrd_im6convert    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivennENV PREFIX=/usr/localnnWORKDIR /buildnCOPY ocrd-im6convert .nCOPY ocrd-tool.json .nCOPY Makefile .nnRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install apt-utils &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    makennRUN make deps-ubuntu installnnENV DEBIAN_FRONTEND teletypenn# no fixed entrypoint (e.g. also allow `convert` etc)nCMD [&quot;/usr/local/bin/ocrd-im6convert&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_imageconvertnn&amp;gt; Thin wrapper around convert(1)nn## Introductionnn[ImageMagick&#39;s](https://imagemagick.org) `convert` CLI contains a treasure trove of image operations. This wrapper aims to provide much of that as an [OCR-D compliant processor](https://ocr-d.github.io/CLI).nn## InstallationnnThis module requires GNU make (for installation) and the ImageMagick command line tools (at runtime). On Ubuntu 18.04 (or similar), you can install them by running:nn    sudo apt-get install maken    sudo make deps-ubuntu # or: apt-get install imagemagicknnMoreover, an installation of [OCR-D core](https://github.com/OCR-D/core) is needed:nn    make deps # or: pip install ocrdnnThis will install the Python package `ocrd` in your current environment. (Setting up a [venv](https://ocr-d.github.io/docs/guide#python-setup) is strongly recommended.)nnLastly, the provided shell script `ocrd-im6convert` works best when copied into your `PATH`, referencing its ocrd-tool.json under a known path. This can be done by running:nn    make installnnThis will copy the binary and JSON file under `$PREFIX`, which variable you can override to your needs. The default value is to use `PREFIX=$VIRTUAL_ENV` if you have already activated a venv, or `PREFIX=$PWD/.local` (i.e. under the current working directory).nn## UsagennThis package provides `ocrd-im6convert` as a [OCR-D processor](https://ocr-d.github.com/cli) (command line interface). It uses the following parameters:nn```JSONn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }n```nnCf. [IM documentation](https://imagemagick.org/script/command-line-options.php) or man-page `convert(1)` for formats and options.nn### Examplenn    ocrd-im6convert -I OCR-D-IMG -O OCR-D-IMG-SMALL -p &#39;{ &quot;output-format&quot;: &quot;image/png&quot;, &quot;output-options&quot;: &quot;-resize 24%&quot; }&#39;nn(This downscales the images in the input file group `OCR-D-IMG` to 24% and stores them as PNG files under the output file group `OCR-D-IMG-SMALL`.)nn## TestingnnNone yetn&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_im6convert&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {nn    &quot;ocrd-im6convert&quot;: {n      &quot;executable&quot;: &quot;ocrd-im6convert&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization&quot;],n      &quot;description&quot;: &quot;Convert and transform images&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;n      ],n      &quot;parameters&quot;: {n        &quot;input-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;,n          &quot;default&quot;: &quot;&quot;n        },n        &quot;output-format&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Desired media type of output&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;]n        },n        &quot;output-options&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;,n          &quot;default&quot;: &quot;&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Dec 27 13:38:58 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;27&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_im6convert&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-im6convert&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Convert and transform images&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-im6convert&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;input-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -density 600x600 -wavelet-denoise 1%x0.1&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-format&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Desired media type of output&quot;, &quot;enum&quot;=&amp;gt;[&quot;image/tiff&quot;, &quot;image/jp2&quot;, &quot;image/png&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;output-options&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;e.g. -resample 300x300 -alpha deactivate -normalize -despeckle -noise 2 -negate -morphology close diamond&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_im6convert&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_im6convert&quot;}         ocrd_keraslm    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnn[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_keraslm.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_keraslm)nn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;für die Wiſſen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so schädlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_keraslm&quot;,n  &quot;version&quot;: &quot;0.3.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 10n        },n        &quot;lm_weight&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;share of the LM scores over the input confidences&quot;,n          &quot;default&quot;: 0.5n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:n    - keraslm-raten    - ocrd-keraslm-raten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_keraslm&#39;,n    version=&#39;0.3.2&#39;,n    description=&#39;character-level language modelling in Keras&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&#39;,n    author_email=&#39;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_keraslm&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    extras_require={n        &#39;plotting&#39;: [n            &#39;sklearn&#39;,n            &#39;matplotlib&#39;,n            ]n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;keraslm-rate=ocrd_keraslm.scripts.run:cli&#39;,n            &#39;ocrd-keraslm-rate=ocrd_keraslm.wrapper.cli:ocrd_keraslm_rate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 9 10:13:52 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;0.3.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;91&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-keraslm-rate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-keraslm-rate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;, &quot;OCR-D-OCR-KRAK&quot;, &quot;OCR-D-OCR-OCRO&quot;, &quot;OCR-D-OCR-CALA&quot;, &quot;OCR-D-OCR-ANY&quot;, &quot;OCR-D-COR-CIS&quot;, &quot;OCR-D-COR-ASV&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-COR-LM&quot;], &quot;parameters&quot;=&amp;gt;{&quot;alternative_decoding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;beam_width&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lm_weight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;share of the LM scores over the input confidences&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model_file&quot;=&amp;gt;{&quot;cacheable&quot;=&amp;gt;true, &quot;content-type&quot;=&amp;gt;&quot;application/x-hdf;subtype=bag&quot;, &quot;description&quot;=&amp;gt;&quot;path of h5py weight/config file for model trained with keraslm&quot;, &quot;format&quot;=&amp;gt;&quot;uri&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;glyph&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.3.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_keraslm&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_keraslm&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Robert Sachunsky, Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_keraslmn    character-level language modelling using Kerasnnn## IntroductionnnThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.nnIn addition to character sequences, (meta-data) context variables can be configured as extra input. nn### ArchitecturennThe model consists of:nn0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, n3. character and context vector sequences are concatenated,n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.nn![model graph depiction](model-graph.png &quot;graph with 1 context variable&quot;)nnThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (&quot;one-hot coding&quot;) as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).nnThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.nnn### Modes of operationnnNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).nnBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).nn### Context conditioningnnEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. nnThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).nn### UnderspecificationnnIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.nnn## InstallationnnRequired Ubuntu packages:nn* Python (``python`` or ``python3``)n* pip (``python-pip`` or ``python3-pip``)n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)nnCreate and activate a virtualenv as usual.nnIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.nnTo install Python dependencies and this module, then do:n```shellnmake deps installn```nWhich is the equivalent of:n```shellnpip install -r requirements.txtnpip install -e .n```nnUseful environment variables are:n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow&#39;s messagesn- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)nnn## UsagennThis packages has two user interfaces:nn### command line interface `keraslm-rate`nnTo be used with string arguments and plain-text files.nn```shellnUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...nnOptions:n  --help  Show this message and exit.nnCommands:n  train                           train a language modeln  test                            get overall perplexity from language modeln  apply                           get individual probabilities from language modeln  generate                        sample characters from language modeln  print-charset                   Print the mapped charactersn  prune-charset                   Delete one character from mappingn  plot-char-embeddings-similarityn                                  Paint a heat map of character embeddingsn  plot-context-embeddings-similarityn                                  Paint a heat map of context embeddingsn  plot-context-embeddings-projectionn                                  Paint a 2-d PCA projection of context embeddingsn```nnExamples:n```shellnkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txtnkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 &quot;für die Wiſſen&quot;nkeraslm-rate apply -m model_dta_64_4_256.h5 &quot;so schädlich ist es Borkickheile zu pflanzen&quot;nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txtn```nn### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`nnTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).nn```jsonn  &quot;tools&quot;: {n    &quot;ocrd-keraslm-rate&quot;: {n      &quot;executable&quot;: &quot;ocrd-keraslm-rate&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;Rate elements of the text with a character-level LSTM language model in Keras&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;,n        &quot;OCR-D-OCR-KRAK&quot;,n        &quot;OCR-D-OCR-OCRO&quot;,n        &quot;OCR-D-OCR-CALA&quot;,n        &quot;OCR-D-OCR-ANY&quot;,n        &quot;OCR-D-COR-CIS&quot;,n        &quot;OCR-D-COR-ASV&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-COR-LM&quot;n      ],n      &quot;parameters&quot;: {n        &quot;model_file&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;uri&quot;,n          &quot;content-type&quot;: &quot;application/x-hdf;subtype=bag&quot;,n          &quot;description&quot;: &quot;path of h5py weight/config file for model trained with keraslm&quot;,n          &quot;required&quot;: true,n          &quot;cacheable&quot;: truen        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;glyph&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to evaluate TextEquiv sequences on&quot;n        },n        &quot;alternative_decoding&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative&quot;,n          &quot;default&quot;: truen        },n        &quot;beam_width&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;maximum number of best partial paths to consider during search with alternative_decoding&quot;,n          &quot;default&quot;: 100n        }n      }n    }n  }n```nnExamples:n```shellnmake deps-test # installs ocrd_tesserocrnmake test/assets # downloads GT, imports PageXML, builds workspacesnocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1ncd ws1nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCKnocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINEnocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p &#39;{ &quot;textequiv_level&quot; : &quot;word&quot;, &quot;model&quot; : &quot;Fraktur&quot; }&#39;nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p &#39;{ &quot;textequiv_level&quot; : &quot;glyph&quot;, &quot;model&quot; : &quot;deu-frak&quot; }&#39;n# get confidences and perplexity:nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;word&quot;, &quot;alternative_decoding&quot;: false }&#39;n# also get best path:nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p &#39;{ &quot;model_file&quot;: &quot;model_dta_64_4_256.h5&quot;, &quot;textequiv_level&quot;: &quot;glyph&quot;, &quot;alternative_decoding&quot;: true, &quot;beam_width&quot;: 10 }&#39;n```nn## Testingnn```shellnmake deps-test testn```nWhich is the equivalent of:n```shellnpip install -r requirements_test.txtntest -e test/assets || test/prepare_gt.bash test/assetsntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txtnkeraslm-rate test -m model_dta_test.h5 test/assets/*.txtnpython -m pytest test $(PYTEST_ARGS)n```nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-keraslm&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-keraslm/0.3.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0)&quot;, &quot;click&quot;, &quot;keras (&amp;gt;=2.2.4)&quot;, &quot;numpy&quot;, &quot;tensorflow (&amp;lt;2.0)&quot;, &quot;h5py&quot;, &quot;networkx (&amp;gt;=2.0)&quot;, &quot;sklearn; extra == &#39;plotting&#39;&quot;, &quot;matplotlib; extra == &#39;plotting&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;character-level language modelling in Keras&quot;, &quot;version&quot;=&amp;gt;&quot;0.3.2&quot;}, &quot;last_serial&quot;=&amp;gt;6158523, &quot;releases&quot;=&amp;gt;{&quot;0.3.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3ec82a615434e90028722586c6123e4a1887e36b0a57f06566a291892280e88&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0da1139d7b62ee27b9bb3af2b4e38929&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2.py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34192, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:09&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:09.567407Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/ba/8f5f0f1801ea99221c772357e2c79d9935a88e89873924e557e24aea6c33/ocrd_keraslm-0.3.1-py2.py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;sha256&quot;=&amp;gt;&quot;665a9bf1d7bc46f497d71638b2d33608062edd16ac11b9cff05be56eacda53c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e8d597a8dbf64e45dcbf19196e73bbf8&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32287, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-25T22:53:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-25T22:53:12.437293Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/79/0e/744edc5497d706ac558b90d8d85b2e52ad5fb6b794c6f9cb44fc0aaa341a/ocrd_keraslm-0.3.1.tar.gz&quot;}], &quot;0.3.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;sha256&quot;=&amp;gt;&quot;45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e8927b5ca560a990cb924c7a01e7280&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34190, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:01&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:01.036117Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;sha256&quot;=&amp;gt;&quot;ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_keraslm-0.3.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7eb11946732e6410d4ba18dad3fbaf20&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;32294, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T22:03:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T22:03:06.384019Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_keraslm&quot;}         ocrd_kraken    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_kraken ./ocrd_krakennRUN pip3 install --upgrade pipnRUN pip3 install .nnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_kraken&quot;,n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-kraken-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-binarize&quot;,n      &quot;input_file_grp&quot;: &quot;OCR-D-IMG&quot;,n      &quot;output_file_grp&quot;: &quot;OCR-D-IMG-BIN&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;description&quot;: &quot;Binarize images with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;level-of-operation&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;page&quot;,n          &quot;enum&quot;: [&quot;page&quot;, &quot;block&quot;, &quot;line&quot;]n        }n      }n    },n    &quot;ocrd-kraken-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-segment&quot;,n      &quot;categories&quot;: [n        &quot;Layout analysis&quot;n      ],n      &quot;steps&quot;: [n        &quot;layout/segmentation/region&quot;n      ],n      &quot;description&quot;: &quot;Block segmentation with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;text_direction&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;Sets principal text direction&quot;,n          &quot;enum&quot;: [&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;],n          &quot;default&quot;: &quot;horizontal-lr&quot;n        },n        &quot;script_detect&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;description&quot;: &quot;Enable script detection on segmenter output&quot;,n          &quot;default&quot;: falsen        },n        &quot;maxcolseps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2},n        &quot;scale&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0},n        &quot;black_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false},n        &quot;white_colseps&quot;: {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false}n      }n    },n    &quot;ocrd-kraken-ocr&quot;: {n      &quot;executable&quot;: &quot;ocrd-kraken-ocr&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ],n      &quot;description&quot;: &quot;OCR with kraken&quot;,n      &quot;parameters&quot;: {n        &quot;lines-json&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;format&quot;: &quot;url&quot;,n          &quot;required&quot;: &quot;true&quot;,n          &quot;description&quot;: &quot;URL to line segmentation in JSON&quot;n        }n      }n    }nn  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls two binaries:nn    - ocrd-kraken-binarizen    - ocrd-kraken-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_kraken&#39;,n    version=&#39;0.1.1&#39;,n    description=&#39;kraken bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael Würzner&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_kraken&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0a4&#39;,n        &#39;kraken == 0.9.16&#39;,n        &#39;click &amp;gt;= 7&#39;,n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-kraken-binarize=ocrd_kraken.cli:ocrd_kraken_binarize&#39;,n            &#39;ocrd-kraken-segment=ocrd_kraken.cli:ocrd_kraken_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Oct 21 20:52:26 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;85&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-kraken-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG&quot;, &quot;output_file_grp&quot;=&amp;gt;&quot;OCR-D-IMG-BIN&quot;, &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;block&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-kraken-ocr&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;OCR with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-ocr&quot;, &quot;parameters&quot;=&amp;gt;{&quot;lines-json&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;URL to line segmentation in JSON&quot;, &quot;format&quot;=&amp;gt;&quot;url&quot;, &quot;required&quot;=&amp;gt;&quot;true&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-kraken-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Block segmentation with kraken&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-kraken-segment&quot;, &quot;parameters&quot;=&amp;gt;{&quot;black_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;script_detect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Enable script detection on segmenter output&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;text_direction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;horizontal-lr&quot;, &quot;description&quot;=&amp;gt;&quot;Sets principal text direction&quot;, &quot;enum&quot;=&amp;gt;[&quot;horizontal-lr&quot;, &quot;horizontal-rl&quot;, &quot;vertical-lr&quot;, &quot;vertical-rl&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;white_colseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-kraken-binarize.input_file_grp] &#39;OCR-D-IMG&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.output_file_grp] &#39;OCR-D-IMG-BIN&#39; is not of type &#39;array&#39;n  [tools.ocrd-kraken-binarize.parameters.level-of-operation] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.maxcolseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.scale] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.black_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-segment.parameters.white_colseps] &#39;description&#39; is a required propertyn  [tools.ocrd-kraken-ocr] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-kraken-ocr.parameters.lines-json.required] &#39;true&#39; is not of type &#39;boolean&#39;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_kraken&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_kraken&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_krakennn&amp;gt; Wrapper for the kraken OCR enginenn[![image](https://travis-ci.org/OCR-D/ocrd_kraken.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_kraken)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/kraken.svg)](https://hub.docker.com/r/ocrd/kraken/tags/)n[![image](https://circleci.com/gh/OCR-D/ocrd_kraken.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_kraken)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-kraken&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-kraken/0.1.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0a4)&quot;, &quot;kraken (==0.9.16)&quot;, &quot;click (&amp;gt;=7)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;kraken bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.1&quot;}, &quot;last_serial&quot;=&amp;gt;6008613, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;sha256&quot;=&amp;gt;&quot;a0de30df5e8b7d9fe1ed3343a8fa3a413620828a2cdf46bcab8d77e864869d53&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b065398af77f4804763665f50503e141&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10691, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:30&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:30.728403Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b4/52/aea22b8cfab48546e10118e0eb7e70dc108fe633af3e07194dfd04e00fb2/ocrd_kraken-0.0.2-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;sha256&quot;=&amp;gt;&quot;805fb1aa976f9ee1275e347b1fee2413af3ea7cc8972af84464c6f4253ebdd6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67b290066697cbaddb71a4ff92eeb9f5&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9634, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:32.808242Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/00/a9843c2c73a086c1f66e28d6b0d64053ecd66995daddfb5c0f28e566c9f7/ocrd_kraken-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;sha256&quot;=&amp;gt;&quot;a6464f3559acfb36947687d4e2e70cd7cb7e655d70234696e2e7c1b07f99bab8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;351d10f31667ec43d9a117b9dd19e861&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5003, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:42:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:42:34.101144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/32/bb/9e4299ec1d5f494e7bf14de447f361455f36ea0255181871ee937aae0528/ocrd_kraken-0.0.2.tar.gz&quot;}], &quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;sha256&quot;=&amp;gt;&quot;4e6b7e9d1930de1f0bd57dfd63f9418c4345842e7cc8fdd9b147e7d378b8fe51&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;67161c2e535ac409369978252333eb35&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10442, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:43.225080Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/4b/d7027ac27e1228cf9aa3ecd94e412b371b2a63ab2c93c1b77ad5414380c1/ocrd_kraken-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;sha256&quot;=&amp;gt;&quot;9bec610685e29d29e0614f2dfc300d201fbbff3f728140536031f14e4e65584c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;f1ec0ad2a8e1d655410e4321c7dfae60&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4121, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T09:37:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T09:37:44.655031Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cb/35/7be3dd70b97e276ce2300dddf165bfc21c0e469c2626d7d531a07b8bf0fb/ocrd_kraken-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;sha256&quot;=&amp;gt;&quot;4d6a4a969ad43711cd22febfe2cc63c966b48b033537f87b433ea8254bb86a1a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d6cc67071fe7db22ee35c58e6df6cb7c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10595, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:21.215930Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/20/af/393dbc0767398429e08adb761289656516ab18d4f65d8e5c81791c6cafdc/ocrd_kraken-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;sha256&quot;=&amp;gt;&quot;67cad5aa4ce098262051f84c2f98a5a03be4b62e8bc4c2af1654f00b41caae25&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_kraken-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;22813065ca842796d8d53a2ae148b7c9&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;4209, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-21T18:20:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-21T18:20:22.550782Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bb/18/1c305cd6dc5b38880a3240bdca9f3ac53c2780a292b2a02812075ddddff7/ocrd_kraken-0.1.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_kraken&quot;}         ocrd_ocropy    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nENV LC_ALL C.UTF-8nENV LANG C.UTF-8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n    ca-certificates n    make n    gitnCOPY ocrd_ocropy ./ocrd_ocropynRUN pip3 install --upgrade pipnRUN make deps installnnENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginen&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_ocropy&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-ocropy-segment&quot;: {n      &quot;executable&quot;: &quot;ocrd-ocropy-segment&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;description&quot;: &quot;Segment page&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE&quot;],n      &quot;parameters&quot;: {n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 10},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.1},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 3},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;,&quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: false},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.2},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 8},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 0.0},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;has an effect&quot;, &quot;default&quot;: 1.0}n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls one binary:nn    - ocrd-ocropy-segmentn&quot;&quot;&quot;nimport codecsnnfrom setuptools import setupnnsetup(n    name=&#39;ocrd_ocropy&#39;,n    version=&#39;0.0.3&#39;,n    description=&#39;ocropy bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_ocropy&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=[&#39;ocrd_ocropy&#39;],n    install_requires=[n        &#39;ocrd &amp;gt;= 1.0.0b8&#39;,n        &#39;ocrd-fork-ocropy &amp;gt;= 1.4.0a3&#39;,n        &#39;click&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-ocropy-segment=ocrd_ocropy.cli:ocrd_ocropy_segment&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jun 11 14:51:00 2019 +0200&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;66&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;has an effect&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_ocropy&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_ocropy&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_ocropynn[![image](https://travis-ci.org/OCR-D/ocrd_ocropy.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_ocropy)nn[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/ocropy.svg)](https://hub.docker.com/r/ocrd/ocropy/tags/)nn&amp;gt; Wrapper for the ocropy OCR enginennn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-ocropy&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-ocropy/0.0.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b8)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;click&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;ocropy bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.3&quot;}, &quot;last_serial&quot;=&amp;gt;4979689, &quot;releases&quot;=&amp;gt;{&quot;0.0.1a1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;sha256&quot;=&amp;gt;&quot;1dc3926e7c28ecb52260c42d0b3b6b3cc3d2964b13ea994601219269c8072d89&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;955580b46dea69b4880f95f90076cfb3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6462, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:48&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:48.327057Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c7/ce/9f578c500afbffba6de78fb1fb0d881c23ddb794256a276e4277d5ad7c25/ocrd_ocropy-0.0.1a1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fc72a46a9e3bc7fd601aa6c00992debe566f1838b95bbd61e8c746b3abd0d673&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.1a1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;39723d9e4f1734de4a7f1fdd9e7008fc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;6105, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-19T17:02:50&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-19T17:02:50.204116Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8f/a1/2030fb1c2c08cac624a7640daa6a12c3d115a52a9d7d66de5c6b427bbbde/ocrd_ocropy-0.0.1a1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;sha256&quot;=&amp;gt;&quot;a1827b7fb49a27e297fb01ceea45c2272d996f498c576637e42d8008d28dfe9b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a5b84192f6eb88c34a6e64528526d98&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10625, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:23&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:23.779614Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7f/46/222d127fe28c522ab65448bd552f9b9b66ec6e5582f8cc7e2ee57f5450a5/ocrd_ocropy-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;sha256&quot;=&amp;gt;&quot;d1e4cd90fff395e332814f51de1b46533ac88ea72f99f4502524c0c659572519&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9e83b8f7b5d686f6bcc032a8ca532ed6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5855, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:17:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:17:25.438144Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/18/c634cc95db36cfa523a75f3ae4e5ee3055b8bcf56969bc3231cdddb3d082/ocrd_ocropy-0.0.2.tar.gz&quot;}], &quot;0.0.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;sha256&quot;=&amp;gt;&quot;2eb914d948f0dcf543560e9c2cb13eccd8d96f335febef1753e108279d0fdc7e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8a0d325dd9a10aea746f05824d30ce5c&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10632, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:40.405082Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/7b/0a/dd552d4077fe60652b1fe30e0fe4363686838bc8b88aa852d080e667d370/ocrd_ocropy-0.0.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;sha256&quot;=&amp;gt;&quot;f7b3f421f34d2cb4637b864709349ee508e859d1f512ce65be8bc3f2ab35374c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_ocropy-0.0.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;69fe2b3b78a357940f17678bdc78a80b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;5867, &quot;upload_time&quot;=&amp;gt;&quot;2019-03-24T19:53:41&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-03-24T19:53:41.685748Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6b/5a/d711492c2f10b241069361df84544145dab22654a173ac566645cec0bb9f/ocrd_ocropy-0.0.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_ocropy&quot;}         ocrd_olena    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# Patch and build Olena from Git, thenn# Install OCR-D wrapper for binarizationnFROM ocrd/corennMAINTAINER OCR-DnnENV PREFIX=/usr/localnnWORKDIR /build-olenanCOPY .gitmodules .nCOPY Makefile .nCOPY ocrd-tool.json .nCOPY ocrd-olena-binarize .nnENV DEPS=&quot;g++ make automake git&quot;nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends $DEPS &amp;amp;&amp;amp; n    make deps-ubuntu &amp;amp;&amp;amp; n    git init &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/olena.git repo/olena &amp;amp;&amp;amp; n    git submodule add https://github.com/OCR-D/assets.git repo/assets &amp;amp;&amp;amp; n    make build-olena install clean-olena &amp;amp;&amp;amp; n    apt-get -y remove $DEPS &amp;amp;&amp;amp; n    apt-get -y autoremove &amp;amp;&amp;amp; apt-get clean &amp;amp;&amp;amp; n    rm -fr /build-olenannWORKDIR /datanVOLUME /datann#ENTRYPOINT [&quot;/usr/bin/ocrd-olena-binarize&quot;]n#CMD [&quot;--help&quot;]nCMD [&quot;/usr/bin/ocrd-olena-binarize&quot;, &quot;--help&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_olenann&amp;gt; Binarize with Olena/scribonn[![Build Status](https://travis-ci.org/OCR-D/ocrd_olena.svg?branch=master)](https://travis-ci.org/OCR-D/ocrd_olena)n[![CircleCI](https://circleci.com/gh/OCR-D/ocrd_olena.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_olena)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/core.svg)](https://hub.docker.com/r/ocrd/olena/tags/)nn## Requirementsnn```nmake deps-ubuntun```nn...will try to install the required packages on Ubuntu.nn## Installationnn```nmake build-olenan```nn...will download, patch and build Olena/scribo from source, and install locally (in VIRTUAL_ENV or in CWD/local).nn```nmake installn```nn...will do that, but additionally install `ocrd-binarize-olena` (the OCR-D wrapper).nn## Testingnn```nmake testn```nn...will clone the assets repository from Github, make a workspace copy, and run checksum tests for binarization on them.nn## UsagennThis package has the following user interfaces:nn### command line interface `scribo-cli`nnConverts images in any format to netpbm (monochrome portable bitmap).nn```nUsage: scribo-cli [version] [help] COMMAND [ARGS]nnList of available COMMAND argument:nn  Full Toolchainsn  ---------------nnn   * On documentsnn     doc-ppct       Common preprocessing before looking for text.nn     doc-ocr           Find and recognize text. Output: the actual textn     tt       and its location.nn     doc-dia           Analyse the document structure and extract then     tt       text. Output: an XML file with region and textn     tt       information.nnnn   * On picturesnn     pic-loc           Try to localize text if there&#39;s any.nn     pic-ocr           Localize and try to recognize text.nnnn  Toolsn  -----nnn     * xml2doct       Convert the XML results of document toolchainsn       tt       into user documents (HTML, PDF...).nnn  Algorithmsn  ----------nnn   * Binarizationnn     sauvola           Sauvola&#39;s algorithm.nn     sauvola-ms        Multi-scale Sauvola&#39;s algorithm.nn     sauvola-ms-fg     Extract foreground objects and run multi-scalen                       Sauvola&#39;s algorithm.nn     sauvola-ms-split  Run multi-scale Sauvola&#39;s algorithm on each colorn                       component and merge results.nn---------------------------------------------------------------------------nSee &#39;scribo-cli COMMAND --help&#39; for more information on a specific command.n```nnFor example:nn```shnscribo-cli sauvola-ms path/to/input.tif path/to/output.png --enable-negate-outputn```nn### [OCR-D processor](https://ocr-d.github.com/cli) interface `ocrd-olena-binarize`nnTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.github.io) annotation workflow. Input could be any valid workspace with source images available. Currently covers the `Page` hierarchy level only. Uses either (the last) `AlternativeImage`, if any, or `imageFilename`, otherwise. Adds an `AlternativeImage` with the result of binarization for every page.nn```jsonn    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n```nn## LicensennCopyright 2018-2020 Project OCR-DnnLicensed under the Apache License, Version 2.0 (the &quot;License&quot;);nyou may not use this file except in compliance with the License.nYou may obtain a copy of the License atnn   http://www.apache.org/licenses/LICENSE-2.0nnUnless required by applicable law or agreed to in writing, softwarendistributed under the License is distributed on an &quot;AS IS&quot; BASIS,nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.nSee the License for the specific language governing permissions andnlimitations under the License.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;1.1.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_olena&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-olena-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-olena-binarize&quot;,n      &quot;description&quot;: &quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;,n      &quot;categories&quot;: [n        &quot;Image preprocessing&quot;n      ],n      &quot;steps&quot;: [n        &quot;preprocessing/optimization/binarization&quot;n      ],n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;parameters&quot;: {n        &quot;impl&quot;: {n          &quot;description&quot;: &quot;The name of the actual binarization algorithm&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: true,n          &quot;enum&quot;: [&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;]n        },n        &quot;win-size&quot;: {n          &quot;description&quot;: &quot;Window size&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 101n        },n        &quot;k&quot;: {n          &quot;description&quot;: &quot;Sauvola&#39;s formulae parameter&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;default&quot;: 0.34n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;nil}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Wed Jan 8 18:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v1.1.1&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;117&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_olena&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-olena-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;OLENA&#39;s binarization algos for OCR-D (on page-level)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-olena-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;impl&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The name of the actual binarization algorithm&quot;, &quot;enum&quot;=&amp;gt;[&quot;sauvola&quot;, &quot;sauvola-ms&quot;, &quot;sauvola-ms-fg&quot;, &quot;sauvola-ms-split&quot;, &quot;kim&quot;, &quot;wolf&quot;, &quot;niblack&quot;, &quot;singh&quot;, &quot;otsu&quot;], &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;k&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.34, &quot;description&quot;=&amp;gt;&quot;Sauvola&#39;s formulae parameter&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;win-size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;101, &quot;description&quot;=&amp;gt;&quot;Window size&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}}, &quot;version&quot;=&amp;gt;&quot;1.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_olena&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_olena&quot;}         ocrd_segment    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_segment&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-segment-repair&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-repair&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Analyse and repair region segmentation&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-EVAL-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;sanitize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;n        },n        &quot;plausibilize&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;n        },n        &quot;plausibilize_merge_min_overlap&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 0.90,n          &quot;description&quot;: &quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-regions&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-regions&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract region segmentation as image+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-extract-lines&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-extract-lines&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Extract line segmentation as image+txt+JSON&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-IMG-CROP&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n        &quot;transparency&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;Add alpha channels with segment masks to the images&quot;n        }n      }n    },n    &quot;ocrd-segment-evaluate&quot;: {n      &quot;executable&quot;: &quot;ocrd-segment-evaluate&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Compare region segmentations&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/analysis&quot;],n      &quot;parameters&quot;: {n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls:nn    - ocrd-segment-repairn    - ocrd-segment-extract-pagesn    - ocrd-segment-extract-regionsn    - ocrd-segment-extract-linesn    - ocrd-segment-evaluaten&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_segment&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Page segmentation and segmentation evaluation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    author=&#39;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_segment&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-segment-repair=ocrd_segment.cli:ocrd_segment_repair&#39;,n            &#39;ocrd-segment-extract-pages=ocrd_segment.cli:ocrd_segment_extract_pages&#39;,n            &#39;ocrd-segment-extract-regions=ocrd_segment.cli:ocrd_segment_extract_regions&#39;,n            &#39;ocrd-segment-extract-lines=ocrd_segment.cli:ocrd_segment_extract_lines&#39;,n            &#39;ocrd-segment-evaluate=ocrd_segment.cli:ocrd_segment_evaluate&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 10:42:42 2020 +0000&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;60&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-segment-evaluate&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Compare region segmentations&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-evaluate&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-lines&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract line segmentation as image+txt+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-lines&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-extract-regions&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Extract region segmentation as image+JSON&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-extract-regions&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;transparency&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;Add alpha channels with segment masks to the images&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/analysis&quot;]}, &quot;ocrd-segment-repair&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analyse and repair region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-segment-repair&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-EVAL-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;plausibilize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove redundant (almost equal or almost contained) regions, and merge overlapping regions&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;plausibilize_merge_min_overlap&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.9, &quot;description&quot;=&amp;gt;&quot;When merging a region almost contained in another, require at least this ratio of area is shared with the other&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sanitize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Shrink and/or expand a region in such a way that it coordinates include those of all its lines&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_segment&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_segment&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_segmentnnThis repository aims to provide a number of [OCR-D-compliant processors](https://ocr-d.github.io/cli) for layout analysis and evaluation.nn## InstallationnnIn your virtual environment, run:n```bashnpip install .n```nn## Usagenn  - extracting page images (including results from preprocessing like cropping, deskewing or binarization) along with region polygon coordinates and metadata:n    - [ocrd-segment-extract-regions](ocrd_segment/extract_regions.py)n  - extracting line images (including results from preprocessing like cropping, deskewing, dewarping or binarization) along with line polygon coordinates and metadata:n    - [ocrd-segment-extract-lines](ocrd_segment/extract_lines.py)n  - comparing different layout segmentations (input file groups N = 2, compute the distance between two segmentations, e.g. automatic vs. manual):n    - [ocrd-segment-evaluate](ocrd_segment/evaluate.py) :construction: (very early stage)n  - repairing layout segmentations (input file groups N &amp;gt;= 1, based on heuristics implemented using Shapely):n    - [ocrd-segment-repair](ocrd_segment/repair.py) :construction: (much to be done)n  - pattern-based segmentation (input file groups N=1, based on a PAGE template, e.g. from Aletheia, and some XSLT or Python to apply it to the input file group)n    - `ocrd-segment-via-template` :construction: (unpublished)n  - data-driven segmentation (input file groups N=1, based on a statistical model, e.g. Neural Network)  n    - `ocrd-segment-via-model` :construction: (unpublished)nnFor detailed description on input/output and parameters, see [ocrd-tool.json](ocrd_segment/ocrd-tool.json)nn## TestingnnNone yet.nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-segment&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-segment/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=1.0.0b19)&quot;, &quot;click&quot;, &quot;shapely&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Page segmentation and segmentation evaluation&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6235446, &quot;releases&quot;=&amp;gt;{&quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;sha256&quot;=&amp;gt;&quot;9b549066f46f26a147b726066712a423f9fcf64b8274dd8285447c564f361783&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e9bc6112469e53afd56563d862000228&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;14529, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:29&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:29.761485Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/90/34/4825c12fa6e8238ce350fc766f6aaa0d591705c8f426160eb59ec7513541/ocrd_segment-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;sha256&quot;=&amp;gt;&quot;284557d2fd985bf4be93b4bbbe08ba3fc2668300f5c9694af6c93f0be7a7c1c9&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_segment-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6b258735d218ef459887c4d8d23382c7&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10335, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T11:50:34&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T11:50:34.482743Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d0/e8/ab967b490f8cc4f70438b278530042a4eb5a9237941cd084fece279cb507/ocrd_segment-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_segment&quot;}         ocrd_tesserocr    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .nCOPY requirements_test.txt .nCOPY ocrd_tesserocr ./ocrd_tesserocrnCOPY Makefile .nRUN make deps-ubuntu &amp;amp;&amp;amp; n    apt-get install -y --no-install-recommends n    g++ n    tesseract-ocr-script-frak n    tesseract-ocr-deu n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; rm -rf /build-ocrd n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove g++ libtesseract-dev maken&quot;, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / tables / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, table, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some Tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nnTo run with docker:nn```ndocker run -v path/to/workspaces:/data ocrd/tesserocr ocrd-tesserocrd-crop ...n```nnn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnsudo make deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-table](ocrd_tesserocr/segment_table.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.8.0&quot;,n  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,n  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-tesserocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-DESKEW-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        },n        &quot;min_orientation_confidence&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;default&quot;: 1.5,n          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-recognize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,n      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],n      &quot;description&quot;: &quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-SEG-WORD&quot;,n        &quot;OCR-D-SEG-GLYPH&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-TESS&quot;n      ],n      &quot;steps&quot;: [&quot;recognition/text-recognition&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;textequiv_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],n          &quot;default&quot;: &quot;word&quot;,n          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;n        },n        &quot;raw_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;n        },n        &quot;char_whitelist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;n        },n        &quot;char_blacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;n        },n        &quot;char_unblacklist&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;default&quot;: &quot;&quot;,n          &quot;description&quot;: &quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;n        },n        &quot;model&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-region&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-PAGE&quot;,n        &quot;OCR-D-GT-SEG-PAGE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,n          &quot;default&quot;: 0n        },n        &quot;crop_polygons&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;n        },n        &quot;find_tables&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;n        }n      }n    },n     &quot;ocrd-tesserocr-segment-table&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the region level&quot;n        }n      }n     },n     &quot;ocrd-tesserocr-segment-line&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-GT-SEG-BLOCK&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_lines&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextRegion level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-segment-word&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-SEG-LINE&quot;,n        &quot;OCR-D-GT-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-WORD&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],n      &quot;parameters&quot;: {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;overwrite_words&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the TextLine level&quot;n        }n      }n    },n    &quot;ocrd-tesserocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,n      &quot;input_file_grp&quot;: [nt&quot;OCR-D-IMG&quot;n      ],n      &quot;output_file_grp&quot;: [nt&quot;OCR-D-SEG-PAGE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;parameters&quot; : {n        &quot;dpi&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;float&quot;,n          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,n          &quot;default&quot;: -1n        },n        &quot;padding&quot;: {n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,n          &quot;default&quot;: 4n        }n      }n    },n    &quot;ocrd-tesserocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG&quot;,n        &quot;OCR-D-SEG-BLOCK&quot;,n        &quot;OCR-D-SEG-LINE&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-BIN-BLOCK&quot;,n        &quot;OCR-D-BIN-LINE&quot;n      ],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {n          &quot;type&quot;: &quot;string&quot;,n          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],n          &quot;default&quot;: &quot;region&quot;,n          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-n&quot;&quot;&quot;nInstalls five executables:nn    - ocrd_tesserocr_recognizen    - ocrd_tesserocr_segment_regionn    - ocrd_tesserocr_segment_tablen    - ocrd_tesserocr_segment_linen    - ocrd_tesserocr_segment_wordn    - ocrd_tesserocr_cropn    - ocrd_tesserocr_deskewn    - ocrd_tesserocr_binarizen&quot;&quot;&quot;nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_tesserocr&#39;,n    version=&#39;0.8.0&#39;,n    description=&#39;Tesserocr bindings&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&#39;,n    author_email=&#39;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&#39;,n    url=&#39;https://github.com/OCR-D/ocrd_tesserocr&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-tesserocr-recognize=ocrd_tesserocr.cli:ocrd_tesserocr_recognize&#39;,n            &#39;ocrd-tesserocr-segment-region=ocrd_tesserocr.cli:ocrd_tesserocr_segment_region&#39;,n            &#39;ocrd-tesserocr-segment-table=ocrd_tesserocr.cli:ocrd_tesserocr_segment_table&#39;,n            &#39;ocrd-tesserocr-segment-line=ocrd_tesserocr.cli:ocrd_tesserocr_segment_line&#39;,n            &#39;ocrd-tesserocr-segment-word=ocrd_tesserocr.cli:ocrd_tesserocr_segment_word&#39;,n            &#39;ocrd-tesserocr-crop=ocrd_tesserocr.cli:ocrd_tesserocr_crop&#39;,n            &#39;ocrd-tesserocr-deskew=ocrd_tesserocr.cli:ocrd_tesserocr_deskew&#39;,n            &#39;ocrd-tesserocr-binarize=ocrd_tesserocr.cli:ocrd_tesserocr_binarize&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Fri Jan 24 15:20:03 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.8.0&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;334&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;dockerhub&quot;=&amp;gt;&quot;ocrd/tesserocr&quot;, &quot;git_url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-tesserocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BIN-BLOCK&quot;, &quot;OCR-D-BIN-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-tesserocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Poor man&#39;s cropping via region segmentation&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-PAGE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;extend detected border by this many (true) pixels on every side&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-tesserocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Detect script, orientation and skew angle for pages or regions&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-DESKEW-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_orientation_confidence&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.5, &quot;description&quot;=&amp;gt;&quot;Minimum confidence score to apply orientation as detected by OSD&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-tesserocr-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in lines with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-TESS&quot;], &quot;parameters&quot;=&amp;gt;{&quot;char_blacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_unblacklist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow inclusively.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;char_whitelist&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;description&quot;=&amp;gt;&quot;Enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Remove existing layout and text annotation below the TextLine level (regardless of textequiv_level).&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;raw_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;Do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) when using line images (i.e. when textequiv_level&amp;lt;region). Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;word&quot;, &quot;description&quot;=&amp;gt;&quot;Lowest PAGE XML hierarchy level to add the TextEquiv results to; when below `region`, implicitly adds segmentation below the line level, but requires existing line segmentation&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-tesserocr-segment-line&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment regions into lines with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-line&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextRegion level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-tesserocr-segment-region&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-region&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-PAGE&quot;, &quot;OCR-D-GT-SEG-PAGE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;crop_polygons&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;annotate polygon coordinates instead of bounding box rectangles&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;find_tables&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;padding&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;extend detected region rectangles by this many (true) pixels&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-table&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment table regions into cell text regions with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-table&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-GT-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the region level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}, &quot;ocrd-tesserocr-segment-word&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment lines into words with Tesseract&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-tesserocr-segment-word&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-GT-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-WORD&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_words&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the TextLine level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/word&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.8.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_tesserocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author-email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_tesserocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Konstantin Baierer, Kay-Michael Würzner, Robert Sachunsky&quot;, &quot;author_email&quot;=&amp;gt;&quot;unixprog@gmail.com, wuerzner@gmail.com, sachunsky@informatik.uni-leipzig.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_tesserocrnn&amp;gt; Crop, deskew, segment into regions / lines / words, or recognize with tesserocrnn[![image](https://circleci.com/gh/OCR-D/ocrd_tesserocr.svg?style=svg)](https://circleci.com/gh/OCR-D/ocrd_tesserocr)n[![image](https://img.shields.io/pypi/v/ocrd_tesserocr.svg)](https://pypi.org/project/ocrd_tesserocr/)n[![image](https://codecov.io/gh/OCR-D/ocrd_tesserocr/branch/master/graph/badge.svg)](https://codecov.io/gh/OCR-D/ocrd_tesserocr)n[![Docker Automated build](https://img.shields.io/docker/automated/ocrd/tesserocr.svg)](https://hub.docker.com/r/ocrd/tesserocr/tags/)nn## IntroductionnnThis offers [OCR-D](https://ocr-d.github.io) compliant workspace processors for (much of) the functionality of [Tesseract](https://github.com/tesseract-ocr) via its Python API wrapper [tesserocr](https://github.com/sirfz/tesserocr) . (Each processor is a step in the OCR-D functional model, and can be replaced with an alternative implementation. Data is represented within METS/PAGE.)nnThis includes image preprocessing (cropping, binarization, deskewing), layout analysis (region, line, word segmentation) and OCR proper. Most processors can operate on different levels of the PAGE hierarchy, depending on the workflow configuration. Image results are referenced (read and written) via `AlternativeImage`, text results via `TextEquiv`, deskewing via `@orientation`, cropping via `Border` and segmentation via `Region` / `TextLine` / `Word` elements with `Coords/@points`.nn## Installationnn### Required ubuntu packages:nn- Tesseract headers (`libtesseract-dev`)n- Some tesseract language models (`tesseract-ocr-{eng,deu,frk,...}` or script models (`tesseract-ocr-script-{latn,frak,...}`)n- Leptonica headers (`libleptonica-dev`)nn### From PyPInnThis is the best option if you want to use the stable, released version.nn---nn**NOTE**nnocrd_tesserocr requires **Tesseract &amp;gt;= 4.1.0**. The Tesseract packagesnbundled with **Ubuntu &amp;lt; 19.10** are too old. If you are on Ubuntu 18.04 LTS,nplease enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr) whichnhas up-to-date builds of Tesseract and its dependencies:nn```shnsudo add-apt-repository ppa:alex-p/tesseract-ocrnsudo apt-get updaten```nn---nn```shnsudo apt-get install git python3 python3-pip libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetnpip install ocrd_tesserocrn```nn### With dockernnThis is the best option if you want to run the software in a container.nnYou need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)nn```shndocker pull ocrd/tesserocrn```nn### From git nnThis is the best option if you want to change the source code or install the latest, unpublished changes.nnWe strongly recommend to use [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).nn```shngit clone https://github.com/OCR-D/ocrd_tesserocrncd ocrd_tesserocrnmake deps-ubuntu # or manually with apt-getnmake deps        # or pip install -r requirementsnmake install     # or pip install .n```nn## UsagennSee docstrings and in the individual processors and [ocrd-tool.json](ocrd_tesserocr/ocrd-tool.json) descriptions.nnAvailable processors are:nn- [ocrd-tesserocr-crop](ocrd_tesserocr/crop.py)n- [ocrd-tesserocr-deskew](ocrd_tesserocr/deskew.py)n- [ocrd-tesserocr-binarize](ocrd_tesserocr/binarize.py)n- [ocrd-tesserocr-segment-region](ocrd_tesserocr/segment_region.py)n- [ocrd-tesserocr-segment-line](ocrd_tesserocr/segment_line.py)n- [ocrd-tesserocr-segment-word](ocrd_tesserocr/segment_word.py)n- [ocrd-tesserocr-recognize](ocrd_tesserocr/recognize.py)nn## TestingnnTo run with docker:nn```ndocker run ocrd/tesserocr ocrd-tesserocrd-crop ...n```nn## Testingnn```shnmake testn```nnThis downloads some test data from https://github.com/OCR-D/assets under `repo/assets`, and runs some basic test of the Python API as well as the CLIs.nnSet `PYTEST_ARGS=&quot;-s --verbose&quot;` to see log output (`-s`) and individual test results (`--verbose`).nn## DevelopmentnnLatest changes that require pre-release of [ocrd &amp;gt;= 2.0.0](https://github.com/OCR-D/core/tree/edge) are kept in branch [`edge`](https://github.com/OCR-D/ocrd_tesserocr/tree/edge).nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-tesserocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-tesserocr/0.7.0/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;tesserocr (&amp;gt;=2.4.1)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Tesserocr bindings&quot;, &quot;version&quot;=&amp;gt;&quot;0.7.0&quot;}, &quot;last_serial&quot;=&amp;gt;6506849, &quot;releases&quot;=&amp;gt;{&quot;0.1.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;sha256&quot;=&amp;gt;&quot;64ec4e7a43ddaf199af7da8966996e260454dae4d30f79cb112149cddf5b8fd2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e12ea0e2f580c6e152d334c470029dc2&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17089, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:24&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:24.592860Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/07/63/e617002f9c2013f8a9ce10baeab48acffc0dff3d21ab160ee67428e08ebd/ocrd_tesserocr-0.1.0-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2a7fd61a97bb222f2ac5a6f85b3d2ce43da843509993eef189f09b48f44027f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;ad528712e13eecf578b236a7ab8457cd&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15424, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:25.913866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4d/48/282d1d793137f1ec30118a9a0bd48534a6a8053bc74a830b6c4eb389653f/ocrd_tesserocr-0.1.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;sha256&quot;=&amp;gt;&quot;3fecd0a93d9a711552fbd2cf15af1f150f04f503f7b3f09d9c025267601bb42d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d45fa7a24f23d22313e4314df42cf984&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9234, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:13:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:13:27.040863Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/eb/a7/66775daafba5937821fd643b6d1069570b262af3a48d701712d2a94350a2/ocrd_tesserocr-0.1.0.tar.gz&quot;}], &quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;sha256&quot;=&amp;gt;&quot;d474e372af4266ab4343570c47a448f9f68b3c002f970717663b64acabe1dbe4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;fab719d99117d974ca24e63cdf6af83e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:51.905308Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/5c/95/7f29b87ff5be4fdd149400855862840de4681b669d3fda60a2ce8bf24127/ocrd_tesserocr-0.1.1-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;sha256&quot;=&amp;gt;&quot;3c0f56fc2c88ec1ea2461eb0610763443b9af279c5260b08a1be079c92bed5c6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;cfef79e48dc96f865deff1b89fa28aa6&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15461, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:53&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:53.535866Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/da/23/fb5e1e125f1fda3b1069960426c5b40a9c5e12fe8f73ac29244888cf110b/ocrd_tesserocr-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;sha256&quot;=&amp;gt;&quot;2de460c4d3218ac6e3133b498c01ee7428770edcd60a02f65793ae4006f3db82&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0dbecd3bc62199f7294a039c4c8557c3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9251, &quot;upload_time&quot;=&amp;gt;&quot;2018-08-31T14:18:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-08-31T14:18:54.917641Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/31/73/c2044ae57f402e21947ceb97f574625cf534eccbf432f6916c419cf3d7e7/ocrd_tesserocr-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;sha256&quot;=&amp;gt;&quot;b2409adbb5c529b05eba8be5a9d1c7e11660dc2626bcaf61b407b617d5c7c99e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;215dd5bba309954a15fc1be4919cd018&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15453, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:20&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:20.618650Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/c1/ca/38355a461d8e29d7039391f5051be291d6a425b078783adb1ebb6ba10e55/ocrd_tesserocr-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;sha256&quot;=&amp;gt;&quot;fbde4fc1a5a0340507b6d96bd529a42162e732b7cca31e968b28f6a4fcdccd12&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b59d049bbfc890edd7a17f3bd596b42a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9242, &quot;upload_time&quot;=&amp;gt;&quot;2018-09-03T13:14:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2018-09-03T13:14:21.805810Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1b/fe/b365c2ffddea53e616408f0213e45614ce3791ead2058df33a795ddc3d21/ocrd_tesserocr-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;sha256&quot;=&amp;gt;&quot;1549fbf8d314dc1f5ea20b45842e971a97b3c276f78d4d167a463432d5b77b18&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f69aed68ca01cf1018b35d91227d74a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py2&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;17420, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:12.698851Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/18/7f/fd08ca819e6f3980220ac680b5c931080247544c2704963e518db6f7a3d0/ocrd_tesserocr-0.1.3-py2-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;sha256&quot;=&amp;gt;&quot;1648df71d28a9b3388f1e701256037eb9023f149a17a22d0a9c2dec4a0510002&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;bbc586d5a04c44b640d7782a84e2de83&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15729, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:14&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:14.276437Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/34/08/ea3ebc9476e1d28672e23b8d1332dbbc95ac9a3246cd7d02be2375995da6/ocrd_tesserocr-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;sha256&quot;=&amp;gt;&quot;6ec1b6c5cb4395f6f4e7356219e7019612fdcda685b511de7171dcaf4f39a439&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3f7f434d236449d567213324856c521a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9442, &quot;upload_time&quot;=&amp;gt;&quot;2019-01-04T13:36:15&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-01-04T13:36:15.802793Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/10/d1b3c66b891193ccc07200d93391cbcfe9c4c5ea2bb1cac045e7d1cf1fa6/ocrd_tesserocr-0.1.3.tar.gz&quot;}], &quot;0.2.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;sha256&quot;=&amp;gt;&quot;f61661e4cba7b77336dcabc6117d1e4fa90357ec98f263eacfc2c836e3a477f4&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e5e19ec5b8786ef3ae8b456e8180b3da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;16547, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:21&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:21.318896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d1/94/606de830cdba1f81928dc42a71f7e58cc6510d6a8b0f9e945c01f56ee3e7/ocrd_tesserocr-0.2.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;sha256&quot;=&amp;gt;&quot;05cc4be3ae1404afd45d8b9278d19fcd6a1ea86d376f52f571fefc4af4d96b86&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9a06170c3773b520b13c9516b0497a33&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10356, &quot;upload_time&quot;=&amp;gt;&quot;2019-02-28T10:12:22&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-02-28T10:12:22.854225Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/50/1c/eda34c75846857877176db4f4f0564e8b7c979a872e4c2a521fa8c389fbb/ocrd_tesserocr-0.2.0.tar.gz&quot;}], &quot;0.2.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;sha256&quot;=&amp;gt;&quot;fd8c18ce5d170e766bccd34c2214e5de22ea13f795bc79642e8be2414c550f2a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;43d7c9b609a3d2e27bcb05bd409cebbc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15963, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:44&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:44.123075Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/39/af/10f4d710bde5515131fc16ea3408670af8e786998a1e0f6d127e800fbc17/ocrd_tesserocr-0.2.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;sha256&quot;=&amp;gt;&quot;40f4776bc548be14245de726e744f827742f02e568f6062cc465d6a585624cae&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b9d79ed8396cc81728525c6e66bc2883&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;9534, &quot;upload_time&quot;=&amp;gt;&quot;2019-04-16T14:58:45&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-04-16T14:58:45.820115Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/df/cc/fd5b999abcae94ff2116a25e31f593b95f0dda4486d89bd4e83d6671b805/ocrd_tesserocr-0.2.1.tar.gz&quot;}], &quot;0.2.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;sha256&quot;=&amp;gt;&quot;7ccdeb2a24f9d93ec6668d02807a4f5fa31d88789a3101ad1fd4ea003128ca65&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;df13430385faf1faeb9d8bca34e1ca08&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;18334, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:06.855632Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/4e/5f/37ec32a07681542a1d34fa9764c76ef34d201a82489335d154d34e8b46b2/ocrd_tesserocr-0.2.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;sha256&quot;=&amp;gt;&quot;ad96c009bcf39b8f9e99f3e58b736ab385e5683935b9146ed9e39e8e8883b4c2&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.2.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;d985dfeeedd9946a32e30ec079c3dac3&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;10990, &quot;upload_time&quot;=&amp;gt;&quot;2019-05-20T10:24:08&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-05-20T10:24:08.563041Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/53/c0186de6ad8429e6b8e0f5e5ac51a8a3d51a2c71bcb597a5879313bf2a2d/ocrd_tesserocr-0.2.2.tar.gz&quot;}], &quot;0.3.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;sha256&quot;=&amp;gt;&quot;09f23770905034ed00f7cb516a907288512a4d21305914b6e2dd7215b9138c6e&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;06790327b49f97d4ed656fb842b36511&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34706, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:39.261053Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b2/b5/8a890997a3f874498a1f596f3ebdb765daa181858a46cc5a66949945adf8/ocrd_tesserocr-0.3.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;sha256&quot;=&amp;gt;&quot;11b6742c4c398ea800d0b17276f0efd8a91ccbd6f0c1df05d7046c3e401a33c8&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.3.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;40be922772cb0f0ad188aa4345bbad9a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;22743, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T14:42:40&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T14:42:40.918776Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f3/fa/10af8e05b04c55680b20582c18bed55ffa846bfa65948c6b6138252a8434/ocrd_tesserocr-0.3.0.tar.gz&quot;}], &quot;0.4.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;sha256&quot;=&amp;gt;&quot;4822713547e696dbb327a80f9dd5bad705be4b7dc1f44fdef1d44f9e03c21c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9d5ea4deb4c75bae31b7d44a4a8fdd0a&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37231, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:05&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:05.083051Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/ee/2b/483b44bf3180e81aa8a5bf7307ae47da4d1656e69dec1a704f9a8d558b88/ocrd_tesserocr-0.4.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;sha256&quot;=&amp;gt;&quot;616bf420794ef71bcc372fa4c29775c48d6909d01b6849e2d0be83766cd0ed90&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;91e09cbc5208905353c22f07029db316&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;19943, &quot;upload_time&quot;=&amp;gt;&quot;2019-08-21T16:47:06&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-08-21T16:47:06.605798Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/87/09/b994a5d7310f73b04b7dd840a5fbdd726da42b7980ac0a07595b6c56ef00/ocrd_tesserocr-0.4.0.tar.gz&quot;}], &quot;0.4.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;sha256&quot;=&amp;gt;&quot;d21818eceac8bcdc1fdb38d4a58bfd1620cef8e7a5d0e6276afbd7695c2cac31&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e634e1792d14a33a6bdde296483f0817&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38864, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:27&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:27.102775Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/1d/78/93c90d9593f62546fea5e2ef9b5edbb5a47121582db724ca41f93830ec87/ocrd_tesserocr-0.4.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;sha256&quot;=&amp;gt;&quot;bbf3843361c4807c5790790d8a8fc0a0325b2fb9817cd4fa70210659dde8c8cb&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.4.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;3de4e2c8fcb66eb6a3cb32a1a1cd361b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20535, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T14:58:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T14:58:28.641792Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/a7/2e/de857738105ed9f1888d3f6724c0c314404b67582652a91b060d25cff808/ocrd_tesserocr-0.4.1.tar.gz&quot;}], &quot;0.5.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;sha256&quot;=&amp;gt;&quot;f3bca0adcb9fce640a010d38d7e1d04b4fc423ec0cc958ff3980afbf74a5711f&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4a807653bdfacd7d22b6c303dc1ac04f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;33343, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:17&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:17.958444Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/36/98/a6c6b46903a3b25b1740cde4aedaf62de6441ac887536e36ad24a3c3bf12/ocrd_tesserocr-0.5.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;sha256&quot;=&amp;gt;&quot;aaf012b2c6adcd9a34b6fa9351dcd16fed3ab848d4d8a563b3825f9b7103be42&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;b4885925db28012b94b5fa3c86d80e28&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;21170, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-26T18:40:19&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-26T18:40:19.386827Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/85/5b/7c5c21b78ccd00d49f7747ad5b2a381d9860aeed41fe545a24a361544837/ocrd_tesserocr-0.5.0.tar.gz&quot;}], &quot;0.5.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;sha256&quot;=&amp;gt;&quot;18cef805014268db86fd6c32bca83069cdf536298fe8151f59f9197d255a9d14&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8835763816200fbfec9b58670bd69d8f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;38309, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:42&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:42.078476Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/06/84/b5aca7d06e31dcb91683ab60e154b73a8d0e1cb4d5ae22debf55922573df/ocrd_tesserocr-0.5.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;sha256&quot;=&amp;gt;&quot;7dd6a5fd556395deb58070d5f6196871a241d89434a26d0a0fc7e106404aa90a&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.5.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;1c203160eddb792cdbd706ccbb5e35bb&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20350, &quot;upload_time&quot;=&amp;gt;&quot;2019-10-31T16:43:43&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-10-31T16:43:43.864345Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/1f/ed95415ee91659222301aa77e4f8c27be33df8e258972059bc031a2c0e3b/ocrd_tesserocr-0.5.1.tar.gz&quot;}], &quot;0.6.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;sha256&quot;=&amp;gt;&quot;41d5309efc4f886569d47dede504cea5e14ffd8e27a33acb69e15c775d34f754&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0f1c539e4ffd53d67a3b891586c7be48&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;37693, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:55.328581Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/89/a9/431c3ad62ac4612b6be3f5cad58b49910a9c00b5f28dd62f8d535ed0c0cf/ocrd_tesserocr-0.6.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;sha256&quot;=&amp;gt;&quot;3a1aeff23dbf42cc8c003039cc8695cd4e01807245f935c9323e6df2832855a7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.6.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;9c454a4d508b6d43a1551b517c125d5b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;20588, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:14:57&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:14:57.128983Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/48/30/6c8253739ee61d4a42b6512be3fcfe0ce7190ff2835ee1210b1c483da025/ocrd_tesserocr-0.6.0.tar.gz&quot;}], &quot;0.7.0&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;sha256&quot;=&amp;gt;&quot;19e81e1ff8344c6766bf41e8968e14efceb2902c7bb4fd2b7c811b3697e0f589&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c70cf04587dbacd64f10e58706852630&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;44435, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:55.259065Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0d/74/404359c05892e1123e1e6cbbd07d237e11bf42f3aa75cf41db87f4920a42/ocrd_tesserocr-0.7.0-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;sha256&quot;=&amp;gt;&quot;640504e049c3ccfe046c912109ca0354fe414004c5afb1fc9e9bb6e0651509d6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_tesserocr-0.7.0.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;0bc1167c26f1fad3e0a1dfc79ebca1e4&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;24991, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-23T14:31:56&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-23T14:31:56.649512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/16/e7/f6f57abfef6c662cd4cde8f02f2f49639e4075211776e069543c2ca3d484/ocrd_tesserocr-0.7.0.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_tesserocr&quot;}         ocrd_cis    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/core:latestnENV VERSION=&quot;Mi 9. Okt 13:26:16 CEST 2019&quot;nENV GITURL=&quot;https://github.com/cisocrgroup&quot;nENV DOWNLOAD_URL=&quot;http://cis.lmu.de/~finkf&quot;nENV DATA=&quot;/apps/ocrd-cis-post-correction&quot;nn# depsnCOPY data/docker/deps.txt ${DATA}/deps.txtnRUN apt-get update nt&amp;amp;&amp;amp; apt-get -y install --no-install-recommends $(cat ${DATA}/deps.txt)nn# localesnRUN sed -i -e &#39;s/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/&#39; /etc/locale.gen n    &amp;amp;&amp;amp; dpkg-reconfigure --frontend=noninteractive locales n    &amp;amp;&amp;amp; update-locale LANG=en_US.UTF-8nn# install the profilernRUNtgit clone ${GITURL}/Profiler --branch devel --single-branch /tmp/profiler nt&amp;amp;&amp;amp; cd /tmp/profiler nt&amp;amp;&amp;amp; mkdir build nt&amp;amp;&amp;amp; cd build nt&amp;amp;&amp;amp; cmake -DCMAKE_BUILD_TYPE=release .. nt&amp;amp;&amp;amp; make compileFBDic trainFrequencyList profiler nt&amp;amp;&amp;amp; cp bin/compileFBDic bin/trainFrequencyList bin/profiler /apps/ nt&amp;amp;&amp;amp; cd / n    &amp;amp;&amp;amp; rm -rf /tmp/profilernn# install the profiler&#39;s language backendnRUNtgit clone ${GITURL}/Resources --branch master --single-branch /tmp/resources nt&amp;amp;&amp;amp; cd /tmp/resources/lexica nt&amp;amp;&amp;amp; make FBDIC=/apps/compileFBDic TRAIN=/apps/trainFrequencyList nt&amp;amp;&amp;amp; mkdir -p /${DATA}/languages nt&amp;amp;&amp;amp; cp -r german latin greek german.ini latin.ini greek.ini /${DATA}/languages nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/resourcesnn# install ocrd_cis (python)nCOPY Manifest.in Makefile setup.py ocrd-tool.json /tmp/build/nCOPY ocrd_cis/ /tmp/build/ocrd_cis/nCOPY bashlib/ /tmp/build/bashlib/n# COPY . /tmp/ocrd_cisnRUN cd /tmp/build nt&amp;amp;&amp;amp; make install nt&amp;amp;&amp;amp; cd / nt&amp;amp;&amp;amp; rm -rf /tmp/buildnn# download ocr models and pre-trainded post-correction modelnRUN mkdir /apps/models nt&amp;amp;&amp;amp; cd /apps/models nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/model.zip &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur1-00085000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 nt&amp;amp;&amp;amp; wget ${DOWNLOAD_URL}/fraktur2-00062000.pyrnn.gz &amp;gt;/dev/null 2&amp;gt;&amp;amp;1nnVOLUME [&quot;/data&quot;]n&quot;, &quot;README.md&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{nt&quot;git_url&quot;: &quot;https://github.com/cisocrgroup/ocrd_cis&quot;,nt&quot;version&quot;: &quot;0.0.6&quot;,nt&quot;tools&quot;: {ntt&quot;ocrd-cis-ocropy-binarize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-binarize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/binarization&quot;,ntttt&quot;preprocessing/optimization/grayscale_normalization&quot;,ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-BIN&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;method&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;],nttttt&quot;description&quot;: &quot;binarization method to use (only ocropy will include deskewing)&quot;,nttttt&quot;default&quot;: &quot;ocropy&quot;ntttt},ntttt&quot;grayscale&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;description&quot;: &quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;,nttttt&quot;default&quot;: falsentttt},ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 0.0ntttt},ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-deskew&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-deskew&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/deskewing&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;maxskew&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;description&quot;: &quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;,nttttt&quot;default&quot;: 5.0ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-denoise&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-denoise&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/despeckling&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-IMG&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-IMG-DESPECK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Despeckle pages / regions / lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;noise_maxsize&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;,nttttt&quot;default&quot;: 3.0ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;page&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-clip&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-clip&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Clip text regions / lines at intersections with neighbours&quot;,nttt&quot;parameters&quot;: {ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to annotate images for&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.7ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-resegment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-resegment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;min_fraction&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;share of foreground pixels that must be retained by the largest label&quot;,nttttt&quot;default&quot;: 0.8ntttt},ntttt&quot;extend_margins&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;,nttttt&quot;default&quot;: 3ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-dewarp&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-dewarp&quot;,nttt&quot;categories&quot;: [ntttt&quot;Image preprocessing&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;preprocessing/optimization/dewarping&quot;nttt],nttt&quot;description&quot;: &quot;Dewarp line images with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;range&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;,nttttt&quot;default&quot;: 4.0ntttt},ntttt&quot;max_neighbour&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;,nttttt&quot;default&quot;: 0.05ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-recognize&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-recognize&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;,nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;,ntttt&quot;OCR-D-SEG-WORD&quot;,ntttt&quot;OCR-D-SEG-GLYPH&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-OCR-OCRO&quot;nttt],nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;,nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-rec&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-rec&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;recognition/text-recognition&quot;nttt],nttt&quot;description&quot;: &quot;Recognize text snippets&quot;,nttt&quot;parameters&quot;: {ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-segment&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-segment&quot;,nttt&quot;categories&quot;: [ntttt&quot;Layout analysis&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;layout/segmentation/region&quot;,ntttt&quot;layout/segmentation/line&quot;nttt],nttt&quot;input_file_grp&quot;: [ntttt&quot;OCR-D-GT-SEG-BLOCK&quot;,ntttt&quot;OCR-D-SEG-BLOCK&quot;nttt],nttt&quot;output_file_grp&quot;: [ntttt&quot;OCR-D-SEG-LINE&quot;nttt],nttt&quot;description&quot;: &quot;Segment pages into regions or regions into lines with ocropy&quot;,nttt&quot;parameters&quot;: {ntttt&quot;dpi&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,nttttt&quot;default&quot;: -1ntttt},ntttt&quot;level-of-operation&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;page&quot;, &quot;region&quot;],nttttt&quot;description&quot;: &quot;PAGE XML hierarchy level to read images from&quot;,nttttt&quot;default&quot;: &quot;region&quot;ntttt},ntttt&quot;maxcolseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 2,nttttt&quot;description&quot;: &quot;number of white/background column separators to try (when operating on the page level)&quot;ntttt},ntttt&quot;maxseps&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 5,nttttt&quot;description&quot;: &quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_regions&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextRegion elements (when operating on the page level)&quot;ntttt},ntttt&quot;overwrite_lines&quot;: {nttttt&quot;type&quot;: &quot;boolean&quot;,nttttt&quot;default&quot;: true,nttttt&quot;description&quot;: &quot;remove any existing TextLine elements (when operating on the region level)&quot;ntttt},ntttt&quot;spread&quot;: {nttttt&quot;type&quot;: &quot;number&quot;,nttttt&quot;format&quot;: &quot;float&quot;,nttttt&quot;default&quot;: 2.4,nttttt&quot;description&quot;: &quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-ocropy-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-ocropy-train&quot;,nttt&quot;categories&quot;: [ntttt&quot;lstm ocropy model training&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;training&quot;nttt],nttt&quot;description&quot;: &quot;train model with ground truth from mets data&quot;,nttt&quot;parameters&quot;: {ntttt&quot;textequiv_level&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;enum&quot;: [&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;],nttttt&quot;default&quot;: &quot;line&quot;ntttt},ntttt&quot;model&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;ntttt},ntttt&quot;ntrain&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;lines to train before stopping&quot;,nttttt&quot;default&quot;: 1000000ntttt},ntttt&quot;outputpath&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;description&quot;: &quot;(existing) path for the trained model&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-align&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-align&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Align multiple OCRs and/or GTs&quot;ntt},ntt&quot;ocrd-cis-wer&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-wer&quot;,nttt&quot;categories&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;evaluation&quot;nttt],nttt&quot;description&quot;: &quot;calculate the word error rate for aligned page xml files&quot;,nttt&quot;parameters&quot;: {ntttt&quot;testIndex&quot;: {nttttt&quot;description&quot;: &quot;text equiv index for the test/ocr tokens&quot;,nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;default&quot;: 0ntttt},ntttt&quot;gtIndex&quot;: {nttttt&quot;type&quot;: &quot;integer&quot;,nttttt&quot;description&quot;: &quot;text equiv index for the gt tokens&quot;,nttttt&quot;default&quot;: -1ntttt}nttt}ntt},ntt&quot;ocrd-cis-jar&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-jar&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Output path to the ocrd-cis.jar file&quot;ntt},ntt&quot;ocrd-cis-profile&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-profile&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Add a correction suggestions and suspicious tokens (profile)&quot;,nttt&quot;parameters&quot;: {ntttt&quot;executable&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;backend&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: truentttt},ntttt&quot;language&quot;: {ntttt    &quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;german&quot;ntttt},ntttt&quot;additionalLexicon&quot;: {nttttt&quot;type&quot;: &quot;string&quot;,nttttt&quot;required&quot;: false,nttttt&quot;default&quot;: &quot;&quot;ntttt}nttt}ntt},ntt&quot;ocrd-cis-train&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-train.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Train post correction model&quot;,nttt&quot;parameters&quot;: {ntttt&quot;gtArchives&quot;: {nttttt&quot;description&quot;: &quot;List of ground truth archives&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Path (or URL) to a ground truth archive&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;imagePreprocessingSteps&quot;: {nttttt&quot;description&quot;: &quot;List of image preprocessing steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;training&quot;: {nttttt&quot;description&quot;: &quot;Configuration of training command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;trigrams&quot;,ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;leFeatures&quot;,ntttttt&quot;rrFeatures&quot;,ntttttt&quot;dmFeatures&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;trigrams&quot;: {nttttttt&quot;description&quot;: &quot;Path to character trigrams csv file (format: n,trigram)&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;filterClasses&quot;: {nttttttt&quot;description&quot;: &quot;List of filtered feature classes&quot;,nttttttt&quot;required&quot;: false,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Class name of feature class to filter&quot;,ntttttttt&quot;type&quot;: &quot;string&quot;nttttttt}ntttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;leFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the lexicon extension features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;rrFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the reranker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt},ntttttt&quot;dmFeatures&quot;: {nttttttt&quot;description&quot;: &quot;List of the desicion maker features&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;array&quot;,nttttttt&quot;items&quot;: {ntttttttt&quot;description&quot;: &quot;Feature configuration&quot;,ntttttttt&quot;type&quot;: &quot;object&quot;,ntttttttt&quot;required&quot;: [nttttttttt&quot;type&quot;,nttttttttt&quot;name&quot;ntttttttt],ntttttttt&quot;properties&quot;: {nttttttttt&quot;name&quot;: {ntttttttttt&quot;description&quot;: &quot;Name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;type&quot;: {ntttttttttt&quot;description&quot;: &quot;Fully qualified java class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt},nttttttttt&quot;class&quot;: {ntttttttttt&quot;description&quot;: &quot;Class name of the feature&quot;,ntttttttttt&quot;type&quot;: &quot;string&quot;nttttttttt}ntttttttt}nttttttt}ntttttt}nttttt}ntttt}nttt}ntt},ntt&quot;ocrd-cis-post-correct&quot;: {nttt&quot;executable&quot;: &quot;ocrd-cis-post-correct.sh&quot;,nttt&quot;categories&quot;: [ntttt&quot;Text recognition and optimization&quot;nttt],nttt&quot;steps&quot;: [ntttt&quot;postprocessing/alignment&quot;nttt],nttt&quot;description&quot;: &quot;Post correct OCR results&quot;,nttt&quot;parameters&quot;: {ntttt&quot;ocrSteps&quot;: {nttttt&quot;description&quot;: &quot;List of additional ocr steps&quot;,nttttt&quot;type&quot;: &quot;array&quot;,nttttt&quot;required&quot;: true,nttttt&quot;items&quot;: {ntttttt&quot;description&quot;: &quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;,ntttttt&quot;type&quot;: &quot;string&quot;nttttt}ntttt},ntttt&quot;postCorrection&quot;: {nttttt&quot;description&quot;: &quot;Configuration of post correction command&quot;,nttttt&quot;type&quot;: &quot;object&quot;,nttttt&quot;required&quot;: [ntttttt&quot;maxCandidate&quot;,ntttttt&quot;profiler&quot;,ntttttt&quot;model&quot;,ntttttt&quot;runLE&quot;,ntttttt&quot;runDM&quot;nttttt],nttttt&quot;properties&quot;: {ntttttt&quot;maxCandidate&quot;: {nttttttt&quot;description&quot;: &quot;Maximum number of considered profiler candidates per token&quot;,nttttttt&quot;type&quot;: &quot;integer&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;profiler&quot;: {nttttttt&quot;description&quot;: &quot;Profiler configuration&quot;,nttttttt&quot;type&quot;: &quot;object&quot;,nttttttt&quot;required&quot;: [ntttttttt&quot;path&quot;,ntttttttt&quot;config&quot;nttttttt],nttttttt&quot;properties&quot;: {ntttttttt&quot;path&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler executable&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt},ntttttttt&quot;config&quot;: {nttttttttt&quot;description&quot;: &quot;Path to the profiler language config file&quot;,nttttttttt&quot;required&quot;: true,nttttttttt&quot;type&quot;: &quot;string&quot;ntttttttt}nttttttt}ntttttt},ntttttt&quot;model&quot;: {nttttttt&quot;description&quot;: &quot;Path to the post correction model file&quot;,nttttttt&quot;type&quot;: &quot;string&quot;,nttttttt&quot;required&quot;: truentttttt},ntttttt&quot;runLE&quot;: {nttttttt&quot;description&quot;: &quot;Do run the lexicon extension step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt},ntttttt&quot;runDM&quot;: {nttttttt&quot;description&quot;: &quot;Do run the ranking and the decision step for the post correction&quot;,nttttttt&quot;required&quot;: true,nttttttt&quot;type&quot;: &quot;boolean&quot;ntttttt}nttttt}ntttt}nttt}ntt}nt}n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;&quot;&quot;&quot;nInstalls:n    - ocrd-cis-alignn    - ocrd-cis-trainingn    - ocrd-cis-profilen    - ocrd-cis-wern    - ocrd-cis-datan    - ocrd-cis-ocropy-clipn    - ocrd-cis-ocropy-denoisen    - ocrd-cis-ocropy-deskewn    - ocrd-cis-ocropy-binarizen    - ocrd-cis-ocropy-resegmentn    - ocrd-cis-ocropy-segmentn    - ocrd-cis-ocropy-dewarpn    - ocrd-cis-ocropy-recognizen    - ocrd-cis-ocropy-trainn&quot;&quot;&quot;nnimport codecsnfrom setuptools import setupnfrom setuptools import find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_cis&#39;,n    version=&#39;0.0.6&#39;,n    description=&#39;CIS OCR-D command line tools&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Florian Fink, Tobias Englmeier, Christoph Weber&#39;,n    author_email=&#39;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&#39;,n    url=&#39;https://github.com/cisocrgroup/ocrd_cis&#39;,n    license=&#39;MIT&#39;,n    packages=find_packages(),n    include_package_data=True,n    install_requires=[n        &#39;ocrd&amp;gt;=2.0.0&#39;,n        &#39;click&#39;,n        &#39;scipy&#39;,n        &#39;numpy&amp;gt;=1.17.0&#39;,n        &#39;pillow&amp;gt;=6.2.0&#39;,n        &#39;shapely&#39;,n        &#39;matplotlib&amp;gt;3.0.0&#39;,n        &#39;python-Levenshtein&#39;,n        &#39;calamari_ocr == 0.3.5&#39;n    ],n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;, &#39;*.csv.gz&#39;, &#39;*.jar&#39;],n    },n    scripts=[n        &#39;bashlib/ocrd-cis-lib.sh&#39;,n        &#39;bashlib/ocrd-cis-train.sh&#39;,n        &#39;bashlib/ocrd-cis-post-correct.sh&#39;,n    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-cis-align=ocrd_cis.align.cli:ocrd_cis_align&#39;,n            &#39;ocrd-cis-profile=ocrd_cis.profile.cli:ocrd_cis_profile&#39;,n            &#39;ocrd-cis-wer=ocrd_cis.wer.cli:ocrd_cis_wer&#39;,n            &#39;ocrd-cis-data=ocrd_cis.data.__main__:main&#39;,n            &#39;ocrd-cis-ocropy-binarize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_binarize&#39;,n            &#39;ocrd-cis-ocropy-clip=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_clip&#39;,n            &#39;ocrd-cis-ocropy-denoise=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_denoise&#39;,n            &#39;ocrd-cis-ocropy-deskew=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_deskew&#39;,n            &#39;ocrd-cis-ocropy-dewarp=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_dewarp&#39;,n            &#39;ocrd-cis-ocropy-recognize=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_recognize&#39;,n            &#39;ocrd-cis-ocropy-rec=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_rec&#39;,n            &#39;ocrd-cis-ocropy-resegment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_resegment&#39;,n            &#39;ocrd-cis-ocropy-segment=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_segment&#39;,n            &#39;ocrd-cis-ocropy-train=ocrd_cis.ocropy.cli:ocrd_cis_ocropy_train&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 23 15:42:32 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;436&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-cis-align&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Align multiple OCRs and/or GTs&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-align&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-jar&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Output path to the ocrd-cis.jar file&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-jar&quot;, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-ocropy-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize (and optionally deskew/despeckle) pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;grayscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;for the ocropy method, produce grayscale-normalized instead of thresholded image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;method&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;ocropy&quot;, &quot;description&quot;=&amp;gt;&quot;binarization method to use (only ocropy will include deskewing)&quot;, &quot;enum&quot;=&amp;gt;[&quot;none&quot;, &quot;global&quot;, &quot;otsu&quot;, &quot;gauss-otsu&quot;, &quot;ocropy&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;maximum pixel number for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;, &quot;preprocessing/optimization/grayscale_normalization&quot;, &quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-clip&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Clip text regions / lines at intersections with neighbours&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-clip&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.7, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-denoise&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Despeckle pages / regions / lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-denoise&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESPECK&quot;, &quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;noise_maxsize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;maximum size in points (pt) for connected components to regard as noise (0 will deactivate denoising)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/despeckling&quot;]}, &quot;ocrd-cis-ocropy-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew regions with ocropy (by annotating orientation angle and adding AlternativeImage)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;, &quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to annotate images for&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5.0, &quot;description&quot;=&amp;gt;&quot;modulus of maximum skewing angle to detect (larger will be slower, 0 will deactivate deskewing)&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-cis-ocropy-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Dewarp line images with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;max_neighbour&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;maximum rate of foreground pixels intruding from neighbouring lines (line will not be processed above that)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4.0, &quot;description&quot;=&amp;gt;&quot;maximum vertical disposition or maximum margin (will be multiplied by mean centerline deltas to yield pixels)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-cis-ocropy-rec&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text snippets&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-rec&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-recognize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Recognize text in (binarized+deskewed+dewarped) lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-recognize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;, &quot;OCR-D-SEG-WORD&quot;, &quot;OCR-D-SEG-GLYPH&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-OCRO&quot;], &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;ocropy model to apply (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level granularity to add the TextEquiv results to&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}, &quot;ocrd-cis-ocropy-resegment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Resegment lines with ocropy (by shrinking annotated polygons)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-resegment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;extend_margins&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;number of pixels to extend the input polygons horizontally and vertically before intersecting&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;min_fraction&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.8, &quot;description&quot;=&amp;gt;&quot;share of foreground pixels that must be retained by the largest label&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-segment&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment pages into regions or regions into lines with ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-segment&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-SEG-BLOCK&quot;, &quot;OCR-D-SEG-BLOCK&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE&quot;], &quot;parameters&quot;=&amp;gt;{&quot;dpi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;level-of-operation&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;region&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to read images from&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;number of white/background column separators to try (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;number of black/foreground column separators to try, counted individually as lines (when operating on the page level)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;overwrite_lines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextLine elements (when operating on the region level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove any existing TextRegion elements (when operating on the page level)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;spread&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2.4, &quot;description&quot;=&amp;gt;&quot;distance in points (pt) from the foreground to project text line (or text region) labels into the background&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;]}, &quot;ocrd-cis-ocropy-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;lstm ocropy model training&quot;], &quot;description&quot;=&amp;gt;&quot;train model with ground truth from mets data&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-ocropy-train&quot;, &quot;parameters&quot;=&amp;gt;{&quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;load model or crate new one (e.g. fraktur.pyrnn)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;ntrain&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1000000, &quot;description&quot;=&amp;gt;&quot;lines to train before stopping&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;outputpath&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;(existing) path for the trained model&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;textequiv_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;line&quot;, &quot;enum&quot;=&amp;gt;[&quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;training&quot;]}, &quot;ocrd-cis-post-correct&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Post correct OCR results&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-post-correct.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of additional ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;postCorrection&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of post correction command&quot;, &quot;properties&quot;=&amp;gt;{&quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;model&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the post correction model file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;runDM&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the ranking and the decision step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;runLE&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Do run the lexicon extension step for the post correction&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}}, &quot;required&quot;=&amp;gt;[&quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;model&quot;, &quot;runLE&quot;, &quot;runDM&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-profile&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Add a correction suggestions and suspicious tokens (profile)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-profile&quot;, &quot;parameters&quot;=&amp;gt;{&quot;additionalLexicon&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;backend&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;executable&quot;=&amp;gt;{&quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;language&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;german&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-train&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Train post correction model&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-train.sh&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtArchives&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ground truth archives&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path (or URL) to a ground truth archive&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;imagePreprocessingSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of image preprocessing steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Image preprocessing command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $IMG_OUTPUT_FILE_GRP, $IMG_INPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;ocrSteps&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of ocr steps&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;OCR command that is evaled using the bash eval command (available parameters: $METS, $LOG_LEVEL, $XML_INPUT_FILE_GRP, $XML_OUTPUT_FILE_GRP, $PARAMETER)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;training&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Configuration of training command&quot;, &quot;properties&quot;=&amp;gt;{&quot;dmFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the desicion maker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;filterClasses&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of filtered feature classes&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of feature class to filter&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;leFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the lexicon extension features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;maxCandidate&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Maximum number of considered profiler candidates per token&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;profiler&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Profiler configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;config&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler language config file&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;path&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to the profiler executable&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;path&quot;, &quot;config&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;rrFeatures&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;List of the reranker features&quot;, &quot;items&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Feature configuration&quot;, &quot;properties&quot;=&amp;gt;{&quot;class&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;name&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;type&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Fully qualified java class name of the feature&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;type&quot;, &quot;name&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;array&quot;}, &quot;trigrams&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;Path to character trigrams csv file (format: n,trigram)&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;required&quot;=&amp;gt;[&quot;trigrams&quot;, &quot;maxCandidate&quot;, &quot;profiler&quot;, &quot;leFeatures&quot;, &quot;rrFeatures&quot;, &quot;dmFeatures&quot;], &quot;type&quot;=&amp;gt;&quot;object&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;postprocessing/alignment&quot;]}, &quot;ocrd-cis-wer&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;evaluation&quot;], &quot;description&quot;=&amp;gt;&quot;calculate the word error rate for aligned page xml files&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-cis-wer&quot;, &quot;parameters&quot;=&amp;gt;{&quot;gtIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;-1, &quot;description&quot;=&amp;gt;&quot;text equiv index for the gt tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;testIndex&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;text equiv index for the test/ocr tokens&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;evaluation&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.6&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-cis-ocropy-rec] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.textequiv_level] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-ocropy-train.parameters.ntrain.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-ocropy-train.categories.0] &#39;lstm ocropy model training&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-ocropy-train.steps.0] &#39;training&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-align] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-align.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-wer] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-wer.parameters.testIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.parameters.gtIndex.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-wer.categories.0] &#39;evaluation&#39; is not one of [&#39;Image preprocessing&#39;, &#39;Layout analysis&#39;, &#39;Text recognition and optimization&#39;, &#39;Model training&#39;, &#39;Long-term preservation&#39;, &#39;Quality assurance&#39;]n  [tools.ocrd-cis-wer.steps.0] &#39;evaluation&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-jar] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-jar.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-profile] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.executable] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.backend] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.language] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.parameters.additionalLexicon] &#39;description&#39; is a required propertyn  [tools.ocrd-cis-profile.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-train] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-train.parameters.gtArchives] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.gtArchives.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.imagePreprocessingSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-train.parameters.training.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-train.parameters.training.required] [&#39;trigrams&#39;, &#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;leFeatures&#39;, &#39;rrFeatures&#39;, &#39;dmFeatures&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-train.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-cis-post-correct] &#39;input_file_grp&#39; is a required propertyn  [tools.ocrd-cis-post-correct.parameters.ocrSteps] Additional properties are not allowed (&#39;items&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.ocrSteps.type] &#39;array&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection] Additional properties are not allowed (&#39;properties&#39; was unexpected)n  [tools.ocrd-cis-post-correct.parameters.postCorrection.type] &#39;object&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-cis-post-correct.parameters.postCorrection.required] [&#39;maxCandidate&#39;, &#39;profiler&#39;, &#39;model&#39;, &#39;runLE&#39;, &#39;runDM&#39;] is not of type &#39;boolean&#39;n  [tools.ocrd-cis-post-correct.steps.0] &#39;postprocessing/alignment&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;cisocrgroup/ocrd_cis&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author-email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_cis&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Florian Fink, Tobias Englmeier, Christoph Weber&quot;, &quot;author_email&quot;=&amp;gt;&quot;finkf@cis.lmu.de, englmeier@cis.lmu.de, web_chris@msn.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/context:python)n[![Total alerts](https://img.shields.io/lgtm/alerts/g/cisocrgroup/ocrd_cis.svg?logo=lgtm&amp;amp;logoWidth=18)](https://lgtm.com/projects/g/cisocrgroup/ocrd_cis/alerts/)n# ocrd_cisnn[CIS](http://www.cis.lmu.de) [OCR-D](http://ocr-d.de) command linentools for the automatic post-correction of OCR-results.nn## Introductionn`ocrd_cis` contains different tools for the automatic post correctionnof OCR-results.  It contains tools for the training, evaluation andnexecution of the post correction.  Most of the tools are following then[OCR-D cli conventions](https://ocr-d.github.io/cli).nnThere is a helper tool to align multiple OCR results as well as anversion of ocropy that works with python3.nn## InstallationnThere are multiple ways to install the `ocrd_cis` tools:n * `make install` uses `pip` to install `ocrd_cis` (see below).n * `make install-devel` uses `pip -e` to install `ocrd_cis` (seen   below).n * `pip install --upgrade pip ocrd_cis_dir`n * `pip install -e --upgrade pip ocrd_cis_dir`nnIt is possible to install `ocrd_cis` in a custom directory usingn`virtualenv`:n```shn python3 -m venv venv-dirn source venv-dir/bin/activaten make install # or any other command to install ocrd_cis (see above)n # use ocrd_cisn deactivaten```nn## UsagenMost tools follow the [OCR-D clinconventions](https://ocr-d.github.io/cli).  They accept then`--input-file-grp`, `--output-file-grp`, `--parameter`, `--mets`,n`--log-level` command line arguments (short and long).  For some toolsn(most notably the alignment tool) expect a comma seperated list ofnmultiple input file groups.nnThe [ocrd-tool.json](ocrd_cis/ocrd-tool.json) contains a schemandescription of the parameter config file for the different tools thatnaccept the `--parameter` argument.nn### ocrd-cis-post-correct.shnThis bash script runs the post correction using a pre-trainedn[model](http://cis.lmu.de/~finkf/model.zip).  If additional supportnOCRs should be used, models for these OCR steps are required and mustnbe configured in an according configuration file (see ocrd-tool.json).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the master-OCR file groupn * `--output-file-grp` name of the post-correction file groupn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-alignnAligns tokens of multiple input file groups to one output file group.nThis tool is used to align the master OCR with any additional supportnOCRs.  It accepts a comma-separated list of input file groups, whichnit aligns in order.nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` comma seperated list of the input file groups;n   first input file group is the master OCRn * `--output-file-grp` name of the file group for the aligned resultn * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-train.shnScript to train a model from a list of ground-truth archives (seenocrd-tool.json) for the post correction.  The tool somewhat mimics thenbehaviour of other ocrd tools:n * `--mets` for the workspacen * `--log-level` is passed to other toolsn * `--parameter` is used as configurationn * `--output-file-grp` defines the output file group for the modelnn### ocrd-cis-datanHelper tool to get the path of the installed data files. Usage:n`ocrd-cis-data [-jar|-3gs]` to get the path of the jar library or thenpath to th default 3-grams language model file.nn### ocrd-cis-wernHelper tool to calculate the word error rate aligned ocr files.  Itnwrites a simple JSON-formated stats file to the given output file group.nnArguments:n * `--input-file-grp` input file group of aligned ocr results withn   their respective ground truth.n * `--output-file-grp` name of the file group for the stats filen * `--log-level` set log leveln * `--mets` path to METS file in workspacenn### ocrd-cis-profilenRun the profiler over the given files of the according the given inputnfile grp and adds a gzipped JSON-formatted profile to the output filengroup of the workspace.  This tools requires an installed [languagenprofiler](https://github.com/cisocrgroup/Profiler).nnArguments:n * `--parameter` path to configuration filen * `--input-file-grp` name of the input file group to profilen * `--output-file-grp` name of the output file group where the profilen   is storedn * `--log-level` set log leveln * `--mets` path to METS file in the workspacenn### ocrd-cis-ocropy-trainnThe ocropy-train tool can be used to train LSTM models.nIt takes ground truth from the workspace and saves (image+text) snippets from the corresponding pages.nThen a model is trained on all snippets for 1 million (or the given number of) randomized iterations from the parameter file.n```shnocrd-cis-ocropy-train n  --input-file-grp OCR-D-GT-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-clipnThe ocropy-clip tool can be used to remove intrusions of neighbouring segments in regions / lines of a workspace.nIt runs a (ad-hoc binarization and) connected component analysis on every text region / line of every PAGE in the input file group, as well as its overlapping neighbours, and for each binary object of conflict, determines whether it belongs to the neighbour, and can therefore be clipped to white. It references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-clip n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-CLIP n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-resegmentnThe ocropy-resegment tool can be used to remove overlap between lines of a workspace.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and for each line already annotated, determines the label of largest extent within the original coordinates (polygon outline) in that line, and annotates the resulting coordinates in the output PAGE.n```shnocrd-cis-ocropy-resegment n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-RES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-segmentnThe ocropy-segment tool can be used to segment regions into lines.nIt runs a (ad-hoc binarization and) line segmentation on every text region of every PAGE in the input file group, and adds a TextLine element with the resulting polygon outline to the annotation of the output PAGE.n```shnocrd-cis-ocropy-segment n  --input-file-grp OCR-D-SEG-BLOCK n  --output-file-grp OCR-D-SEG-LINE n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-deskewnThe ocropy-deskew tool can be used to deskew pages / regions of a workspace.nIt runs the Ocropy thresholding and deskewing estimation on every segment of every PAGE in the input file group and annotates the orientation angle in the output PAGE.n```shnocrd-cis-ocropy-deskew n  --input-file-grp OCR-D-SEG-LINE n  --output-file-grp OCR-D-SEG-LINE-DES n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-denoisenThe ocropy-denoise tool can be used to despeckle pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-denoise n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-DEN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-binarizenThe ocropy-binarize tool can be used to binarize, denoise and deskew pages / regions / lines of a workspace.nIt runs the Ocropy &quot;nlbin&quot; adaptive thresholding, deskewing estimation and denoising on every segment of every PAGE in the input file group and references the resulting segment image files in the output PAGE (as AlternativeImage). (If a deskewing angle has already been annotated in a region, the tool respects that and rotates accordingly.) Images can also be produced grayscale-normalized.n```shnocrd-cis-ocropy-binarize n  --input-file-grp OCR-D-SEG-LINE-DES n  --output-file-grp OCR-D-SEG-LINE-BIN n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-dewarpnThe ocropy-dewarp tool can be used to dewarp text lines of a workspace.nIt runs the Ocropy baseline estimation and dewarping on every line in every text region of every PAGE in the input file group and references the resulting line image files in the output PAGE (as AlternativeImage).n```shnocrd-cis-ocropy-dewarp n  --input-file-grp OCR-D-SEG-LINE-BIN n  --output-file-grp OCR-D-SEG-LINE-DEW n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### ocrd-cis-ocropy-recognizenThe ocropy-recognize tool can be used to recognize lines / words / glyphs from pages of a workspace.nIt runs the Ocropy optical character recognition on every line in every text region of every PAGE in the input file group and adds the resulting text annotation in the output PAGE.n```shnocrd-cis-ocropy-recognize n  --input-file-grp OCR-D-SEG-LINE-DEW n  --output-file-grp OCR-D-OCR-OCRO n  --mets mets.xmln  --parameter file:///path/to/config.jsonn```nn### TesserocrnInstall essential system packages for Tesserocrn```shnsudo apt-get install python3-tk n  tesseract-ocr libtesseract-dev libleptonica-dev n  libimage-exiftool-perl libxml2-utilsn```nnThen install Tesserocr from: https://github.com/OCR-D/ocrd_tesserocrn```shnpip install -r requirements.txtnpip install .n```nnDownload and move tesseract models from:nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Filesnor use your own models andnplace them into: /usr/share/tesseract-ocr/4.00/tessdatann## Workflow configurationnnA decent pipeline might look like this:nn1. page-level croppingn2. page-level binarizationn3. page-level deskewingn4. page-level dewarpingn5. region segmentationn6. region-level clippingn7. region-level deskewingn8. line segmentationn9. line-level clipping or resegmentationn10. line-level dewarpingn11. line-level recognitionn12. line-level alignmentnnIf GT is used, steps 1, 5 and 8 can be omitted. Else if a segmentation is used in 5 and 8 which does not produce overlapping sections, steps 6 and 9 can be omitted.nn## TestingnTo run a few basic tests type `make test` (`ocrd_cis` has to beninstalled in order to run any tests).nn## OCR-D workspacenn* Create a new (empty) workspace: `ocrd workspace init workspace-dir`n* cd into `workspace-dir`n* Add new file to workspace: `ocrd workspace add file -G group -i idn  -m mimetype`nn## OCR-D linksnn- [OCR-D](https://ocr-d.github.io)n- [Github](https://github.com/OCR-D)n- [Project-page](http://www.ocr-d.de/)n- [Ground-truth](http://www.ocr-d.de/sites/all/GTDaten/IndexGT.html)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;MIT&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-cis&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-cis/0.0.7/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;click&quot;, &quot;scipy&quot;, &quot;numpy (&amp;gt;=1.17.0)&quot;, &quot;pillow (&amp;gt;=6.2.0)&quot;, &quot;matplotlib (&amp;gt;3.0.0)&quot;, &quot;python-Levenshtein&quot;, &quot;calamari-ocr (==0.3.5)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;CIS OCR-D command line tools&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.7&quot;}, &quot;last_serial&quot;=&amp;gt;6235442, &quot;releases&quot;=&amp;gt;{&quot;0.0.6&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;sha256&quot;=&amp;gt;&quot;ac2ada13f48b301831e41cba1e9a86b8e10ac2e8f4036ecdda9eb3524e36461c&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;a186d34dad8d16c13d12af2d0b6d889b&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044792, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:33.819139Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/f7/e0/5e3953c9243d05859e679bb83bef9c6f08e10fe0eef736fce90bc42657bc/ocrd_cis-0.0.6-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;sha256&quot;=&amp;gt;&quot;97aea3f172a5eda7272113eb99d55fddda0a96069a20173ea17563d0532bbd55&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.6.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;5c8c3934a2a4fe764c112d8fd12a5ffc&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96645, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-05T19:37:38&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-05T19:37:38.406783Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/8a/a9/1fab502623c41529c13b4ecbedfe224f35843160ddcef4c527a18cfe73b8/ocrd_cis-0.0.6.tar.gz&quot;}], &quot;0.0.7&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;sha256&quot;=&amp;gt;&quot;c3d5898c869ae8c88db28fd52907bcabf1ac0d5cd474f73a30a1ff06615c3dbe&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;539c82850462be8013eb31938e7779cf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;34044484, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:28.430896Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/38/c3/10637d7c51e3d6a0e5e5004476dcf2de093e1e3bec8452e241dcf1fa595c/ocrd_cis-0.0.7-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;sha256&quot;=&amp;gt;&quot;3629b49d32e1626830b6890f6d47793474fcb3232e4b12c43d5d3f38bb33f08d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_cis-0.0.7.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7df03598c04d60203afb00c61ff836da&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;96590, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-02T15:30:33&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-02T15:30:33.037095Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/b8/cb/3fdc4daee6b85b732913c012cf41cafaab708b367c3fd5883d0d8e99c1b1/ocrd_cis-0.0.7.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/cisocrgroup/ocrd_cis&quot;}         ocrd_anybaseocr    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;FROM ocrd/corenMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-layouterkennungnCOPY setup.py .nCOPY requirements.txt .nCOPY README.md .nCOPY ocrd_anybaseocr ./ocrd_anybaseocrnRUN pip3 install .n&quot;, &quot;README.md&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.n n #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.n n #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)n n #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.n n #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.n n #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.n n #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.n n #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.n n #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/mjenckel/LAYoutERkennung/&quot;,n  &quot;version&quot;: &quot;0.0.1&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-anybaseocr-binarize&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-binarize&quot;,n      &quot;description&quot;: &quot;Binarize images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;parameters&quot;: {n        &quot;nocheck&quot;:         {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;disable error checking on inputs&quot;},n        &quot;show&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;display final results&quot;},n        &quot;raw_copy&quot;:        {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;also copy the raw image&quot;},n        &quot;gray&quot;:            {&quot;type&quot;: &quot;boolean&quot;,                     &quot;default&quot;: false, &quot;description&quot;: &quot;force grayscale processing even if image seems binary&quot;},n        &quot;bignore&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1,   &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;debug&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,     &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;escale&quot;:          {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0,   &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;hi&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 90,    &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;lo&quot;:              {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 5,     &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;perc&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 80,    &quot;description&quot;: &quot;percentage for filters&quot;},n        &quot;range&quot;:           {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 20,    &quot;description&quot;: &quot;range for filters&quot;},n        &quot;threshold&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;zoom&quot;:            {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5,   &quot;description&quot;: &quot;zoom for page background estimation, smaller=faster&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-deskew&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-deskew&quot;,n      &quot;description&quot;: &quot;Deskew images with the algorithm from ocropy&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-BIN&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;parameters&quot;: {n        &quot;escale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;scale for estimating a mask over the text region&quot;},n        &quot;bignore&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.1, &quot;description&quot;: &quot;ignore this much of the border for threshold estimation&quot;},n        &quot;threshold&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 0.5, &quot;description&quot;: &quot;threshold, determines lightness&quot;},n        &quot;maxskew&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;,   &quot;default&quot;: 1.0, &quot;description&quot;: &quot;skew angle estimation parameters (degrees)&quot;},n        &quot;skewsteps&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8,   &quot;description&quot;: &quot;steps for skew angle estimation (per degree)&quot;},n        &quot;debug&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;display intermediate results&quot;},n        &quot;parallel&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,   &quot;description&quot;: &quot;???&quot;},n        &quot;lo&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 5,   &quot;description&quot;: &quot;percentile for black estimation&quot;},n        &quot;hi&quot;:        {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 90,   &quot;description&quot;: &quot;percentile for white estimation&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-crop&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-crop&quot;,n      &quot;description&quot;: &quot;Image crop using non-linear processing&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-DESKEW&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;parameters&quot;: {n        &quot;colSeparator&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.04, &quot;description&quot;: &quot;consider space between column. 25% of width&quot;},n        &quot;maxRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Consider maximum rular area&quot;},n        &quot;minArea&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.05, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;minRularArea&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Consider minimum rular area&quot;},n        &quot;positionBelow&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.75, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;positionLeft&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.4, &quot;description&quot;: &quot;rular position in left&quot;},n        &quot;positionRight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.6, &quot;description&quot;: &quot;rular position in right&quot;},n        &quot;rularRatioMax&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 10.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularRatioMin&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 3.0, &quot;description&quot;: &quot;rular position in below&quot;},n        &quot;rularWidth&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.95, &quot;description&quot;: &quot;maximum rular width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-dewarp&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-dewarp&quot;,n      &quot;description&quot;: &quot;dewarp image with anyBaseOCR&quot;,n      &quot;categories&quot;: [&quot;Image preprocessing&quot;],n      &quot;steps&quot;: [&quot;preprocessing/optimization/dewarping&quot;],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-IMG-DEWARP&quot;],n      &quot;parameters&quot;: {n        &quot;imgresize&quot;:    { &quot;type&quot;: &quot;string&quot;,                      &quot;default&quot;: &quot;resize_and_crop&quot;, &quot;description&quot;: &quot;run on original size image&quot;},n        &quot;pix2pixHD&quot;:    { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;: &quot;Path to pix2pixHD library&quot;},n        &quot;model_name&quot;:t{ &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models&quot;, &quot;description&quot;: &quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;},n        &quot;checkpoint_dir&quot;:   { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;./&quot;, &quot;description&quot;: &quot;Path to where to look for dir with model name&quot;},n        &quot;gpu_id&quot;:       { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0,    &quot;description&quot;: &quot;gpu id&quot;},n        &quot;resizeHeight&quot;: { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image height&quot;},n        &quot;resizeWidth&quot;:  { &quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 1024, &quot;description&quot;: &quot;resized image width&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-tiseg&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-tiseg&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;separate text and non-text part with anyBaseOCR&quot;,n      &quot;parameters&quot;: {n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-textline&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-textline&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-SEG-TISEG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LINE-ANY&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],n      &quot;description&quot;: &quot;separate each text line&quot;,n      &quot;parameters&quot;: {n        &quot;minscale&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 12.0, &quot;description&quot;: &quot;minimum scale permitted&quot;},n        &quot;maxlines&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 300, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;scale&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.0, &quot;description&quot;: &quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;},n        &quot;hscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.0, &quot;description&quot;: &quot;non-standard scaling of horizontal parameters&quot;},n        &quot;vscale&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.7, &quot;description&quot;: &quot;non-standard scaling of vertical parameters&quot;},n        &quot;threshold&quot;:   {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 0.2, &quot;description&quot;: &quot;baseline threshold&quot;},n        &quot;noise&quot;:       {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 8, &quot;description&quot;: &quot;noise threshold for removing small components from lines&quot;},n        &quot;usegauss&quot;:    {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;use gaussian instead of uniform&quot;},n        &quot;maxseps&quot;:     {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum black column separators&quot;},n        &quot;sepwiden&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 10, &quot;description&quot;: &quot;widen black separators (to account for warping)&quot;},n        &quot;blackseps&quot;:   {&quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;description&quot;: &quot;also check for black column separators&quot;},n        &quot;maxcolseps&quot;:  {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 2, &quot;description&quot;: &quot;maximum # whitespace column separators&quot;},n        &quot;csminaspect&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 1.1, &quot;description&quot;: &quot;minimum aspect ratio for column separators&quot;},n        &quot;csminheight&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;float&quot;, &quot;default&quot;: 6.5, &quot;description&quot;: &quot;minimum column height (units=scale)&quot;},n        &quot;pad&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;padding for extracted lines&quot;},n        &quot;expand&quot;:      {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 3, &quot;description&quot;: &quot;expand mask for grayscale extraction&quot;},n        &quot;parallel&quot;:    {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 0, &quot;description&quot;: &quot;number of CPUs to use&quot;},n        &quot;libpath&quot;:     {&quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;.&quot;, &quot;description&quot;: &quot;Library Path for C Executables&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }n    },n    &quot;ocrd-anybaseocr-layout-analysis&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-layout-analysis&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG-CROP&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-LAYOUT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {n        &quot;batch_size&quot;:         {&quot;type&quot;: &quot;number&quot;, &quot;format&quot;: &quot;integer&quot;, &quot;default&quot;: 4, &quot;description&quot;: &quot;Batch size for generating test images&quot;},n        &quot;model_path&quot;:         { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/structure_analysis.h5&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classification Model&quot;},n        &quot;class_mapping_path&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;:&quot;models/mapping_DenseNet.pickle&quot;,&quot;required&quot;: false, &quot;description&quot;: &quot;Path to Layout Structure Classes&quot;}n      }n    },n    &quot;ocrd-anybaseocr-block-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-anybaseocr-block-segmentation&quot;,n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;output_file_grp&quot;: [&quot;OCR-D-BLOCK-SEGMENT&quot;],n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;steps&quot;: [&quot;layout/segmentation/text-image&quot;],n      &quot;description&quot;: &quot;Analysis of the input document&quot;,n      &quot;parameters&quot;: {        n        &quot;block_segmentation_model&quot;:   { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/&quot;, &quot;required&quot;: false, &quot;description&quot;: &quot;Path to block segmentation Model&quot;},n        &quot;block_segmentation_weights&quot;: { &quot;type&quot;: &quot;string&quot;,&quot;default&quot;:&quot;mrcnn/block_segmentation_weights.h5&quot;,  &quot;required&quot;: false, &quot;description&quot;: &quot;Path to model weights&quot;},n        &quot;operation_level&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;, &quot;line&quot;], &quot;default&quot;: &quot;page&quot;,&quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;}n      }       n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd-anybaseocr&#39;,n    version=&#39;0.0.1&#39;,n    author=&quot;DFKI&quot;,n    author_email=&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;,n    url=&quot;https://github.com/mjenckel/LAYoutERkennung&quot;,n    license=&#39;Apache License 2.0&#39;,n    long_description=open(&#39;README.md&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    packages=find_packages(exclude=[&quot;work_dir&quot;, &quot;src&quot;]),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;]n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-anybaseocr-binarize           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_binarize&#39;,n            &#39;ocrd-anybaseocr-deskew             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_deskew&#39;,n            &#39;ocrd-anybaseocr-crop               = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_cropping&#39;,        n            &#39;ocrd-anybaseocr-dewarp             = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_dewarp&#39;,n            &#39;ocrd-anybaseocr-tiseg              = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_tiseg&#39;,n            &#39;ocrd-anybaseocr-textline           = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_textline&#39;,n            &#39;ocrd-anybaseocr-layout-analysis    = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_layout_analysis&#39;,n            &#39;ocrd-anybaseocr-block-segmentation = ocrd_anybaseocr.cli.cli:ocrd_anybaseocr_block_segmentation&#39;n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Dec 17 13:28:07 2019 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;111&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_anybaseocr&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung/&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-anybaseocr-binarize&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Binarize images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-binarize&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;gray&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;force grayscale processing even if image seems binary&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;nocheck&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;disable error checking on inputs&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;perc&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;80, &quot;description&quot;=&amp;gt;&quot;percentage for filters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;range&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;20, &quot;description&quot;=&amp;gt;&quot;range for filters&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;raw_copy&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also copy the raw image&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;show&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;display final results&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;zoom&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;zoom for page background estimation, smaller=faster&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/binarization&quot;]}, &quot;ocrd-anybaseocr-block-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-block-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-BLOCK-SEGMENT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;block_segmentation_model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/&quot;, &quot;description&quot;=&amp;gt;&quot;Path to block segmentation Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;block_segmentation_weights&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;mrcnn/block_segmentation_weights.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to model weights&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-crop&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Image crop using non-linear processing&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-crop&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;colSeparator&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.04, &quot;description&quot;=&amp;gt;&quot;consider space between column. 25% of width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.3, &quot;description&quot;=&amp;gt;&quot;Consider maximum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.05, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minRularArea&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.01, &quot;description&quot;=&amp;gt;&quot;Consider minimum rular area&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;positionBelow&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.75, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionLeft&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.4, &quot;description&quot;=&amp;gt;&quot;rular position in left&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;positionRight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.6, &quot;description&quot;=&amp;gt;&quot;rular position in right&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMax&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularRatioMin&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3.0, &quot;description&quot;=&amp;gt;&quot;rular position in below&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;rularWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.95, &quot;description&quot;=&amp;gt;&quot;maximum rular width&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/cropping&quot;]}, &quot;ocrd-anybaseocr-deskew&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;Deskew images with the algorithm from ocropy&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-deskew&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DESKEW&quot;], &quot;parameters&quot;=&amp;gt;{&quot;bignore&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.1, &quot;description&quot;=&amp;gt;&quot;ignore this much of the border for threshold estimation&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;debug&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;display intermediate results&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;escale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;scale for estimating a mask over the text region&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hi&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;90, &quot;description&quot;=&amp;gt;&quot;percentile for white estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;lo&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;5, &quot;description&quot;=&amp;gt;&quot;percentile for black estimation&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxskew&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;skew angle estimation parameters (degrees)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;???&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;skewsteps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;steps for skew angle estimation (per degree)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.5, &quot;description&quot;=&amp;gt;&quot;threshold, determines lightness&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/deskewing&quot;]}, &quot;ocrd-anybaseocr-dewarp&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Image preprocessing&quot;], &quot;description&quot;=&amp;gt;&quot;dewarp image with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-dewarp&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-DEWARP&quot;], &quot;parameters&quot;=&amp;gt;{&quot;checkpoint_dir&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;./&quot;, &quot;description&quot;=&amp;gt;&quot;Path to where to look for dir with model name&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;gpu_id&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;gpu id&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;imgresize&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;resize_and_crop&quot;, &quot;description&quot;=&amp;gt;&quot;run on original size image&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_name&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models&quot;, &quot;description&quot;=&amp;gt;&quot;name of dir with trained pix2pixHD model (latest_net_G.pth)&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pix2pixHD&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;/home/ahmed/project/pix2pixHD&quot;, &quot;description&quot;=&amp;gt;&quot;Path to pix2pixHD library&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;resizeHeight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image height&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;resizeWidth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1024, &quot;description&quot;=&amp;gt;&quot;resized image width&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;preprocessing/optimization/dewarping&quot;]}, &quot;ocrd-anybaseocr-layout-analysis&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Analysis of the input document&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-layout-analysis&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LAYOUT&quot;], &quot;parameters&quot;=&amp;gt;{&quot;batch_size&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;4, &quot;description&quot;=&amp;gt;&quot;Batch size for generating test images&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;class_mapping_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/mapping_DenseNet.pickle&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classes&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;model_path&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;models/structure_analysis.h5&quot;, &quot;description&quot;=&amp;gt;&quot;Path to Layout Structure Classification Model&quot;, &quot;required&quot;=&amp;gt;false, &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}, &quot;ocrd-anybaseocr-textline&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate each text line&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-textline&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-LINE-ANY&quot;], &quot;parameters&quot;=&amp;gt;{&quot;blackseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;also check for black column separators&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;csminaspect&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.1, &quot;description&quot;=&amp;gt;&quot;minimum aspect ratio for column separators&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;csminheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;6.5, &quot;description&quot;=&amp;gt;&quot;minimum column height (units=scale)&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;expand&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;expand mask for grayscale extraction&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;hscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.0, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;libpath&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;.&quot;, &quot;description&quot;=&amp;gt;&quot;Library Path for C Executables&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;maxcolseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum # whitespace column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxlines&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of horizontal parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;maxseps&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;2, &quot;description&quot;=&amp;gt;&quot;maximum black column separators&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;minscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;12.0, &quot;description&quot;=&amp;gt;&quot;minimum scale permitted&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;noise&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;noise threshold for removing small components from lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;pad&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;3, &quot;description&quot;=&amp;gt;&quot;padding for extracted lines&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;parallel&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0, &quot;description&quot;=&amp;gt;&quot;number of CPUs to use&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;scale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.0, &quot;description&quot;=&amp;gt;&quot;the basic scale of the document (roughly, xheight) 0=automatic&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;sepwiden&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;10, &quot;description&quot;=&amp;gt;&quot;widen black separators (to account for warping)&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;threshold&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;0.2, &quot;description&quot;=&amp;gt;&quot;baseline threshold&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}, &quot;usegauss&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;use gaussian instead of uniform&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;vscale&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;1.7, &quot;description&quot;=&amp;gt;&quot;non-standard scaling of vertical parameters&quot;, &quot;format&quot;=&amp;gt;&quot;float&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/line&quot;]}, &quot;ocrd-anybaseocr-tiseg&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;separate text and non-text part with anyBaseOCR&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-anybaseocr-tiseg&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-CROP&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-TISEG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;operation_level&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;page&quot;, &quot;description&quot;=&amp;gt;&quot;PAGE XML hierarchy level to operate on&quot;, &quot;enum&quot;=&amp;gt;[&quot;page&quot;, &quot;region&quot;, &quot;line&quot;], &quot;type&quot;=&amp;gt;&quot;string&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/text-image&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-anybaseocr-tiseg.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-layout-analysis.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n  [tools.ocrd-anybaseocr-block-segmentation.steps.0] &#39;layout/segmentation/text-image&#39; is not one of [&#39;preprocessing/characterization&#39;, &#39;preprocessing/optimization&#39;, &#39;preprocessing/optimization/cropping&#39;, &#39;preprocessing/optimization/deskewing&#39;, &#39;preprocessing/optimization/despeckling&#39;, &#39;preprocessing/optimization/dewarping&#39;, &#39;preprocessing/optimization/binarization&#39;, &#39;preprocessing/optimization/grayscale_normalization&#39;, &#39;recognition/text-recognition&#39;, &#39;recognition/font-identification&#39;, &#39;recognition/post-correction&#39;, &#39;layout/segmentation&#39;, &#39;layout/segmentation/text-nontext&#39;, &#39;layout/segmentation/region&#39;, &#39;layout/segmentation/line&#39;, &#39;layout/segmentation/word&#39;, &#39;layout/segmentation/classification&#39;, &#39;layout/analysis&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_anybaseocr&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author-email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;DFKI&quot;, &quot;author_email&quot;=&amp;gt;&quot;Saqib.Bukhari@dfki.de, Mohammad_mohsin.reza@dfki.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# Document Preprocessing and Segmentationnn[![CircleCI](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung.svg?style=svg)](https://circleci.com/gh/mjenckel/OCR-D-LAYoutERkennung)nn&amp;gt; Tools for preprocessing scanned images for OCRnn# InstallingnnTo install anyBaseOCR dependencies system-wide:nn    $ sudo pip install .nnAlternatively, dependencies can be installed into a Virtual Environment:nn    $ virtualenv venvn    $ source venv/bin/activaten    $ pip install -e .nn#Toolsnn## Binarizernn### Method Behaviour n This function takes a scanned colored /gray scale document image as input and do the black and white binarize image.nn #### Usage:n```shnocrd-anybaseocr-binarize -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-binarize n   -m mets.xml n   -I OCR-D-IMG n   -O OCR-D-PAGE-BINn```nn## Deskewernn### Method Behaviour n This function takes a document image as input and do the skew correction of that document.nn #### Usage:n```shnocrd-anybaseocr-deskew -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-deskew n  -m mets.xml n  -I OCR-D-PAGE-BIN n  -O OCR-D-PAGE-DESKEWn```nn## Croppernn### Method Behaviour n This function takes a document image as input and crops/selects the page content area only (that&#39;s mean remove textual noise as well as any other noise around page content area)nn #### Usage:n```shnocrd-anybaseocr-crop -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-crop n   -m mets.xml n   -I OCR-D-PAGE-DESKEW n   -O OCR-D-PAGE-CROPn```nnn## Dewarpernn### Method Behaviour n This function takes a document image as input and make the text line straight if its curved.nn #### Usage:n```shnocrd-anybaseocr-dewarp -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nnn#### Example: n```shnCUDA_VISIBLE_DEVICES=0 ocrd-anybaseocr-dewarp n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-DEWARPn```nn## Text/Non-Text Segmenternn### Method Behaviour n This function takes a document image as an input and separates the text and non-text part from the input document image.nn #### Usage:n```shnocrd-anybaseocr-tiseg -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-tiseg n   -m mets.xml n   -I OCR-D-PAGE-CROP n   -O OCR-D-PAGE-TISEGn```nn## Textline Segmenternn### Method Behaviour n This function takes a cropped document image as an input and segment the image into textline images.nn #### Usage:n```shnocrd-anybaseocr-textline -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-textline n   -m mets.xml n   -I OCR-D-PAGE-TISEG n   -O OCR-D-PAGE-TLn```nn## Block Segmenternn### Method Behaviour n This function takes raw document image as an input and segments the image into the different text blocks.nn #### Usage:n```shnocrd-anybaseocr-block-segmenter -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-block-segmenter n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nn## Document Analysernn### Method Behaviour n This function takes all the cropped document images of a single book and its corresponding text regions as input and generates the logical structure on the book level.nn #### Usage:n```shnocrd-anybaseocr-layout-analysis -m (path to METs input file) -I (Input group name) -O (Output group name) [-p (path to parameter file) -o (METs output filename)]n```nn#### Example: n```shnocrd-anybaseocr-layout-analysis n   -m mets.xml n   -I OCR-IMG n   -O OCR-D-PAGE-BLOCKn```nnn## TestingnnTo test the tools, download [OCR-D/assets](https://github.com/OCR-D/assets). Innparticular, the code is tested with then[dfki-testdata](https://github.com/OCR-D/assets/tree/master/data/dfki-testdata)ndataset.nnRun `make test` to run all tests.nn## Licensennn```n Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);n you may not use this file except in compliance with the License.n You may obtain a copy of the License atnn     http://www.apache.org/licenses/LICENSE-2.0nn Unless required by applicable law or agreed to in writing, softwaren distributed under the License is distributed on an &quot;AS IS&quot; BASIS,n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.n See the License for the specific language governing permissions andn limitations under the License.n ```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-anybaseocr&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-anybaseocr/0.0.1/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0)&quot;, &quot;opencv-python-headless (&amp;gt;=3.4)&quot;, &quot;ocrd-fork-ocropy (&amp;gt;=1.4.0a3)&quot;, &quot;ocrd-fork-pylsd (&amp;gt;=0.0.3)&quot;, &quot;setuptools (&amp;gt;=41.0.0)&quot;, &quot;torch (&amp;gt;=1.1.0)&quot;, &quot;torchvision&quot;, &quot;pandas&quot;, &quot;keras&quot;, &quot;tensorflow-gpu (==1.14.0)&quot;, &quot;scikit-image&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.1&quot;}, &quot;last_serial&quot;=&amp;gt;6317222, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;sha256&quot;=&amp;gt;&quot;021a114defc9702fa99988308277cab92bad1a95a8472395b8e38fde23569dc6&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;e70acb5331cd2daece04bc114622ec39&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;95755, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:51&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:51.079869Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d6/2c/9417ad5fb850c2eb52a86e822f64741d4df65831580104a68196e0c5cbcf/ocrd_anybaseocr-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;sha256&quot;=&amp;gt;&quot;077b3f59f09f1e315aee5fafbeef8184706d45c0a5863224f5ebef941b682281&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_anybaseocr-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;84203839fe06916bc281097251eba50f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;77823, &quot;upload_time&quot;=&amp;gt;&quot;2019-12-17T13:15:54&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-12-17T13:15:54.116419Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/15/cf/fc744aa2323538a7a980a44af16d86ab68feba42f78ba6069763e9ed125d/ocrd_anybaseocr-0.0.1.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/mjenckel/LAYoutERkennung&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_anybaseocr&quot;}         ocrd_pc_segmentation    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;&quot;# WORK IN PROGRESS - NOT READYnFROM ocrd/corenVOLUME [&quot;/data&quot;]nMAINTAINER OCR-DnENV DEBIAN_FRONTEND noninteractivenENV PYTHONIOENCODING utf8nnWORKDIR /build-ocrdnCOPY setup.py .nCOPY README.md .nCOPY requirements.txt .n#COPY requirements_test.txt .nCOPY ocrd_pc_segmentation ./ocrd_pc_segmentationnCOPY Makefile .nRUN apt-get update &amp;amp;&amp;amp; n    apt-get -y install --no-install-recommends n        build-essential n    &amp;amp;&amp;amp; make deps install n    &amp;amp;&amp;amp; apt-get -y remove --auto-remove build-essentialn&quot;, &quot;README.md&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depsn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;,n  &quot;version&quot;: &quot;0.1.0&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-pixelclassifier-segmentation&quot;: {n      &quot;executable&quot;: &quot;ocrd-pc-segmentation&quot;,n      &quot;categories&quot;: [&quot;Layout analysis&quot;],n      &quot;description&quot;: &quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-IMG-BIN&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-SEG-BLOCK&quot;n      ],n      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],n      &quot;parameters&quot;: {n        &quot;overwrite_regions&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: true,n          &quot;description&quot;: &quot;remove existing layout and text annotation below the Page level&quot;n        },n        &quot;xheight&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;description&quot;: &quot;height of character x in pixels used during training&quot;,n          &quot;default&quot;: 8n        },n        &quot;model&quot;:  {n          &quot;type&quot;: &quot;string&quot;,n          &quot;description&quot;: &quot;trained model for pixel classifier&quot;,n          &quot;default&quot;: &quot;__DEFAULT__&quot;n        },n        &quot;gpu_allow_growth&quot;: {n          &quot;type&quot;: &quot;boolean&quot;,n          &quot;default&quot;: false,n          &quot;description&quot;: &quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;nn        },n        &quot;resize_height&quot;: {n          &quot;type&quot;: &quot;integer&quot;,n          &quot;default&quot;: 300,n          &quot;description&quot;: &quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;n        }nn      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnsetup(n    name=&#39;ocrd_pc_segmentation&#39;,n    version=&#39;0.1.3&#39;,n    description=&#39;pixel-classifier based page segmentation&#39;,n    long_description=codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Alexander Gehrke, Christian Reul, Christoph Wick&#39;,n    author_email=&#39;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&#39;,n    url=&#39;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    install_requires=open(&quot;requirements.txt&quot;).read().split(),n    extras_require={n        &#39;tf_cpu&#39;: [&#39;ocr4all_pixel_classifier[tf_cpu]&amp;gt;=0.0.1&#39;],n        &#39;tf_gpu&#39;: [&#39;ocr4all_pixel_classifier[tf_gpu]&amp;gt;=0.0.1&#39;],n    },n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.yml&#39;, &#39;*.yaml&#39;],n    },n    classifiers=[n        &quot;Programming Language :: Python :: 3&quot;,n        &quot;License :: OSI Approved :: Apache Software License&quot;,n        &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;nn    ],n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;ocrd-pc-segmentation=ocrd_pc_segmentation.cli:ocrd_pc_segmentation&#39;,n        ]n    },n    data_files=[(&#39;&#39;, [&quot;requirements.txt&quot;])],n    include_package_data=True,n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Mon Jan 20 10:00:24 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.1.3&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;29&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd-pixelclassifier-segmentation.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-pixelclassifier-segmentation&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Layout analysis&quot;], &quot;description&quot;=&amp;gt;&quot;Segment page into regions using a pixel classifier based on a Fully Convolutional Network (FCN)&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG-BIN&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-SEG-BLOCK&quot;], &quot;parameters&quot;=&amp;gt;{&quot;gpu_allow_growth&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;false, &quot;description&quot;=&amp;gt;&quot;required for GPU use with some graphic cards (set to true, if you get CUDNN_INTERNAL_ERROR)&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;model&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;&quot;__DEFAULT__&quot;, &quot;description&quot;=&amp;gt;&quot;trained model for pixel classifier&quot;, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;overwrite_regions&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;true, &quot;description&quot;=&amp;gt;&quot;remove existing layout and text annotation below the Page level&quot;, &quot;type&quot;=&amp;gt;&quot;boolean&quot;}, &quot;resize_height&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;300, &quot;description&quot;=&amp;gt;&quot;scale down pixelclassifier output to this height for postprocessing (performance/quality tradeoff). Independent of training.&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}, &quot;xheight&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;8, &quot;description&quot;=&amp;gt;&quot;height of character x in pixels used during training&quot;, &quot;type&quot;=&amp;gt;&quot;integer&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;layout/segmentation/region&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.1.0&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [tools.ocrd-pixelclassifier-segmentation.parameters.xheight.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n  [tools.ocrd-pixelclassifier-segmentation.parameters.resize_height.type] &#39;integer&#39; is not one of [&#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;]n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author-email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_pc_segmentation&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Alexander Gehrke, Christian Reul, Christoph Wick&quot;, &quot;author_email&quot;=&amp;gt;&quot;alexander.gehrke@uni-wuerzburg.de, christian.reul@uni-wuerzburg.de, christoph.wick@uni-wuerzburg.de&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[&quot;License :: OSI Approved :: Apache Software License&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Topic :: Scientific/Engineering :: Image Recognition&quot;], &quot;description&quot;=&amp;gt;&quot;# page-segmentation module for OCRdnn## IntroductionnnThis module implements a page segmentation algorithm based on a FullynConvolutional Network (FCN). The FCN creates a classification for each pixel inna binary image. This result is then segmented per class using XY cuts.nn## Requirementsnn- For GPU-Support: [CUDA](https://developer.nvidia.com/cuda-downloads) andn  [CUDNN](https://developer.nvidia.com/cudnn)n- other requirements are installed via Makefile / pip, see `requirements.txt`n  in repository root.nn## InstallationnnIf you want to use GPU support, set the environment variable `TENSORFLOW_GPU`,notherwise leave it unset. Then:nn```bashnmake depn```nnto install dependencies andnn```shnmake installn```nnto install the package.nnBoth are python packages installed via pip, so you may want to activatena virtalenv before installing.nn## Usagenn`ocrd-pc-segmentation` follows the [ocrd CLI](https://ocr-d.github.io/cli).nnIt expects a binary page image and produces region entries in the PageXML file.nn## ConfigurationnnThe following parameters are recognized in the JSON parameter file:nn- `overwrite_regions`: remove previously existing text regionsn- `xheight`: height of character &quot;x&quot; in pixels used during training.n- `model`: pixel-classifier model pathn- `gpu_allow_growth`: required for GPU use with some graphic cardsn  (set to true, if you get CUDNN_INTERNAL_ERROR)n- `resize_height`: scale down pixelclassifier output to this height before postprocessing. Independent of training / used model.n  (performance / quality tradeoff, defaults to 300)nn## TestingnnThere is a simple CLI test, that will run the tool on a single image from the assets repository.nn`make test-cli`nn## TrainingnnTo train models for the pixel classifier, see [its README](https://github.com/ocr-d-modul-2-segmentierung/page-segmentation/blob/master/README.md)nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-pc-segmentation&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-pc-segmentation/0.1.3/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.0a1)&quot;, &quot;click&quot;, &quot;ocr4all-pixel-classifier (&amp;gt;=0.1.3)&quot;, &quot;numpy&quot;, &quot;ocr4all-pixel-classifier[tf_cpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_cpu&#39;&quot;, &quot;ocr4all-pixel-classifier[tf_gpu] (&amp;gt;=0.0.1) ; extra == &#39;tf_gpu&#39;&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;pixel-classifier based page segmentation&quot;, &quot;version&quot;=&amp;gt;&quot;0.1.3&quot;}, &quot;last_serial&quot;=&amp;gt;6169845, &quot;releases&quot;=&amp;gt;{&quot;0.1.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;sha256&quot;=&amp;gt;&quot;c22e9fad55a01f29bea78943c8ac93bc1a0780cbc6b606cbf81bac5f888d2294&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;7cd68c8c55c0110fbfb6de61877fd60e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559195, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:31&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:31.585016Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/72/f6/5936ad2bdc878920ae26b448bd68eb580f04632b373d5fba62c79a8c8148/ocrd_pc_segmentation-0.1.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;sha256&quot;=&amp;gt;&quot;9f908f54f86d85a10b5d1d339e9f964f1b2ade3b4032ee8dadeeaa474dc299b7&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;8469f2af2217a526828000b4af13f7f0&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547673, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-18T12:31:39&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-18T12:31:39.690671Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/d9/82/c3fee56b73554529fe319dd596df56758e5429b1d5ee4b8603d404f7c94e/ocrd_pc_segmentation-0.1.1.tar.gz&quot;}], &quot;0.1.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;sha256&quot;=&amp;gt;&quot;026be378afb3104e0f2367254da1da0f3ba212f5d4d5c8f6a7880b4eddc5b9a5&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;aceed390bfeffbaf723ca96961ed5d7f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559196, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:53:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:53:55.306178Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/cc/d6/396ad6297c509445f03fddedc5efcd6f882ce5bb223c050157d675574858/ocrd_pc_segmentation-0.1.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;sha256&quot;=&amp;gt;&quot;e2dcd0b641accb8c6594d6dd24dcf1899c3cefbc033c5860b4ff72c20f1ad4ca&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;c982401d1a8ab607bf6ed1871df87826&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547671, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-19T13:54:12&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-19T13:54:12.122137Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/0c/47/46c39455cc4c5739e4599f7715c4b618193b561885aa302777fd7b11c1b5/ocrd_pc_segmentation-0.1.2.tar.gz&quot;}], &quot;0.1.3&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;sha256&quot;=&amp;gt;&quot;30442df84ae140871ed32549d7f0e5472f02783614bd4b627bceafdd540ca266&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;6f80c4823630b6a94f3b013ec6eab69e&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7559081, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:32&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:32.354512Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/45/9c/3d1dc9c772ea9446f372837318f1e55b76c6a2cb1368579592c6b3fe9326/ocrd_pc_segmentation-0.1.3-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;sha256&quot;=&amp;gt;&quot;b58ab36e89213735fcf0b9376ce97e342626fbf8892d302c5feb3dbd5b1c73a3&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_pc_segmentation-0.1.3.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;16b1c95e3235cf1d9f2b971bc4684daf&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;7547532, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-20T16:45:36&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-20T16:45:36.370537Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/3a/66/bad782febb7496d089df1520d08a241af4875d6d656e68d93cfaa4fa6cf2/ocrd_pc_segmentation-0.1.3.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/segmentation-runner&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/ocr-d-modul-2-segmentierung/ocrd_pc_segmentation&quot;}         dinglehopper    {&quot;compliant_cli&quot;=&amp;gt;false, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;dinglehoppern============nndinglehopper is an OCR evaluation tool and reads [ALTO](https://github.com/altoxml), [PAGE](https://github.com/PRImA-Research-Lab/PAGE-XML) and text files.nn[![Build Status](https://travis-ci.org/qurator-spk/dinglehopper.svg?branch=master)](https://travis-ci.org/qurator-spk/dinglehopper)nnGoalsn-----n* Usefuln  * As a UI tooln  * For an automated evaluationn  * As a libraryn* Unicode supportnnInstallationn------------nIt&#39;s best to use pip, e.g.:n~~~nsudo pip install .n~~~nnUsagen-----n~~~ndinglehopper some-document.gt.page.xml some-document.ocr.alto.xmln~~~nThis generates `report.html` and `report.json`.nnnAs a OCR-D processor:n~~~nocrd-dinglehopper -m mets.xml -I OCR-D-GT-PAGE,OCR-D-OCR-TESS -O OCR-D-OCR-TESS-EVALn~~~nThis generates HTML and JSON reports in the `OCR-D-OCR-TESS-EVAL` filegroup.nnn![dinglehopper displaying metrics and character differences](.screenshots/dinglehopper.png?raw=true)nnTestingn-------nUse `pytest` to run the tests in [the tests directory](qurator/dinglehopper/tests):n~~~nvirtualenv -p /usr/bin/python3 venvn. venv/bin/activatenpip install -r requirements.txtnpip install pytestnpytestn~~~n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;git_url&quot;: &quot;https://github.com/qurator-spk/dinglehopper&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-dinglehopper&quot;: {n      &quot;executable&quot;: &quot;ocrd-dinglehopper&quot;,n      &quot;description&quot;: &quot;Evaluate OCR text against ground truth with dinglehopper&quot;,n      &quot;input_file_grp&quot;: [n        &quot;OCR-D-GT-PAGE&quot;,n        &quot;OCR-D-OCR&quot;n      ],n      &quot;output_file_grp&quot;: [n        &quot;OCR-D-OCR-EVAL&quot;n      ],n      &quot;categories&quot;: [n        &quot;Quality assurance&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/text-recognition&quot;n      ]n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;from io import opennfrom setuptools import find_packages, setupnnwith open(&#39;requirements.txt&#39;) as fp:n    install_requires = fp.read()nnsetup(n    name=&#39;dinglehopper&#39;,n    author=&#39;Mike Gerber, The QURATOR SPK Team&#39;,n    author_email=&#39;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&#39;,n    description=&#39;The OCR evaluation tool&#39;,n    long_description=open(&#39;README.md&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read(),n    long_description_content_type=&#39;text/markdown&#39;,n    keywords=&#39;qurator ocr&#39;,n    license=&#39;Apache&#39;,n    namespace_packages=[&#39;qurator&#39;],n    packages=find_packages(exclude=[&#39;*.tests&#39;, &#39;*.tests.*&#39;, &#39;tests.*&#39;, &#39;tests&#39;]),n    install_requires=install_requires,n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;templates/*&#39;],n    },n    entry_points={n      &#39;console_scripts&#39;: [n        &#39;dinglehopper=qurator.dinglehopper.cli:main&#39;,n        &#39;ocrd-dinglehopper=qurator.dinglehopper.ocrd_cli:ocrd_dinglehopper&#39;,n      ]n    }n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Tue Jan 14 13:22:42 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;56&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper.git&quot;}, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-dinglehopper&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Quality assurance&quot;], &quot;description&quot;=&amp;gt;&quot;Evaluate OCR text against ground truth with dinglehopper&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-dinglehopper&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-GT-PAGE&quot;, &quot;OCR-D-OCR&quot;], &quot;output_file_grp&quot;=&amp;gt;[&quot;OCR-D-OCR-EVAL&quot;], &quot;steps&quot;=&amp;gt;[&quot;recognition/text-recognition&quot;]}}}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;false&quot;&amp;gt;n  [] &#39;version&#39; is a required propertyn&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;false, &quot;org_plus_name&quot;=&amp;gt;&quot;qurator-spk/dinglehopper&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Mike Gerber, The QURATOR SPK Team&quot;, &quot;author-email&quot;=&amp;gt;&quot;mike.gerber@sbb.spk-berlin.de, qurator@sbb.spk-berlin.de&quot;, &quot;name&quot;=&amp;gt;&quot;dinglehopper&quot;, &quot;pypi&quot;=&amp;gt;nil, &quot;url&quot;=&amp;gt;&quot;UNKNOWN&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/qurator-spk/dinglehopper&quot;}         ocrd_typegroups_classifier    {&quot;compliant_cli&quot;=&amp;gt;true, &quot;files&quot;=&amp;gt;{&quot;Dockerfile&quot;=&amp;gt;nil, &quot;README.md&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebräisch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```n&quot;, &quot;ocrd-tool.json&quot;=&amp;gt;&quot;{n  &quot;version&quot;: &quot;0.0.2&quot;,n  &quot;git_url&quot;: &quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;,n  &quot;tools&quot;: {n    &quot;ocrd-typegroups-classifier&quot;: {n      &quot;executable&quot;: &quot;ocrd-typegroups-classifier&quot;,n      &quot;description&quot;: &quot;Classification of 15th century type groups&quot;,n      &quot;categories&quot;: [n        &quot;Text recognition and optimization&quot;n      ],n      &quot;steps&quot;: [n        &quot;recognition/font-identification&quot;n      ],n      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;],n      &quot;parameters&quot;: {n        &quot;network&quot;: {n          &quot;description&quot;: &quot;The file name of the neural network to use, including sufficient path information&quot;,n          &quot;type&quot;: &quot;string&quot;,n          &quot;required&quot;: truen        },n        &quot;stride&quot;: {n          &quot;description&quot;: &quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;,n          &quot;type&quot;: &quot;number&quot;,n          &quot;format&quot;: &quot;integer&quot;,n          &quot;default&quot;: 112n        }n      }n    }n  }n}n&quot;, &quot;setup.py&quot;=&amp;gt;&quot;# -*- coding: utf-8 -*-nimport codecsnnfrom setuptools import setup, find_packagesnnwith codecs.open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f:n    README = f.read()nnsetup(n    name=&#39;ocrd_typegroups_classifier&#39;,n    version=&#39;0.0.2&#39;,n    description=&#39;Typegroups classifier for OCR&#39;,n    long_description=README,n    long_description_content_type=&#39;text/markdown&#39;,n    author=&#39;Matthias Seuret, Konstantin Baierer&#39;,n    author_email=&#39;seuretm@users.noreply.github.com&#39;,n    url=&#39;https://github.com/seuretm/ocrd_typegroups_classifier&#39;,n    license=&#39;Apache License 2.0&#39;,n    packages=find_packages(exclude=(&#39;tests&#39;, &#39;docs&#39;)),n    include_package_data=True,n    install_requires=open(&#39;requirements.txt&#39;).read().split(&#39;n&#39;),n    package_data={n        &#39;&#39;: [&#39;*.json&#39;, &#39;*.tgc&#39;],n    },n    entry_points={n        &#39;console_scripts&#39;: [n            &#39;typegroups-classifier=ocrd_typegroups_classifier.cli.simple:cli&#39;,n            &#39;ocrd-typegroups-classifier=ocrd_typegroups_classifier.cli.ocrd_cli:cli&#39;,n        ]n    },n)n&quot;}, &quot;git&quot;=&amp;gt;{&quot;last_commit&quot;=&amp;gt;&quot;Thu Jan 16 11:38:59 2020 +0100&quot;, &quot;latest_tag&quot;=&amp;gt;&quot;v0.0.2&quot;, &quot;number_of_commits&quot;=&amp;gt;&quot;77&quot;, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier.git&quot;}, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;ocrd_tool&quot;=&amp;gt;{&quot;git_url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;tools&quot;=&amp;gt;{&quot;ocrd-typegroups-classifier&quot;=&amp;gt;{&quot;categories&quot;=&amp;gt;[&quot;Text recognition and optimization&quot;], &quot;description&quot;=&amp;gt;&quot;Classification of 15th century type groups&quot;, &quot;executable&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;input_file_grp&quot;=&amp;gt;[&quot;OCR-D-IMG&quot;], &quot;parameters&quot;=&amp;gt;{&quot;network&quot;=&amp;gt;{&quot;description&quot;=&amp;gt;&quot;The file name of the neural network to use, including sufficient path information&quot;, &quot;required&quot;=&amp;gt;true, &quot;type&quot;=&amp;gt;&quot;string&quot;}, &quot;stride&quot;=&amp;gt;{&quot;default&quot;=&amp;gt;112, &quot;description&quot;=&amp;gt;&quot;Stride applied to the CNN on the image. Should be between 1 and 224. Smaller values increase the computation time.&quot;, &quot;format&quot;=&amp;gt;&quot;integer&quot;, &quot;type&quot;=&amp;gt;&quot;number&quot;}}, &quot;steps&quot;=&amp;gt;[&quot;recognition/font-identification&quot;]}}, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;ocrd_tool_validate&quot;=&amp;gt;&quot;&amp;lt;report valid=&quot;true&quot;&amp;gt;n&amp;lt;/report&amp;gt;&quot;, &quot;official&quot;=&amp;gt;true, &quot;org_plus_name&quot;=&amp;gt;&quot;OCR-D/ocrd_typegroups_classifier&quot;, &quot;python&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author-email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd_typegroups_classifier&quot;, &quot;pypi&quot;=&amp;gt;{&quot;info&quot;=&amp;gt;{&quot;author&quot;=&amp;gt;&quot;Matthias Seuret, Konstantin Baierer&quot;, &quot;author_email&quot;=&amp;gt;&quot;seuretm@users.noreply.github.com&quot;, &quot;bugtrack_url&quot;=&amp;gt;nil, &quot;classifiers&quot;=&amp;gt;[], &quot;description&quot;=&amp;gt;&quot;# ocrd_typegroups_classifiernn&amp;gt; Typegroups classifier for OCRnn## Installationnn### From PyPInn```shnpip3 install ocrd_typegroup_classifiern```nn### From sourcennIf needed, create a virtual environment for Python 3 (it was testednsuccessfully with Python 3.7), activate it, and install ocrd.nn```shnvirtualenv -p python3 ocrd-venv3nsource ocrd-venv3/bin/activatenpip3 install ocrdn```nnEnter in the folder containing the tool:nn```ncd ocrd_typegroups_classifier/n```nnInstall the module and its dependenciesnn```nmake installn```nnFinally, run the test:nn```nsh test/test.shn```nn** Important: ** The test makes sure that the system does work. Fornspeed reasons, a very small neural network is used and applied only tonthe top-left corner of the image, therefore the quality of the resultsnwill be of poor quality.nn## ModelsnnThe model classifier-1.tgc is based on a ResNet-18, with less neuronsnper layer than the usual model. It was briefly trained on 12 classes:nAdornment, Antiqua, Bastarda, Book covers and other irrelevant data,nEmpty Pages, Fraktur, Griechisch, Hebräisch, Kursiv, Rotunda, Textura,nand Woodcuts - Engravings.nn## Heatmap Generation ##nGiven a trained model, it is possible to produce heatmaps correspondingnto classification results. Syntax:nn```npython3 tools/heatmap.py ocrd_typegroups_classifier/models/classifier.tgc sample.jpg outn```nnn&quot;, &quot;description_content_type&quot;=&amp;gt;&quot;text/markdown&quot;, &quot;docs_url&quot;=&amp;gt;nil, &quot;download_url&quot;=&amp;gt;&quot;&quot;, &quot;downloads&quot;=&amp;gt;{&quot;last_day&quot;=&amp;gt;-1, &quot;last_month&quot;=&amp;gt;-1, &quot;last_week&quot;=&amp;gt;-1}, &quot;home_page&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;, &quot;keywords&quot;=&amp;gt;&quot;&quot;, &quot;license&quot;=&amp;gt;&quot;Apache License 2.0&quot;, &quot;maintainer&quot;=&amp;gt;&quot;&quot;, &quot;maintainer_email&quot;=&amp;gt;&quot;&quot;, &quot;name&quot;=&amp;gt;&quot;ocrd-typegroups-classifier&quot;, &quot;package_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;platform&quot;=&amp;gt;&quot;&quot;, &quot;project_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/&quot;, &quot;project_urls&quot;=&amp;gt;{&quot;Homepage&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;release_url&quot;=&amp;gt;&quot;https://pypi.org/project/ocrd-typegroups-classifier/0.0.2/&quot;, &quot;requires_dist&quot;=&amp;gt;[&quot;ocrd (&amp;gt;=2.0.1)&quot;, &quot;pandas&quot;, &quot;scikit-image&quot;, &quot;torch (&amp;gt;=1.4.0)&quot;, &quot;torchvision (&amp;gt;=0.5.0)&quot;], &quot;requires_python&quot;=&amp;gt;&quot;&quot;, &quot;summary&quot;=&amp;gt;&quot;Typegroups classifier for OCR&quot;, &quot;version&quot;=&amp;gt;&quot;0.0.2&quot;}, &quot;last_serial&quot;=&amp;gt;6465300, &quot;releases&quot;=&amp;gt;{&quot;0.0.1&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;sha256&quot;=&amp;gt;&quot;d469964e37069a2dab403bbf7400eec4ddabcf4ee83c86d6e88bda1bd96e9c1d&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;19437f8f76a7e346479a2bea163b164f&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26290742, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:55&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:55.449239Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/e6/1b/5d0e6967985a7e23d01f558677bd7de4385dacc0186e4896ad23cb4e2f0d/ocrd_typegroups_classifier-0.0.1-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;sha256&quot;=&amp;gt;&quot;6b339f6b52cb62acc93f64d11637aa895a2cfbe7958df3391e4d6480d8c87d28&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.1.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;48c202c02d301243c8e9f365e9dcad1d&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15969, &quot;upload_time&quot;=&amp;gt;&quot;2019-11-29T15:27:59&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2019-11-29T15:27:59.723574Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/6a/d0/620fd50f319ef68ec959b67d0c048bb0f1d602ca5cc0baa0ff46fd235382/ocrd_typegroups_classifier-0.0.1.tar.gz&quot;}], &quot;0.0.2&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;urls&quot;=&amp;gt;[{&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;sha256&quot;=&amp;gt;&quot;75057c3c0c8be6f664f04c903ce3fd4337a5f87dea8c825a423e006a2c406a03&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;733fcd5009cf54a7349aa314bf9a6e47&quot;, &quot;packagetype&quot;=&amp;gt;&quot;bdist_wheel&quot;, &quot;python_version&quot;=&amp;gt;&quot;py3&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;26294951, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:25&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:25.132553Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/bc/82/1b0976ef56d24962249dd9c4ff1c8dff259413cb52cc99bb08bbea15e1f8/ocrd_typegroups_classifier-0.0.2-py3-none-any.whl&quot;}, {&quot;comment_text&quot;=&amp;gt;&quot;&quot;, &quot;digests&quot;=&amp;gt;{&quot;md5&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;sha256&quot;=&amp;gt;&quot;8c9b0f8253a2b34985128201ff155329ce23a5094e21f5f162d9ffa12ce8230b&quot;}, &quot;downloads&quot;=&amp;gt;-1, &quot;filename&quot;=&amp;gt;&quot;ocrd_typegroups_classifier-0.0.2.tar.gz&quot;, &quot;has_sig&quot;=&amp;gt;false, &quot;md5_digest&quot;=&amp;gt;&quot;4e597d6a3f75c4991392b11e88f89f40&quot;, &quot;packagetype&quot;=&amp;gt;&quot;sdist&quot;, &quot;python_version&quot;=&amp;gt;&quot;source&quot;, &quot;requires_python&quot;=&amp;gt;nil, &quot;size&quot;=&amp;gt;15988, &quot;upload_time&quot;=&amp;gt;&quot;2020-01-16T10:39:28&quot;, &quot;upload_time_iso_8601&quot;=&amp;gt;&quot;2020-01-16T10:39:28.744590Z&quot;, &quot;url&quot;=&amp;gt;&quot;https://files.pythonhosted.org/packages/13/6c/ad140f1e282941da373f19236cfffdc7b4dfe8190cef547175d33c3de8d9/ocrd_typegroups_classifier-0.0.2.tar.gz&quot;}]}, &quot;url&quot;=&amp;gt;&quot;https://github.com/seuretm/ocrd_typegroups_classifier&quot;}, &quot;url&quot;=&amp;gt;&quot;https://github.com/OCR-D/ocrd_typegroups_classifier&quot;}         ",
      "url": " /en/kwalitee.html"
    },
  

    {
      "slug": "en-spec-logging-html",
      "title": "THIS IS JUST A FIRST DRAFT!",
      "content"	 : "THIS IS JUST A FIRST DRAFT!Conventions for LOGGINGThis section specifies how the output of the digitization workflow is logged.Target AudienceUsers and developers of digitization workflows in libraries and/or digitization centers.IntroductionLogging is essential for developers and users to debug applications.You always have to choose between two contradictory goals:  Runtime  InformationMany issues make troubleshooting easier but have a negative effect on the runtime.Therefore, log levels have been introduced to customize the output to suit your needs.When a workflow is executed, the output of the applications MAY be stored in files.All log files were stored in a subfolder ‘metadata/log’.The file name SHOULD contain the ID of the activity in the provenance and the name of the stream.FILENAME := ACTIVITY_ID + “_” + OUTPUT_STREAM + “.log”‘&#39;’Example:’’’ ocrd-kraken-bin_0001_stdout.logLog LevelsA more detailed description will be found hereTRACE / ALLThis is the most verbose logging level to trace the path of the algorithm.E.g.: Start/End/Duration of a method, even loops may be logged. Note that logging within loops can dramatically affect performance.DEBUGThis level is used if parameters and or the status of an algorithm/method should be logged.INFOLog information about used settings on application level.WARNThis level is used to log information about missing/wrong configurations which may lead to errors.ERRORLog all events that produce no or a wrong result. It is useful to output all the information that helps to determine the cause without the need for further investigation.FormatThe format of the logging output has to be formatted like this:TIMESTAMP LEVEL LOGGERNAME - MESSAGEExample:08:03:40.017 WARN edu.kit.ocrd.MyTestClass - A warn messageMETSThe log files are not referenced inside METS.If they are listed in the provenance, their content must be included in the provenance.Ingest Workspace to OCR-D RepositoriumNo log files will be stored in repositoryUse CasesLog during the WorkflowAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are also stored in the provenance, only information that is important for later analysis should be provided.STDERR only contains error messages that cause the program to terminate (see Loglevel ERROR).STDOUT should only contain outputs that are maximum of the log level INFO (see Loglevel INFO).For automated workflows it is recommended to save STDERR only.Analyze applicationsAll applications executed during workflow have to write there logging to STDERR and STDOUT.Since both outputs are used to analyze the program flow and possible errors and the performance is not important, all information should be output here.",
      "url": " /en/spec/logging.html"
    },
  

    {
      "slug": "en-spec-mets-html",
      "title": "Requirements on handling METS/PAGE",
      "content"	 : "Requirements on handling METS/PAGEOCR-D has decided to base its data exchange format on METS. This document defines a set of conventions and mechanism for using METS files within OCR-D. The basis for this is the METS Application Profilefor digitised media by the DFG-Viewer (current version: 2.3.1).For layout and text recognition results, the primary exchange format in OCR-D is PAGE. Conventions for PAGE are outlined in a separate document.1) Metadata1.1 Unique ID for the document processedMETS documents must be uniquely addressable within the global library community.For this purpose, the METS file MUST contain a mods:identifier that must contain a globally unique identifier for the document and have a type attribute with a value of (in order of preference):  purl  urn  handle  url1.2 Always use URL or relative filenamesAlways use URL, except for files located in the directory or any subdirectories of the METS file.Example/tmp/foo/ws1├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/mets.xml:  /tmp/foo/ws1/foo.xml (absolute path)  file:///tmp/foo/ws1/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)1.3 Recording processing information in METSOCR-D processors should add information to the METS metadata header to indicate thatthey changed the METS. This information is mainly for human consumption to getan overview of the software agents involved in the METS file’s creation. Moredetailed or machine-actionable provenance information is outside the scope ofthe processor.To add agent information, a processor must:1) locate the first mets:metsHdr M.2) Add to M a new mets:agent A with these attributes  TYPE must be the string OTHER  OTHERTYPE must be the string SOFTWARE  ROLE must be the string OTHER  OTHERROLE must be the processing step this processor provided, from the list in the ocrd-tool.json spec3) Add to A a mets:name N that should include, in free-text form, these data points  Name of the processor, e.g. the name of the executable from ocrd-tool.json  Version of the processor, e.g. from ocrd-tool.jsonExample&amp;lt;mets:agent TYPE=&quot;OTHER&quot; OTHERTYPE=&quot;SOFTWARE&quot; ROLE=&quot;OTHER&quot; OTHERROLE=&quot;preprocessing/optimization/binarization&quot;&amp;gt;  &amp;lt;mets:name&amp;gt;ocrd_tesserocr v0.1.2&amp;lt;/mets:name&amp;gt;&amp;lt;/mets:agent&amp;gt;2) Images2.1 If in PAGE then in METSAll URLs used in the imageFilename / filename attributes ofpc:Page / pc:AlternativeImage of all PAGE files MUST be referenced in a mets:fileGrp as the@xlink:href attribute of a mets:file in the METS. For derived images (pc:AlternativeImage), this MUST be the same file group asthe PAGE-XML that was the result of the processing step that produced thepc:AlternativeImage.In other words: New pc:AlternativeImage files must bewritten to the same mets:fileGrp as their target PAGE-XML files, which in mostimplementations will mean the same directory. Existing images must be read from the filesystem, and could reside in any fileGrp prior to the source PAGE-XML file in the workflow (including the original image in the first fileGrp).2.2 Pixel density of images must be explicit and high enoughThe pixel density is the ratio of the number of pixels that represent a a unit of measure of the scanned object. It is typically measured in pixels per inch (PPI, a.k.a. DPI).The original input images MUST have &amp;gt;= 150 ppi.Every processing step that generates new images and changes their dimensions MUST make sure to adapt the density explicitly when serialising the image.$&amp;gt; exiftool input.tif |grep &#39;X Resolution&#39;&quot;300&quot;# WRONG (ppi unchanged)$&amp;gt; convert input.tif -resize 50% output.tif# RIGHT:$&amp;gt; convert input.tif -resize 50% -density 150 -unit inches output.tif$&amp;gt; exiftool output.tif |grep &#39;X Resolution&#39;&quot;150&quot;However, since technical metadata about pixel density is so often lost inconversion or inaccurate, processors should assume 300 ppi for images withmissing or suspiciously low pixel density metadata.2.3 No multi-page imagesImage formats like TIFF support encoding multiple images in a single file.Data providers MUST provide single-image TIFF files.OCR-D processors MUST raise an exception if they encounter multi-image TIFF files.2.4 Images and coordinatesCoordinates in a PAGE-XML are always absolute, i.e. relative to extent defined in the imageWidth / imageHeight attributes of the top-level pc:Page.When a processor wants to access the image of a layout element like a pc:TextRegion or pc:TextLine, the algorithm should be:  If the element in question has an attribute imageFilename, resolve this value  Else if the element in question has a subelement pc:AlternativeImage with attribute filename, resolve this value  Otherwise, resolve by passing the attribute imageFilename of the top-level pc:Page and the points attribute of the element’s pc:Coords subelement(For details on coordinate handling, see PAGE-XML specs.)3) File groups mets:fileGrpReferenced files MUST be organised into file groups without recursion.3.1 File Group @USE syntaxAll mets:fileGrp MUST have a unique USE attribute that hints at the provenance of the filesand MUST be a valid xsd:ID.It SHOULD have the structureID := &quot;OCR-D-&quot; + PREFIX? + WORKFLOW_STEP + (&quot;-&quot; + PROCESSOR)?PREFIX := (&quot;&quot; | &quot;GT-&quot;)WORKFLOW_STEP := (&quot;IMG&quot; | &quot;SEG&quot; | &quot;OCR&quot; | &quot;COR&quot;)PROCESSOR := [A-Z0-9-]{3,}PREFIX can be GT- to indicate that these files are ground truth.WORKFLOW_STEP can be one of:  IMG: Image(s)  SEG: Segmented regions / lines / words  OCR: OCR produced from image  COR: Post-correctionPROCESSOR should be a mnemonic of the processor or result type in a terse,all-caps form, such as the name of the tool (KRAKEN) or the organisationCIS or the type of manipulation (CROP) or a combination of both startingwith the type of manipulation (BIN-KRAKEN).Examples            &amp;lt;mets:fileGrp USE&amp;gt;      Type of use for OCR-D                  &amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;      The unmanipulated source images              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-BIN&quot;&amp;gt;      Binarization preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-CROP&quot;&amp;gt;      Cropping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESKEW&quot;&amp;gt;      Deskewing preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DESPECK&quot;&amp;gt;      Despeckling preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-PRE-DEWARP&quot;&amp;gt;      Dewarping preprocessing              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-REGION&quot;&amp;gt;      Region segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-LINE&quot;&amp;gt;      Line segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-WORD&quot;&amp;gt;      Word segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-TESS&quot;&amp;gt;      Tesseract OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR-OCRO&quot;&amp;gt;      Ocropus OCR              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-CIS&quot;&amp;gt;      CIS post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-COR-ASV&quot;&amp;gt;      ASV post-correction              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-REGION&quot;&amp;gt;      Region segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-LINE&quot;&amp;gt;      Line segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-WORD&quot;&amp;gt;      Word segmentation ground truth              &amp;lt;mets:fileGrp USE=&quot;OCR-D-GT-SEG-GLYPH&quot;&amp;gt;      Glyph segmentation ground truth      4) Files mets:file4.1 File ID syntaxEach mets:file must have an ID attribute. The ID attribute of a mets:file SHOULD be the USE of the containing mets:fileGrp combined with its corresponding page ID or a 4-zero-padded number.The ID MUST be unique inside the METS file.FILEID := ID + &quot;_&quot; + [0-9]{4}ID := FILEGRP + (&quot;.IMG&quot;)?Examples            &amp;lt;mets:file ID&amp;gt;      ID of the file for OCR-D                  &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;      The unmanipulated source image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN_0001&quot;&amp;gt;      PAGE encapsulating the result from binarization              &amp;lt;mets:file ID=&quot;OCR-D-PRE-BIN.IMG_0001&quot;&amp;gt;      Black-and-white image              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP_0001&quot;&amp;gt;      PAGE encapsulating the result from (binarization and) cropping              &amp;lt;mets:file ID=&quot;OCR-D-PRE-CROP.IMG_0001&quot;&amp;gt;      Cropped black-and-white image      4.2 @MIMETYPE syntaxEvery mets:file element representing a PAGE file MUST have its MIMETYPE attribute set to application/vnd.prima.page+xml.4.3 File Group @USE=&quot;FULLDOWNLOAD_...&quot;For mets:file entries representative of the publication as a whole, the ID attribute MUST have  prefix FULLDOWNLOAD_, followed by the file format (TEI, ALTO, hOCR, HTML, TXT, COCO, PDF).These entries SHOULD be referenced in the structMap under /mets:mets/mets:structMap[@TYPE=&quot;PHYSICAL&quot;]/mets:div/mets:fptr (i.e. the top-level mets:div element).They MAY reside in the same mets:fileGrp as per-page files of the same type, or in a separate one.Example&amp;lt;mets:file ID&amp;gt; | ID of the file for OCR-D–               | –&amp;lt;mets:file ID=&quot;FULLDOWNLOAD_TEI&quot;  MIMETYPE=&quot;application/tei+xml&quot;&amp;gt;            | The digitised publication or book in TEI format.&amp;lt;mets:file ID=&quot;FULLDOWNLOAD_TEI_01&quot;  MIMETYPE=&quot;application/tei+xml&quot;&amp;gt;            | The digitised publication or book in TEI format. Version one.&amp;lt;mets:file ID=&quot;FULLDOWNLOAD_TEI_02&quot;  MIMETYPE=&quot;application/tei+xml&quot;&amp;gt;            | The digitised publication or book in TEI format, a second Version.5) Grouping files by page mets:structMap5.1 Grouping files by pageThe METS file MUST have exactly one physical map that contains a singlemets:div[@TYPE=&quot;physSequence&quot;] which in turn must contain amets:div[@TYPE=&quot;page&quot;] for every page in the work.These mets:div[@TYPE=&quot;page&quot;] can contain an arbitrary number of mets:fptrpointers to mets:file elements to signify that all the files within a div areencodings of the same page.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot;&amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;5.2 OCR-D mets:structMapThe METS may contain different mets:structMap entries, differentiated by their TYPE attribute (e.g. LOGICAL, PHYSICAL, ...).  A mets:structMap with TYPE=&quot;PHYSICAL&quot; is mandatory.  The logical document structure detected by library or archive MUST be described by TYPE=&quot;LOGICAL&quot;.  The logical document structure detected by OCR-D software MUST be described by TYPE=&quot;OCR-D-LOGICAL&quot;.            attributes in structMap      description                  LABEL      contains the recognized text of a structuring component, e.g. the title of a chapter              TYPE      contains the type of a structuring component according to some standardized, controlled vocabulary (see DFG-Viewer: structural data set), e.g. chapter      6) Ranges of pages mets:structLinkThe mets:structLink describes the ranges of pages in parts of a document by linking mets:div elements of the logical mets:structMap to mets:div elements of the  physical mets:structMap.Example&amp;lt;mets:fileGrp USE=&quot;OCR-D-IMG&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-IMG_0001&quot; &amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:fileGrp USE=&quot;OCR-D-OCR&quot;&amp;gt;    &amp;lt;mets:file ID=&quot;OCR-D-OCR_0001&quot; &amp;gt;...&amp;lt;/mets:file&amp;gt;&amp;lt;/mets:fileGrp&amp;gt;&amp;lt;mets:structMap TYPE=&quot;OCR-D-LOGICAL&quot;&amp;gt;  &amp;lt;mets:div DMDID=&quot;dmdSec_0001&quot; ADMID=&quot;amdSec_0001&quot; ID=&quot;OCR-D-loc_0001&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;OCR-D-loc_d5e320&quot; TYPE=&quot;chapter&quot; LABEL=&quot;KapıteI 1&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;OCR-D-loc_d7e560&quot; TYPE=&quot;chapter&quot; LABEL=&quot;Unterkapitel&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;    &amp;lt;mets:div ID=&quot;OCR-D-loc_d9e376&quot; TYPE=&quot;chapter&quot; LABEL=&quot;Kapidel 2&quot;/&amp;gt;   &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;&amp;lt;mets:structMap TYPE=&quot;LOGICAL&quot;&amp;gt;   &amp;lt;mets:div TYPE=&quot;Monograph&quot; DMDID=&quot;dmdSec_0001&quot; ADMID=&quot;amdSec_0001&quot; ID=&quot;loc_0001&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;loc_d1e410&quot; TYPE=&quot;chapter&quot; LABEL=&quot;Kapitel 1&quot;/&amp;gt;    &amp;lt;mets:div ID=&quot;loc_d1e451&quot; TYPE=&quot;chapter&quot; LABEL=&quot;Kapitel 2&quot;/&amp;gt;   &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;&amp;lt;mets:structMap TYPE=&quot;PHYSICAL&quot;&amp;gt;  &amp;lt;mets:div ID=&quot;PHYS_0000&quot; TYPE=&quot;physSequence&quot;&amp;gt;    &amp;lt;mets:div ID=&quot;PHYS_0001&quot; TYPE=&quot;page&quot;&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-IMG_0001&quot;/&amp;gt;      &amp;lt;mets:fptr FILEID=&quot;OCR-D-OCR_0001&quot;/&amp;gt;    &amp;lt;/mets:div&amp;gt;  &amp;lt;/mets:div&amp;gt;&amp;lt;/mets:structMap&amp;gt;&amp;lt;mets:structLink&amp;gt;&amp;lt;!-- Library-Part--&amp;gt;   &amp;lt;mets:smLink xlink:from=&quot;loc_0001&quot; xlink:to=&quot;PHYS_0000&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e410&quot; xlink:to=&quot;PHYS_0001&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e410&quot; xlink:to=&quot;PHYS_0002&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e410&quot; xlink:to=&quot;PHYS_0003&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e410&quot; xlink:to=&quot;PHYS_0004&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e451&quot; xlink:to=&quot;PHYS_0005&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;loc_d1e451&quot; xlink:to=&quot;PHYS_0006&quot;/&amp;gt; &amp;lt;!-- OCR-D-Part--&amp;gt;   &amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_0001&quot; xlink:to=&quot;PHYS_0000&quot;/&amp;gt;&amp;lt;!-- Kapitel 1--&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d5e320&quot; xlink:to=&quot;PHYS_0001&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d5e320&quot; xlink:to=&quot;PHYS_0002&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d5e320&quot; xlink:to=&quot;PHYS_0003&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d5e320&quot; xlink:to=&quot;PHYS_0004&quot;/&amp;gt;&amp;lt;!-- Unter-Kapitel zu 1--&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d7e560&quot; xlink:to=&quot;PHYS_0002&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d7e560&quot; xlink:to=&quot;PHYS_0003&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d7e560&quot; xlink:to=&quot;PHYS_0004&quot;/&amp;gt;&amp;lt;!-- Kapitel 2--&amp;gt;    &amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d7e560&quot; xlink:to=&quot;PHYS_0005&quot;/&amp;gt;&amp;lt;mets:smLink xlink:from=&quot;OCR-D-loc_d7e560&quot; xlink:to=&quot;PHYS_0006&quot;/&amp;gt;&amp;lt;/mets:structLink&amp;gt;",
      "url": " /en/spec/mets.html"
    },
  

    {
      "slug": "de-modelle-html",
      "title": "OCR-Modelle",
      "content"	 : "OCR-ModelleFür die Texterkennung wird ein geeignetes OCR-D-Modul und ein dazu passendesSprach-/Schriftmodell benötigt. Diese Seite gibt einen Überblick über diewichtigsten Modelle und Modell-Repositorien.ocrd-tesserocr-recognizeDieser Prozessor verwendet Tesseract (ab Version 4.1) für die Texterkennung. Tesseract benötigtSprach- oder Schriftmodelle. Dies sind Dateien in einem speziellen Format (*.traineddata). Sie enthaltenmindestens eine Liste mit dem Erkennungs-Zeichensatz (“unicharset”) und die Gewichte des neuronalen Erkennungs-Modells (“lstm”), optional auch noch Wörterbücher (“wordlist”/”dawg”) und weitere Komponenten.Sprachmodelle sind im Zeichensatz und im Wörterbuch auf eine Muttersprache (z. B. deu = Deutsch) beschränkt.Schriftmodelle dagegen enthalten einen umfangreicheren Zeichensatz und Wörterbücher aus mehreren Sprachen mit der gleichen Schrift (z. B. Latin = lateinische Schrift mit Englisch, Deutsch, Französisch,Spanisch, Italienisch, …).Für Tesseract gibt es mehr als 100 Sprach- und Schriftmodelle, die von Google mittels synthetischer Daten(d.h. per Rasterung großer Mengen von Text mit vielen verschiedenen Vektorfonts) erzeugt (“trainiert”)wurden. Daneben gibt es aber auch noch weitere Modelle von anderen Anbietern, und man kann auch eigeneModelle entweder komplett neu oder auf Basis vorhandener Modelle erstellen. Eigenes Training wird durchtesstrain gut unterstützt.Die Modelle von Google gibt es jeweils in drei Varianten:tessdata_fast Diese Variante wird auch von den meistenLinux-Distributionen angeboten und ist besonders schnell bei der Texterkennung. Sie verwendet neuronale Netzwerke.tessdata_best Diese Variante braucht deutlich mehr Zeit bei derTexterkennung, kann aber im Einzelfall(nicht generell!) bessere Ergebnisse liefern. Sie verwendet neuronale Netzwerke.Eigenes Training neuer Modelle auf Basis vorhandener Modelle setzt ebenfalls diese Variante voraus.tessdata Diese Variante ist ähnlich schnell wie tessdata_fast, enthältaber zusätzlich zu den neuronalen Netzwerken auch noch die musterbasierte Zeichenerkennung von Tesseract 3.Man kann damit also zwei unterschiedliche Texterkennungsmethoden kombinieren, was in Einzelfällen zu besseren Ergebnissenführen kann.Schrift- und Sprachmodelle für historische DruckeDie folgenden Modelle für Tesseract gibt es:  deu_frak Älteres Sprachmodell für deutsche Fraktur. Dieses Modell war mit Tesseract 3 gebräuchlich, ist aber heute nicht mehr zu empfehlen.  deu Sprachmodell für deutsche Antiqua, das aber auch etwas Fraktur erkennen kann.  deu_latf (ehem. frk) Sprachmodell für deutsche Fraktur, das aber auch etwas Antiqua erkennen kann.  Latin Schriftmodell für lateinische Antiqua-Schriften, das aber auch etwas Fraktur erkennen kann.  Fraktur Schriftmodell für Fraktur-Schriften, das aber auch Antiqua-Schriften ganz gut erkennt. Fehler beimErzeugen dieses Modells haben zur Folge, dass es kein Paragraphzeichen kennt und die Ligaturen ch und ckhäufig als Kleiner- und Größerzeichen “erkennt”.Weitere Frakturmodelle. Ausgehend von Fraktur sind mit Hilfe von GT4HistOCRweitere Modelle der UB Mannheimerzeugt worden, die für ein breites Spektrum historischer Drucke gute Ergebnisse liefern.Schrift- und Sprachmodelle können in Tesseract auch kombiniert werden, was inder Regel noch bessere Ergebnisse bringt, allerdings dann auch mehr Zeitkostet.",
      "url": " /de/modelle.html"
    },
  

    {
      "slug": "en-models-html",
      "title": "Models for OCR-D processors",
      "content"	 : "Models for OCR-D processorsOCR engines rely on pre-trained models for their recognition. Every engine hasits own internal format(s) for models. Some support central storage of modelsat a specific location (Tesseract, Ocropy, Kraken) while others require the fullpath to a model (Calamari).Moreover, many processors provide other file resources like configuration files or presets.Since v2.22.0, OCR-D/corecomes with a framework for managing file resources uniformly. This meansthat processors can delegate to OCR-D/core to resolve specific file resources by name,looking in well-defined places in the filesystem. This also includes downloading and cachingfile parameters passed as a URL. Furthermore, OCR-D/core comes with a bundled databaseof known resources, such as models, dictionaries, configurations and otherprocessor-specific data files. Processors can add their own specifications to that.This means that OCR-D users should be able to concentrate on fine-tuning their OCR workflowsand not bother with implementation details like “where do I get models from and where do I put them”.In particular, users can reference file parameters by name now.All of the above mentioned functionality can be accessed using the ocrd resmgr command line tool.What models are available?To get a list of the (available or installed) file resources that OCR-D/coreis aware of:ocrd resmgr list-available# alternatively, using Docker:docker run --volume ocrd-models:/models -- ocrd/all:maximum ocrd resmgr list-availableThe output will look similar to this:ocrd-calamari-recognize- qurator-gt4hist-0.3 (https://qurator-data.de/calamari-models/GT4HistOCR/2019-07-22T15_49+0200/model.tar.xz)  Calamari model trained with GT4HistOCR- qurator-gt4hist-1.0 (https://qurator-data.de/calamari-models/GT4HistOCR/2019-12-11T11_10+0100/model.tar.xz)  Calamari model trained with GT4HistOCRocrd-cis-ocropy-recognize- LatinHist.pyrnn.gz (https://github.com/chreul/OCR_Testdata_EarlyPrintedBooks/raw/master/LatinHist-98000.pyrnn.gz)  ocropy historical latin model by github.com/chreulAs you can see, resources are grouped by the processors which make use of them.The word after the list symbol, e.g. qurator-gt4hist-0.3,LatinHist.pyrnn.gz, defines the name of the resource, which is a shorthand you canuse in parameters without having to specify the full URL (in brackets after thename).The second line of each entry contains a short description of the resource.Installing resourcesOn installing resources in OCR-D, read the follow-up sectionsInstalling known resources andInstalling unknown resources.Known resources are resources that are provided by processor developers in the ocrd-tool.jsonand are available by name to ocrd resmgr download.Unknown resources, in contrast, are models, configurations, parameter sets etc. that you provide yourselfor found elsewhere on the Internet, which require passing a URL (or local path) to ocrd resmgr download.If you installed OCR-D via Docker, read the section Models and Docker additionally.Installing known resourcesYou can install resources with the ocrd resmgr download command. It expectsthe name of the processor as the 1st argument and the name of a resource as a 2nd argument.Since model distribution is decentralised within OCR-D, every processor can advertise itsown known resources, which the resource manager then picks up.For example, to install the LatinHist.pyrnn.gz resource for ocrd-cis-ocropy-recognize:ocrd resmgr download ocrd-cis-ocropy-recognize LatinHist.pyrnn.gzThis will look up the resource in the bundled resource and user databases, download,unarchive (where applicable) and store it in the proper location.  Note: The special name * can be used instead of a resource name/url todownload all known resources for this processor. To download all tesseract models:ocrd resmgr download ocrd-tesserocr-recognize &#39;*&#39;  Note: Equally, the special processor * can be used instead of a processor and a resourceto download all known resources for all installed processors:ocrd resmgr download &#39;*&#39;  (In either case, * must be in quotes or escaped to avoid wildcard expansion by the shell.)Installing unknown resourcesIf you need to install a resource which OCR-D does not know of, that can be achieved by passingits URL in combination with the --any-url/-n flag to ocrd resmgr download.For example, to install the same model for ocrd-cis-ocropy-recognize as above:ocrd resmgr download -n https://github.com/chreul/OCR_Testdata_EarlyPrintedBooks/raw/master/LatinHist-98000.pyrnn.gz ocrd-cis-ocropy-recognize LatinHist.pyrnn.gzOr to install a model for ocrd-tesserocr-recognize that is located at https://my-server/mymodel.traineddata:ocrd resmgr download -n https://my-server/mymodel.traineddata ocrd-tesserocr-recognize mymodel.traineddataThis will download and store the resource in the proper location and create a stub entry in theuser database.  You can then use it as the parameter value for the model parameter:ocrd-tesserocr-recognize -P model mymodelModels and DockerIf you are using OCR-D with Docker, we recommend keeping all downloaded resources persistentlyin a host directory, independent of both:  the Docker container’s internal storage (which is transient, i.e. any change over the imagegets lost with each new docker run),  the host’s data directory (which may be on a different filesystem).That resource directory needs to be mounted into a specific path in the container, as does the data directory:  /models: resource files (to be mounted as a named volume, e.g. -v ocrd-models:/models),  /data: input/output files (to be mounted any way you like, probably a bind mount, e.g. -v $PWD:/data),  /tmp: temporary files (ideally as tmpfs, e.g. --tmpfs /tmp)Initially, (if you use a named volume, not a bind mount,) the host resource directory will contain onlythose resources that have been pre-installed into the processors’ module directories. Each time you runthe Docker container, the Resource Manager and the processors will access that directory from the insideto resolve resources, so you can download additional models into that location using ocrd resmgr, andlater use them in workflows.The following will assume (without loss of generality) that your host-side datapath is under ./data, and the host-side volume is called ocrd-models:To download models to ocrd-models in the host FS and /models in the container FS:docker run --user $(id -u)   --volume ocrd-models:/models   ocrd/all   ocrd resmgr download ocrd-tesserocr-recognize eng.traineddata;   ocrd resmgr download ocrd-calamari-recognize default;   ...To run processors, then as usual do:docker run --user $(id -u)   --tmpfs /tmp   --volume $PWD/data:/data   --volume ocrd-models:/models   ocrd/all ocrd-tesserocr-recognize -I IN -O OUT -P model engThis principle applies to all ocrd/* Docker images, e.g. you can replace ocrd/all above with ocrd/tesserocr as well.List installed resourcesThe ocrd resmgr list-installed command has the same output format as ocrd resmgr list-available. But insteadof the database, it scans the filesystem locations where data is searched for existingresources and lists URL and description if a database entry exists.User databaseWhenever the OCR-D/core resource manager encounters an unknown resource in the filesystem, or when you installa resource with ocrd resmgr download, it will add a new stub entry in the user database, which is found at$XDG_CONFIG_HOME/ocrd/resources.yml (where $XDG_CONFIG_HOME defaults to $HOME/.config if unset) andgets created if it does not exist.This allows you to use the OCR-D/core resource manager mechanics, includinglookup of known resources by name or URL, without relying (only) on thedatabase maintained by the OCR-D/core developers.  Note: If you produced or found resources that are interesting for the widerOCR(-D) community, please tell us in the OCR-D gitter chator open an issue in the respective Github repository, so we can add it to the database.Where is the dataThe lookup algorithm is defined in our specificationsIn order of preference, a resource &amp;lt;name&amp;gt; for a processor ocrd-foo is searched at:  $PWD/&amp;lt;name&amp;gt;  $XDG_DATA_HOME/ocrd-resources/ocrd-foo/&amp;lt;name&amp;gt;  /usr/local/share/ocrd-resources/ocrd-foo/&amp;lt;name&amp;gt;  $VIRTUAL_ENV/lib/python3.6/site-packages/ocrd-foo/&amp;lt;name&amp;gt; or $VIRTUAL_ENV/share/ocrd-foo/&amp;lt;name&amp;gt; (or whatever the processor’s internal module location is)(where $XDG_DATA_HOME defaults to $HOME/.local/share if unset).We recommend using the $XDG_DATA_HOME location, which is also the default. Butyou can override the location to store data with the --location option, which canbe cwd, data, system and module resp.In Docker though, $XDG_CONFIG_HOME (set to $XDG_DATA_HOME/ocrd-resources,which ultimately is /usr/local/share/ocrd-resources) gets symlinked to/models for easier volume handling (and persistency).# will download to $PWD/latest_net_G.pthocrd resmgr download --location cwd ocrd-anybaseocr-dewarp latest_net_G.pth# will download to /usr/local/share/ocrd-resources/ocrd-anybaseocr-dewarp/latest_net_G.pthocrd resmgr download --location system ocrd-anybaseocr-dewarp latest_net_G.pthChanging the default resource directoryThe $XDG_DATA_HOME default location is reasonable becausemodels are usually large files which should persist across different deployments,both native and containerized, both single-module and ocrd_all.Moreover, that variable can easily be overridden during installation.However, there are use cases where system or even cwd should beused as location to store resources, hence the --location option.Notes on specific processorsOcropy / ocrd_cisAn Ocropy model is simply the neural network serialized with Python’s picklemechanism and is generally distributed in a gzipped form, with a .pyrnn.gzextension and can be used as such, no need to unarchive.To use a specific model with OCR-D’s ocropus wrapper inocrd_cis and more specifically, theocrd-cis-ocropy-recognize processor, use the model parameter:ocrd-cis-ocropy-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-OCRO -P model fraktur-jze.pyrnn.gz  Note: The model must have been downloaded before withocrd resmgr download ocrd-cis-ocropy-recognize fraktur-jze.pyrnn.gzCalamari / ocrd_calamariCalamari models are Tensorflow model directories. For distribution, thisdirectory is usually packed to a tarball or ZIP file. Once downloaded, thesecontainers must be unpacked to a directory again. ocrd resmgr handles thisfor you, so you just need the name of the resource in the database.The Calamari-OCR project also maintains a repository of models.To use a specific model with OCR-D’s calamari wrapperocrd_calamari and more specifically,the ocrd-calamari-recognize processor, use the checkpoint_dir parameter:# To use the &quot;default&quot; model, i.e. the one trained on GT4HistOCR by QURATORocrd-calamari-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-CALA# To use your own trained modelocrd-calamari-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-CALA -P checkpoint_dir /path/to/modeldirTesseract / ocrd_tesserocrTesseract models are single files with a .traineddata extension.Since Tesseract only supports model lookup in a single directory, and we want to share the tessdata directory with the standalone CLI,ocrd_tesserocr resources must be stored in the module location.If the default path of that location is not the place you want to use for Tesseract models,then either recompile Tesseract with the tessdata path you had in mind,or use the TESSDATA_PREFIX environment variable to override the module location at runtime.NOTE: For reasons of efficiency and to avoid duplicate models, all ocrd-tesserocr-* processorsre-use the resource directory for ocrd-tesserocr-recognize.OCR-D’s Tesseract wrapper,ocrd_tesserocr and morespecifically, the ocrd-tesserocr-recognize processor, expects the name of themodel(s) to be provided as the model parameter. Multiple models can becombined by concatenating with + (which generally improves accuracy but always slows processing):# Use the deu and deu_latf modelsocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS -P model &#39;deu+deu_latf&#39;# Use the Fraktur modelocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS -P FrakturModel trainingWith the pretrained models mentioned above, good results can be obtained for many originals. Nevertheless, therecognition rate can usually be improved significantly by fine-tuning an existing model or even training a newmodel on your own particular originals.TesstrainFor training Tesseract models, tesstrain can be used. As it isnot included in ocrd_all, you will still have to install it, first. For information on the setup and the training process itself see the Readme in the GithHub Repository.okralactWhile tesstrain only allows you to train models for Tesseract, with okralactyou can train models for four engines compatible with OCR-D - namely Tesseract, Ocropus, Kraken and Calamari - at once. Especially if you want to use several OCR engines for your workflows or are not sure which OCR engine will give you the bestresults, this might be particularly effective for you. Just like tesstrain it is not included in ocrd_all, meaning you will still have to install it, first. For information on the setup and the training process itself see theReadme in the GithHub Repository.Further readingIf you just installed OCR-D and want to know how to process your own data, please see the user guide.",
      "url": " /en/models.html"
    },
  

    {
      "slug": "en-module-processors-html",
      "title": "Module Processors",
      "content"	 : "            cor-asv-ann                                                        cor-asv-fst                                                        ocrd_calamari                                                        ocrd_im6convert                                                        ocrd_keraslm                                                        ocrd_kraken                                                        ocrd_ocropy                                                        ocrd_olena                                                        ocrd_segment                                                        ocrd_tesserocr                                                        ocrd_cis                                                        ocrd_anybaseocr                                                        ocrd_pc_segmentation                                                        dinglehopper                                                        ocrd_typegroups_classifier                                            ",
      "url": " /en/module-processors.html"
    },
  

    {
      "slug": "en-spec-nextflow-html",
      "title": "Workflow Format (Nextflow)",
      "content"	 : "Workflow Format (Nextflow)One key task in phase III of OCR-D was to define a workflow format describing sequences of OCR-D processors processing the images. Our solution is based on the open source project Nextflow (NF).The files (with the extension .nf) describing this sequence can be generated and read both by humans and algorithms and can be more complex and flexible than the temporary solution withocrd process.Nextflow is a workflow framework that allows the integration of various scripting languages into a single cohesive pipeline. Nextflow also has its own Domain Specific Language (DSL) that extends Groovy (extension of Java).We choose it due to its rich set of features:  Stream oriented: Promotes programming approach extending Unix pipes model.  Fast Prototyping: Lets you write a computational pipeline from smaller tasks.  Reproducibility: Supports Docker, Singularity, and 3 other types of containers.  Portable: Can run locally, Slurm, SGE, PBS, and cloud (Google, Kubernetes, and AWS).  Continuous checkpoints: Each process in the workflow is checkpointed. It is possible to retry failed workflows and start from the last checkpoint.  Supports various scripting languages including Bash, Python, Perl, and others.  Enables separation between configuration (how to do) and workflow logic (what to do).  Modularization of tasks possible via workflow, sub-workflows, and processes.  Provides detailed logs and various types of execution reports.Structure of the Nextflow scriptThe NF script contains the following structures:DSL and ParametersExample:// enables a syntax extension that allows definition of module librariesnextflow.enable.dsl=2// pipeline parametersparams.venv = &quot;$HOME/venv37-ocrd/bin/activate&quot;params.workspace = &quot;$projectDir/ocrd-workspace/&quot;params.mets = &quot;$projectDir/ocrd-workspace/mets.xml&quot;params.reads = &quot;$projectDir/ocrd-workspace/OCR-D-IMG&quot; // The first input directoryparams.outs = &quot;$projectDir/ocrd-workspace/OCR-D-BIN&quot;// nextflow run &amp;lt;my script&amp;gt; --foo Hello// Then, the parameter is accessed with: params.foo// log pipeline parameters to the consolelog.info &quot;&quot;&quot;         O P E R A N D I - T E S T   P I P E L I N E 4         ===========================================         venv          : ${params.venv}         ocrd-workpace : ${params.workspace}         mets          : ${params.mets}         reads         : ${params.reads}         outs          : ${params.outs}         &quot;&quot;&quot;         .stripIndent()Definition of processesprocess tesserocr_deskew {  maxForks 1  input:    path mets_file    val input_dir    val output_dir  output:    val output_dir  script:  &quot;&quot;&quot;  source &quot;${params.venv}&quot;  ocrd-tesserocr-deskew -I ${input_dir} -O ${output_dir} -P operation_level page  deactivate  &quot;&quot;&quot;}Definition of workflowsMain workflow// This is the main workflowworkflow {  main:    ocr_d_img = Channel.value(&quot;OCR-D-IMG&quot;)    ocr_d_bin = Channel.value(&quot;OCR-D-BIN&quot;)    ocr_d_crop = Channel.value(&quot;OCR-D-CROP&quot;)    ocr_d_bin2 = Channel.value(&quot;OCR-D-BIN2&quot;)    ocr_d_denoise = Channel.value(&quot;OCR-D-BIN-DENOISE&quot;)    ocr_d_deskew = Channel.value(&quot;OCR-D-BIN-DENOISE-DESKEW&quot;)    ocr_d_seg = Channel.value(&quot;OCR-D-SEG&quot;)    ocr_d_dewarp = Channel.value(&quot;OCR-D-SEG-LINE-RESEG-DEWARP&quot;)    ocr_d_oc = Channel.value(&quot;OCR-D-OC&quot;)        // input_dir_ch = Channel.fromPath(params.reads, type: &#39;dir&#39;)    ocropy_binarize(params.mets, ocr_d_img, ocr_d_bin)    anybaseocr_crop(params.mets, ocropy_binarize.out, ocr_d_crop)    skimage_binarize(params.mets, anybaseocr_crop.out, ocr_d_bin2)    skimage_denoise(params.mets, skimage_binarize.out, ocr_d_denoise)    tesserocr_deskew(params.mets, skimage_denoise.out, ocr_d_deskew)    cis_ocropy_segment(params.mets, tesserocr_deskew.out, ocr_d_seg)    cis_ocropy_dewarp(params.mets, cis_ocropy_segment.out, ocr_d_dewarp)    calamari_recognize(params.mets, cis_ocropy_dewarp.out, ocr_d_oc)}Code exampleCheck this source code example: seq_ocrd_wf_many.nfTODO: We will provide more structure-related details here based on the example above.For users and developers:Detailed instructions for local executions and example Nextflow workflow scripts can be found here: NextflowConvert the existing OCR-D process workflows we reference to NFYou can use OtoN (OCR-D to Nextflow) converter which converts basic OCR-D process workflows to Nextflow workflow scripts. Check here: OtoNIt is still very fresh and there are still known problems (related to the produced Nextflow scripts) and we are trying to fix them. It is also not convenient to use (no proper CLI and no usage instructions yet). Stay tuned for more updates.Most edge cases for the lexer/parser of the OCR-D process file have been tested while implementing. There may be input OCR-D process files that are not handled well enough. Feel free to report any bugs, errors, or lack of errors (when an error is expected).The tool will probably be a part of the OCR-D software in the future when it is stable enough for general use.Conventions for Nextflow scriptsTry to stick to the structure provided in section Structure of the Nextflow script when writing Nextflow scripts. You can also check the Nextflow examples provided in section Main workflow. The naming conventions for variables, function names, process names, and workflow names are encouraged to follow the snake case.For developers: Nextflow implementation in OCR-D related projectsThe minimally used features for local runs are the parameters, processes, process decorators, and workflows.ParallelizationA Nextflow workflow script contains several processes. Processes are executed independently and are isolated from each other (i.e. they do not have a shared memory space). Communication between the processes is possible only through data channels (similar to the pipes model in Unix). These channels are basically asynchronous FIFO queues. Any process can define one or more channels as input and output. The order of interaction between these processes, and ultimately the order of workflow execution depends on the communication channel dependencies between processes. For example, if process A writes data to channel A and process B reads data from channel A, then Nextflow knows that process A must be executed before process B.Check this source code example: seq_ocrd_wf_many.nfTODO: We will provide more parallelization details here based on the example above.Interaction with the processing serverAs of Aug 2022 the processing server implementation in OCR-D/core is not yet finished, cf. https://github.com/OCR-D/core/pull/884 and https://github.com/OCR-D/core/pull/652. The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.Interaction with the METS serverAs of August, 2022, the METS server implementation is still unfinished.The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.",
      "url": " /en/spec/nextflow.html"
    },
  

    {
      "slug": "de-nextflow-html",
      "title": "Web Api",
      "content"	 : "NextflowWhat is NF and why did we choose it?Nextflow is a workflow framework that allows the integration of various scripting languages into a single cohesive pipeline. Nextflow also has its own Domain Specific Language (DSL) that extends Groovy (extension of Java).We choose it due to its rich set of features:  Stream oriented: Promotes programming approach extending Unix pipes model.  Fast Prototyping: Let’s you write a computational pipeline from smaller tasks.  Reproducibility: Supports Docker, Singularity, and 3 other types of containers.  Portable: Can run locally, Slurm, SGE, PBS, and cloud (Google, Kubernetes, and AWS).  Continuous checkpoints: Each process in the workflow is checkpointed. It is possible to retry failed workflows and start from the last checkpoint.  Supports various scripting languages including Bash, Python, Perl, and others.  Enables separation between configuration (how to do) and workflow logic (what to do).  Modularization of tasks possible via workflow, sub-workflows, and processes.  Provides detailed logs and various types of execution reports.How is the NF script structured?The NF script contains the following structures:DSL and ParametersDefinition of processesDefinition of workflowsMain workflowCheck this source code example: seq_ocrd_wf_many.nfTODO: I will provide more structure-related details here based on the example above.Which features of NF do we use, i.e. what features have to be implemented in potential implementations?The minimally used features for local runs are the parameters, processes, process decorators, and workflows. I will provide further answers to any following questions related to this main question. I am not sure what else to cover here for now.How does parallelization work, both within works and across works?A Nextflow workflow script contains several processes. Processes are executed independently and are isolated from each other (i.e. they do not have a shared memory space). Communication between the processes is possible only through data channels (similar to the pipes model in Unix). These channels are basically asynchronous FIFO queues. Any process can define one or more channels as input and output. The order of interaction between these processes, and ultimately the order of workflow execution depends on the communication channel dependencies between processes. For example, if process A writes data to channel A and process B reads data from channel A, then Nextflow knows that process A must be executed before process B.Check this source code example: seq_ocrd_wf_many.nfTODO: I will provide more parallelization details here based on the example above.How does the NF script interact with the processing server?There is still no running processing server. More details will be announced once there is more to talk about. The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.How does the NF script interact with the METS server?There is still no running METS server. More details will be announced once there is more to talk about. The interaction will most probably happen with curl through a bash script inside the Nextflow process. Of course, if it is integrated inside the OCR-D core, then no direct interactions will be needed from inside the Nextflow script.How to convert the existing OCR-D process workflows we reference to NF?I have written an OtoN (OCR-D to Nextflow) converter which converts basic OCR-D process workflows to Nextflow workflow scripts. Check here: OtoNCurrently, there are no known issues or bugs. Feel free to report any bugs, errors, or lack of errors (when an error is expected). The tool will probably be a part of the OCR-D software in the future when it is stable enough for general use.How should NF scripts be written, tested, deployed, and evaluated?Depends on the use case. Detailed instructions for local executions and example Nextflow workflow scripts can be found here: NextflowI will provide further answers to any following questions related to this main question.What conventions do we encourage, naming, structure, documentation, etc.?Try to stick to the structure provided in point 2 when writing Nextflow scripts. You can also check the Nextflow examples provided in point 8. The naming conventions for variables, function names, process names, and workflow names are encouraged to follow the snake case. I will provide further answers to any following questions related to this main question.",
      "url": " /de/nextflow.html"
    },
  

    {
      "slug": "en-spec-ocrd-eval-html",
      "title": "Quality Assurance in OCR-D",
      "content"	 : "Quality Assurance in OCR-DRationaleEvaluating the quality of OCR requires comparing the OCR results on representative ground truth (GT)– i.e. realistic data (images) with manual transcriptions (segmentation, text).OCR results can be obtained via several distinct OCR workflows.The comparison requires evaluation tools which themselves build on a number of establishedevaluation metrics.The evaluation results must be presented in a way that allows factorising and localising aberrations,both within documents (page types, individual pages, region types, individual regions) and across classes of similar documents.All this needs to  work together in a well-defined and automatically repeatable manner, so users can make informed decisions about which OCR workflow works best for which material and use case.Evaluation MetricsThe evaluation of the quality (accuracy and precision) of OCR is a complex task, for which multiple methods and metrics are available.It needs to capture several aspects corresponding to the interdependent subtasks of an OCR workflow, viz. layout analysis and text recognition, which themselves require different methods and metrics.Furthermore, the time and resources required for OCR processing also have to be captured. Here we describe the metrics that were selected for use in OCR-D, how exactly they are applied, and what was the motivation.Scope of These DefinitionsAt this stage (Q3 2022) these definitions serve as a basis of common understanding for the metrics used in the benchmarking presented in OCR-D QUIVER. Further implications for evaluation tools do not yet apply.Text EvaluationThe most important measure to assess the quality of OCR is the accuracy of the recognized text.The majority of metrics for this are based on the Levenshtein distance, an algorithm to compute the distance between two strings.In OCR, one of these strings is generally the Ground Truth text and the other the recognized text which is the result of an OCR.The text is concatenated at page level from smaller constituents in reading order.CharactersA text consists of a set of characters that have a certain meaning.In OCR-D, a character is technically defined as a grapheme cluster, i.e. one or more Unicode (or Private Use Area) codepoint(s) that represents an element of a writing system in NFC (see Unicode Normalization).White spaces are considered as characters.Special codepoints like Byte-Order Marks or directional marks are ignored.Examples  the character ä in the word Kälte is encoded by Unicode U+00E4  the character ܡܿ in the word ܡܿܢ is encoded by Unicode U+0721 + U+073FLevenshtein Distance (Edit Distance)Levenshtein distance between two strings is defined as the (minimum) number of (single-character) edit operations needed to turn the one into the other.Edit operations depend on the specific variant of the algorithm but for OCR, relevant operations are deletion, insertion and substitution.To calculate the edit distance, the two strings first have to be (optimally) aligned.The Levenshtein distance forms the basis for the calculation of CER/WER.As there are different implementations of the edit distance available (e.g. rapidfuzz, jellyfish, …), the OCR-D coordination project will provide a recommendation in the final version of this document.ExampleGiven a Ground truth that reads ſind and the recognized text fmd.The Levenshtein distance between these texts is 3, because 3 single-character edit operations are necessary to turn fmd into ſind. For example:  fmd –&amp;gt; ſmd (substitution)  ſmd –&amp;gt; ſimd (insertion)  ſimd –&amp;gt; ſind (substitution)CER and WERCharacter Error Rate (CER)The character error rate (CER) is defined as the quotient of the edit distance over the lengthwith respect to the character string pair of GT and OCR text. It thus describes an empirical estimateof the probability of some random character to be misrecognised.Thus, CER defines a (single-character) error in terms of the above three categories of edit operations:  deletion: a character that is present in the text has been deleted from the output.Example:This reads Sonnenfinſterniſſe:. The output contains Sonnenfinſterniſſe, deleting :.  substitution: a character is replaced by another character in the output.Example:This heading reads Die Finſterniſſe des 1801ſten Jahrs. The output contains 180iſten, replacing 1 with i.  insertion: a new character is introduced in the output.Example:This reads diese Strahlen, und. The output contains Strahlen ,, inserting a white space before the comma.CER can be defined in multiple ways, depending on what exactly counts as the length of the text.Given $i$ as the number of insertions, $d$ the number of deletions, $s$ the number of substitutions of the OCR text,and $n$ the total number of characters of the GT text, the CER can be obtained by$CER = frac{i + s+ d}{n}$If the CER value is calculated this way, it represents the percentage of characters incorrectly recognized by the OCR engine. Also, we can easily reach error rates beyond 100% when the output contains a lot of insertions.Sometimes, this is mitigated by defining $n$ as the maximum of both lengths, or by clipping the rate at 100%.Neither of these strategies yields an unbiased estimate.The normalized CER avoids this effect by considering the number of correct characters (or identity operations), $c$:$CER_n = frac{i + s+ d}{i + s + d + c}$In OCR-D’s benchmarking we calculate the normalized CER where values naturally range between 0 and 100%.CER GranularityIn OCR-D we distinguish between the CER per page and the overall CER of a document. The reasoning behind this is that the material OCR-D mainly aims at (historical prints) is very heterogeneous: Some pages might have an almost simplistic layout while others can be highly complex and difficult to process. Providing only an overall CER would cloud these differences between pages.Currently we only provide CER per page; higher-level CER results might be calculated as a weighted aggregate at a later stage.Word Error Rate (WER)Word error rate (WER) is analogous to CER: While CER operates on (differences between) characters,WER measures the percentage of incorrectly recognized words in a text.A word in that context is usually defined as any sequence of characters between white space (including line breaks), with leading and trailing punctuation removed (according to Unicode TR29 Word Boundary algorithm).CER and WER share categories of errors, and the WER is similarly calculated:$WER = frac{i_w + s_w + d_w}{i_w + s_w + d_w + c_w}$where $i_w$ is the number of inserted, $s_w$ the number of substituted, $d_w$ the number of deleted and $c_w$ the number of correct words.More specific cases of WER consider only the “significant” words, omitting e.g. stopwords from the calculation.WER GranularityIn OCR-D we distinguish between the WER per page and the overall WER of a document. The reasoning here follows the one of CER granularity.Currently we only provide WER per page; higher-level WER results might be calculated at a later stage.Bag of WordsIn the “Bag of Words” (BaW) model, a text is represented as a multiset of the words (as defined in the previous section) it contains, regardless of their order.Example:  Eine Mondfinsternis ist die Himmelsbegebenheit welche sich zur Zeit des Vollmondes ereignet, wenn die Erde zwischen der Sonne und dem Monde steht, so daß die Strahlen der Sonne von der Erde aufgehalten werden, und daß man so den Schatten der Erde in dem Monde siehet. In diesem Jahre sind zwey Monfinsternisse, davon ist ebenfalls nur Eine bey uns sichtbar, und zwar am 30sten März des Morgens nach 4 Uhr, und währt bis nach 6 Uhr.To get the Bag of Words of this paragraph a multiset containing each word and its number of occurrence is created:$BoW_{GT}$ ={    &quot;Eine&quot;: 2, &quot;Mondfinsternis&quot;: 1, &quot;ist&quot;: 2, &quot;die&quot;: 2, &quot;Himmelsbegebenheit&quot;: 1,     &quot;welche&quot;: 1, &quot;sich&quot;: 1, &quot;zur&quot;: 1,  &quot;Zeit&quot;: 1, &quot;des&quot;: 2, &quot;Vollmondes&quot;: 1,    &quot;ereignet,&quot;: 1, &quot;wenn&quot;: 1, &quot;Erde&quot;: 3, &quot;zwischen&quot;: 1, &quot;der&quot;: 4, &quot;Sonne&quot;: 2,    &quot;und&quot;: 4, &quot;dem&quot;: 2, &quot;Monde&quot;: 2, &quot;steht,&quot;: 1, &quot;so&quot;: 2, &quot;daß&quot;: 2,     &quot;Strahlen&quot;: 1, &quot;von&quot;: 1, &quot;aufgehalten&quot;: 1, &quot;werden,&quot;: 1, &quot;man&quot;: 1, &quot;den&quot;: 1,     &quot;Schatten&quot;: 1, &quot;in&quot;: 1, &quot;siehet.&quot;: 1, &quot;In&quot;: 1, &quot;diesem&quot;: 1, &quot;Jahre&quot;: 1,     &quot;sind&quot;: 1, &quot;zwey&quot;: 1, &quot;Monfinsternisse,&quot;: 1, &quot;davon&quot;: 1, &quot;ebenfalls&quot;: 1, &quot;nur&quot;: 1,     &quot;bey&quot;: 1, &quot;uns&quot;: 1, &quot;sichtbar,&quot;: 1, &quot;zwar&quot;: 1, &quot;am&quot;: 1, &quot;30sten&quot;: 1,     &quot;März&quot;: 1, &quot;Morgens&quot;: 1, &quot;nach&quot;: 2, &quot;4&quot;: 1, &quot;Uhr,&quot;: 1, &quot;währt&quot;: 1,     &quot;bis&quot;: 1, &quot;6&quot;: 1, &quot;Uhr.&quot;: 1}Bag-of-Words Error RateBased on the above concept, the Bag-of-Words Error Rate is defined as the sum over the modulus of the GT count minus OCR count of each word, divided by the sum total of words in GT and OCR.The BoW error therefore describes how many words are misrecognized (positively or negatively), independent of a page’s layout (order/segmentation).[BWE = frac{|BoW_{GT} - BoW_{OCR}|}{ {n_w}_{GT} + {n_w}_{OCR} }]ExampleGiven the GT text der Mann steht an der Ampel, recognised by OCR as cer Mann fteht an der Ampel:[BoW_{GT} = { text{Ampel}: 1, text{an}: 1, text{der}: 2, text{Mann}: 1, text{steht}: 1 }]and[BoW_{OCR} = { text{Ampel}: 1, text{an}: 1, text{cer}: 1, text{der}: 1, text{Mann}: 1, text{fteht}: 1 }]results in:[BWE = frac{|1 - 1| + |1 - 1| + |2 - 1| + |0 - 1| + |1 - 1| + |1 - 0| + |0 - 1|}{12} = frac{0 + 0 + 1 + 1 + 0 + 1 + 1}{12} = 0.33]In this example, 66% of the words have been correctly recognized.Layout EvaluationA good text segmentation is the basis for measuring text accuracy.An example can help to illustrate this:Given in a document containing two columns these two columns are detected by layout analysis as just one.The OCR result will then contain the text for the first lines of the first and second column, followed by the second lines of the first and second column asf. which does not correspond to the sequence of words and paragraphs given in the Ground Truth.Even if all characters and words may be recognized correctly, all downstream processes to measure text accuracy will be defeated.While the comprehensive evaluation of OCR with consideration of layout analysis is still a research topic, several established metrics can be used to capture different aspects of it.For pragmatic reasons we set aside errors resulting from misdetecting the reading order for the moment (though this might be implemented in the future).Any layout evaluation in the context of OCR-D focuses on region level which should be sufficient for most use cases.Reading Order (Definition)Reading order describes the order in which segments on a page are intended to be read. While the reading order might be easily obtained in monographs with a single column where only a few page segments exist, identifying the reading order in more complex layouts (e.g. newspapers or multi-column layouts) can be more challenging.Example of a simple page layout with reading order:(http://resolver.sub.uni-goettingen.de/purl?PPN1726778096)Example of a complex page layout with reading order:(http://resolver.sub.uni-goettingen.de/purl?PPN1726778096)See Reading Order Evaluation for the actual metric.IoU (Intersection over Union)Intersection over Union is a term which describes the degree of overlap of two regions of a (document) image defined either by a bounding box or polygon. Example:(where green represents the Ground Truth and red the detected bounding box)Given a region A with an area $area_1$, a region B with the area $area_2$, and their overlap (or intersection) $area_o$, the IoU can then be expressed as$IoU = frac{area_o}{area_1+area_2-area_o}$where $area_1+area_2-area_o$ expresses the union of the two regions ($area_1+area_2$) while not counting the overlapping area twice.The IoU ranges between 0 (no overlap at all) and 1 (the two regions overlap perfectly). Users executing object detection can choose a threshold that defines which degree of overlap must be given to define a prediction as correct. If e.g. a threshold of 0.6 is chosen, all prediction that have an IoU of 0.6 or higher are correct.In OCR-D we use IoU to measure how well segments on a page are recognized during the segmentation step. The area of one region represents the area identified in the Ground Truth, while the second region represents the area identified by an OCR-D processor.Resource UtilizationLast but not least, it is important to collect information about the resource utilization of each processing step, so that informed decisions can be made when e.g. having to decide between results quality and throughput speed.CPU TimeCPU time is the time taken by the CPU(s) on the processors. It does not include idle time, but does grow with the number of threads/processes.Wall TimeWall-clock time (or elapsed time) is the time taken on the processors including idle time but ignoring concurrency.I/OI/O (input / output) bandwidth is the (average/peak) number of bytes per second read and written from disk during processing.Memory UsageMemory usage is the (average/peak) number of bytes the process allocates in memory (RAM), i.e. resident set size (RSS) or proportional set size (PSS).Disk UsageDisk usage is the total number of bytes the process reads and writes on disk.Unicode NormalizationIn Unicode there can be multiple ways to express characters that have multiple components, such as a base letter and an accent. For evaluation it is essential that both Ground Truth and OCR results are normalized in the same way before evaluation.For example, the letter ä can be expressed directly as ä (U+00E4 in Unicode) or as a combination of a and ◌̈ (U+0061 + U+0308). Both encodings are semantically equivalent but technically different.Unicode has the notion of normalization forms to provide canonically normalized text. The most common forms are NFC (Normalization Form Canonical Composed) and NFD (Normalization Form Canonical Decomposed). When a Unicode string is in NFC, all decomposed codepoints are replaced with their decomposed equivalent (e.g. U+0061 + U+0308 to U+00E4). In an NFD encoding, all decomposed codepoints are replaced with their composed equivalents (e.g. U+00E4 to U+0061 + U+0308).In accordance with the concept of GT levels in OCR-D, it is preferable for strings to be normalized as NFC.The Unicode normalization algorithms rely on data from the Unicode database on equivalence classes and other script- and language-related metadata. For graphemes from the Private Use Area (PUA), such as MUFI, this information is not readily available and can lead to inconsistent normalization. Therefore, it is essential that evaluation tools normalize PUA codepoints in addition to canonical Unicode normalization.Metrics Not in Use YetThe following metrics are not part of the MVP (minimal viable product) and will (if ever) be implemented at a later stage.GPU MetricsGPU Avg MemoryGPU avg memory refers to the average amount of memory of the GPU (in GiB) that was used during processing.GPU Peak MemoryGPU peak memory is the maximum GPU memory allocated during the execution of a workflow in MB.Text EvaluationLetter AccuracyLetter Accuracy is a metric that focuses on a pre-defined set of characters classes for evaluation while ignoring others.Letters in a common sense do not include white spaces and punctuations or Arabic and Indic digits.Furthermore, even letter capitalization might be ignored.The relevant character classes must be removed from both the candidate text and the ground truth before evaluation.Letter Accuracy can be calculated as follows:            Let $      L_{GT}      $ be the number of relevant letters in the ground truth, $      L_{r}      $ the number of recognized letters, then                  $LA = 1 - frac{      L_{GT}      -      L_{r}      }{      L_{GT}      }$      Flexible Character Accuracy MeasureThe Flexible Character Accuracy (FCA) measure has been introduced to mitigate a major drawback of CER:CER (if applied naively by comparing concatenated page-level texts) is heavily dependent on the reading order an OCR engine detects.Thus, where text blocks are rearranged or merged, no suitable text alignment can be made, so CER is very low,even if single characters, words and even lines have been perfectly recognized.FCA avoids this by splitting the recognized text and GT into lines and, if necessary, sub-line chunks,finding pairs that align maximally until only unmatched lines remain (which must be treated as errors),and measuring average CER of all pairs.The algorithm can be summarized as follows:      Split both input texts into text lines    Sort the GT lines by length  (in descending order)    For the top GT line, find the best fully or partially matching OCR line  (by lowest edit distance and highest coverage)    If full match (i.e. full length of line) a. Mark as done and remove line from both lists b. Else mark matching part as done,     then cut off unmatched part and add to respective list of text lines; resort    If any more lines available repeat step 3    Count remaining unmatched lines as insertions or deletions (depending on origin – GT or OCR)    Calculate the (micro-)average CER of all marked pairs and return as overall FCER  (paraphrase of C. Clausner, S. Pletschacher and A. Antonacopoulos / Pattern Recognition Letters 131 (2020) 390–397, p. 392)Layout EvaluationReading Order EvaluationClausner, Pletschacher and Antonacopoulos 2013 propose a method to evaluate reading order by classifying relations between any two regions:direct or indirect successor / predecessor, unordered, undefined.Next, text regions on both sides, ground truth and detected reading order, are matched and assigned (depending on overlap area). A GT region can have multiple corresponding detections. Then, for each pair of regions, the relation typeon GT is compared to the relation types of the corresponding predictions. Any deviation introduces costs,depending both on the kind of relation (e.g. direct vs indirect, or successor vs predecessor)and the relative size of the overlap.The authors introduce a predefined penalty matrix where the cost for each misclassification is given.(Direct opposition is more expensive than indirect.)For example, if the relation given in GT is “somewhere after (but unordered group involved)”,but the detected relation is “directly before”, then the penalty will be lower (10) thanif the GT relation is “directly after” (40) – because the latter is more specific than the former.To calculate the success measure $s$ of the detected reading order, first the costs obtained from comparing all GT to all detected relations are summed up ($e$).Then this error value is normalised by the hypothetical error value at 50% agreement ($e_{50}$):$e_{50} = p_{max} * n_{GT} / 2$where $p_{max}$ is the highest single penalty and $n_{GT}$ is the number of regions in the ground truth.The success measure is then given by$s = frac{1}{e * (1/e_{50}) + 1}$mAP (mean Average Precision)This score was originally devised for object detection in photo scenery (where overlaps are allowed and cannot conflict with text flow).It is not adequate for document layout for various reasons, but since it is a standard metric in the domain of neural computer vision,methods and tools of which are increasingly used for layout analysis as well, it is still somewhat useful for reference.The following paragraphs will first introduce the intermediate concepts needed to define the mAP metric itself.Precision and RecallPrecision describes to which degree the predictions of a model are correct.The higher the precision of a model, the more confidently we can assume that each prediction is correct(e.g. the model having identified a bicycle in an image actually depicts a bicycle).A precision of 1 (or 100%) indicates all predictions are correct (true positives) and no predictions are incorrect (false positives). The lower the precision value, the more false positives.In the context of object detection in images, it measures either  the ratio of correctly detected segments over all detected segments(where correct is defined as having sufficient overlap with some GT segment), or  the ratio of correctly segmented pixels over the image size(assuming all predictions can be combined into some coherent segmentation).Recall, on the other hand,  describes to which degree a model predicts what is actually present.The higher the recall of a model, the more confidently we can assume that it covers everything to be found(e.g. the model having identified every bicycle, car, person etc. in an image).A recall of 1 (or 100%) indicates that all objects have a correct prediction (true positives) and no predictions are missing or mislabelled (false negatives). The lower the recall value, the more false negatives.In the context of object detection in images, it measures either  the ratio of correctly detected segments over all actual segments, or  the ratio of correctly segmented pixels over the image size.Notice that both goals are naturally conflicting each other. A good predictor needs both high precision and recall.But the optimal trade-off depens on the application.For layout analysis though, the underlying notion of sufficient overlap itself is inadequate:  it does not discern oversegmentation from undersegmentation  it does not discern splits/merges that are allowable (irrelevant w.r.t. text flow) or not (break up or conflate lines)  it does not discern foreground from background, or when partial overlap starts breaking character legibility or introducing ghost charactersPrediction ScoreMost types of model can output a confidence score alongside each predicted object,which represents the model’s certainty that the prediction is correct.For example, when a model tries to identify ornaments on a page, if it returns a segment (polygon / mask)with a prediction score of 0.6, the model asserts there is a 60% probability that there is an ornament at that location.Whether this prediction is then considered to be a positive detection, depends on the chosen threshold.IoU ThresholdsFor object detection, the metrics precision and recall are usually defined in terms of a threshold for the degree of overlap(represented by the IoU as defined above), ranging between 0 and 1)above which pairs of detected and GT segments are qualified as matches.(Predictions that are non-matches across all GT objects – false positives – and GT objects that are non-matches across all predictions – false negatives – contribute indirectly in the denominator.)Example:Given a prediction threshold of 0.8, an IoU threshold of 0.6 and a model that tries to detect bicycles in an image which depicts two bicycles.The model returns two areas in an image that might be bicycles, one with a confidence score of 0.4 and one with 0.9. Since the prediction threshold equals 0.8, the first candidate gets immediately tossed out. The otheris compared to both bicycles in the GT. One GT object is missed (false negative), the other intersects the remaining prediction, but the latter is twice as large.Therefore, the union of that pair is more than double the intersection. But since the IoU threshold equals 0.6, even the second candidate is not regarded as a match and thus also counted as false negative. Overall, both precision and recall are zero (because 1 kept prediction is a false positive and 2 GTs are false negatives).Precision-Recall CurveBy varying the prediction threshold (and/or the IoU threshold), the tradeoff between precision and recall can be tuned.When the full range of combinations has been gauged, the result can be visualised in a precision-recall curve (or receiver operator characteristic, ROC).Usually the optimum balance is where the product of precision and recall (i.e. area under the curve) is maximal.Given a dataset with 100 images in total of which 50 depict a bicycle. Also given a model trying to identify bicycles on images. The model is run 7 times using the given dataset while gradually increasing the threshold from 0.1 to 0.7.            run      threshold      true positives      false positives      false negatives      precision      recall                  1      0.1      50      25      0      0.66      1              2      0.2      45      20      5      0.69      0.9              3      0.3      40      15      10      0.73      0.8              4      0.4      35      5      15      0.88      0.7              5      0.5      30      3      20      0.91      0.6              6      0.6      20      0      30      1      0.4              7      0.7      10      0      40      1      0.2      For each threshold a pair of precision and recall can be computed and plotted to a curve:This graph is called Precision-Recall-Curve.Average PrecisionAverage Precision (AP) describes how well (flexible and robust) a model can detect objects in an image,by averaging precision over the full range (from 0 to 1) of confidence thresholds (and thus, recall results).It is equal to the area under the Precision-Recall Curve.The Average Precision can be computed with the weighted mean of precision at each confidence threshold:$AP = displaystylesum_{k=0}^{k=n-1}[r(k) - r(k+1)] * p(k)$with $n$ being the number of thresholds and $r(k)/p(k)$ being the respective recall/precision values for the current confidence threshold $k$.Example:Given the example above, we get:[begin{array}{ll}AP &amp;amp;  = displaystylesum_{k=0}^{k=n-1}[r(k) - r(k+1)] * p(k) &amp;amp; = displaystylesum_{k=0}^{k=6}[r(k) - r(k+1)] * p(k) &amp;amp; = (1-0.9) * 0.66 + (0.9-0.8) * 0.69 + text{...} + (0.2-0) * 1&amp;amp; = 0.878end{array}]Usually, AP calculation also involves smoothing (i.e. clipping local minima) and interpolation (i.e. adding data points between the measured confidence thresholds).Mean Average PrecisionMean Average Precision (mAP) is a metric used to measure the full potential of an object detector over various conditions.AP is merely an average over confidence thresholds. But as stated earlier, the IoU threshold can be chosen freely,so AP only reflects the performance under that particular choice. In general though, how accurately every object must be matched may depend on the use-case, and on the class or size of the objects.That’s why the mAP metric has been introduced: It is calculated by computing the AP over a range of IoU thresholds, and averaging over them:$mAP = displaystylefrac{1}{N}sum_{i=1}^{N}AP_i$ with $N$ being the number of thresholds.Often, this mAP for a range of IoU thresholds gets complemented by additional mAP runs for a set of fixed values, or for various classes and object sizes only.The common understanding is that those different measures collectively allow drawing better conclusions and comparisons about the model’s quality.Scenario-Driven Performance EvaluationScenario-driven, layout-dedicated, text-flow informed performance evaluation as described inClausner et al., 2011is currently the most comprehensive and sophisticated approach to evaluate the quality of layout analysis.It is not a single metric, but comprises a multitude of measures derived in a unified method, which considersthe crucial effects that segmentation can have on text flow, i.e. which kinds of overlaps (merges and splits)amount to benign deviations (extra white-space) or pathological ones (breaking lines and words apart).In this approach, all the derived measures are aggregated under various sets of weights, called evaluation scenarios,which target specific use cases (like headline or keyword extraction, linear fulltext, newspaper or figure extraction).Evaluation JSON schemaThe results of an evaluation should be expressed in JSON according tothe ocrd-eval.json.ToolsSee OCR-D workflow guide.References  CER/WER:          https://sites.google.com/site/textdigitisation/qualitymeasures      https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510#5aec        IoU:          https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef        mAP:          https://blog.paperspace.com/mean-average-precision/      https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173        BoW:          https://en.wikipedia.org/wiki/Bag-of-words_model        FCA:          https://www.primaresearch.org/www/assets/papers/PRL_Clausner_FlexibleCharacterAccuracy.pdf        Letter Accuary:          https://www.o-bib.de/bib/article/view/5888/8845        Reading Order Evaluation:          https://www.primaresearch.org/www/assets/papers/ICDAR2013_Clausner_ReadingOrder.pdf        More background on evaluation of OCR          https://doi.org/10.1145/3476887.3476888      https://doi.org/10.1515/9783110691597-009      ",
      "url": " /en/spec/ocrd_eval.html"
    },
  

    {
      "slug": "en-spec-ocrd-tool-html",
      "title": "ocrd-tool.json",
      "content"	 : "ocrd-tool.jsonTools MUST be described in a file ocrd-tool.json in the root of the repository.It must contain a JSON object adhering to the ocrd-tool JSON Schema.In particular, every tool provided must be described in an array item under thetools key. These definitions drive the CLI and the webservices.To validate a ocrd-tool.json file, use ocrd ocrd-tool /path/to/ocrd-tool.json validate.File parametersTo mark a parameter as expecting the address of a file, it MUST declare thecontent-type property as a valid mediatype.Optionally, workflow processors can be notified that this file is potentiallylarge and static (e.g. a fixed dataset or a precomputed model) and should be cachedindefinitely after download by setting the cacheable property to true.The filename itself, i.e. the concrete value &amp;lt;fpath&amp;gt; of a file parameter,should be resolved in the following way (with &amp;lt;cwd&amp;gt; representing the current working directory, and &amp;lt;mod&amp;gt; representing the distribution directory of the module):  If &amp;lt;fpath&amp;gt; is an http/https URL: Download to a temporary directory (ifcacheable==False) or a semi-temporary cache directory (if cacheable==True)  If &amp;lt;fpath&amp;gt; is an absolute path: Use as-is.  If &amp;lt;fpath&amp;gt; is a relative path (with or without directory components):Try resolving via the following directories, and return the first one found if any,otherwise abort with an error message stating so:          &amp;lt;cwd&amp;gt;/&amp;lt;fpath&amp;gt; (Note that the file is expected to be directly under &amp;lt;cwd&amp;gt;, not in a subdirectory)      If an environment variable is defined that has the name of the processor inupper-case and with - replaced with - and followed by _PATH (e.g. for a processorocrd-dummy, the variable would need to be called OCRD_DUMMY_PATH):                  Split the variable value at : and try to resolve by appending &amp;lt;fpath&amp;gt;to each token and return the first found file if any                    $XDG_DATA_HOME/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt; (with $HOME/.local/share instead of $XDG_DATA_HOME if unset)      /usr/local/share/ocrd-resources/&amp;lt;name-of-processor&amp;gt;/&amp;lt;fpath&amp;gt;      &amp;lt;mod&amp;gt;/&amp;lt;fpath&amp;gt; (Note that the file is expected to be directly under &amp;lt;mod&amp;gt;, not in a subdirectory)      The path of the &amp;lt;mod&amp;gt; directory is implementation specific and allows modulesto distribute small resources along with the code, i.e. pre-installed files likepresets.Input / Output file groupsTools should define the names of both expected input and produced output filegroups as a list of USE attributes of mets:fileGrp elements. If more thanone file group is expected or produced, this should be explained in thedescription of the tool.NOTE: Both input and output file groups can be overridden atruntime. Tools must therefore ensure not tohardcode file group names. When multiple groups are expected, the order of theoverride reflects the order in which they are defined in the ocrd-tool.json.Definitiontype: objectdescription: Schema for tools by OCR-D MPrequired:  - version  - git_url  - toolsadditionalProperties: falseproperties:  version:    description: &quot;Version of the tool, expressed as MAJOR.MINOR.PATCH.&quot;    type: string    pattern: &#39;^[0-9]+.[0-9]+.[0-9]+$&#39;  git_url:    description: GitHub/GitLab URL    type: string    format: url  dockerhub:    description: DockerHub image    type: string  tools:    type: object    additionalProperties: false    patternProperties:      &#39;ocrd-.*&#39;:        type: object        additionalProperties: false        required:          - description          - steps          - executable          - categories          - input_file_grp          # Not required because not all processors produce output files          # - output_file_grp        properties:          executable:            description: The name of the CLI executable in $PATH            type: string          input_file_grp:            description: Input fileGrp@USE this tool expects by default            type: array            items:              type: string              # pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          output_file_grp:            description: Output fileGrp@USE this tool produces by default            type: array            items:              type: string              # pattern: &#39;^OCR-D-[A-Z0-9-]+$&#39;          parameters:            description: Object describing the parameters of a tool. Keys are parameter names, values sub-schemas.            type: object            default: {}            patternProperties:              &quot;.*&quot;:                type: object                additionalProperties: false                required:                  - description                  - type                  # also either &#39;default&#39; or &#39;required&#39;                properties:                  type:                    type: string                    description: Data type of this parameter                    enum:                      - string                      - number                      - boolean                      - object                      - array                  format:                    description: Subtype, such as `float` for type `number` or `uri` for type `string`.                  description:                    description: Concise description of syntax and semantics of this parameter                  items:                    type: object                    description: describe the items of an array further                  minimum:                    type: number                    description: Minimum value for number parameters, including the minimum                  maximum:                    type: number                    description: Maximum value for number parameters, including the maximum                  exclusiveMinimum:                    type: number                    description: Minimum value for number parameters, excluding the minimum                  exclusiveMaximum:                    type: number                    description: Maximum value for number parameters, excluding the maximum                  multipleOf:                    type: number                    description: For number values, those values must be multiple of this number                  properties:                    type: object                    description: Describe the properties of an object value                  additionalProperties:                    type: boolean                    description: Whether an object value may contain properties not explicitly defined                  required:                    type: boolean                    description: Whether this parameter is required                  default:                    description: Default value when not provided by the user                  enum:                    type: array                    description: List the allowed values if a fixed list.                  content-type:                    type: string                    default: &#39;application/octet-stream&#39;                    description: &amp;gt;                      The media type of resources this processor expects for                      this parameter. Most processors use files for resources                      (e.g.  `*.traineddata` for `ocrd-tesserocr-recognize`)                      while others use directories of files (e.g. `default` for                      `ocrd-eynollah-segment`).  If a parameter requires                      directories, it must set `content-type` to                      `text/directory`.                  cacheable:                    type: boolean                    description: &quot;If parameter is reference to file: Whether the file should be cached, e.g. because it is large and won&#39;t change.&quot;                    default: false          description:            description: Concise description of what the tool does          categories:            description: Tools belong to these categories, representing modules within the OCR-D project structure            type: array            items:              type: string              enum:                - Image preprocessing                - Layout analysis                - Text recognition and optimization                - Model training                - Long-term preservation                - Quality assurance          steps:            description: This tool can be used at these steps in the OCR-D functional model            type: array            items:              type: string              enum:                - preprocessing/characterization                - preprocessing/optimization                - preprocessing/optimization/cropping                - preprocessing/optimization/deskewing                - preprocessing/optimization/despeckling                - preprocessing/optimization/dewarping                - preprocessing/optimization/binarization                - preprocessing/optimization/grayscale_normalization                - recognition/text-recognition                - recognition/font-identification                - recognition/post-correction                - layout/segmentation                - layout/segmentation/text-nontext                - layout/segmentation/region                - layout/segmentation/line                - layout/segmentation/word                - layout/segmentation/classification                - layout/analysis          resource_locations:            type: array            description: The locations in the filesystem this processor supports for resource lookup            default: [&#39;data&#39;, &#39;cwd&#39;, &#39;system&#39;, &#39;module&#39;]            items:              type: string              enum: [&#39;data&#39;, &#39;cwd&#39;, &#39;system&#39;, &#39;module&#39;]          resources:            type: array            description: Resources for this processor            items:              type: object              additionalProperties: false              required:                - url                - description                - name                - size              properties:                url:                  type: string                  description: URLs of all components of this resource                description:                  type: string                  description: A description of the resource                name:                  type: string                  description: Name to store the resource as                type:                  type: string                  enum: [&#39;file&#39;, &#39;directory&#39;, &#39;archive&#39;]                  default: file                  description: Type of the URL                parameter_usage:                  type: string                  description: Defines how the parameter is to be used                  enum: [&#39;as-is&#39;, &#39;without-extension&#39;]                  default: &#39;as-is&#39;                path_in_archive:                  type: string                  description: If type is archive, the resource is at this location in the archive                  default: &#39;.&#39;                version_range:                  type: string                  description: Range of supported versions, syntax like in PEP 440                  default: &#39;&amp;gt;= 0.0.1&#39;                size:                  type: number                  description: &quot;Size of the resource in bytes to be retrieved (for archives: size of the archive)&quot;ExampleThis is from the ocrd_tesserocr project:{  &quot;version&quot;: &quot;0.10.0&quot;,  &quot;git_url&quot;: &quot;https://github.com/OCR-D/ocrd_tesserocr&quot;,  &quot;dockerhub&quot;: &quot;ocrd/tesserocr&quot;,  &quot;tools&quot;: {    &quot;ocrd-tesserocr-deskew&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-deskew&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Detect script, orientation and skew angle for pages or regions&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-DESKEW-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/deskewing&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;page&quot;,&quot;region&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        },        &quot;min_orientation_confidence&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;default&quot;: 1.5,          &quot;description&quot;: &quot;Minimum confidence score to apply orientation as detected by OSD&quot;        }      }    },    &quot;ocrd-tesserocr-fontshape&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-fontshape&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Recognize font shapes (family/monospace/bold/italic) and size in segmented words with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons), annotating TextStyle&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-OCR&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-OCR-STYLE&quot;      ],      &quot;steps&quot;: [&quot;recognition/font-identification&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Number of background-filled pixels to add around the word image (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) on each side before recognition.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;osd&quot;,          &quot;description&quot;: &quot;tessdata model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or osd); must be an old (pre-LSTM) model&quot;        }      }    },    &quot;ocrd-tesserocr-recognize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-recognize&quot;,      &quot;categories&quot;: [&quot;Text recognition and optimization&quot;],      &quot;description&quot;: &quot;Segment and/or recognize text with Tesseract (using annotated derived images, or masking and cropping images from coordinate polygons) on any level of the PAGE hierarchy.&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-REGION&quot;,        &quot;OCR-D-SEG-TABLE&quot;,        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-SEG-WORD&quot;,        &quot;OCR-D-SEG-GLYPH&quot;,        &quot;OCR-D-OCR-TESS&quot;      ],      &quot;steps&quot;: [        &quot;layout/segmentation/region&quot;,        &quot;layout/segmentation/line&quot;,        &quot;recognition/text-recognition&quot;      ],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;default&quot;: 0,          &quot;description&quot;: &quot;Extend detected region/cell/line/word rectangles by this many (true) pixels, or extend existing region/line/word images (i.e. the annotated AlternativeImage if it exists or the higher-level image cropped to the bounding box and masked by the polygon otherwise) by this many (background/white) pixels on each side before recognition.&quot;        },        &quot;segmentation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Highest PAGE XML hierarchy level to remove existing annotation from and detect segments for (before iterating downwards); if ``none``, does not attempt any new segmentation; if ``cell``, starts at table regions, detecting text regions (cells). Ineffective when lower than ``textequiv_level``.&quot;        },        &quot;textequiv_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;cell&quot;, &quot;line&quot;, &quot;word&quot;, &quot;glyph&quot;, &quot;none&quot;],          &quot;default&quot;: &quot;word&quot;,          &quot;description&quot;: &quot;Lowest PAGE XML hierarchy level to re-use or detect segments for and add the TextEquiv results to (before projecting upwards); if ``none``, adds segmentation down to the glyph level, but does not attempt recognition at all; if ``cell``, stops short before text lines, adding text of text regions inside tables (cells) or on page level only.&quot;        },        &quot;overwrite_segments&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;If ``segmentation_level`` is not none, but an element already contains segments, remove them and segment again. Otherwise use the existing segments of that element.&quot;        },        &quot;overwrite_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;If ``textequiv_level`` is not none, but a segment already contains TextEquivs, remove them and replace with recognised text. Otherwise add new text as alternative. (Only the first entry is projected upwards.)&quot;        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, annotate polygon coordinates instead of bounding box rectangles.&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;When detecting regions, recognise tables as table regions (Tesseract&#39;s ``textord_tabfind_find_tables=1``).&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting regions, use &#39;sparse text&#39; page segmentation mode (finding as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space.&quot;        },        &quot;raw_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;When detecting lines, do not attempt additional segmentation (baseline+xheight+ascenders/descenders prediction) on line images. Can increase accuracy for certain workflows. Disable when line segments/images may contain components of more than 1 line, or larger gaps/white-spaces.&quot;        },        &quot;char_whitelist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow exclusively; overruled by blacklist if set.&quot;        },        &quot;char_blacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to suppress; overruled by unblacklist if set.&quot;        },        &quot;char_unblacklist&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;default&quot;: &quot;&quot;,          &quot;description&quot;: &quot;When recognizing text, enumeration of character hypotheses (from the model) to allow inclusively.&quot;        },        &quot;model&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;description&quot;: &quot;The tessdata text recognition model to apply (an ISO 639-3 language specification or some other basename, e.g. deu-frak or Fraktur).&quot;        }      }    },     &quot;ocrd-tesserocr-segment&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions and lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;, &quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 4        },        &quot;block_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }   },   &quot;ocrd-tesserocr-segment-region&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-region&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment page into regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-PAGE&quot;,        &quot;OCR-D-GT-SEG-PAGE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_regions&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the Page level (otherwise skip page; no incremental annotation yet).&quot;        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected region rectangles by this many (true) pixels&quot;,          &quot;default&quot;: 0        },        &quot;crop_polygons&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;annotate polygon coordinates instead of bounding box rectangles&quot;        },        &quot;find_tables&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;recognise tables as table regions (textord_tabfind_find_tables)&quot;        },        &quot;sparse_text&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: false,          &quot;description&quot;: &quot;use &#39;sparse text&#39; page segmentation mode (find as much text as possible in no particular order): only text regions, single lines without vertical or horizontal space&quot;        }      }    },     &quot;ocrd-tesserocr-segment-table&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-table&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment table regions into cell text regions with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/region&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_cells&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TableRegion level (otherwise skip table; no incremental annotation yet).&quot;        }      }     },     &quot;ocrd-tesserocr-segment-line&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-line&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment regions into lines with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-GT-SEG-BLOCK&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/line&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_lines&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextRegion level (otherwise skip region; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-segment-word&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-segment-word&quot;,      &quot;categories&quot;: [&quot;Layout analysis&quot;],      &quot;description&quot;: &quot;Segment lines into words with Tesseract&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-SEG-LINE&quot;,        &quot;OCR-D-GT-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-SEG-WORD&quot;      ],      &quot;steps&quot;: [&quot;layout/segmentation/word&quot;],      &quot;parameters&quot;: {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;overwrite_words&quot;: {          &quot;type&quot;: &quot;boolean&quot;,          &quot;default&quot;: true,          &quot;description&quot;: &quot;Remove existing layout and text annotation below the TextLine level (otherwise skip line; no incremental annotation yet).&quot;        }      }    },    &quot;ocrd-tesserocr-crop&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-crop&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Poor man&#39;s cropping via region segmentation&quot;,      &quot;input_file_grp&quot;: [&quot;OCR-D-IMG&quot;      ],      &quot;output_file_grp&quot;: [&quot;OCR-D-SEG-PAGE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/cropping&quot;],      &quot;parameters&quot; : {        &quot;dpi&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;float&quot;,          &quot;description&quot;: &quot;pixel density in dots per inch (overrides any meta-data in the images); disabled when negative&quot;,          &quot;default&quot;: 0        },        &quot;padding&quot;: {          &quot;type&quot;: &quot;number&quot;,          &quot;format&quot;: &quot;integer&quot;,          &quot;description&quot;: &quot;extend detected border by this many (true) pixels on every side&quot;,          &quot;default&quot;: 4        }      }    },    &quot;ocrd-tesserocr-binarize&quot;: {      &quot;executable&quot;: &quot;ocrd-tesserocr-binarize&quot;,      &quot;categories&quot;: [&quot;Image preprocessing&quot;],      &quot;description&quot;: &quot;Binarize regions or lines with Tesseract&#39;s global Otsu&quot;,      &quot;input_file_grp&quot;: [        &quot;OCR-D-IMG&quot;,        &quot;OCR-D-SEG-BLOCK&quot;,        &quot;OCR-D-SEG-LINE&quot;      ],      &quot;output_file_grp&quot;: [        &quot;OCR-D-BIN-BLOCK&quot;,        &quot;OCR-D-BIN-LINE&quot;      ],      &quot;steps&quot;: [&quot;preprocessing/optimization/binarization&quot;],      &quot;parameters&quot;: {        &quot;operation_level&quot;: {          &quot;type&quot;: &quot;string&quot;,          &quot;enum&quot;: [&quot;region&quot;, &quot;line&quot;],          &quot;default&quot;: &quot;region&quot;,          &quot;description&quot;: &quot;PAGE XML hierarchy level to operate on&quot;        }      }    }  }}",
      "url": " /en/spec/ocrd_tool.html"
    },
  

    {
      "slug": "en-spec-ocrd-zip-html",
      "title": "OCRD-ZIP",
      "content"	 : "OCRD-ZIPThis document describes an exchange format to bundle a workspace described by aMETS file following OCR-D’s conventions.RationaleMETS is the exchange format of choice by OCR-D for describing relations offiles such as images and metadata about those images such as PAGE or ALTOfiles. METS is a textual format, not suitable for embedding arbitrary,potentially binary, data. For various use cases (such as transfer via network,long-term preservation, reproducible tests etc.) it is desirable to have aself-contained representation of a workspace.With such a representation, data producers are not forced to providedereferenceable HTTP-URL for the files they produce and data consumers are notforced to dereference all HTTP-URL.While METS does have mechanisms for embedding XML data and even base64-encodedbinary data, the tradeoffs in file size, parsing speed and readability are toogreat to make this a viable solution for a mass digitization scenario.Instead, we propose an exchange format (“OCRD-ZIP”) based on the BagIt specused for data ingestion adopted in the web archiving community.BagIt profileAs a baseline, an OCRD-ZIP must adhere to v0.97+ of the BagItspecs, i.e.  all files in data/  a file bagit.txt  a file bag-info.txtIn accordance with the BagIt standard, bagit.txt MUST consist of exactlythese two lines:BagIt-Version: 1.0Tag-File-Character-Encoding: UTF-8In addition, OCRD-ZIP adhere to a BagItprofile (see Appendix A forthe full definition):  bag-info.txt MUST additionally contain these tags:          BagIt-Profile-Identifier: URL of the OCR-D BagIt profile      Ocrd-Identifier: A globally unique identifier for this bag      Ocrd-Base-Version-Checksum: Checksum of the version this bag is based on        bag-info.txt MAY additionally contain these tags:          Ocrd-Mets: Alternative path to the mets.xml file, relative to /data, if its path IS NOT mets.xml      BagIt-Profile-IdentifierThe BagIt-Profile-Identifier must be the string https://ocr-d.de/en/spec/bagit-profile.json.Ocrd-MetsOcrd-Mets can be provided to declare that the METS file will not be thestandard mets.xml but another path relative to /data/.Implementations MUST check for the Ocrd-Mets tag: If it has a value, look for theMETS file at that location, relative to /data. Otherwise, assume the defaultmets.xml.Ocrd-IdentifierA globally unique identifier identifying the work/works/parts of works thisbundle of file represents.This is to be used for repositories to identify new ingestions of existing works.To ensure global uniqueness, the identifier should be prefixed with anidentifier of the organization, e.g. an ISIL or domain name.Ocrd-Base-Version-ChecksumThe SHA512 checksum of the manifest-sha512.txt file of the version this bagwas based on, if any.InvariantsZIPAn OCRD-ZIP MUST be a serialized as a ZIP file.manifest-sha512.txtChecksums for the files in /data must be calculated with the SHA512algorithm only and provided as manifest-sha512.txt.Since the checksum of this manifest file can be relevant (seeOcrd-Base-Version-Checksum), in addition to the requirementsof the BagIt spec, the entries MUST be sorted.NOTE: These checksums can be generated with find data -type f | sort -sf |xargs sha512sum &amp;gt; manifest-sha512.txt.File names must be relative to METSWithin an OCRD-ZIP, all local file resources referenced in the METS (andconsequently all those referenced in other files within the workspace – seerule “If in PAGE then in METS” must berelative to the location of the METS file.Example/tmp/foo/ws1/data├── mets.xml├── foo.tif└── foo.xmlValid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  foo.xml  foo.tif  file://foo.tifInvalid mets:FLocat/@xlink:href in /tmp/foo/ws1/data/mets.xml:  /tmp/foo/ws1/data/foo.xml (absolute path)  file:///tmp/foo/ws1/data/foo.tif (file URL scheme with absolute path)  file:///foo.tif (relative path written as absolute path)When in data then in METSAll files except mets.xml itself that are contained in data directory mustbe referenced in a mets:file/mets:Flocat in the mets.xml.When in METS and not in dataAll local files (mets:file/mets:FLocat/@xlink:href that represent file paths) must be part of the OCRD-ZIP.Optional metadata about the payloadIn addition to the actual data files in /data, the following metadata filesare allowed to be present in the root of the bag:  README.md: An extended, human-readable description of the dataset in the Markdown syntax  Makefile: A GNU make build file to reproduce the data in /data.  build.sh: A bash script to reproduce the data in /data.  sources.csv: A comma-separated values list to be used in the scripts.These files are purely for documentation and should not be used by processors in any way.AlgorithmsPacking a workspace as OCRD-ZIPTo pack a workspace to OCRD-ZIP:  Create a temporary folder TMP  Foreach mets:file f in the source METS:          Strip file:// from the beginning of the xlink:href of f      If it is not a file path (begins with http:// or https://):                  continue                    Download/Copy the file to a location within TMP/data. The structure SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt; where                  &amp;lt;USE&amp;gt; is the USE attribute of the parent mets:fileGrp          &amp;lt;ID&amp;gt; is the ID attribute of the mets:file                    Replace the URL of f with the path relative to /data (SHOULD be &amp;lt;USE&amp;gt;/&amp;lt;ID&amp;gt;) in                  all mets:FLocat of the METS          all other files in the workspace, esp. PAGE-XML                      Write out the changed METS to TMP/data/mets.xml  Package TMP as a BagIt bagUnpacking OCRD-ZIP to a workspace  Unzip OCRD-ZIP z to a folder TMP  If the value M of Ocrd-Mets is different from mets.xml:          Rename TMP/data/mets.xml to TMP/data/ + M        Move TMP/data to an appropriate location to use as a workspaceAppendix A - BagIt profile definitionBagIt-Profile-Info:  BagIt-Profile-Identifier: https://ocr-d.de/en/spec/bagit-profile.json  BagIt-Profile-Version: &#39;1.2.0&#39;  Source-Organization: OCR-D  External-Description: BagIt profile for OCR data  Contact-Name: Konstantin Baierer  Contact-Email: konstantin.baierer@sbb.spk-berlin.de  Version: 0.1Bag-Info:  Bagging-Date:    required: false  Source-Organization:    required: false  Ocrd-Mets:    required: false    default: &#39;mets.xml&#39;  Ocrd-Identifier:    required: true  Ocrd-Checksum:    required: false    # echo -n | sha512sum    default: &#39;cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e&#39;Manifests-Required: [&#39;sha512&#39;]Tag-Manifests-Required: []Tag-Files-Required: []Tag-Files-Allowed:  - README.md  - Makefile  - build.sh  - sources.csv  - metadata/*.xml  - metadata/*.txtAllow-Fetch.txt: falseSerialization: requiredAccept-Serialization: application/zipAccept-BagIt-Version:  - &#39;1.0&#39;Appendix B - IANA considerationsProposed media type of OCRD-ZIP: application/vnd.ocrd+zipProposed extension: .ocrd.zip",
      "url": " /en/spec/ocrd_zip.html"
    },
  

    {
      "slug": "en-spec-page-html",
      "title": "Conventions for PAGE",
      "content"	 : "Conventions for PAGEIn addition to these conventions, refer to the PAGE APIdocs for extensivedocumentation on the PAGE XML format itself.Media TypeThe preliminary media type of a PAGEdocument is application/vnd.prima.page+xml, which MUST be used as the MIMETYPE of a &amp;lt;mets:file&amp;gt;representing a PAGE document.One page in one PAGEA single PAGE XML file represents one page in the original document.Every &amp;lt;pc:Page&amp;gt; element MUST have an attribute image which MUST always be the source image.The PAGE XML root element &amp;lt;pc:PcGts&amp;gt; MUST have exactly one &amp;lt;pc:Page&amp;gt;.ImagesURL for imageFilename / filenameThe imageFilename of the &amp;lt;pg:Page&amp;gt; and filename of the&amp;lt;pg:AlternativeImage&amp;gt; element MUST be a filename relative to the mets.xml.All URL used in imageFilename and filename MUST be referenced in a fileGrpin METS. This MUST bethe same file group as the PAGE-XML that was the result of the processing stepthat produced the &amp;lt;pg:AlternativeImage&amp;gt;. In other words:&amp;lt;pg:AlternativeImage&amp;gt; should be written to the same &amp;lt;mets:fileGrp&amp;gt; as its sourcePAGE-XML, which in most implementations will mean the same folder.Original image as imageFilenameThe imageFilename attribute of the &amp;lt;pg:Page&amp;gt; MUST reference the originalimage and MUST NOT change between processing steps.AlternativeImage for derived imagesTo encode images derived from the original image, the &amp;lt;pc:AlternativeImage&amp;gt;should be used. Its filename attribute should reference the URL of thederived image.The comments attribute SHOULD be used according to the AlternativeImage classification.AlternativeImage: classificationThe comments attribute of the &amp;lt;pg:AlternativeImage&amp;gt; attribute should be used  binarized  grayscale_normalized  deskewed  despeckled  cropped  rotated-90 / rotated-180 / rotated-270  dewarpedAlternativeImage on sub-page level elementsFor the results of image processing that changes the positions of pixels (e.g.cropping, rotation, dewarping), AlternativeImage on page level and polygon ofrecognized zones is not sufficient for accessing the section of the image that a region is based onsince coordinates are always relative to the original image.For such use cases, &amp;lt;pg:AlternativeImage&amp;gt; may be used as a child of&amp;lt;pg:TextRegion&amp;gt;, &amp;lt;pg:TextLine&amp;gt;, &amp;lt;pg:Word&amp;gt; or &amp;lt;pg:Glyph&amp;gt;.Attaching text recognition results to elementsA PAGE document can attach recognized text to typographical units ofa page at different levels, such as region (&amp;lt;pg:TextRegion&amp;gt;), line(&amp;lt;pg:TextLine&amp;gt;), word (&amp;lt;pg:Word&amp;gt;) or glyph (&amp;lt;pg:Glyph&amp;gt;).To attach recognized text to an element E, it must be encoded asUTF-8 in a single &amp;lt;pg:Unicode&amp;gt; element U within a &amp;lt;pg:TextEquiv&amp;gt;element T of E.T must be the last element of E.Leading and trailing whitespace (U+0020, U+000A) in the content of a&amp;lt;pg:Unicode&amp;gt; is not significant and must be removed from the string byprocessors.To encode an actual space character at the start or end of the content&amp;lt;pg:Unicode&amp;gt;, use a non-breaking space U+00A0.Text recognition confidenceThe confidence score describing the assumed correctness of the text recognition results in a&amp;lt;pg:TextEquiv&amp;gt; can be expressed in an attribute @conf as a float valuebetween 0 and 1, where 0 means “certainly wrong” and 1 means “certainlycorrect”.Attaching multiple text recognition results to elementsAlternative text recognition results can be expressed by using multiple&amp;lt;pg:TextEquiv&amp;gt; wherever a single &amp;lt;pg:TextEquiv&amp;gt; would be allowed. Whenusing multiple &amp;lt;pg:TextEquiv&amp;gt;, they each must have an attribute @index withan integer number unique per set of &amp;lt;pg:TextEquiv&amp;gt; that allows ranking themin order of preference. @index of the first (preferred) &amp;lt;pg:TextEquiv&amp;gt; must bethe value 1.Consistency of text results on different levelsSince text results can be defined on different levels and those levels can benested, text results information can be redundant. To avoid inconsistencies,the following assertions must be true:  text of &amp;lt;pg:Word&amp;gt; must be equal to the text of all &amp;lt;pg:Glyph&amp;gt;    contained within, concatenated with empty string  text of &amp;lt;pg:TextLine&amp;gt; must be equal to the text of all    &amp;lt;pg:Word&amp;gt; contained  within, concatenated with a single space (U+0020).  text of &amp;lt;pg:TextRegion&amp;gt; must be equal to the text of all    &amp;lt;pg:TextLine&amp;gt; contained within, concatenated with a newline (U+000A).NOTE: “Concatenation” means joining a list of strings with a separator, noseparator is added to the start or end of the resulting string.These assertions are only to be enforced for the first &amp;lt;pg:TextEquiv&amp;gt; of bothcontaining and contained elements, i.e. the only &amp;lt;pg:TextEquiv&amp;gt; of an elementor the &amp;lt;pg:TextEquiv&amp;gt; with @index = 1 if multiple textresults are attached.Consistency strictnessA consistency checker must support four levels of strictness:strictIf any of the assertions fail for a PAGE document, an exceptionshould be raised and the document no further processedlaxIf any of the assertions fail for a PAGE document, another comparisondisregarding all whitespace shall be made. If this still fails, an exceptionshould be raised and the document no further processedfixIf any of the assertions fail for a specific element in PAGE document, the textresults of this element must be recreated, by concatenating the text results ofits children elements. This algorithm needs to be recursive, i.e. if any of thechildren elements is itself inconsistent, its text results must be recreated inthe same way before concatenation.offThese consistency checks are so restrictive to spot data that cannot beunambiguously processed. However, there are valid use cases where the“index-1-consistency” is too narrow, esp. in post-correction with languagemodels. For such use cases, it must be possible to disable the consistencyvalidation altogether in the workflow.Example&amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;f&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;    &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;o&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;Glyph&amp;gt;    &amp;lt;TextEquiv&amp;gt;&amp;lt;Unicode&amp;gt;t&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;/Glyph&amp;gt;  &amp;lt;TextEquiv index=&quot;1&quot;&amp;gt;&amp;lt;Unicode&amp;gt;foof&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;  &amp;lt;TextEquiv index=&quot;2&quot;&amp;gt;&amp;lt;Unicode&amp;gt;toot&amp;lt;/Unicode&amp;gt;&amp;lt;/TextEquiv&amp;gt;&amp;lt;/Word&amp;gt;In this example, the &amp;lt;pg:Word&amp;gt; has text foof butthe concatenation of the first text results of the contained &amp;lt;pg:Glyphs&amp;gt;spells foot. As a result:  Validation should raise an exception for inconsistency.  Data consumers should assume the text result to be foot.TextStyleTypographical information (type, cut etc.) must be documented in PAGE XML using the&amp;lt;TextStyle&amp;gt; element.See the PAGE documentation on TextStyle for all possible values.The &amp;lt;TextStyle&amp;gt; element can be used in all relevant elements:  &amp;lt;TextRegion&amp;gt;  &amp;lt;TextLine&amp;gt;  &amp;lt;Word&amp;gt;  &amp;lt;Glyph&amp;gt;Example:&amp;lt;Word&amp;gt;  &amp;lt;TextStyle fontFamily=&quot;Arial&quot; fontSize=&quot;17.0&quot; bold=&quot;true&quot;/&amp;gt;  &amp;lt;!-- [...] --&amp;gt;&amp;lt;/Word&amp;gt;Font familiesThe pg:TextStyle/@fontFamily attribute can list one or more fontfamilies, separated by comma (,).font-families    := font-family (&quot;,&quot; font-family)*font-family      := font-family-name (&quot;:&quot; confidence)?font-family-name := [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot;]+ | &#39;&quot;&#39; [&quot;A&quot; - &quot;Z&quot; | &quot;a&quot; - &quot;z&quot; | &quot;0&quot; - &quot;9&quot; | &quot; &quot;]+ &#39;&quot;&#39;confidence       := (&quot;0&quot; | &quot;1&quot;)? &quot;.&quot; [&quot;0&quot; - &quot;9&quot;]+Font family names that contain a space must be quoted with double quotes (&quot;).Clusters of typesetsSometimes it is necessary to not express that an element is typeset in aspecific font family but in font family from a cluster of related font groups.For such typeset clusters, the pg:TextStyle/@fontFamily attribute should be re-used.This specification doesn’t restrict the naming of font families.However, we recommend to choose one of the following list of type groups names ifapplicable:  textura  rotunda  bastarda  antiqua  greek  hebrew  italic  frakturFont families and confidenceProviding multiple font families means that the element inquestion is set in one of the font families listed.It is not possible to declare that multiple font families are used in anelement. Instead, data producers are advised to increase output granularityuntil every element is set in a single font family.The degree of confidence in the font family can be expressed by concatenatingfont family names with colon (:) followed by a float between 0 (informationis certainly wrong) and 1 (information is certainly correct).If a font family is not suffixed with a confidence value, the confidence isconsidered to be 1.Examples&amp;lt;TextStyle fontFamily=&quot;Arial:0.8, Times:0.7, Courier:0.4&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:.8, Times:0.5&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial:1&quot;/&amp;gt;&amp;lt;TextStyle fontFamily=&quot;Arial&quot;/&amp;gt;ColumnsTo model columns, use constructs in the &amp;lt;pg:ReadingOrder&amp;gt; of the PAGEdocument.A grid layout must be wrapped in a &amp;lt;pg:OrderedGroup&amp;gt; with a@caption that has the form column_&amp;lt;horizontal&amp;gt;_&amp;lt;vertical&amp;gt; where&amp;lt;vertical&amp;gt; is the number of columns and &amp;lt;horizontal&amp;gt; is the number of rows.&amp;lt;OrderedGroup caption=&quot;column_1_1&quot;&amp;gt; &amp;lt;!-- the default: single column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_2&quot;&amp;gt; &amp;lt;!-- two-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_1_3&quot;&amp;gt; &amp;lt;!-- three-column layout --&amp;gt;&amp;lt;OrderedGroup caption=&quot;column_2_3&quot;&amp;gt; &amp;lt;!-- three-column layout split in top and bottom --&amp;gt;Regions that belong to the same column must be grouped within&amp;lt;pg:OrderedGroupIndexed&amp;gt; with a caption that begins with column_&amp;lt;y&amp;gt;_&amp;lt;x&amp;gt;where &amp;lt;y&amp;gt; is the row position and &amp;lt;x&amp;gt; is the column position (counting starts at 1):&amp;lt;OrderedGroup caption=&quot;column_2_2&quot;&amp;gt; &amp;lt;!-- two-column two-row layout --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_1_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- upper-right column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_1&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-left column --&amp;gt;    &amp;lt;OrderedGroupIndexed caption=&quot;column_2_2&quot;&amp;gt;...&amp;lt;/OrderedGroupIndexed&amp;gt; &amp;lt;!-- lower-right column --&amp;gt;&amp;lt;/OrderedGroup&amp;gt;",
      "url": " /en/spec/page.html"
    },
  

    {
      "slug": "en-phase2-html",
      "title": "Phase II",
      "content"	 : "Module Projects Phase IIIn the first project phase, a functional model for the OCR-D workflow wasdeveloped. Full text recognition is seen as a complex process that includesseveral upstream and downstream steps in addition to the actual textrecognition. First, a digital image is preprocessed for text recognition bycropping, deskewing, dewarping, despeckling and binarizing it into a black andwhite image. This is followed by layout recognition, which identifies the textareas of a page down to line level. The recognition of the lines or thebaseline is particularly important for the subsequent text recognition, whichis based on neural networks in all modern approaches. The individual structuresor elements of the fully text-recognized document are then classified accordingto their typographic function before the OCR result is improved in thepost-correction, if necessary. Finally the end result is transferred torepositories for long-term archiving.From the project proposals for the DFG’s module project call in March 2017,eight projects were approved:Scalable methods of text and structure recognition for full text digitization of historical prints: Image Optimisation        German Research Center for Artificial Intelligence (DFKI)Project participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungDFKI was involved in the OCR-D project with two modules: Image optimization andlayout recognition. In both modules several processors were developed andintegrated into the OCR-D software system.The first module project image optimization focused on the pre-processing ofthe digitized material with the aim of improving the image quality and thus theperformance of the subsequent OCR modules. For this purpose, tools forbinarization, deskewing, cropping and dewarping were implemented.The cropping tool based on computer vision is particularly noteworthy for itsperformance. It predominantly achieves very good results on the entire projectdata. The dewarping tool is also interesting due to its novel architecture.Generative neural networks are used to generate equalized variants of imagesinstead of determining explicit transformations for the equalization.Scalable text and structure recognition methods for the full text digitization of historical prints: Layout Recognition        DFKIProject participants: Andreas Dengel, Martin Jenckel, Khurram HashmiGitHub: mjenckel/OCR-D-LAYoutERkennungIn the second DFKI module project layout recognition, the aim was to extractthe document structure, both of individual document pages and the entiredocument. On the one hand, the metadata obtained in this way helps to digitizethe document as a whole, on the other hand, the extraction of certain documentstructures is necessary. For example, most OCR methods can only processindividual lines of text. The tools developed are used for text-non-textsegmentation, block segmentation and classification, text line detection andstructure analysis.One focus of development was the combined block segmentation and classificationbased on the MaskRCNN architecture known from video and image segmentation.This tool works with the unprocessed raw data, so that on the one hand nopre-processing is necessary and on the other hand the full information spectrumcan be used.Further development of a semi-automated open source tool for layout analysis and region extraction and classification (LAREX) of early printing        Julius-Maximilians-University of Würzburg   Institute of Computer Science: Chair of Artificial Intelligence and Applied Computer ScienceProject participants: Frank Puppe, Alexander GehrkeGitHub: ocr-d-modul-2-segmentierungAt the Department of Computer Science VI at the University of Würzburg, LAREXwas developed in the preliminary work. LAREX is a comfortable editor forannotating regions and layout elements on book pages. In the furtherdevelopment of the OCR-D module project, the focus was not only on improvingefficient operability but also on expanding automatic procedures.For this purpose a Convolutional-Neural-Net (CNN) was implemented and trained,which assigns each pixel of a page scan a classification in different classesin order to separate image and text. By considering the pixels of only oneclass each, a segmentation of the page is then carried out with classicalmethods. Another tested approach first used classical segmentation methods andthen classified the segments.The segmentation method based on CNN output was adapted to the OCR-Dinterfaces. Good results were achieved on pure text pages or pages with clearlyseparated images. There is potential for improvement especially in therecognition of decorative initials of older prints and other images close tothe text as well as multi-column layouts.NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers        University of Leipzig Institut für Informatik: Department of Automatic Language ProcessingProject participants: Gerhard Heyer, Robert SachunskyGitHub: ASVLeipzig/cor-asv-fstA fully automatic post-correction separately from the actual OCR only makessense if statistical knowledge about “correct text” and about typical OCRerrors is added a priori. Neural networks (NN) as well as weighted finitetransducers (WFST), which can be trained on corresponding additional data, aresuitable for this purpose.For the implementation of a combined architecture of NN and FST it was decided to implement three modules:  a pure NN solution with continuously (end-to-end) trained model on thecharacter level alone - as a deep (multi-layer), bidirectional recurrentnetwork according to the encoder-decoder scheme (for different input andoutput lengths) with an attention mechanism and A*-Beamsearch withadjustable rejection threshold (against overcorrection), i.e.post-correction of text lines is treated like machine translation,  a NN language model (LM) at the character level - as a deep (multi-layer),bidirectional recurrent network with interface for graph input andincremental decoding  a WFST component with an error model to be trained explicitly on glyph leveland word model/lexicon, as well as connection to 2. - via WFST compositionof input graph with error and word model according to the sliding windowprinciple, conversion of the single windows to one hypothesis graph per textline, and combination of the respective output weights with LM-evaluationsin an efficient search for the best path.The combination of 3. with 2. thus represents a hybrid solution. But also 1.can benefit from 2. (if the same network topology is used) by initializing theweights from a language model trained on larger amounts of pure text (transferlearning).Both approaches benefit from a close connection to the OCR search space, i.e. atransfer of alternative character hypotheses and their confidence (as so faronly possible with Tesseract and realized in cooperation with the moduleproject of the Mannheim University Library). However, they also deliver goodresults on pure full text (with CER reduction of up to 5%), provided thatsufficient suitable training data is available and the OCR itself deliversuseful results (below 10% CER).Command line interfaces for training and evaluation as well as full OCR-Dinterfaces for processing and evaluation are available for all modules.Optimized use of OCR processes – Tesseract as a component in the OCR-D workflow        University of Mannheim University Library MannheimProject participants: Stefan Weil, Noah MetzgerGitHub: tesseract-ocr/tesseract/The module project focused on the OCR software Tesseract, which has beendeveloped by Ray Smith since 1985, since 2005 as open source under a freelicense.The project had two main goals: The integration of Tesseract into the OCR-Dworkflow including support of the other module projects by providinginterfaces, and the general improvement of stability, code quality andperformance of Tesseract.The integration into the OCR-D workflow required much less effort thanoriginally planned; mainly because most of the work had already been doneoutside the module project and the existing Python interface tesserocr could beused.For the OCR-D module project of the University of Leipzig, Tesseract wasextended to generate alternative OCR results for the single characters. Asinput data for an OCR post-correction model, text recognition can thus befurther improved. A valuable side-effect of the new code are more accuratecharacter and word coordinates.With several hundred corrections, the code quality was significantly improvedand a much more stable program flow was achieved. Tesseract is now moremaintainable, requires less memory and is faster than before.A significant improvement in recognition accuracy for most of the printingunits relevant for OCR-D was achieved by new generic models for Tesseract.These were trained from September 2019 until January 2020 on the basis of thedata collection GT4HistOCR.Automatic post-correction of historical OCR captured prints with integrated optional interactive correction        Ludwig-Maximilians-University of MunichCentre for Information and Language Processing (CIS)Project participants: Klaus Schulz, Floran Fink, Tobias EnglmeierGitHub: https://github.com/cisocrgroup/ocrd-postcorrection, https://github.com/cisocrgroup/cis-ocrd-pyThe result of the project is a A-I-PoCoTo system integrated into the OCR-D workflow for fully automatic post-correction of full text recognized historical prints. The system also includes an optional interactive post-correction (I-PoCoTo), which is integrated into the interactive post-correction system PoCoWeb. The system can thus be used alternatively as a stand-alone tool for collaborative web-based post-correction of OCR documents.The basis of the fully automatic post-correction is a flexible, feature-based Machine Learning (ML) procedure for fully automatic OCR post-correction with a special focus on avoiding the problem of disimprovement. The system uses the document-dependent profiling technology developed at CIS to detect errors and to generate correction candidates. In addition to various confidence values, the features of the system also use information from additional auxiliary OCRs.The system logs all correction decisions. Via this protocol mechanism the automatic post correction in PoCoWeb can be checked interactively. You can manually undo individual correction decisions that have been made, and also subsequently execute correction decisions that have not been made.The entire system is integrated into the OCR-D workflow and follows the conventions valid there.Development of a model repository and an automatic font recognition for OCR-D        University LeipzigInstitute of Computer Science: Chair of Digital HumanitiesFriedrich Alexander University Erlangen-NurembergDepartment of Computer Science: Chair of Computer Science 5: Pattern RecognitionJohannes Gutenberg University MainzGutenberg Institute for World Literature and Writing-Oriented Media: Department of Book ScienceProject participants: Gregory Crane, Nikolaus Weichselbaumer, Saskia Limbach, Andreas Meier, Vincent Christlein, Mathias Seuret, Rui DongGitHub: OCR-D/okralact, https://github.com/seuretm/ocrd_typegroups_classifierThe recognition rates of OCR for prints produced before 1800 vary greatly, asthe diversity of historical fonts is either not taken into account at all oronly insufficiently in the training data. Therefore this module project,consisting of computer scientists and book historians, has set itself threegoals:On the one hand, we have developed a tool for the automatic recognition offonts in digitised images. Here, we have concentrated especially on brokenfonts besides fracture, which have received little attention so far, but werewidely used in the 15th and 16th centuries: Bastarda, Rotunda, Textura andSchwabacher. The tool has been trained with 35,000 images and achieves anaccuracy of 98% in determining fonts. Overall, it can not only differentiatebetween the above mentioned fonts, but also distinguish between Hebrew, Greek,Fraktur, Antiqua and Italic.In a second step, an online training infrastructure was created (Okralact). Itsimplifies the use of different OCR engines (Tesseract, Ocropus, Octopus,Calamari) and at the same time makes it possible to train specific models forcertain fonts.Finally, a model repository has been set up that contains already developedfont-specific OCR models. To lay a foundation here, we have transcribed a totalof about 2,500 lines for Bastarda, Textura and Schwabacher from a variety ofdifferent books.The high accuracy of the font recognition tool opens up the possibility ofhaving the tool even distinguish between the fonts of individual printers inthe future through further training data, which would address severaldesiderata of historical research.OLA-HD – An OCR-D long-term archive for historical books        Georg-August-University of GöttingenState and University Library of Lower SaxonySociety for Scientific Data Processing mbH GöttingenProject participants: Mustafa Dogan, Kristine Schima-Voigt, Philip Wieder, Triet Doan, Jörg-Holger PanzerGitHub: subugoe/OLA-HD-IMPLIn September 2018 the Digital Library Department of the State and University Library of Lower Saxony and the Gesellschaft für wissenschaftliche Datenverarbeitung Göttingen started the DFG project OLA-HD - Ein OCR-D Langzeitarchiv für historische Drucke.The aim of OLA-HD is to develop an integrated concept for long-term archivingand persistent identification of OCR objects, as well as a prototypicalimplementation.In regular exchange with the project partners, the basic requirements forlong-term archiving and persistent identification were determined and recordedin the form of a specification for technical and economic-organizationalimplementation.With the prototype the user can upload OCR results of a work as OCRD-ZIP intothe system. The system validates the zip file, assigns a PID and sends the fileto the archive manager (CDSTAR - GWDG Common Data StorageArchitecture). This writes the Zip file to thearchive (tape storage). Depending on the configuration (file type, file size,etc.), files are also written to an online storage (hard disk) for fast access.The user has access to all OCR versions and can download versions as BagIt-Zipfiles. All works and versions have their own PIDs. The PIDs are generated bythe European Persistent Identifier Consortium(ePIC) service. The different OCR versions ofa work are linked via the PID, so that the system can map the versioning in atree structure.Users who are not logged in can browse the inventory and preview text and - ifavailable - images in the file structure or navigate through the differentversions. Users can register and log in via the GWDG portal and manage theirfiles via a dashboard.By March 2020, minor optimizations will be made to the user interface and theconcept will be finalized. The concept will describe further expansion stagesthat may be useful for transferring the prototype software into a product.",
      "url": " /en/phase2.html"
    },
  

    {
      "slug": "de-phase2-html",
      "title": "Phase II",
      "content"	 : "Modulprojekte Phase IIAus den Projektanträgen für die Modulprojektausschreibung der DFG im März 2017 wurden acht Projekte bewilligt:Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Bildoptimierung        Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)Das DFKI war als Projektpartner im OCR-D Projekt mit zwei Modulen vertreten:Bildoptimierung und Layouterkennung. In beiden Modulen wurden mehrereProzessoren entwickelt und in das OCR-D-Softwaresystem integriert.Das erste Modul-Projekt Bildoptimierung fokussierte sich auf dieVorverarbeitung der Digitalisate mit dem Ziel, die Bildqualität und somit auchdie Performanz der nachfolgenden OCR-Module zu verbessern. Dafür wurdenWerkzeuge für die Binarisierung, das Deskewing, das Cropping und das Dewarpingimplementiert.Das auf Computer Vision basierte Cropping-Werkzeug ist als besonders performanthervorzuheben. Es erzielt auf den gesamten Projektdaten vorwiegend sehr guteErgebnisse. Auch das Dewarping-Werkzeug ist aufgrund seiner neuartigenArchitektur interessant. Mit Hilfe generativer neuronaler Netze werdenentzerrte Varianten von Bildern generiert, anstatt explizite Transformationenfür die Entzerrung zu bestimmen.Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Layouterkennung        DFKIGitHub: mjenckel/OCR-D-LAYoutERkennung/tree/masterIm zweiten Modul-Projekt des DFKI Layouterkennung galt es, dieDokumentstruktur, sowohl einzelner Dokumentseiten als auch im Gesamtdokument,zu extrahieren. Die dabei gewonnenen Metadaten helfen zum einen, das Dokumentals Ganzes zu digitalisieren, zum anderen ist das Extrahieren bestimmterDokumentstrukturen notwendig. Die meisten OCR-Methoden können z.B. nur einzelneTextzeilen verarbeiten. Die entwickelten Werkzeuge dienen derText-Nicht-Text-Segmentierung, der Blocksegmentierung und -klassifizierung, derTextzeilenerkennung sowie der Strukturanalyse.Ein Entwicklungsschwerpunkt war die kombinierte Blocksegmentierung und-klassifizierung, welche auf der, aus der Video- und Bildsegmentierungbekannten, MaskRCNN-Architektur basiert. Dieses Werkzeug arbeitet mit denunbearbeiteten Rohdaten, sodass einerseits keine Vorverarbeitung notwendig istund andererseits das volle Informationsspektrum ausgenutzt werden kann.Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken        Julius-Maximilians-Universität Würzburg Institut für Informatik: Lehrstuhl für Künstliche Intelligenz und angewandte InformatikGitHub: ocr-d-modul-2-segmentierungAm Lehrstuhl für Informatik VI der Uni Würzburg wurde in den Vorarbeiten LAREXentwickelt, ein komfortabler Editor zur Annotation von Regionen undLayout-Elementen auf Buchseiten. Bei der Weiterentwicklung imOCR-D-Modulprojekt lag der Schwerpunkt neben der Verbesserung der effizientenBedienbarkeit vor allem auch in dem Ausbau der automatischen Verfahren.Hierzu wurde ein Convolutional-Neural-Net (CNN) implementiert und trainiert,welches jedem Pixel eines Seitenscans eine Einordnung in verschiedene Klassenzuweist, um so Bild und Text zu trennen. Unter Betrachtung der Pixel je nureiner Klasse wird anschließend mit klassischen Verfahren eine Segmentierung derSeite durchgeführt. Ein weiterer getesteter Ansatz nutzte zuerst klassischeSegmentierungsverfahren und ordnete die Segmente anschließend ein.Das auf der CNN-Ausgabe basierende Segmentierungsverfahren wurde an dieOCR-D-Schnittstellen angepasst. Auf reinen Textseiten oder Seiten mit deutlichabgetrennten Bildern wurden gute Ergebnisse erzielt. Verbesserungspotentialbesteht vor allem bei der Erkennung von Zierinitialen älterer Drucke undweiteren nah am Text liegenden Bildern sowie mehrspaltigen Layouts.NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers        Universität Leipzig   Institut für Informatik: Abteilung Automatische Sprachverarbeitung_GitHub: ASVLeipzig/cor-asv-fstEine vollautomatische Nachkorrektur separat von der eigentlichen OCR ist immernur dann sinnvoll, wenn dabei statistisches Wissen über “richtigen Text” undüber typische OCR-Fehler a priori hinzukommt. Dafür eignen sich neuronaleNetze (NN) ebenso wie gewichtete endliche Transduktoren (WFST), die aufentsprechenden zusätzlichen Daten trainiert werden können.Für die Umsetzung einer kombinierten Architektur aus NN und FST wurdeentschieden, drei Module zu implementieren:  eine reine NN-Lösung mit durchgehend (end-to-end) trainiertem Modell  allein auf Zeichenebene – als tiefes (mehrschichtiges), bidirektionales  rekurrentes Netzwerk nach dem Encoder-Decoder-Schema (für verschiedene  Eingabe- und Ausgabelänge) mit Attention-Mechanismus und A*-Beamsearch mit  einstellbarer Rückweisungsschwelle (gegen Überkorrektur), d.h. die  Nachkorrektur von Textzeilen wird wie maschinelle Übersetzung behandelt,  ein NN-Sprachmodell (LM) auf Zeichenebene – als tiefes (mehrschichtiges),  bidirektionales rekurrentes Netzwerk mit Schnittstelle für Graph-Eingabe  und inkrementeller Dekodierung,  eine WFST-Komponente mit explizit zu trainierendem Fehlermodell auf  Zeichenebene und Wortmodell/Lexikon, sowie Anbindung an 2. – per  WFST-Komposition von Eingabegraph mit Fehler- und Wortmodell nach  Sliding-Window-Prinzip, Konversion der Einzelfenster zu einem  Hypothesengraph pro Textzeile, und Kombination der jeweiligen  Ausgabegewichte mit LM-Bewertungen in einer effizienten Suche nach dem  besten Pfad.Die Kombination von 3. mit 2. stellt also eine hybride Lösung dar. Aber auch 1.kann von 2. profitieren (sofern die gleiche Netzwerk-Topologie benutzt wird),indem die Gewichte aus einem auf größeren Mengen reinem Text trainiertenSprachmodell initialisiert werden (Transfer-Learning).Beide Ansätze profitieren von einer engen Anbindung an den OCR-Suchraum, d.h.eine Übergabe alternativer Zeichen-Hypothesen und ihrer Konfidenz (wie bishernur mit Tesseract möglich und in Zusammenarbeit mit dem Modulprojekt der UBMannheim realisiert). Sie liefern aber auch auf reinem Volltext bereits guteErgebnisse (mit CER-Reduktion von bis zu 5%), sofern genügend passendeTrainingsdaten zur Verfügung stehen und die OCR ihrerseits brauchbareErgebnisse (unterhalb 10% CER) liefert.Für alle Module stehen Kommandozeilen-Schnittstellen für Training undEvaluierung, sowie volle OCR-D-Schnittstellen für Prozessierung und Evaluierungzur Verfügung.Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow        Universität MannheimUniversitätsbibliothek MannheimGitHub: tesseract-ocr/tesseract/Im Fokus des Modulprojekts stand die OCR-Software Tesseract, die seit 1985 vonRay Smith entwickelt wurde, seit 2005 als Open Source unter einer freienLizenz.Das Projekt umfasste zwei Hauptziele: Die Einbindung von Tesseract in denOCR-D-Workflow inklusive Unterstützung der anderen Modulprojekte durch dieBereitstellung von Schnittstellen, sowie die allgemeine Verbesserung derStabilität, Codequalität und Performance von Tesseract.Die Einbindung in den OCR-D-Workflow erforderte wesentlich weniger Aufwand alsursprünglich geplant; hauptsächlich, weil die meiste Arbeit bereits außerhalbdes Modulprojekts geleistet war und dabei die schon vorhandenePython-Schnittstelle tesserocr genutzt werden konnte.Für das OCR-D-Modulprojekt der Universität Leipzig wurde Tesseract um dieGenerierung von alternativen OCR-Ergebnissen für die Einzelzeichen erweitert.Als Eingabedaten für ein OCR-Postkorrektur-Modell lässt sich damit dieTexterkennung weiter verbessern. Ein wertvoller Nebeneffekt des neuen Codessind genauere Zeichen- und Wortkoordinaten.Mit mehreren hundert Korrekturen konnte die Codequalität signifikant gesteigertund ein deutlich stabilerer Programmfluss erreicht werden. Tesseract ist jetztwartbarer, braucht weniger Speicher und ist schneller als zuvor.Eine wesentliche Verbesserung der Erkennungsgenauigkeit für die meisten der fürOCR-D relevanten Druckwerke konnte durch neue generische Modelle für Tesseracterreicht werden. Diese wurden ab Septem-ber 2019 bis Januar 2020 auf Basis derDatensammlung GT4HistOCR trainiert.Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur        Ludwig-Maximilians-Universität MünchenCentrum für Informations- und Sprachverarbeitung (CIS)GitHub: cisocrgroup/ocrd-postcorrection](https://github.com/cisocrgroup/ocrd-postcorrection), cisocrgroup/cis-ocrd-pyDas Ergebnis des Projekts ist ein in den OCR-D-Workflow integriertes SystemA-I-PoCoTo zur vollautomati-schen Nachkorrektur OCR-erfasster historischerDrucke. Das System beinhaltet zudem eine optional nachge-schaltete interaktiveNachkorrektur (I-PoCoTo), die in das interaktive NachkorrektursystemPoCoWeb einge-bunden ist. Das System kann damit auch alternativ alsStand-Alone-Tool zur gemeinschaftlichen webbasierten Nachkorrektur vonOCR-Dokumenten eingesetzt werden.Die Grundlage der vollautomatischen Nachkorrektur ist ein flexibles,featurebasiertes Machine-Learning (ML) Verfahren zur vollautomatischenOCR-Nachkorrektur mit einem besonderen Fokus auf die Vermeidung derVerschlimmbesserungsproblematik. Zur Erkennung von Fehlern und für dieErzeugung von Korrekturkandida-ten verwendet das System die am CIS entwickeltedokumentenabhängige Profilierungstechnologie. Die Fea-tures des Systemsverwenden neben verschiedenen Konfidenzwerten insbesondere auch Informationenaus zusätzlichen Hilfs-OCRs.Das System protokolliert sämtliche Korrekturentscheidungen. Über diesenProtokollmechanismus kann die automatische Postkorrektur in PoCoWebinteraktiv überprüft werden. Dabei können sowohl einzelne getätigteKorrekturentscheidungen manuell rückgängig gemacht werden, als auch nichtgetätigte Korrekturentschei-dungen nachträglich ausgeführt werden.Das gesamte System ist in den OCR-D-Workflow eingebunden und folgt den dortgültigen Konventionen.Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D        Universität LeipzigInstitut für Informatik: Lehrstuhl für Digital HumanitiesFriedrich-Alexander-Universität Erlangen-Nürnberg Department Informatik: Lehrstuhl für Informatik 5: MustererkennungJohannes Gutenberg-Universität Mainz Gutenberg-Institut für Weltliteratur und schriftorientierte Medien: Abteilung Buchwissenschaft_GitHub: OCR-D/okralact, seuretm/ocrd_typegroups_classifierDie Erkennungsquoten von OCR für Drucke, die vor 1800 produziert wurden,variieren sehr stark, da die Diversität historischer Schriftarten in denTrainingsdaten entweder gar nicht oder nur unzureichend berücksichtigt wird.Daher hat sich dieses Modulprojekt, bestehend aus Informatikerinnen undBuchhistorikerinnen, drei Ziele gesteckt:Zum einen haben wir ein Tool zur automatischen Erkennung von Schriftarten inBilddigitalisaten entwickelt. Hier haben wir uns besonders auf gebrocheneSchriften neben der Fraktur konzentriert, die bisher wenig Beachtung gefundenhaben, jedoch im 15. und 16. Jahrhundert weit verbreitet waren: Bastarda,Rotunda, Textura und Schwabacher. Das Tool wurde mit 35.000 Bildern trainiertund erreicht eine Genauigkeit von 98% bei der Bestimmung von Schriftarten.Insgesamt kann es nicht nur zwischen den o.g. Schriftarten differenzieren,sondern auch Hebräisch, Griechisch, Fraktur, Antiqua und Kursiv unterscheiden.In einem zweiten Schritt wurde eine Online-Trainingsinfrastruktur geschaffen(Okralact). Sie vereinfacht die Benutzung verschiedener OCR-engines (Tesseract,Ocropus, Kraken, Calamari) und ermöglicht es zugleich, spezifische Modelle fürbestimmte Schriftarten zu trainieren.Zum Abschluss wurde ein Modellrepositorium eingerichtet, das bereitserarbeitete schriftartspezifische OCR-Modelle enthält. Um hier einen Grundstockzu legen, haben wir insgesamt ca. 2.500 Zeilen für Bastarda, Textura undSchwabacher aus einer Vielzahl verschiedener Bücher transkribiert.Die hohe Genauigkeit des Tools zur Erkennung der Schriftarten eröffnet dieMöglichkeit, in Zukunft durch weitere Trainingsdaten das Tool sogar zwischenden Schriften einzelner Drucker unterscheiden zu lassen, was mehrere Desiderateder historischen Forschung adressieren würde.OLA-HD – Ein OCR-D-Langzeitarchiv für historische Drucke        Georg-August-Universität Göttingen  Niedersächsische Staats- und Universitätsbibliothek   Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen GitHub: subugoe/OLA-HD-IMPLIm September 2018 starteten die Abteilung Digitale Bibliothek derNiedersächsischen Staats- und Universi-tätsbibliothek und die Gesellschaft fürwissenschaftliche Datenverarbeitung Göttingen das DFG-Projekt OLA-HD – EinOCR-D Langzeitarchiv für historischeDrucke.Ziel von OLA-HD ist die Entwicklung eines integrierten Konzepts für dieLangzeitarchivierung und persistente Identifizierung von OCR-Objekten, sowieeine prototypische Implementierung.Im regelmäßigen Austausch mit den Projektpartnern wurden dieBasis-Anforderungen für die Langzeitarchivierung und persistente Identifikationermittelt und in Form einer Spezifikation zur technischen undwirtschaftlich-organisatorischen Umsetzung festgehalten.Mit dem Prototypen kann der Anwender OCR-Ergebnisse eines Werkes als OCRD-ZIPin das System laden. Das System validiert die Zip-Datei, vergibt eine PID undschickt die Datei an den Archiv-Manager (CDSTAR – GWDG Common Data StorageArchitecture). Dieser schreibt die Zip-Datei in dasArchiv (Bandspeicher). Abhängig von der Konfiguration (Datei-Typ, Datei-Größeetc.) werden Dateien zusätzlich in ein Online Storage geschrieben (Festplatte),um einen schnellen Zugriff zu ermöglichen. Der Nutzer hat Zugriff auf alleOCR-Versionen und kann Versionen als BagIt-Zip Dateien herunterladen. AlleWerke und Versionen haben eigene PIDs. Die PIDs werden vom European PersistentIdentifier Consortium (ePIC) Servicegeneriert. Die verschiedenen OCR-Versionen eines Werkes sind über die PIDverknüpft, sodass das System die Versionierung in einer Baumstruktur abbildenkann.Nicht angemeldete Anwender können den Bestand durchsuchen und in derDateistruktur eine Vorschau von Text und – sofern vorhanden – Bild erhaltenbzw. über die verschiedenen Versionen navigieren. Die Anwender können sich überdas GWDG-Portal registrieren und anmelden und können über ein Dashboard ihreDateien verwalten.Bis März 2020 werden kleinere Optimierungen am User-Interface vorgenommen unddas Konzept finalisiert. Im Konzept werden weitere Ausbaustufen beschrieben,die sinnvoll sein können, um die prototypische Soft-ware in ein Produkt zuüberführen.",
      "url": " /de/phase2.html"
    },
  

    {
      "slug": "en-phase3-html",
      "title": "",
      "content"	 : "OCR-D - Phase IIIIn February 2020, the DFG published a call for proposals to continue the OCR-D project in a third project phase. The goal of this phase is the implementation of the OCR-D software in institutions that maintain and process collections. Four implementation and three module projects were approved by the DFG.On 30 July, our kick-off workshop took place, heralding phase III of OCR-D. The team of the Coordination Project introduced objectives and communications channels of phase III, gave an insight into the current status and plans of the OCR-D software, the Web API and the handling of Ground Truth Data in OCR-D. Furthermore, the Coordination Project presented Best Practices of Software Developing in OCR-D, including ideas for the community on how to contribute. In addition, the implementation and module projects presented themselves to the interested community and to our cooperation partners.Implementation ProjectsIntegration of Kitodo and OCR-D for productive mass digitisationUB Braunschweig, SLUB Dresden, UB MannheimWith the workflow management system (WMS) Kitodo.Production and the TYPO3-based presentation module Kitodo.Presentation, Kitodo is a widespread and open solution for mass digitization in culture-preserving institutions, which allows suitable operating models for large and small institutions. A process for text recognition based on the tools and workflows of OCR-D must therefore be designed as a distributed system that does justice to the flexibility of the various operating models, the complex workflows and the needs-based scalability for small to very large digitization projects.The project pursues four project goals that build on one another and complement each other, which should ultimately enable the use of OCR-D in mass digitization with Kitodo:  Construction and documentation of a web-based, scalable OCR-D server  Development of a quality-based workflow optimization for OCR-D  Implementation of an OCR module for Kitodo  Extension of Kitodo.Presentation and DFG-Viewer for OCR on DemandFurther information: Projekt page of the University Library MannheimOPERANDI: OCR-D Performance Optimisation and IntegrationSUB Göttingen, GWDGThe aim of the project is the development of an OCR-D-based implementation package for mass full text capture with an improved throughput and better quality of the results. The aim is to ensure the implementation package can also be used by other projects and institutions with comparable requirements. Two scenarios were identified in the pilot phase. In the first scenario, OCR generation is to take place for works that have already been digitised. In the second scenario, OCR generation for new works takes place as part of the digitisation process.Further information: Project page of the Göttingen State and University LibraryOCR4all libraries – full text recognition of historical collectionsGEI Braunschweig, HCI and ZPD of the University of WürzburgThe project aims to upgrade and adjust the open-source GUI-based tool OCR4all, developed at the ZPD of the University of Würzburg, helping libraries and archives undertaking mass-digitization to implement the solutions developed as part of the OCR-D project in a low-threshold, flexible and self-sufficient manner. In addition, a visual explanation component will support the creation and configuration of optimal OCR workflows.The research library of the GEI Braunschweig, with its collection of digitized 17th and 18th century textbooks, will serve as use-case. All solutions developed during the project will be evaluated every step of the way in close cooperation with and under the supervision of the HCI chair of the University of Würzburg, to ensure that lay users in libraries and archives are able to use OCR-D solutions comfortably and autonomously.Further information Project page of the German Research FoundationODEM: OCR-D extension for mass digitizationULB Sachsen-AnhaltThe University and State Library Saxony-Anhalt has been a partner in the digitization of VD18 holdings for many years now. This project is the next step in the development of these holdings, because the 6.13 million digitized pages will be enriched with fulltext information, using the tools developed in the earlier OCR-D project phases. The amount of data and the diversity of these holdings already show that this is a project under realistic conditions: In mass digitization, many languages are encountered and individual peculiarities of publications must be considered, when using and developing the OCR-D tools to gain fulltext information to increase their usefulness. Since all of these holdings have already been digitized, updating and reformatting the metadata as well as the reuse of already existing information, such as structural metadata, for new viewing formats are key focal points of this project.Further information: Project page of the German Research FoundationModule ProjectsWorkflow for work-specific training based on generic models with OCR-D and ground truth enhancementUB MannheimThe goal of this project is to enable institutions (for example, libraries) to retrain the modules of the OCR-D workflow as easily as possible so that better recognition rates can be achieved for specific works.Further information: Projekt page of the University Library MannheimFont Group Recognition for Improved OCRJGU Mainz, FAU Erlangen-NürnbergThis project builds on the Module Projekt »Entwicklung eines Modellrepositoriums und einer automatischen Schriftarterkennung für OCR-D« in which we developed a tool that identifies the main font group present on a given page.This continuation aims for three main goals:  Development of a more fine-granular font group recognition tool  Generation of font-group-specific OCR training-data  Training of font-group-specific OCR-modelsFurther information: Project page of the German Research FoundationOLA-HD Service – A Generic Service for Long-Term Archiving of Historical PrintsSUB Göttingen, GWDGThe primary goal of this project is the development of a productive service for the long-term archiving of historical prints in the context of OCR-D. This OLA-HD Service is based on the corresponding prototype from OCR-D Phase II, expands it according to the requirements of the implementation projects, is integrated into the OCR-D framework and is generically designed and implemented according to the tender requirements.Further information: Project page of the Göttingen State and University Library",
      "url": " /en/phase3.html"
    },
  

    {
      "slug": "de-phase3-html",
      "title": "OCR-D - Phase III",
      "content"	 : "OCR-D - Phase IIIIm Februar 2020 hat die DFG eine Ausschreibung zur Fortsetzung des OCR-D Projektes in einer dritten Projektphase veröffentlicht. Ziel dieser Phase ist die Implementierung der OCR-D-Software in bestandshaltenden und -verarbeitenden Einrichtungen. Vier Implementierungs- und drei Modulprojekte wurden von der DFG bewilligt.Am 30. Juli fand unser Kick-off-Workshop statt, der Phase III einläutete.Das Team gab eine Einführung in die Ziele und öffentlichen Kommunikationskanäle von OCR-D in Phase III, in Status und Pläne der OCR-Software und der Web-API und in den Umgang mit Ground Truth Daten in OCR-D. Zudem gab das Koordinierungsprojekt einen Einblick in die bisherige Praxis der Softwareentwicklung in OCR-D mit Möglichkeiten, mitzuwirken.Außerdem präsentierten sich unsere Implementierungs- und Modulprojekte gegenseitig sowie unseren Kooperationspartnern.ImplementierungsprojekteIntegration von Kitodo und OCR-D zur produktiven MassendigitalisierungUB Braunschweig, SLUB Dresden, UB MannheimKitodo ist mit dem Workflowmanagementsystem (WMS) Kitodo.Production und dem TYPO3-basierten Präsentationsmodul Kitodo.Presentation eine weit verbreitete und offene Lösung für die Massendigitalisierung in kulturbewahrenden Einrichtungen, die für große und kleine Institutionen passende Betriebsmodelle erlaubt. Ein auf den Werkzeugen und Workflows von OCR-D basierendes Verfahren zur Texterkennung muss deshalb als verteiltes System konstruiert werden, das der Flexibilität der verschiedenen Betriebsmodelle, der komplexen Workflows sowie der bedarfs-gerechten Skalierbarkeit für kleine bis sehr große Digitalisierungsprojekte gerecht wird.Das Projekt verfolgt vier aufeinander aufbauende und sich komplementär ergänzende Projektziele, die im Ergebnis den Einsatz von OCR-D in der Massendigitalisierung mit Kitodo ermöglichen sollen:  Aufbau und Dokumentation eines web-basierten, skalierbaren OCR-D-Servers  Entwicklung einer qualitätsbasierten Workflow-Optimierung für OCR-D  Implementierung eines OCR-Moduls für Kitodo  Erweiterung von Kitodo. Presentation und DFG-Viewer um OCR on DemandWeitere Informationen: Projektseite der Universitätsbibliothek MannheimOPERANDI – OCR-D Performanzoptimierung und Integration. Ein Implementierungspaket der OCR-D-Software für die MassendigitalisierungSUB Göttingen, GWDGDas Ziel von OPERANDI ist die Entwicklung und der Aufbau eines auf OCR-D basierenden Implementierungspaketes zur Massenvolltexterfassung mit verbessertem Durchsatz, bei besserer Qualität der Ergebnisse. Zugleich wird das Ziel verfolgt, dass das Implementierungspaket auch von anderen Vorhaben und Einrichtungen mit vergleichbaren Anforderungen nachgenutzt werden kann. Im Rahmen der Pilotierung wurden zwei Szenarien identifiziert. Im ersten Szenario soll die OCR-Erzeugung für bereits digitalisierte Werke stattfinden, was in einer Massenvolltexterfassung mündet. Im zweiten Szenario erfolgt die OCR-Erzeugung für neue zu digitalisierende Werke im Rahmen des Digitalisierungsprozesses.Weitere Informationen: Projektseite der Niedersächsischen Staats- und Universitätsbibliothek GöttingenOCR4all libraries – Volltexterkennung historischer SammlungenGEI Braunschweig, HCI und ZPD der Universität WürzburgIm Projekt soll das am ZPD der Universität Würzburg entwickelte GUI-basierte Open-Source-Werkzeug OCR4all so erweitert und angepasst werden, dass Bibliotheken und Archive bei ihrer Massendigitalisierung die im Rahmen des OCR-D-Projekts erarbeiteten Lösungen niederschwellig, flexibel und eigenständig einsetzen können. Eine zusätzliche visuelle Erklärungskomponente soll darüber hinaus Unterstützung bei der Erstellung und Konfiguration optimaler OCR-Workflows bieten.Als Use Case fungiert die Forschungsbibliothek des GEI Braunschweig mit ihren digitalisierten Schulbüchern des 17. und 18. Jahrhunderts. Um zunehmende Komplexitäten der so entstehenden OCR-Lösung nutzerorientiert aufzufangen, wird die bestehende grafische Benutzerschnittstelle in enger Kooperation und unter Anleitung des HCI Lehrstuhl der Universität Würzburg angepasst und weiterentwickelt.Weitere Informationen: Projektseite der Deutschen ForschungsgemeinschaftODEM: OCR-D Erweiterung für MassendigitalisierungULB Sachsen-AnhaltDie Universitäts- und Landesbibliothek Sachsen-Anhalt ist bereits seit vielen Jahren Partner bei der Digitalisierung von VD18-Beständen. Dieses Projekt stellt die nächste Weiterentwicklung dieses Bestandsaufbaus dar, in dem die 6,13 Millionen Seiten mittels der in den OCR-D Projektphasen entwickelten Tools um Volltexte angereichert werden. Die Datenmenge und große Diversität der Bestände zeigen bereits, dass es sich bei diesem Projekt um eine Implementierung unter Realbedingungen handelt: In der Massendigitalisierung gibt es eine Vielzahl von auftretenden Sprachen und individuellen Besonderheiten bei Publikationen, die nun mithilfe der OCR-D-Tools, die zu diesem Zweck weiterentwickelt und ergänzt werden, um Volltext ergänzt und so besser nutzbar gemacht werden sollen. Da es sich um bereits digitalisierte Bestände handelt, ist insbesondere die Aktualisierung und Anpassung der Metadaten sowie die Weiternutzung vorhandener Informationen, wie etwa der Strukturierung, für die neu erstellten Ausgabeformate ein zentraler Aspekt dieses Projekts.Weitere Informationen: Projektseite der Deutschen ForschungsgemeinschaftModulprojekteWorkflow für werkspezifisches Training auf Basis generischer Modelle mit OCR-D sowie Ground-Truth-AufwertungUB MannheimZiel dieses Projektes ist, dass Einrichtungen (zum Beispiel Bibliotheken) möglichst einfach die Module des OCR-D-Workflows nachtrainieren können, so dass bessere Erkennungs­raten für spezifische Werke erreicht werden können.Weitere Informationen: Projektseite der Universitätsbibliothek MannheimErkennung von Schriftartgruppen zur OCR-VerbesserungJGU Mainz, FAU Erlangen-NürnbergDieses Projekt baut auf den Ergebnissen des Vorgängerprojekts »Entwicklung eines Modellrepositoriums und einer automatischen Schriftarterkennung für OCR-D« auf, in dem wir ein Werkzeug entwickelt haben, das automatisch die dominierende Schriftart auf einer gegebenen Seite erkennt.Diese Fortsetzung verfolgt drei Hauptziele:  Die Entwicklung eines feingranulareren Schriftarterkennungswerkzeugs  Die Generierung schriftartspezifischer OCR-Trainingsdaten  Das Training schriftartspezifischer OCR-ModelleWeitere Informationen: Projektseite der Deutschen ForschungsgemeinschaftOLA-HD Service – Ein generischer Dienst für die Langzeitarchivierung historischer DruckeSUB Göttingen, GWDGDas primäre Projektziel ist die Entwicklung eines produktiven Dienstes für die Langzeitarchivierung von historischen Drucken im Rahmen von OCR-D. Dieser OLA-HD Service baut auf dem entsprechenden Prototypen aus der OCR-D Phase II auf, erweitert diesen gemäß der Anforderungen der Implementierungsprojekte, wird in das OCR-D Framework integriert und wird entsprechend der Ausschreibungsanforderungen generisch konzipiert und umgesetzt.Weitere Informationen: Projektseite der Niedersächsischen Staats- und Universitätsbibliothek Göttingen",
      "url": " /de/phase3.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-existing-mets-html",
      "title": "",
      "content"	 : "Praxis: OCR von digitalisierten BeständenDer Einfachheit halber verwenden wir ein Beispiel aus dem Bestand an Testdateninnerhalb von OCR-D, da diese weniger umfangreichsind. Prinzipiell funktionieren die OCR-D Werkzeuge aber mit allen METS-Dateien, dieauch im DFG-Viewer angezeigt werden können.Ist ocrd installiert?$ ocrd --helpUsage: ocrd [OPTIONS] COMMAND [ARGS]...  CLI to OCR-DOptions:  --version                       Show the version and exit.  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                  Log level  --help                          Show this message and exit.Commands:  bashlib    Work with bash library  ocrd-tool  Work with ocrd-tool.json JSON_FILE  process    Process a series of tasks  workspace  Working with workspace  zip        Bag/Spill/Validate OCRD-ZIP bagsFalls dieser Befehl nicht klappt, überprüfen Sie ob die Software richtig installiert wurde.METS herunterladenWir verwenden eine auf 2 Seiten reduzierte Erstausgabe von Kant “Kritik der reinen Vernunft” aus dem Bestanddes Deutschen Textarchivs, die in GitHub liegt: https://github.com/OCR-D/assets/blob/master/data/kant_aufklaerung_1784/data/mets.xmlDazu nutze den workspace Unterbefehl von ocrd, der es erlaubt, mit METS zuinteragieren, ohne das XML direkt bearbeiten zu müssen.Der Unterbefehl clone erlaubt das Herunterladen von METS und ggf. enthaltenen Dateien.Mit dem Parameter --download werden auch alle referenzierten Dateien mit heruntergeladen (das entfernte Verzeichnis wird “geklont”)$ ocrd workspace clone --download &quot;https://raw.githubusercontent.com/OCR-D/assets/master/data/kant_aufklaerung_1784/data/mets.xml&quot; kant20:47:48.914 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;/home/ich/kant$ find kantkantkant/OCR-D-IMGkant/OCR-D-IMG/INPUT_0017kant/OCR-D-IMG/INPUT_0020kant/OCR-D-GT-ALTOkant/OCR-D-GT-ALTO/PAGE_0020_ALTOkant/OCR-D-GT-ALTO/PAGE_0017_ALTOkant/OCR-D-GT-PAGEkant/OCR-D-GT-PAGE/PAGE_0017_PAGEkant/OCR-D-GT-PAGE/PAGE_0020_PAGEkant/mets.xmlDas METS liegt nun als mets.xml im Unterverzeichnis kant, die Bilder inUnterverzeichnissen, die Dateigruppen repräsentieren.TIPP Kopieren sie das geklonte Verzeichnis an diesem Punkt. Dann müssen Sie nichtalle Dateien neu laden, falls im weiteren Ablauf etwas schief läuft:$ cp -r kant kant.BAKMETS untersuchenDie METS-Datei könenn wir nun untersuchen.Welche Dateigruppen gibt es?$ cd kant$ ocrd workspace list-groupOCR-D-IMGOCR-D-GT-PAGEOCR-D-GT-ALTOEs gibt also drei Dateigruppen, wobei wir uns für die Bilder interessieren, die in OCR-D-IMG gelistet sind.Welche Seiten gibt es?Physische Seiten werden in METS in einer structMap aufgelistet. Wir können uns die IDs der Seiten ausgeben lassen:$ ocrd workspace list-pagePHYS_0017PHYS_0020Es gibt zwei Seiten.Dateien auflistenWir können uns die enthaltenen Dateien auflisten lassen mit dem find Unterbefehl:$ ocrd workspace findOCR-D-IMG/INPUT_0017OCR-D-IMG/INPUT_0020OCR-D-GT-PAGE/PAGE_0017_PAGEOCR-D-GT-PAGE/PAGE_0020_PAGEOCR-D-GT-ALTO/PAGE_0017_ALTOOCR-D-GT-ALTO/PAGE_0020_ALTOEs gibt sechs Dateien: Für jede der zwei Seiten Dateien für jede der drei Dateigruppen.Standardmäßig wird nur die Datei-ID ausgegeben, weitere Felder können über denParameter -k angegeben werden (ocrd workspace find --help liefert weitereInformationen):$ ocrd workspace find -k url -k mimetype -k pageId -k IDOCR-D-IMG/INPUT_0017    image/tiff      PHYS_0017       INPUT_0017OCR-D-IMG/INPUT_0020    image/tiff      PHYS_0020       INPUT_0020OCR-D-GT-PAGE/PAGE_0017_PAGE    application/vnd.prima.page+xml  PHYS_0017       PAGE_0017_PAGEOCR-D-GT-PAGE/PAGE_0020_PAGE    application/vnd.prima.page+xml  PHYS_0020       PAGE_0020_PAGEOCR-D-GT-ALTO/PAGE_0017_ALTO    application/alto+xml    PHYS_0017       PAGE_0017_ALTOOCR-D-GT-ALTO/PAGE_0020_ALTO    application/alto+xml    PHYS_0020       PAGE_0020_ALTOAnatomie eines ocrd-* KommandozeilenaufrufsAlle OCR-D Werkzeuge folgen einer einheitlichen Aufrufsyntax, die Teil derSpezifikationen ist.ocrd-*   -m mets.xml          # Pfad zur METS-Datei  -I INPUT-GROUP       # Eingabe-Dateigruppe  -O OUTPUT-GROUP      # Ausgabe-Dateigruppe  -l DEBUG|INFO|ERROR  # Optional: Logging Detail Level  -p parameter.json    # Optional: Parameter-Datei  -g PAGE-ID           # Optional: ID einer Seite falls nur eine Seite verarbeitet werden sollBinarisierung mit ocrd-kraken-binarizeDer erste Schritt für die OCR ist die Binarisierung des Bildes, bei der alleFarben durch schwarz oder weiss ersetzt werden.Dafür gibt es verschiedene Implementierungen im OCR-D-Ökosystem, wir verwendenhier die Binarisierung von kraken.$ ocrd-kraken-binarize -m mets.xml -I OCR-D-IMG -O OCR-D-IMG-BIN19:49:28.847 INFO processor.KrakenBinarize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=image/tiff, ID=INPUT_0017, url=OCR-D-IMG/INPUT_0017, local_filename=---]/&amp;gt;19:49:28.868 INFO processor.KrakenBinarize - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7effeb96aeb8&amp;gt;19:49:28.868 INFO processor.KrakenBinarize - About to binarize page &#39;None&#39;19:49:28.871 INFO kraken.binarization - Binarizing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG/INPUT_001719:49:32.928 INFO processor.KrakenBinarize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=image/tiff, ID=INPUT_0020, url=OCR-D-IMG/INPUT_0020, local_filename=---]/&amp;gt;19:49:32.931 INFO processor.KrakenBinarize - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7effeb8ac5f8&amp;gt;19:49:32.931 INFO processor.KrakenBinarize - About to binarize page &#39;None&#39;19:49:32.933 INFO kraken.binarization - Binarizing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG/INPUT_002019:49:36.986 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Damit werden alle Dateien in der OCR-D-IMG Dateigruppe binarisiert, dasErgebnis wird in die Dateigruppe OCR-D-IMG-BIN geschrieben.Zeilensegmentierung mit OcropyAls nächsten Schritt müssen die Zeilen bestimmt werden, damit OCR angewandt werden kann.Auch dafür gibt es verschiedene Implementierungen innerhalb von OCR-D, wir verwenden die Zeilensegmentierung von Ocropus.$ ocrd-ocropy-segment -m mets.xml -I OCR-D-IMG-BIN -O OCR-D-SEG-LINE19:52:45.364 INFO processor.ocropySegment - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0001, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001, local_filename=---]/&amp;gt;19:52:45.365 INFO processor.ocropySegment - downloaded_file &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0001, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001, local_filename=/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG-BIN/OCR-D-IMG-BIN_0001]/&amp;gt;19:52:45.383 INFO processor.ocropySegment - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7fcf0fab2208&amp;gt;19:52:50.705 INFO processor.ocropySegment - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0002, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002, local_filename=---]/&amp;gt;19:52:50.705 INFO processor.ocropySegment - downloaded_file &amp;lt;OcrdFile mimetype=image/png, ID=OCR-D-IMG-BIN_0002, url=OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002, local_filename=/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002]/&amp;gt;19:52:50.706 INFO processor.ocropySegment - pcgts &amp;lt;ocrd_models.ocrd_page_generateds.PcGtsType object at 0x7fcf0fa66358&amp;gt;19:52:55.739 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Ausgehend von den binarisierten Bilder in der Dateigruppe OCR-D-IMG-BIN erzeugen wir PAGE-XML mit Segmentierungsinformation in der Dateigruppe und dem lokalen Verzeichnis OCR-D-SEG-LINE.Das PAGE-XML sieht ausschnittsweise so aus:&amp;lt;pc:Page imageFilename=&quot;OCR-D-IMG-BIN/OCR-D-IMG-BIN_0002&quot; imageWidth=&quot;1456&quot; imageHeight=&quot;2084&quot;&amp;gt;    &amp;lt;pc:TextRegion&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0002&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;103,136 122,136 122,188 103,188&quot;/&amp;gt;        &amp;lt;/pc:TextLine&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0003&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;126,125 166,125 166,185 126,185&quot;/&amp;gt;        &amp;lt;/pc:TextLine&amp;gt;        &amp;lt;pc:TextLine id=&quot;line_0004&quot;&amp;gt;            &amp;lt;pc:Coords points=&quot;166,124 209,124 209,168 166,168&quot;/&amp;gt;      &amp;lt;/pc:TextLine&amp;gt;      &amp;lt;!-- ... --&amp;gt;Wir können uns das Ergebnis auch im PAGE Viewer visualisieren:Die Zeilenerkennung ist nicht perfekt (Artefakte im Randbereich, ueberlangeZeilen), aber gut genug, um OCR durchzuführen.Texterkennung mit tesseractDas Projekt ocrd_tesserocr bindet tesseract über die tesserocrPython-Bindings an OCR-D an.Der Aufruf ist analog zu Binarisierung und Segmentierung:$ ocrd-tesserocr-recognize -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS21:09:52.910 INFO processor.TesserocrRecognize - Using model &#39;eng&#39; in /usr/share/tesseract-ocr/4.00/tessdata/ for recognition at the line level21:09:52.910 INFO processor.TesserocrRecognize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0001, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0001.xml, local_filename=---]/&amp;gt;21:09:53.071 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:10:24.127 INFO processor.TesserocrRecognize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0002, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0002.xml, local_filename=---]/&amp;gt;21:10:24.357 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:11:09.617 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Das Ergebnis ist allerdings nicht optimal. Ausschnitt aus kant/OCR-D-OCR-TESS/OCR-D-OCR-TESS_0002: &amp;lt;pc:TextLine id=&quot;line_0006&quot;&amp;gt;     &amp;lt;pc:Coords points=&quot;529,410 1342,410 1342,468 529,468&quot;/&amp;gt;     &amp;lt;pc:TextEquiv conf=&quot;0.47&quot;&amp;gt;         &amp;lt;pc:Unicode&amp;gt;geenegerc wogpe35tro rappucb धाम es Bgrenbeire ﺪﺷﺭ&amp;lt;/pc:Unicode&amp;gt;     &amp;lt;/pc:TextEquiv&amp;gt; &amp;lt;/pc:TextLine&amp;gt;Das liegt daran, dass wir nicht angegeben haben, welches Modell verwendet werden soll. Standardmäßig verwendettesseract daher eng, das Modell für englischsprachige Antiqua. Für deutsche Fraktur benötigen wir dasModell frk, das im tesseract-Modell-GitHub-Repository zu finden ist.$ cd /usr/share/tesseract-ocr/4.00/tessdata/$ sudo wget &#39;https://github.com/tesseract-ocr/tessdata_best/raw/master/frk.traineddata&#39;Nun führen wir den befehl noch einmal aus aber geben als Parameter das zu verwendende Modell mit an:$ ocrd-tesserocr-recognize -m mets.xml -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-FRK -p &amp;lt;(echo &#39;{&quot;model&quot;: &quot;frk&quot;}&#39;)21:09:52.910 INFO processor.TesserocrRecognize - Using model &#39;frk&#39; in /usr/share/tesseract-ocr/4.00/tessdata/ for recognition at the line level21:09:52.910 INFO processor.TesserocrRecognize - INPUT FILE 0 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0001, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0001.xml, local_filename=---]/&amp;gt;21:09:53.071 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:10:24.127 INFO processor.TesserocrRecognize - INPUT FILE 1 / &amp;lt;OcrdFile mimetype=application/vnd.prima.page+xml, ID=OCR-D-SEG-LINE_0002, url=OCR-D-SEG-LINE/OCR-D-SEG-LINE_0002.xml, local_filename=---]/&amp;gt;21:10:24.357 INFO processor.TesserocrRecognize - Recognizing text in page &#39;None&#39;21:11:09.617 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/kant/mets.xml&#39;Vergleichen wir die Erkennung der Textzeile:&amp;lt;pc:TextLine id=&quot;line_0006&quot;&amp;gt;    &amp;lt;pc:Coords points=&quot;529,410 1342,410 1342,468 529,468&quot;/&amp;gt;    &amp;lt;pc:TextEquiv conf=&quot;0.9&quot;&amp;gt;        &amp;lt;pc:Unicode&amp;gt;gewiegelt worden ; ſo ſchädlich iſt es Vorurtheile zu&amp;lt;/pc:Unicode&amp;gt;    &amp;lt;/pc:TextEquiv&amp;gt;&amp;lt;/pc:TextLine&amp;gt;Das sieht nicht nur korrekt aus, auch tesseract ist sich sicherer: Konfidenz 0.9. vs 0.47 oben.“In einem Rutsch”Die einzelnen Prozessschritte nacheinander auszuführen wie beschrieben ist mühselig. Wenn an einemPunkt im Ablauf etwas schief läuft, muss der gesamte Vorgang abgebrochen werden. Daher bietet ocrdeinen Modus, in dem ein kompletter Ablauf von Prozesschritten angegeben werden kann. Wenn innerhalbdes Prozesses ein Fehler auftritt, endet der Gesamtprozess mit der Fehlermeldung.Die Syntax entsprocht weitgehend dem individuellen Aufruf mit der Einschränkung dass ocrd- weggelassenwird:$ ocrd process -m mets.xml   &#39;kraken-binarize -I OCR-D-IMG -O OCR-D-IMG-BIN&#39;   &#39;ocropy-segment -I OCR-D-IMG-BIN -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS&#39;AufgabeWenden Sie die gelernten Schritte an auf die erste Seite des Kommunistischen Manifests.Die URL der METS-Datei: https://raw.githubusercontent.com/OCR-D/assets/master/data/communist_manifesto/data/mets.xmlFalls Sie Python 3.7 verwenden und deshalb ocrd_kraken nicht installieren konnten: Hier ist ein OCRD-ZIPder Aufgabe bis einschliesslich Binarisierung: binarized.zip",
      "url": " /slides/2019-03-25-dhd/praxis-existing-mets.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-gt-html",
      "title": "",
      "content"	 : "OCR-D Ground Truth PraxisRepository-URL: https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagitWerkzeuge  TranskribusTranskribus ist eine Werkzeug mit der auf Basis des PAGE-Formates Transkribtionen erstellt werden können. Diese Transkribtionen  können für das Training von Texterkennungs-Software genutzt werden.  AletheiaAletheia ist die Refenenzsoftware des am PRIma-LAB (Pattern Recognition &amp;amp; Image Analysis Research Lab) entwickelten PAGE-Formates. Mit dieser Software werden Transkribtionen erstellt, die für das Training von Texterkennungs-Software genutzt werden können.Beide Werkzeuge können für die Erstellung von Ground Truth genutzt werden.GuidelinesOCR-D GroundTruth GuidelinesPräsentation: https://ocr-d.github.io/gt/trans_documentation/index.htmlDokumentation: https://github.com/OCR-D/gt-guidelinesPage-XMLDokumentation zum PAGE XML Format for Page Content im Rahmen von OCR-Dhttps://ocr-d.github.io/gt/trans_documentation/trPage.htmlGitHub: https://github.com/PRImA-Research-Lab/PAGE-XMLTools und HelferleinTanskribus Software: https://transkribus.euKurz-Hilfe: https://transkribus.eu/wiki/images/c/cf/Transkribus_in_10_Schritten.pdfAletheiaSoftware: https://www.primaresearch.org/tools/Aletheia/Editions~Hinweis~ Es liegt nur eine Windows-Version vor.makeAletheia_metsErstellung einer Mets-Datei (Page Collections-Datei), um einfach mit Aletheia zu arbeiten.https://github.com/tboenig/makeAletheia_metsTranskribus_mets2Aletheia_metsKonvertierung einer vorhandenen Transkribus-METS-Datei in eine  Aletheia-METS-Datei.https://github.com/tboenig/Transkribus_mets2Aletheia_metsWorkshopAufgabe:  Arbeiten in Transkribus: Erstellung von GroundTruth auf Basis von Digitalisaten          Erstellung einer Transkribtion und der Zuordnung von Seitenregionen.        Arbeiten in Altheia: Bearbeitung in Aletheia          Alternative Aufgabe: Öffnen der Datei im PAGE-Viewer        Arbeiten mit dem GT-OCR-D Repositorium: Download und Bearbeitung von GroundTruth aus dem GT-OCR-D Repositorium.Aufgabe 1:Voraussetzung: installierter Transkribus (https://transkribus.eu/Transkribus/)~Hinweis:~ Dazu ist ein Acount bei Transkribus notwendig.  Starten Sie Transkribus und loggen Sie sich ein.  Öffnen Sie die Collection OCR-D_DHD2019 Collection.  Sie finden darin: a_gehema_feldapotheke_1688_3, Öffnen Sie das Dokument.                  Das PAGE-Format bietet eine strukturelle Transkribtion von Regionen -&amp;gt; Zeilen -&amp;gt; Wörtern an. [Zeichen(Glyphen) können ebenfalls mit PAGE transkibiert werden.]                  Reduzieren Sie die Anzeigen nur auf die Zeilenebene.    Klicken Sie die erste Zeile an und beginnen Sie mit der Transkribtion des Textes.    Zeichen die sich außerhalb des Tastaturlayouts befinden, können Sie durch das Virtual keyboard eingeben.    Neben der Transkribtion ist die Markierung und Klassifizierung der Seitenregionen vorzunehmen.siehe: https://ocr-d.github.io/gt//trans_documentation/lyTextregionen.html  Folgende Regionen sind zu unterscheiden:          Textregion : TextRegion,      Abbildungen, Fotos : ImageRegion,      Buchschmuck, Zeichnungen : GraphicRegion,      Trennlinien, Separatoren : SeparatorRegion,      Tabellen : TableRegion,      Strichzeichnungen : LineDrawingRegion,      Mathematische Formeln : MathsRegion,      Chemische Formeln : ChemRegion,      Noten : MusicRegion,      Werbung : AdvertRegion und      Schmutz, Verfärbungen, Rauschen : NoiseRegion        Diese Regionen sind unter dem Metadaten-&amp;gt;Structural zu finden.                  Zum Abschluss der Arbeiten exportieren Sie Ihren Ground Truth.                Aufgabe 2:Um den GroundTruth aus Transkribus in Aletheia zu bearbeiten, kann die zur Verfügung gestellte METS-Datei genutzt werden. Jedoch muß diese in das Aletheia-Format konvertiert werden.Allgemeine Informationen zu METS: https://de.wikipedia.org/wiki/Metadata_Encoding_%26_Transmission_StandardMETS-Standard: http://www.loc.gov/standards/mets/Voraussetzung: Der SAXON : The XSLT and XQuery Processor ist auf Ihrem Rechner installiert.  Laden Sie sich die entsprechenden Tools (XSL-Transformation) von https://github.com/tboenig/Transkribus_mets2Aletheia_mets herunter  Entpacken Sie die Zip-Datei.  Öffnen Sie die Kommandozeile/Terminal          Windows:                  Tippen Sie in das Programm-Suche-Feld: cmd ein. Es öffnet sich ein DOS/Windows Terminal. In diesem können Sie mit dem SAXON Processor arbeiten.          Nutzen Sie Ihr Linux-Sub-System (Betrifft nur Nutzer von Windows10).                          Starten Sie die Linux-App.              Um Zugriff auf die Dateien Ihres Rechners zu bekommen nutzen Sie in Ihrem Linux-Subsystem das Kommando cd für Change Directory m cd /mnt/[Laufwerks-Angabe]/Users/Path zu den entsprechenden Ordnern/Dateien                                Für beide Systemvarianten gilt, passen Sie den Kommandozeilenaufruf an Ihr Dateisystem an.            java -jar ../saxon9he.jar -xsl:../xsl/Transkribus_mets2Aletheia_mets.xsl -s:../example/mets.xml -o: ../example/mets_aletheia.metsx                                Schreiben Sie den angpassten Kommandozeilenaufruf in das Kommandozeilenfenster/Terminal und starten Sie die Transformation durch drücken der Eingabetaste (Returntaste).                      Linux          Öffnen Sie einen Terminal      Passen Sie den Kommandozeilenaufruf an Ihr Dateisystem an.        java -jar ../saxon9he.jar -xsl:../xsl/Transkribus_mets2Aletheia_mets.xsl -s:../example/mets.xml -o: ../example/mets_aletheia.metsx                    Schreiben Sie den angepassten Kommandozeilenaufruf in das Kommandozeilenfenster/Terminal und starten Sie die Transformation durch drücken der Eingabetaste (Returntaste oder Entertaste).      Hinweis: Wo ist meine Eingabetaste (Returntaste oder Entertaste)?https://upload.wikimedia.org/wikipedia/commons/a/a5/Enter.png            Parameter      kurze Erklärung                  -xsl:      Angabe des XSL-Datei.              -s:      Angabe der Datei die transformiert werden soll.              -o:      Angabe der Ausgabedatei.      Alternative Aufgabe 2:Eine PAGE-Datei kann auch im PAGE-Viewer angezeigt werden. Dazu ist eine valide PAGE-Datei notwendig.Zur Zeit werden PAGE-Dateien in Transkribus mit einigen Erweiterungen ausgeliefert, die zum PAGE-Schema nicht konform sind.Deshalb müssen diese Erweiterungen vor dem Öffnen mit dem PAGE-Viewer zum Beispiel manuell entfernt werden.Ausschnitt aus der PAGE-Datei von Transkribus:&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&amp;gt;&amp;lt;PcGts xmlns=&quot;http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15 http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15/pagecontent.xsd&quot;&amp;gt;    &amp;lt;Metadata&amp;gt;        &amp;lt;Creator&amp;gt;OCR_D&amp;lt;/Creator&amp;gt;        &amp;lt;Created&amp;gt;2016-09-20T13:04:41.875+02:00&amp;lt;/Created&amp;gt;        &amp;lt;LastChange&amp;gt;2018-04-23T12:49:58.191+02:00&amp;lt;/LastChange&amp;gt;        &amp;lt;Comments&amp;gt;                Measurement unit: pixel                PrimaryLanguage: German                Language: GermanStandard                Producer: ABBYY FineReader Engine 11&amp;lt;/Comments&amp;gt;        &amp;lt;TranskribusMetadata docId=&quot;6557&quot; pageId=&quot;213761&quot; pageNr=&quot;1&quot; tsid=&quot;3193617&quot; status=&quot;GT&quot; userId=&quot;2082&quot; imgUrl=&quot;...&quot; xmlUrl=&quot;...&quot; imageId=&quot;205160&quot;/&amp;gt;    &amp;lt;/Metadata&amp;gt;    [...]&amp;lt;/PcGts&amp;gt;Der Eintrag:&amp;lt;TranskribusMetadata docId=&quot;6557&quot; pageId=&quot;213761&quot; pageNr=&quot;1&quot; tsid=&quot;3193617&quot; status=&quot;GT&quot; userId=&quot;2082&quot; imgUrl=&quot;...&quot; xmlUrl=&quot;...&quot; imageId=&quot;205160&quot;/&amp;gt;ist zu löschen.  Laden Sie eine Transkribus-PAGE-Datei in einen Text oder XML-Editor.  Markieren und löschen Sie die gesamten Zeile: &amp;lt;TranskribusMetadata...  Speichern Sie Datei unter einem neuen Namen zum Beispiel: [alter Name]_PAGEViewer.xml  Öffnen Sie den PAGE-Viewer und laden Sie die bearbeitete PAGE-Datei in den Viewer.  Aufgabe 3:Das OCR-D-GT-Repositorium speichert, verwaltet und archiviert GroundTruth-Daten. Für das Training und für die Evaluation können aus diesem Repositorium entsprechende Daten verwendet werden. Aber auch GroundTruth-Daten die in verschiedenen Kontexten erstellt wurden können in diesem Repositorium gespeichert und archiviert werden. Möchten Sie Ihren GroundTruth-Korpus zur Verfügung stellen dann nehmen Sie mit OCR-D Kontakt auf. Schreiben Sie eine E-Mail an ocrd@bbaw.de.  Öffnen Sie die Webseite: https://ocr-d-repo.scc.kit.edu/api/v1/metastore/bagit.  In der Tabelle wählen Sie die erste Zeile aus und clicken Sie auf den Link zur zip-Datei in der Spalte URL.https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/f15fb8c8-3842-4314-9a44-5e8b472d7bfc/data/buerger_gedichte_1778.ocrd.zip  Speichern und entpacken Sie die zip-Datei  Starten Sie den PAGE-Viewer  Laden Sie eine PAGE-Datei, die sich im Ordner: buerger_gedichte_1778.ocrd-1/data/OCR-D-GT-SEG-BLOCK  Da die Dateien keine Dateiendungen besitzen, werden Sie im Datei-Öffnen-Fenster zuerst nicht angezeigt.            Windows-Explorer-Ansicht      Datei-Öffnen-Fenster im PAGE-Viewer                                Damit Sie alle Dateien im Select Document File-Fenster sehen können, geben Sie einen Stern () unter *Dateiname ein und bestätigen die Eingabe mit Enter.  Das Laden der Bilddateien erfolgt auf gleichem Weg.http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221",
      "url": " /slides/2019-03-25-dhd/praxis-gt.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-new-mets-html",
      "title": "",
      "content"	 : "Praxis: OCR von willkürlichen BildernDer Fokus von OCR-D ist auf Massendigitalisierung historischer Bestände.Deswegen wird konsequent immer METS verwendet.Um mit willkürlichen Bildern innerhalb OCR-D zu arbeiten, müssen wir daher METSerzeugen.Als Beispiel verwenden wir die erste Seite der englischen Ausgabe des Kommunistischen Manifests (Quelle: https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Manifesto_of_the_Communist_Party.djvu/page15-2745px-Manifesto_of_the_Communist_Party.djvu.jpg).Neues METS erzeugenDer workspace Unterbefehl erlaubt es, neues METS zu initiieren (init):$ ocrd workspace init communist_manifesto22:58:38.321 INFO ocrd.resolver - Writing /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto/mets.xml22:58:38.322 INFO ocrd.workspace - Saving mets &#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto/mets.xml&#39;/home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/communist_manifesto$ find communist_manifestocommunist_manifestocommunist_manifesto/mets.xmlVerzeichnis für Bild anlegenPer Konvention heisst die Dateigruppe mit dem unkomprimierten Bild innerhalb von OCR-D immer OCR-D-IMG:$ cd communist_manifesto$ mkdir OCR-D-IMGBild herunterladen$ curl &#39;https://raw.githubusercontent.com/OCR-D/assets/master/data/communist_manifesto/data/OCR-D-IMG/OCR-D-IMG_0015&#39; &amp;gt; OCR-D-IMG/OCR-D-IMG_0015.pngBild zum METS hinzufügenDateien können mit dem add Unterbefehl von ocrd workspace hinzugefügt werden.Dafür sind eine Reihe von Parametern notwendig, die Sie der Hilfe entnehmen können:$ ocrd workspace add --helpUsage: ocrd workspace add [OPTIONS] LOCAL_FILENAME  Add a file LOCAL_FILENAME to METS in a workspace.Options:  -G, --file-grp TEXT  fileGrp USE  [required]  -i, --file-id TEXT   ID for the file  [required]  -m, --mimetype TEXT  Media type of the file  [required]  -g, --page-id TEXT   ID of the physical page  --force              If file with ID already exists, replace it  --help               Show this message and exit.Wir fügen also die Datei hinzu:$ ocrd workspace add -g P0015 -G OCR-D-IMG -i OCR-D-IMG_0015 -m image/png OCR-D-IMG/OCR-D-IMG_0015.pngUnd überprüfen ob sie wirklich hinzugefügt wurde:$ ocrd workspace findOCR-D-IMG/OCR-D-IMG_0015Eindeutigen Identifier hinzufügenNun setzen wir noch einen eindeutigen Identifier:$ ocrd workspace set-id &#39;1234567890&#39;ValidierenWir können nun überprüfen, ob das Verzeichnis den Anforderungen von OCR-Dgenügt. Dazu verwenden wir den validate Unterbefehl von ocrd workspace.Bei der Validierung überspringen wir die Untersuchung der Auflösung, dadiese aus technischen Gründen häufig nicht gegeben ist, was zu Fehlern führt, die keine sind.$ ocrd workspace validate --skip pixel_density mets.xml&amp;lt;report valid=&quot;true&quot;&amp;gt;&amp;lt;/report&amp;gt;Fertig!Das Verzeichnis ist nun in einer Form, dass es mit OCR-D Werkzeugen weiterverarbeitet werden kann.Im Ergebnis sollte das von Ihnen erstellte Verzeichnis dem Beispieldatensatz im OCR-D/assets Repository entsprechen.",
      "url": " /slides/2019-03-25-dhd/praxis-new-mets.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-praxis-ocrd-zip-html",
      "title": "",
      "content"	 : "Praxis: OCRD-ZIP erzeugenOCRD-ZIP ist das Austauschformat, in demWorkspaces archiviert werden. Dies ist das Format, mit dem alle Repostorienarbeiten (Ground Truth, Forschungsdaten, Langzeitarchivierung).OCRD-ZIP basiert auf dem [BagIt] Standard, der in der Archiv-Community verbreitet istund von der Library of Congress gepflegt wird.Ein OCRD-ZIP ist ein ZIP-Archiv, das neben den OCR-Daten auch Metadaten enthält,bspw. Prüfsummen für die enthaltenen Dateien und Provenienzinformationen.DateistrukturSiehe Folien vom OCR-D Entwicklerworkshop Februar 2019Erzeugen von OCRD-ZIPOCRD-ZIP kann aus bestehenden Verzeichnissen mit dem zip Unterbefehl von ocrd verarbeitet werden.Zum Erzeugen von OCRD-ZIP verwenden wir den bag Unterbefehl:$ ocrd zip --skip-zip --id foo out23:29:30.973 INFO ocrd.workspace_bagger - Bagging /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo to /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out (temp dir /tmp/ocrd-bagit-n_ag6g3e)23:29:30.974 INFO ocrd.workspace_bagger - Resolving OCR-D-IMG/OCR-D-IMG_0015 (partial)23:29:30.974 INFO ocrd.workspace_bagger - Resolved /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/OCR-D-IMG/OCR-D-IMG_001523:29:30.984 INFO bagit - Using 1 processes to generate manifests: sha51223:29:30.984 INFO bagit - Generating manifest lines for file data/mets.xml23:29:30.985 INFO bagit - Generating manifest lines for file data/OCR-D-IMG/OCR-D-IMG_001523:29:31.014 INFO bagit - Creating /tmp/ocrd-bagit-n_ag6g3e/tagmanifest-sha512.txt23:29:31.018 INFO ocrd.workspace_bagger - Created bag at /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/outDas Ergebnis liegt also nun im Verzeichnis out:$ find outoutout/tagmanifest-sha512.txtout/bagit.txtout/manifest-sha512.txtout/dataout/data/OCR-D-IMGout/data/OCR-D-IMG/OCR-D-IMG_0015out/data/mets.xmlout/bag-info.txtValidierenUm sicherzugehen, dass das OCRD-ZIP auch den Vorgaben entspricht, könen wir den validate Unterbefehl von ocrd zip verwenden.$ ocrd zip validate --skip-unzip out23:33:11.823 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/data/mets.xml23:33:11.824 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/data/OCR-D-IMG/OCR-D-IMG_001523:33:11.846 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/bagit.txt23:33:11.846 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/manifest-sha512.txt23:33:11.847 INFO bagit - Verifying checksum for file /home/kba/build/github.com/OCR-D/monorepo/slides/2019-03-25-dhd/foo/out/bag-info.txtOKDas OCRD-ZIP ist also valide und kann ins Repository!AufgabeErstellen Sie aus einem der Verzeichnisse aus den vorherigen Aufgaben ein OCRD-ZIP.Validieren Sie das OCRD-ZIP",
      "url": " /slides/2019-03-25-dhd/praxis-ocrd-zip.html"
    },
  

    {
      "slug": "en-project-summary-html",
      "title": "Brief Description &amp; Project Goals",
      "content"	 : "Brief Description &amp;amp; Project GoalsCoordinated Funding Initiative for the Further Development of Methods for Optical Character Recognition (OCR) “Short name: OCR-DProject PartnersDuke August Library Wolfenbüttel](http://www.hab.de/), Berlin-Brandenburg Academy of Sciences and Humanities, State Library of Berlin Prussian Cultural Heritage, Karlsruhe Institute of Technology(Project managers and contact persons see Contact)Project Duration2015–2020Funded byGerman Research Foundation Scientific Literature and Information Systems (LIS)Project ProgressThe “Coordinated Funding Initiative for the Further Development of Optical Character Recognition Methods” (OCR-D) began its first project phase in the third quarter of 2015. Requirements for the further development of automatic text recognition were collected and analyzed in six work packages. The work finally led to the DFG’s call for proposals “Scalable methods of text and structure recognition for the full text digitisation of historical prints” in March 2017. The approval of eight (module) projects by the DFG at the end of December 2017 marks the end of the first and the beginning of the second project phase, in which the module projects are coordinated and supported and their project results are tested and integrated. In order to be able to fulfill the tasks of the coordination project over the entire duration of all module projects, the DFG approved an extension of the project until July 2020.GoalsAn essential main goal of OCR-D is the conceptual preparation of the transformation of VD-prints (16th–19th century) into machine-readable form and the provision of the necessary tools.In order to achieve this, the coordination project and the module projects aim to meet the following objectives:  the creation of reference corpora for training and testing  the development of standards in the areas of metadata, documentation and ground truth  the further development of individual processing steps, with a particular focus on Optical Layout Recognition (OLR)  the analysis of existing tools and their further development  the creation of an OCR-D framework  the establishment of quality assurance procedures**At the end of the overall project, a software package for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries will be made available. Furthermore an accompanying concept will provide answers to technical, information scientifical and organisational questions regarding the possible mass processing of these data.",
      "url": " /en/project-summary.html"
    },
  

    {
      "slug": "en-project-html",
      "title": "The OCR-D Project",
      "content"	 : "The OCR-D Project**OCR-D is a coordination project aimed at the further development of Optical Character Recognition (OCR) techniques for historical prints.Workflows and methods of automatic text recognition are examined, described and, if necessary, optimized. An essential goal is the conceptual preparation of the transformation of German prints from the 16th to the 19th century into electronic full texts.This project involves the Duke August Library Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the State Library of Berlin Prussian Cultural Heritage and the Karlsruhe Institute of Technology. The Bavarian State Library was also involved until 31/08/2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have image digitised extensive holdings. With the help of OCR procedures, searchable full texts can be automatically generated from these image data. The use of digital full texts is indispensable today in many scientific disciplines, especially in the field of (digital) humanities.So far, however, access to the electronic full text has often been impossible, or only inadequately possible. Many historical holdings are available in digitised form through the “ Union Catalogues of Books Printed in German Speaking Countries “ (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially gothic types, are difficult to identify.There is a need for development, which we have uncovered in OCR-D. On the basis of existing tools and investigations, the OCR process is to be optimized for VD prints. In addition, answers will be found to the associated technical, information scientific and organizational problems. In contrast to other OCR projects, the focus is not on developing a new, powerful OCR engine. Instead, full text digitization is seen as a process that is implemented in modular open source software. The processes and parameters can be traced and, if required, tailor-made workflows can be defined that deliver optimal results for specific titles.The project is funded by the German Research Foundation](http://www.dfg.de/) and runs until July 2020. In the first phase, needs were identified and concepts for the further course were developed. The cooperation structure was consolidated and continued in the second phase. In this phase, the identified needs are addressed by eight module projects, which partly develop existing tools for the automated processing of early modern printing, partly set up new tools. In all steps, we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digital copies of the printed German cultural heritage of the 16th to 19th centuries is to be developed.",
      "url": " /en/project.html"
    },
  

    {
      "slug": "en-spec-provenance-html",
      "title": "Provenance",
      "content"	 : "ProvenanceProvenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. (Source: The PROV Data Model (PROV-DM))Data ModelThe PROV Data Model (PROV-DM) is used to store all provenance metadata.All provenance have to be stored in files. There’s provenance at the page level, but there’s also provenance at the document level.All files regarding provenance are stored in a subfolder metadata.FormatThe workflow provenance is stored in PROV-XML.TypesAll Activities, Entities belonging to the OCR-D workflow have the same namespace.Namespace  Prefix  ocrd  Namespace  http://www.ocr-d.de            Type      Data Type      Description                  Entity      ocrd:mets      Filename  of METS file              Entity      ocrd:mets_referencedFile      ID of the file referenced inside METS.              Entity      ocrd:parameter_file      Content of the parameter file.              Activity      ocrd:processor      Processor that was executed              Activity      ocrd:workflow      Workflow that was executed      ContentOnly the following information is stored for provenance:(a) General data  Workflow engine          Label including version      Start date      End date      (b) Processor data  Processor          Label including version, conforming to OCR-D mets:agent/mets:name (e.g.: ocrd-kraken-binarize_Version 0.1.0, ocrd/core 1.0.0)      Start date      End date        Content of METS file before executing the processor  Content of METS file after executing processor  ID of the input file(s)  ID of output file(s)  Content of parameter.json (optional)Input/OutputAll files referenced in METS must also be referenced in provenance by their mets:file/@ID.A file may be linked to its location (URL). The location may be replaced due to different uses:  local files  external filesAll files not referenced in METS must be linked to their content in provenance. (e.g.: parameter.json)Ingest Workspace to OCR-D RepositoriumAt least before ingesting into repository/LTA, the entire provenance must be stored in one file (metadata/ocrd_provenance.xml) to make the provenance searchable.Therefore all the provenance files are merged into one big file.This file replaces all provenance files stored in subfolder ‘metadata’ExampleThe file structure could look like this after a workflow with 4 steps has been executed.metadata/   |   +-- mets.xml.&#39;workflowid&#39;_0000   |   +-- mets.xml.&#39;workflowid&#39;_0001   |   +-- mets.xml.&#39;workflowid&#39;_0002   |   +-- mets.xml.&#39;workflowid&#39;_0003   |   +-- mets.xml.&#39;workflowid&#39;_0004   |   +-- ocrd_provenance.xml   |   +-- provenance_&#39;workflowid&#39;.xml (optional)Provenance and BagItThe provenance MAY be stored as tag directory in the bagIt container.E.g.:&amp;lt;base directory&amp;gt;/         |         +-- bagit.txt         |         +-- manifest-&amp;lt;algorithm&amp;gt;.txt         |         +-- [additional tag files]         |         +-- data/         |     |         |     +-- mets.xml         |     |         |     +-- ...         |         +-- metadata                |                +-- mets.xml.&#39;workflowid&#39;_0000                |                +-- ...                |                +-- mets.xml.&#39;workflowid&#39;_XXXX                |                +-- ocrd_provenance.xml",
      "url": " /en/spec/provenance.html"
    },
  

    {
      "slug": "en-publications-html",
      "title": "Publications",
      "content"	 : "Publications  Baierer, Konstantin; Büttner, Andreas; Engl, Elisabeth; Hinrichsen, Lena; Reul, Christian: OCR-D &amp;amp; OCR4all: Two Complementary Approaches for Improved OCR of Historical Sources, in: Proceedings of the 6th International Workshop on Computational History (HistoInformatics 2021) co-located with ACM/IEEE Joint Conference on Digital Libraries 2021 (JCDL 2021), 01.10.2021. Online: &amp;lt;http://ceur-ws.org/Vol-2981/&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Dong, Rui; Burghardt, Manuel; Christlein, Vincent: New Approaches to OCR for Early Printed Books, in: DigItalia 15 (2), 12.2020, S. 74–87. Online: &amp;lt;https://doi.org/10/ghstmx&amp;gt;.    Engl, Elisabeth: OCR-D kompakt: Ergebnisse und Stand der Forschung in der Förderinitiative, in: Bibliothek Forschung und Praxis 44 (2), 29.07.2020, S. 218–230. Online: &amp;lt;https://doi.org/10.1515/bfp-2020-0024&amp;gt; ; Video: &amp;lt;OCR-D kompakt&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Neudecker, Clemens: Volltexte – die Zukunft alter Drucke. Bericht zum Abschlussworkshop des OCR-D-Projekts, in: o-bib 7 (2), 05.05.2020, S. 1–4. Online: &amp;lt;https://doi.org/10.5282/o-bib/5600&amp;gt;.    Boenig, Matthias; Engl, Elisabeth; Baierer, Konstantin; Hartmann, Volker; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, in: DHd 2020: Spielräme - Digital Humanities zwischen Modellierung und Interpretation. Konferenzabstracts, Paderborn 05.03.2020, S. 244–247. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth; Boenig, Matthias; Baierer, Konstantin; Neudecker, Clemens; Hartmann, Volker: Volltexte für die Frühe Neuzeit. Der Beitrag des OCR-D-Projekts zur Volltexterkennung frühneuzeitlicher Drucke, in: Zeitschrift für Historische Forschung 47 (2), 2020, S. 223–250.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 1–6. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352640&amp;gt;.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 25–30. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352638&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open source OCR framework for historical documents, in: EuropeanaTech Insight (13), 31.07.2019. Online: &amp;lt;https://pro.europeana.eu/page/issue-13-ocr#ocr-d-an-end-to-end-open-source-ocr-framework-for-historical-documents&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 19.24. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322908&amp;gt;.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 3–8. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322916&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 53–58. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322917&amp;gt;.    Engl, Elisabeth: Das Projekt OCR-D – Ein Fortschrittsbericht zur Volltextdigitalisierung frühneuzeitlicher Drucke, in: Medium Buch 1, 2019, S. 233–235.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, in: Conference Abstracts, Galway 08.12.2018. Online: &amp;lt;https://eadh2018.exordo.com/files/papers/92/final_draft/EADH_2018_Proposal_Brief_Final.pdf&amp;gt;.    Schulz, Klaus; Fink, Florian: Novel software for cleansing digitised historical texts, in: Scientia, 28.11.2018. Online: &amp;lt;https://doi.org/10.26320/SCIENTIA278&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, in: Konferenzabstracts, Köln 28.02.2018, S. 219–223. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D – Koordinierte Förderinitiative zur Weiterentwicklung von OCR-Verfahren, in: Bibliotheksdienst 52 (1), 05.12.2017. Online: &amp;lt;https://doi.org/10.1515/bd-2018-0007&amp;gt;.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, in: Konferenzabstracts, Leipzig 11.03.2016, S. 103–108. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.  Presentations  Hinrichsen, Lena: Community Building und Community Management in OCR-D, #vBIB21 01.12.2021. Online: &amp;lt;https://nbn-resolving.org/urn:nbn:de:0290-opus4-178109&amp;gt; ; Video: &amp;lt;https://av.tib.eu/media/55592&amp;gt;.  Vom Bild zum Text – praktische OCR für die DH. Abschlussveranstaltung, vDHd2021 15.09.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Abschluss.pdf&amp;gt;.    Hartwig, Uwe: Open Source OCR-Systeme, 06.07.2021. Online: &amp;lt;https://doi.org/10.5281/zenodo.5076012&amp;gt;.    Würzner, Kay-Michael; Sachunsky, Robert: Kollaborative Erstellung von Trainingsmaterialien für OCR – Ein Werkstattbericht, Bibliothekartag 2021 17.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17661&amp;gt;.    Hertling, Anke; Klaes, Sebastian: OCR on demand: Der Ansatz eines User-generated Content, Bibliothekartag 2021 17.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17662&amp;gt; ; Video: &amp;lt;OCR on demand&amp;gt;.    Engl, Elisabeth: OCR-D: Von Prototypen zu Digitalisierungsprojekten, Bibliothekartag 2021 16.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17593&amp;gt;, Stand: 27.10.2021 ; Video: &amp;lt;OCR-D&amp;gt;: Stand: 27.10.2021.    Engl, Elisabeth; Sachunsky, Robert; Fink, Robert; Schäfer, Robin: Vom Bild zum Text – praktische OCR für die DH. Postcorrection, Hackathon, vDHd2021 19.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z3.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Büttner, Andreas; Kamlah, Jan: Vom Bild zum Text – praktische OCR für die DH. Evaluation, Transkription, Training, vDHd2021 12.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z2.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1620823922694?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1620823922694&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Reul, Christian; Büttner, Andreas: Vom Bild zum Text – praktische OCR für die DH. OCR-D und OCR4all, TEI-Konvertierung, vDHd2021 05.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z1.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1620218746064?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1620218746064&amp;gt;.    Vom Bild zum Text –&amp;nbsp; praktische OCR für die DH. Einführungsveranstaltung, vDHd2021 23.03.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/assets/other/OCR@vDHd.pdf&amp;gt;.    Engl, Elisabeth: Massendigitalisierung alter Drucke – OCR-D in Bibliotheken, Vortrag, 3. Workshop zur Retrodigitalisierung. OCR – Prozesse und Entwicklungen 01.03.2021. Online: &amp;lt;https://wiki.zbw.eu/pages/viewpage.action?pageId=33620559&amp;amp;preview=/33620559/33620562/2021-02-24_Engl_Massendigitalisierung%20alter%20Drucke.pdf&amp;gt;.    Neudecker, Clemens: OCR-D: An open ecosystem for improving OCR on historical documents, Vortrag, Mini-ELAG 20.10.2020. Online: &amp;lt;https://elag.org/mini-elag-october-20-2020/ocr-d-an-open-ecosystem-for-improving-ocr-on-historical-documents/&amp;gt;.    Boenig, Matthias: Digitale Transformation: OCR-D, Angebot und Vision, Vortrag, FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft 08.10.2020. Online: &amp;lt;https://docs.google.com/presentation/d/1JCzfGq_Reze7R3TaecYyBocnkD6uNy94eukJTlYUbNI/edit#slide=id.g9d954d5829_0_469&amp;gt;.    Engl, Elisabeth: OCR-D in the wild: Erfahrungen und Erkenntnisse aus der Praxisphase mit Bibliotheken, Vortrag, vbib2020 26.05.2020. Online: &amp;lt;https://doi.org/10.5446/47151&amp;gt;.    Baierer, Konstantin; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, Paderborn 05.03.2020. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth: Die OCR-D-Workflowengine, Vortrag, 2. Workshop Retrodigitalisierung zu Effizienz und Qualitätssicherung in Digitalisierungsworkflows, Hannover 18.02.2020. Online: &amp;lt;/slides/Retrodigitalisierung-2020-02-18/TIB_Retrodigitalisierung.pdf&amp;gt;.    Engl, Elisabeth: Bibliothekarische Digitalisierungspraxis und die OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_in_Bibliotheken.pdf&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin: Funktionen und Möglichkeiten der OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;https://hackmd.io/@cneud/ocrd-bonn#/&amp;gt;.    Engl, Elisabeth: OCR-D in a Nutshell, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/Abschlussworkshop_Überblick.pdf&amp;gt;.    Boenig, Matthias: Spezifikationen und Lessons Learned, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_Spezifikationen_Lessons_Learned.pdf&amp;gt;.    Würzner, Kay-Michael: Multi-source OCR workflows with OCR-D, Vortrag, The Open Islamicate Texts Initiative Workshop, Berwyn 29.01.2020. Online: &amp;lt;https://wrznr.github.io/OpenITI-2020/#1&amp;gt;.    Baierer, Konstantin; Engl, Elisabeth; Luetgen, Michael: OCR(-D) und Kitodo, Vortrag, Kitodo Anwenderworkshop, Hamburg 19.11.2019. Online: &amp;lt;https://hackmd.io/@kba/S1peIVxhH#/&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019. Online: &amp;lt;https://hackmd.io/@kba/SyiQKUCUH#/&amp;gt;.    Metzger, Noah: Projektabschlusspräsentation, Vortrag, Mannheim 19.09.2019. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/52213/&amp;gt;.    Sachunsky, Robert; Würzner, Kay-Michael: Flexible workflows with OCR-D, Vortrag, 3rd OCR-D developer workshop, Berlin 26.08.2019. Online: &amp;lt;https://hackmd.io/@FKFH0M1sR2SdJZwK5U8Cfg/S1YQ4NeNr#/&amp;gt;.    Metzger, Noah; Weil, Stefan: Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow, Workshop, MAD HD, Heidelberg 30.07.2019.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://hackmd.io/@QTT7e4hCTyWxVOvjiS61cA/B1nn7W7jV#/&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents&amp;gt;.    Weil, Stefan: Tesseract OCR – News, Vortrag, ELAG 2019, Berlin 09.05.2019. Online: &amp;lt;https://www.elag2019.de/talks/2019-05-09-tesseract-elag.pdf&amp;gt;.    Kamlah, Jan; Weil, Stefan: Forschungsdaten aus Digitalisaten, Vortrag, E-Science-Tage, Heidelberg 28.03.2019. Online: &amp;lt;https://heibox.uni-heidelberg.de/d/31bb269467/files/?p=%2FVortr%C3%A4ge%2FC3_2019-03-28-Kamlah-Weil.pdf&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Christlein, Vincent; Maier, Andreas: Automatic Font Group Recognition in Early Printed Books, Vortrag, Mainz 25.03.2019.    Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Herrmann, Elisa: Vom gedruckten Werk zu elektronischem Volltext, Workshop, DHd 2019, Mainz 25.03.2019. Online: &amp;lt;http://kba.cloud/2019-03-25-dhd/&amp;gt;.    Weil, Stefan: Hands-On Lab digital / Vom Bild zum Text. Automatisierte Texterkennung in historischen Drucken mit der freien Software Tesseract, Workshop, 108. Bibliothekartag, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16351&amp;gt;.    Boenig, Matthias: OCR-D in der Praxis: Ein gemeinsamer Ausblick mit Dienstleistern und Anwendern, Öffentliche Arbeitssitzung, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/17097/docId/16357/start/0/rows/20&amp;gt;.    Herrmann, Elisa: Von der Vision zur Umsetzung: Der aktuelle Entwicklungsstand von OCR-D, Vortrag, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://www.researchgate.net/publication/332173701_Von_der_Vision_zur_Umsetzung_Der_aktuelle_Entwicklungsstand_von_OCR-D&amp;gt;.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, Posterpräsentation, EADH 2018, Galway 08.12.2018. Online: &amp;lt;https://git.informatik.uni-leipzig.de/ocr-d/poster-eadh2018/blob/master/main.pdf&amp;gt;.    Baierer, Konstantin; Würzner, Kay-Michael: An open-source framework for integrating multi-source layout and text recognition tools into scalable OCR workflows, Vortrag, Bibliotheca Baltica Symposium, Rostock 05.10.2018. Online: &amp;lt;https://ocr-d.github.io/2018-10-05-baltica/index.html#/&amp;gt;.    Weil, Stefan: 126 Jahre Zeitung online – Fundgrube für historisch Interessierte und Motor für die Bibliotheks-IT, Vortrag, 107. Bibliothekartag, Berlin 15.06.2018. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/46507/&amp;gt;.    Herrmann, Elisa: Wieviel sind 85% wert: Qualität von OCR- und NER-Verfahren für die Forschung, Vortrag, MWW / DARIAH-DE Expertenworkshop Suchtechnologien, Weimar 24.05.2018. Online: &amp;lt;/slides/MWW-2018/MWW-Workshop_Wieviel sind 85% wert_2018-05-24.pdf&amp;gt;.    Würzner, Kay-Michael: Neues aus OCR-D, Vortrag, PhilTag 2018, Würzburg 10.04.2018. Online: &amp;lt;/slides/PhilTag-2018/content.md&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, Vortrag, DHd 2018, Köln 28.02.2018. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Perspektiven der automatischen Texterfassung als Grundlage wissenschaftlicher Editionen am Beispiel der Brief- und Schriftenausgabe der Bernd Alois Zimmermann-Gesamtausgabe, Workshop, Workshop der AG eHumanities Mainz. Geisteswissenschaftliche Forschungsdaten. Methoden zur digitalen Erfassung, Mainz 19.10.2017. Online: &amp;lt;/slides/Akademienunion_2017/slides/ocr-perspektiven.pdf&amp;gt;.    Prabhune, Ajinkya; Neudecker, Clemens: OCR-D Technische Systemarchitektur: Workflows, Repository, Schnittstellen, Vortrag, Karlsruhe 26.09.2017. Online: &amp;lt;/slides/OCR-Workshop-2017/slides/systemarchitektur/OCR-D-Workshop-Prabhune-Neudecker.pdf&amp;gt;.    Würzner, Kay-Michael: (Open-Source-)OCR-Workflows, Vortrag, Digital Humanities Kolloquium, Berlin 04.08.2017. Online: &amp;lt;https://edoc.bbaw.de/frontdoor/index/index/docId/2786&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D: Koordinierte Förderinitiative zur Weiterentwicklung von OCR für historische Dokumente, Vortrag, 106. Bibliothekartag, Frankfurt am Main 30.05.2017. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/16521/rows/10/start/0/facetNumber_author_facet/all/author_facetfq/St%C3%A4cker%2C+Thomas/docId/3004&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Compilation of a Large Ground-Truth Data Set: Using Transkribus, Vortrag, Transkribus User Conference, Wien 11.02.2017. Online: &amp;lt;/slides/Transkribus-WS-2017/slides/gt_compilation.pdf&amp;gt;.    Herrmann, Elisa: Aktuelle OCR-Entwicklungen und ihr Einsatz in der Praxis, Vortrag, Berliner Bibliothekswissenschaftliches Kolloquium, Berlin 17.01.2017.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, DHd 2016, Leipzig 11.03.2016. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.    Herrmann, Elisa: OCR-D: Koordinierungsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, Philtag 13, Würzburg 26.02.2016. Online: &amp;lt;/slides/PhilTag-2016/OCR-D_Wurzburg-13PhilTag.pdf&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: Kooperationsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, 12. Workshop zur Texterkennung in historischen Dokumenten, Rostock 09.02.2016. Online: &amp;lt;/slides/Rostock-2016-02-09/OCR-D_Rostock_09-02-2016.pdf&amp;gt;.  ",
      "url": " /en/publications.html"
    },
  

    {
      "slug": "de-publikationen-html",
      "title": "Publikationen",
      "content"	 : "Publikationen  Baierer, Konstantin; Büttner, Andreas; Engl, Elisabeth; Hinrichsen, Lena; Reul, Christian: OCR-D &amp;amp; OCR4all: Two Complementary Approaches for Improved OCR of Historical Sources, in: Proceedings of the 6th International Workshop on Computational History (HistoInformatics 2021) co-located with ACM/IEEE Joint Conference on Digital Libraries 2021 (JCDL 2021), 01.10.2021. Online: &amp;lt;http://ceur-ws.org/Vol-2981/&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Dong, Rui; Burghardt, Manuel; Christlein, Vincent: New Approaches to OCR for Early Printed Books, in: DigItalia 15 (2), 12.2020, S. 74–87. Online: &amp;lt;https://doi.org/10/ghstmx&amp;gt;.    Engl, Elisabeth: OCR-D kompakt: Ergebnisse und Stand der Forschung in der Förderinitiative, in: Bibliothek Forschung und Praxis 44 (2), 29.07.2020, S. 218–230. Online: &amp;lt;https://doi.org/10.1515/bfp-2020-0024&amp;gt; ; Video: &amp;lt;OCR-D kompakt&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Neudecker, Clemens: Volltexte – die Zukunft alter Drucke. Bericht zum Abschlussworkshop des OCR-D-Projekts, in: o-bib 7 (2), 05.05.2020, S. 1–4. Online: &amp;lt;https://doi.org/10.5282/o-bib/5600&amp;gt;.    Boenig, Matthias; Engl, Elisabeth; Baierer, Konstantin; Hartmann, Volker; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, in: DHd 2020: Spielräme - Digital Humanities zwischen Modellierung und Interpretation. Konferenzabstracts, Paderborn 05.03.2020, S. 244–247. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth; Boenig, Matthias; Baierer, Konstantin; Neudecker, Clemens; Hartmann, Volker: Volltexte für die Frühe Neuzeit. Der Beitrag des OCR-D-Projekts zur Volltexterkennung frühneuzeitlicher Drucke, in: Zeitschrift für Historische Forschung 47 (2), 2020, S. 223–250.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 1–6. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352640&amp;gt;.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, in: Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, Sydney 20.09.2019, S. 25–30. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3352631.3352638&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open source OCR framework for historical documents, in: EuropeanaTech Insight (13), 31.07.2019. Online: &amp;lt;https://pro.europeana.eu/page/issue-13-ocr#ocr-d-an-end-to-end-open-source-ocr-framework-for-historical-documents&amp;gt;.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 19.24. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322908&amp;gt;.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 3–8. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322916&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, in: Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, Brüssel 09.05.2019, S. 53–58. Online: &amp;lt;https://dl.acm.org/doi/10.1145/3322905.3322917&amp;gt;.    Engl, Elisabeth: Das Projekt OCR-D – Ein Fortschrittsbericht zur Volltextdigitalisierung frühneuzeitlicher Drucke, in: Medium Buch 1, 2019, S. 233–235.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, in: Conference Abstracts, Galway 08.12.2018. Online: &amp;lt;https://eadh2018.exordo.com/files/papers/92/final_draft/EADH_2018_Proposal_Brief_Final.pdf&amp;gt;.    Schulz, Klaus; Fink, Florian: Novel software for cleansing digitised historical texts, in: Scientia, 28.11.2018. Online: &amp;lt;https://doi.org/10.26320/SCIENTIA278&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, in: Konferenzabstracts, Köln 28.02.2018, S. 219–223. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D – Koordinierte Förderinitiative zur Weiterentwicklung von OCR-Verfahren, in: Bibliotheksdienst 52 (1), 05.12.2017. Online: &amp;lt;https://doi.org/10.1515/bd-2018-0007&amp;gt;.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, in: Konferenzabstracts, Leipzig 11.03.2016, S. 103–108. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.  Vorträge  Hinrichsen, Lena: Community Building und Community Management in OCR-D, #vBIB21 01.12.2021. Online: &amp;lt;https://nbn-resolving.org/urn:nbn:de:0290-opus4-178109&amp;gt; ; Video: &amp;lt;https://av.tib.eu/media/55592&amp;gt;.  Vom Bild zum Text – praktische OCR für die DH. Abschlussveranstaltung, vDHd2021 15.09.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Abschluss.pdf&amp;gt;.    Hartwig, Uwe: Open Source OCR-Systeme, 06.07.2021. Online: &amp;lt;https://doi.org/10.5281/zenodo.5076012&amp;gt;.    Würzner, Kay-Michael; Sachunsky, Robert: Kollaborative Erstellung von Trainingsmaterialien für OCR – Ein Werkstattbericht, Bibliothekartag 2021 17.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17661&amp;gt;.    Hertling, Anke; Klaes, Sebastian: OCR on demand: Der Ansatz eines User-generated Content, Bibliothekartag 2021 17.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17662&amp;gt; ; Video: &amp;lt;OCR on demand&amp;gt;.    Engl, Elisabeth: OCR-D: Von Prototypen zu Digitalisierungsprojekten, Bibliothekartag 2021 16.06.2021. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/17593&amp;gt;, Stand: 27.10.2021 ; Video: &amp;lt;OCR-D&amp;gt;: Stand: 27.10.2021.    Engl, Elisabeth; Sachunsky, Robert; Fink, Robert; Schäfer, Robin: Vom Bild zum Text – praktische OCR für die DH. Postcorrection, Hackathon, vDHd2021 19.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z3.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Büttner, Andreas; Kamlah, Jan: Vom Bild zum Text – praktische OCR für die DH. Evaluation, Transkription, Training, vDHd2021 12.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z2.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1620823922694?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1620823922694&amp;gt;.    Engl, Elisabeth; Baierer, Konstantin; Reul, Christian; Büttner, Andreas: Vom Bild zum Text – praktische OCR für die DH. OCR-D und OCR4all, TEI-Konvertierung, vDHd2021 05.05.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z1.pdf&amp;gt; ; Video: &amp;lt;https://meet.gwdg.de/playback/presentation/2.3/db36b9cd45a79838b121a8b68270a85734c8f026-1620218746064?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1620218746064&amp;gt;.    Vom Bild zum Text –&amp;nbsp; praktische OCR für die DH. Einführungsveranstaltung, vDHd2021 23.03.2021. Online: &amp;lt;https://dhd-ag-ocr.github.io/assets/other/OCR@vDHd.pdf&amp;gt;.    Engl, Elisabeth: Massendigitalisierung alter Drucke – OCR-D in Bibliotheken, Vortrag, 3. Workshop zur Retrodigitalisierung. OCR – Prozesse und Entwicklungen 01.03.2021. Online: &amp;lt;https://wiki.zbw.eu/pages/viewpage.action?pageId=33620559&amp;amp;preview=/33620559/33620562/2021-02-24_Engl_Massendigitalisierung%20alter%20Drucke.pdf&amp;gt;.    Neudecker, Clemens: OCR-D: An open ecosystem for improving OCR on historical documents, Vortrag, Mini-ELAG 20.10.2020. Online: &amp;lt;https://elag.org/mini-elag-october-20-2020/ocr-d-an-open-ecosystem-for-improving-ocr-on-historical-documents/&amp;gt;.    Boenig, Matthias: Digitale Transformation: OCR-D, Angebot und Vision, Vortrag, FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft 08.10.2020. Online: &amp;lt;https://docs.google.com/presentation/d/1JCzfGq_Reze7R3TaecYyBocnkD6uNy94eukJTlYUbNI/edit#slide=id.g9d954d5829_0_469&amp;gt;.    Engl, Elisabeth: OCR-D in the wild: Erfahrungen und Erkenntnisse aus der Praxisphase mit Bibliotheken, Vortrag, vbib2020 26.05.2020. Online: &amp;lt;https://doi.org/10.5446/47151&amp;gt;.    Baierer, Konstantin; Neudecker, Clemens: Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts, Paderborn 05.03.2020. Online: &amp;lt;https://doi.org/10.5281/zenodo.3666690&amp;gt;.    Engl, Elisabeth: Die OCR-D-Workflowengine, Vortrag, 2. Workshop Retrodigitalisierung zu Effizienz und Qualitätssicherung in Digitalisierungsworkflows, Hannover 18.02.2020. Online: &amp;lt;/slides/Retrodigitalisierung-2020-02-18/TIB_Retrodigitalisierung.pdf&amp;gt;.    Engl, Elisabeth: Bibliothekarische Digitalisierungspraxis und die OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_in_Bibliotheken.pdf&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin: Funktionen und Möglichkeiten der OCR-D-Software, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;https://hackmd.io/@cneud/ocrd-bonn#/&amp;gt;.    Engl, Elisabeth: OCR-D in a Nutshell, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/Abschlussworkshop_Überblick.pdf&amp;gt;.    Boenig, Matthias: Spezifikationen und Lessons Learned, Vortrag, Volltexte - Die Zukunft alter Drucke, Bonn 12.02.2020. Online: &amp;lt;/slides/Abschlussworkshop-2020/OCR-D_Spezifikationen_Lessons_Learned.pdf&amp;gt;.    Würzner, Kay-Michael: Multi-source OCR workflows with OCR-D, Vortrag, The Open Islamicate Texts Initiative Workshop, Berwyn 29.01.2020. Online: &amp;lt;https://wrznr.github.io/OpenITI-2020/#1&amp;gt;.    Baierer, Konstantin; Engl, Elisabeth; Luetgen, Michael: OCR(-D) und Kitodo, Vortrag, Kitodo Anwenderworkshop, Hamburg 19.11.2019. Online: &amp;lt;https://hackmd.io/@kba/S1peIVxhH#/&amp;gt;.    Seuret, Mathias; Limbach, Saskia; Weichselbaumer, Nikolaus; Maier, Andreas; Christlein, Vicent: Dataset of Pages from Early Printed Books with Multiple Font Groups, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019.    Baierer, Konstantin; Dong, Rui; Neudecker, Clemens: okralact – a multi-engine Open Source OCR training system, Vortrag, 5. internationaler Workshop zu Historical Document Imaging and Processing HIP 2019 als Teil der ICDAR 2019, Sydney 20.09.2019. Online: &amp;lt;https://hackmd.io/@kba/SyiQKUCUH#/&amp;gt;.    Metzger, Noah: Projektabschlusspräsentation, Vortrag, Mannheim 19.09.2019. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/52213/&amp;gt;.    Sachunsky, Robert; Würzner, Kay-Michael: Flexible workflows with OCR-D, Vortrag, 3rd OCR-D developer workshop, Berlin 26.08.2019. Online: &amp;lt;https://hackmd.io/@FKFH0M1sR2SdJZwK5U8Cfg/S1YQ4NeNr#/&amp;gt;.    Metzger, Noah; Weil, Stefan: Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow, Workshop, MAD HD, Heidelberg 30.07.2019.    Englmeier, Tobias; Fink, Florian; Schulz, Klaus: A-I-PoCoTo – Combining automated and interactive OCR postcorrection, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019.    Boenig, Matthias; Baierer, Konstantin; Hartmann, Volker; Federbusch, Maria; Neudecker, Clemens: Labelling OCR Ground Truth for Usage in Repositories, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://hackmd.io/@QTT7e4hCTyWxVOvjiS61cA/B1nn7W7jV#/&amp;gt;.    Neudecker, Clemens; Baierer, Konstantin; Federbusch, Maria; Würzner, Kay-Michael; Boenig, Matthias; Herrmann, Elisa; Hartmann, Volker: OCR-D: An end-to-end open-source OCR framework for historical documents, Vortrag, DATeCH 2019. 3. internationale Konferenz zu Digital Access to Textual Cultural Heritage 2019, Brüssel 09.05.2019. Online: &amp;lt;https://www.slideshare.net/cneudecker/ocrd-an-endtoend-open-source-ocr-framework-for-historical-printed-documents&amp;gt;.    Weil, Stefan: Tesseract OCR – News, Vortrag, ELAG 2019, Berlin 09.05.2019. Online: &amp;lt;https://www.elag2019.de/talks/2019-05-09-tesseract-elag.pdf&amp;gt;.    Kamlah, Jan; Weil, Stefan: Forschungsdaten aus Digitalisaten, Vortrag, E-Science-Tage, Heidelberg 28.03.2019. Online: &amp;lt;https://heibox.uni-heidelberg.de/d/31bb269467/files/?p=%2FVortr%C3%A4ge%2FC3_2019-03-28-Kamlah-Weil.pdf&amp;gt;.    Weichselbaumer, Nikolaus; Seuret, Mathias; Limbach, Saskia; Christlein, Vincent; Maier, Andreas: Automatic Font Group Recognition in Early Printed Books, Vortrag, Mainz 25.03.2019.    Baierer, Konstantin; Boenig, Matthias; Hartmann, Volker; Herrmann, Elisa: Vom gedruckten Werk zu elektronischem Volltext, Workshop, DHd 2019, Mainz 25.03.2019. Online: &amp;lt;http://kba.cloud/2019-03-25-dhd/&amp;gt;.    Weil, Stefan: Hands-On Lab digital / Vom Bild zum Text. Automatisierte Texterkennung in historischen Drucken mit der freien Software Tesseract, Workshop, 108. Bibliothekartag, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/docId/16351&amp;gt;.    Boenig, Matthias: OCR-D in der Praxis: Ein gemeinsamer Ausblick mit Dienstleistern und Anwendern, Öffentliche Arbeitssitzung, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/17097/docId/16357/start/0/rows/20&amp;gt;.    Herrmann, Elisa: Von der Vision zur Umsetzung: Der aktuelle Entwicklungsstand von OCR-D, Vortrag, 7. Bibliothekskongress, Leipzig 18.03.2019. Online: &amp;lt;https://www.researchgate.net/publication/332173701_Von_der_Vision_zur_Umsetzung_Der_aktuelle_Entwicklungsstand_von_OCR-D&amp;gt;.    Sachunsky, Robert; Schiffer, Lena K.; Efer, Thomas; Heyer, Gerhard: Towards Context-Aware Language Models for Historical OCR Post-Correction, Posterpräsentation, EADH 2018, Galway 08.12.2018. Online: &amp;lt;https://git.informatik.uni-leipzig.de/ocr-d/poster-eadh2018/blob/master/main.pdf&amp;gt;.    Baierer, Konstantin; Würzner, Kay-Michael: An open-source framework for integrating multi-source layout and text recognition tools into scalable OCR workflows, Vortrag, Bibliotheca Baltica Symposium, Rostock 05.10.2018. Online: &amp;lt;https://ocr-d.github.io/2018-10-05-baltica/index.html#/&amp;gt;.    Weil, Stefan: 126 Jahre Zeitung online – Fundgrube für historisch Interessierte und Motor für die Bibliotheks-IT, Vortrag, 107. Bibliothekartag, Berlin 15.06.2018. Online: &amp;lt;https://madoc.bib.uni-mannheim.de/46507/&amp;gt;.    Herrmann, Elisa: Wieviel sind 85% wert: Qualität von OCR- und NER-Verfahren für die Forschung, Vortrag, MWW / DARIAH-DE Expertenworkshop Suchtechnologien, Weimar 24.05.2018. Online: &amp;lt;/slides/MWW-2018/MWW-Workshop_Wieviel sind 85% wert_2018-05-24.pdf&amp;gt;.    Würzner, Kay-Michael: Neues aus OCR-D, Vortrag, PhilTag 2018, Würzburg 10.04.2018. Online: &amp;lt;/slides/PhilTag-2018/content.md&amp;gt;.    Boenig, Matthias; Federbusch, Maria; Herrmann, Elisa; Neudecker, Clemens; Würzner, Kay-Michael: Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?, Vortrag, DHd 2018, Köln 28.02.2018. Online: &amp;lt;http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Perspektiven der automatischen Texterfassung als Grundlage wissenschaftlicher Editionen am Beispiel der Brief- und Schriftenausgabe der Bernd Alois Zimmermann-Gesamtausgabe, Workshop, Workshop der AG eHumanities Mainz. Geisteswissenschaftliche Forschungsdaten. Methoden zur digitalen Erfassung, Mainz 19.10.2017. Online: &amp;lt;/slides/Akademienunion_2017/slides/ocr-perspektiven.pdf&amp;gt;.    Prabhune, Ajinkya; Neudecker, Clemens: OCR-D Technische Systemarchitektur: Workflows, Repository, Schnittstellen, Vortrag, Karlsruhe 26.09.2017. Online: &amp;lt;/slides/OCR-Workshop-2017/slides/systemarchitektur/OCR-D-Workshop-Prabhune-Neudecker.pdf&amp;gt;.    Würzner, Kay-Michael: (Open-Source-)OCR-Workflows, Vortrag, Digital Humanities Kolloquium, Berlin 04.08.2017. Online: &amp;lt;https://edoc.bbaw.de/frontdoor/index/index/docId/2786&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: OCR-D: Koordinierte Förderinitiative zur Weiterentwicklung von OCR für historische Dokumente, Vortrag, 106. Bibliothekartag, Frankfurt am Main 30.05.2017. Online: &amp;lt;https://opus4.kobv.de/opus4-bib-info/frontdoor/index/index/searchtype/collection/id/16521/rows/10/start/0/facetNumber_author_facet/all/author_facetfq/St%C3%A4cker%2C+Thomas/docId/3004&amp;gt;.    Würzner, Kay-Michael; Boenig, Matthias: Compilation of a Large Ground-Truth Data Set: Using Transkribus, Vortrag, Transkribus User Conference, Wien 11.02.2017. Online: &amp;lt;/slides/Transkribus-WS-2017/slides/gt_compilation.pdf&amp;gt;.    Herrmann, Elisa: Aktuelle OCR-Entwicklungen und ihr Einsatz in der Praxis, Vortrag, Berliner Bibliothekswissenschaftliches Kolloquium, Berlin 17.01.2017.    Boenig, Matthias; Würzner, Kay-Michael; Binder, Arne; Springmann, Uwe: Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts, DHd 2016, Leipzig 11.03.2016. Online: &amp;lt;http://dhd2016.de/boa.pdf#page=103&amp;gt;.    Herrmann, Elisa: OCR-D: Koordinierungsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, Philtag 13, Würzburg 26.02.2016. Online: &amp;lt;/slides/PhilTag-2016/OCR-D_Wurzburg-13PhilTag.pdf&amp;gt;.    Stäcker, Thomas; Herrmann, Elisa: Kooperationsprojekt zur Weiterentwicklung von OCR-Verfahren, Vortrag, 12. Workshop zur Texterkennung in historischen Dokumenten, Rostock 09.02.2016. Online: &amp;lt;/slides/Rostock-2016-02-09/OCR-D_Rostock_09-02-2016.pdf&amp;gt;.  ",
      "url": " /de/publikationen.html"
    },
  

    {
      "slug": "search-index-json",
      "title": "",
      "content"	 : "[  {% for post in site.pages %}    {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    },  {% endfor %}  {% for post in site.posts %}  {      &quot;slug&quot;: &quot;{{ post.url | slugify }}&quot;,      &quot;title&quot;: &quot;{{ post.title | xml_escape }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot; | remove: &quot;{&quot; }}&quot;,      &quot;url&quot;: &quot; {{ post.url | xml_escape }}&quot;    }    {% unless forloop.last %},{% endunless %}  {% endfor %}]",
      "url": " /search-index.json"
    },
  

    {
      "slug": "search-html",
      "title": "Search",
      "content"	 : "Search",
      "url": " /search.html"
    },
  

    {
      "slug": "slides-2019-03-25-dhd-setup-time-html",
      "title": "",
      "content"	 : "# Setup Time!## What operating system are you on?### LinuxYou&#39;re set, see [next section](#installation-ocr-d-stack)### Mac OS XWorks mostly like Linux for our purposes, see [next section](#installation-ocr-d-stack).However, you will need to have `homebrew` or `macports` on your system to beable to [install tesseract fromhomebrew](https://formulae.brew.sh/formula/tesseract_) or [withmacports](https://github.com/macports/macports-ports/blob/master/textproc/tesseract/Portfile).In case of problems, install VirtualBox and Ubuntu 18.04 inside.### Windows 7Please install VirtualBox and Ubuntu 18.04 inside### Windows 10Install Windows Subsystem for Linux (WSL)1. Open PowerShell as Administrator and run:    ```bash=bash    Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux    ```    :information_source: Needs reboot!2. Press &#39;Windows&#39;-Key3. Type &#39;Microsoft Store&#39;4. Start Microsoft Store5. Search for &#39;Ubuntu&#39;6. In category &#39;Apps&#39; select &#39;Ubuntu 18.04 LTS&#39;7. Select &#39;Download/Herunterladen&#39;8. Dialog &#39;Geräteübergreifend verwenden&#39;   a) Select &#39;Nein, danke&#39;9. Installation finished! -&gt; Start    ```    Installing, this may take a few minutes...    Please create a default UNIX user account. ...    Enter new UNIX username: ocrd    Enter new UNIX password: dhd2019    Retype new UNIX password: dhd2019    passwd: password updated successfully    [...]    ocrd@hostname:~$    ```10. Done**Link:** https://docs.microsoft.com/de-de/windows/wsl/install-win10## Installation OCR-D Stack### Python```bash=bash# Update package listuser@hostname:~$sudo apt-get update[...]Reading package lists... Done# Install Python3user@hostname:~$sudo apt-get install python3 python3-dev python3-virtualenv build-essential[...]Do you want to continue? [Y/n] &#39;Enter&#39;[...]user@hostname:~$```### Setup a virtualenv:information_source: Setting up virtual environment will take a while!```bash=bash# Setup virtual environment.user@hostname:~$python3 -m virtualenv ~/env-ocrd --python=python3Already using interpreter /usr/bin/python3Using base prefix &#39;/usr&#39;New python executable in /home/ocrd/env-ocrd/bin/python3Also creating executable in /home/ocrd/env-ocrd/bin/pythonInstalling setuptools, pkg_resources, pip, wheel...doneuser@hostname:~$```### Activate virtualenv```bash=bashuser@hostname:~$source ~/env-ocrd/bin/activate(env-ocrd) user@hostname:~$```### Install ocrd core Software:information_source: Setting up ocrd core software will take a while!```bash=bash(env-ocrd) user@hostname:~$pip install ocrd==1.0.0b6Collecting ocrd==1.0.0b5  Downloading https://files.pythonhosted.org/packages/cd/e4/9f56fe9971e04e2e97d6ad27457d6a1c17d798de9c15fd3030f194beca24/ocrd-0.15.2-py3-none-any.whl (138kB)   100% |████████████████████████████████| 143kB 3.0MB/s[...]Successfully installed Deprecated-1.2.0 Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.1.1 Pillow-5.4.1 Werkzeug-0.14.1 attrs-19.1.0 bagit-1.7.0 bagit-profile-1.3.0 certifi-2019.3.9 chardet-3.0.4 click-7.0 idna-2.8 itsdangerous-1.1.0 jsonschema-3.0.1 lxml-4.3.2 numpy-1.16.2 ocrd-0.15.2 opencv-python-4.0.0.21 pyrsistent-0.14.11 pyyaml-5.1 requests-2.21.0 six-1.12.0 urllib3-1.24.1 wrapt-1.11.1(env-ocrd) user@hostname:~$```#### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd --versionocrd, version 1.0.0b6```### Install OCR-D Modules- ocrd_ocropy- ocrd_kraken- ocrd_tesserocr#### ocrd_ocropy```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_ocropyCollecting ocrd_ocropy[...]Successfully installed cycler-0.10.0 imageio-2.5.0 kiwisolver-1.0.1 matplotlib-3.0.3 ocrd-fork-ocropy-1.4.0a3 ocrd-ocropy-0.0.1a1 pyparsing-2.3.1 python-dateutil-2.8.0(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-ocropy-segment --helpUsage: ocrd-ocropy-segment [OPTIONS]                                                                                                                   Options:  -V, --version                   Show version  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                  Log level  -J, --dump-json                 Dump tool description as JSON and exit  -p, --parameter PATH  -g, --page-id TEXT              ID(s) of the pages to process  -O, --output-file-grp TEXT      File group(s) used as output.  -I, --input-file-grp TEXT       File group(s) used as input.  -w, --working-dir TEXT          Working Directory  -m, --mets TEXT                 METS URL to validate  --help                          Show this message and exit.```#### ocrd_kraken```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_krakenCollecting ocrd_kraken[...]Successfully installed kraken-0.9.16 ocrd-kraken-0.1.0 regex-2019.3.12(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-kraken-binarize --helpUsage: ocrd-kraken-binarize [OPTIONS]Options:-V, --version                   Show version-l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]  Log level-J, --dump-json                 Dump tool description as JSON and exit-p, --parameter PATH-g, --page-id TEXT              ID(s) of the pages to process-O, --output-file-grp TEXT      File group(s) used as output.-I, --input-file-grp TEXT       File group(s) used as input.-w, --working-dir TEXT          Working Directory-m, --mets TEXT                 METS URL to validate--help                          Show this message and exit.(env-ocrd) user@hostname:~$```#### ocrd_tesserocr```bash=bash# Install tesseract(env-ocrd) user@hostname:~$sudo apt-get install libtesseract-dev tesseract-ocr[...]Do you want to continue? [Y/n] &#39;Enter&#39;[...](env-ocrd) user@hostname:~$```:information_source: Setting up virtual environment will take a while!```bash=bash(env-ocrd) user@hostname:~$pip install ocrd_tesserocrCollecting ocrd_tesserocr[...]Successfully installed kraken-0.9.16 ocrd-kraken-0.1.0 regex-2019.3.12(env-ocrd) user@hostname:~$```##### Test Installation```bash=bash(env-ocrd) user@hostname:~$ocrd-tesserocr-recognize --helpUsage: ocrd-tesserocr-recognize [OPTIONS]Options:-V, --version                   Show version-l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]                                Log level-J, --dump-json                 Dump tool description as JSON and exit-p, --parameter PATH-g, --page-id TEXT              ID(s) of the pages to process-O, --output-file-grp TEXT      File group(s) used as output.-I, --input-file-grp TEXT       File group(s) used as input.-w, --working-dir TEXT          Working Directory-m, --mets TEXT                 METS URL to validate--help                          Show this message and exit.(env-ocrd) user@hostname:~$```## Install TranskribusTo use Transkribus a registration with Transkribus is necessary. Follow the *Register and Installation Guide* on: https://transkribus.eu/Transkribus/. In this case it is cited here in the following passage.**Register at the website**-  Go to: http://transkribus.eu/-  Read our user agreement: https://transkribus.eu/Transkribus/docs/TranskribusTermsOfUse_v04-2016.pdf-  All documents uploaded toTranskribus are “private”,which means that no one except you has access to them.-  The Transkribus team fully supports all EU directives on data protection and privacy. We will respect your privacy and only use the data to improve our services and support research in humanities and computer science!**Download Transkribus from the website** -  Go to the Transkribus website http://transkribus.eu/ and click “Download”.  -  Transkribus runs on Windows, MacOS and Linux.  If you need help installing the platform, consult the Transkribus wiki: https://transkribus.eu/wiki/index.php/Download_and_Installation -  If you use MacOS an error message may appear when you try to open Transkribus for the first time. To remedy this:-   right click the Track Pad to open the Context Menu and add a security exception for Transkribus.  -  Once you have downloaded Transkribus, make sure you unzip the file. The program cannot be started from the zipped file!## Install PAGE ViewerPAGE Viewer allows you to view a PAGE file together with the image.Download the appropriate version for your OS from the [PRIMA Labs Website](https://www.primaresearch.org/alternative_download_links.html)## Yay :fireworks: :tada::fireworks: :tada:If all steps were successful you are well prepared for the workshop.If not don&#39;t hesitate to contact us via [gitter](https://gitter.im/OCR-D/Lobby).",
      "url": " /slides/2019-03-25-dhd/setup-time.html"
    },
  

    {
      "slug": "en-setup-html",
      "title": "OCR-D setup guide",
      "content"	 : "# OCR-D setup guideOCR-D&#39;s software is a modular collection of many projects (called _modules_)with many tools per module (called _processors_) that you can combine freelyto achieve the workflow best suited for OCRing your content.## System requirementsMinimum system requirements 8 GB RAM (more recommended)  - The more RAM is available, the more concurrent processes can be run  - Exceedingly large images (newspapers, folio-size books...) require a lot of RAM 20 GB free disk space for local installation (more recommended)  - How much disk space is needed depends mainly on the individual purposes of the installation. In addition to the installation itself  you will need space for various pretrained models, training and evaluation data for training, and data to process. Python 3.7    - OCR-D&#39;s target Python version is currently Python 3.7. Python 3.8 also works. Python   - Python 3.9 and newer versions are not yet fully supported, since there are no pre-built Python packages for Tensorflow 2.5 and  Operating system: Ubuntu 18.04 (or Docker)  - For installation on Windows 10 (WSL) and macOS see the setup guides in the OCR-D Wiki  - Ubuntu 18.04 is our target platform because it was the most up-to-date Ubuntu LTS release when we started developing and will be supported for the foreseeable future  - Ubuntu 22.04 is now (2022) the current Ubuntu LTS, seems to work, too, and will be our next target platform.  - Other Linux distributions or Ubuntu versions can also be used, though some instructions have to be adapted (e.g. package management, locations of some files)  - With Windows Subsystem for Linux (WSL), a feature of Windows 10, it is also possible to set up an Ubuntu 18.04 installation within Microsoft Windows  - OCR-D can be deployed on an Apple MacOSX machine using Homebrew## Installation### ocrd_all`ocrd_all` is the main way to distribute and install the OCR-D software.If you want to produce OCR output from image data,this is what you need.      Tell me more about ocrd_allThe [`ocrd_all`](https://github.com/OCR-D/ocrd_all) project is an effort by theOCR-D community, now maintained by the OCR-D coordination team. It streamlinesthe native installation of OCR-D modules with a versatile Makefile approach.Besides allowing native installation of the full OCR-D stack (or any subset),it is also the base for the [`ocrd/all`](https://hub.docker.com/r/ocrd/all)Docker images available from DockerHub that contain the full stack (or certain subsets)of OCR-D modules ready for deployment.Technically, [`ocrd_all`](https://github.com/OCR-D/ocrd_all) is a Git repositorythat keeps all the necessary software as Git submodules at specific revisions.This way, the software tools are known to be at a stable version and guaranteed tobe interoperable with one another.  ### Installation: Docker or NativeThere are two methods to install OCR-D: 1. **[Docker Installation of OCR-D](#ocrd_all-via-docker)** using the prebuilt `ocrd/all` [Docker images](https://hub.docker.com/r/ocrd/all) to install a module collection (**recommended**) 2. **[Native Installation of OCR-D](#ocrd_all-natively)** using the `ocrd_all` [git repository](https://github.com/OCR-D/ocrd_all) to install selected modules nativelyWe recommend using the prebuilt Docker images, since this does not require any changes tothe host system besides [installing Docker](https://hub.docker.com/r/ocrd/all).      Installation of individual OCR-D modulesSometimes it can be useful to [install the modules individually](#individual-installation-experts-only), either via Docker or natively.Beware that we do not recommend installing modules individually, as it can be difficult to catch all dependencies,keep the software versions up-to-date and ensure that all components are at a usable and interoperable state.  ## ocrd_all via Docker### PrerequisitesIf you want to use the OCR-D-via-Docker solution, [docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository) has to be installed.After installing docker you may have to set up and start the docker daemon and add your user to the `docker` group:```sh# Start docker daemon at startupsudo systemctl enable docker# Add user to group &#39;docker&#39;sudo usermod -aG docker $USER``` Please log out and log in again.To test access to docker try the following command:```shdocker images```Now you should see an (empty) list of available images.### mini medi maxiThere are three versions of the[`ocrd/all`](https://hub.docker.com/r/ocrd/all) Docker image:`minimum`, `medium` and `maximum`. They differ in which modules are includedand hence the size of the image:* `minimum` is comprised of the essential OCR-D components, with Tesseract and OCRopus as OCR engines.* `medium` adds the Calamari OCR engine, as well as extra segmentation, pre- and postprocessing options.* `maximum` includes all modules for best performance and full flexibility, but requires the most disk space.We encourage the use of the relatively large but complete `maximum` image.The `minimum` or `medium` images should only be used when certain that none but the included OCR-Dmodules are needed.  Click here for a table showing the modules included in each version| Module                      | `minimum` | `medium` | `maximum` || -----                       | ----      | ----     | ----      || core                        | ☑         | ☑        | ☑         || ocrd_cis                    | ☑         | ☑        | ☑         || ocrd_fileformat             | ☑         | ☑        | ☑         || ocrd_im6convert             | ☑         | ☑        | ☑         || ocrd_pagetopdf              | ☑         | ☑        | ☑         || ocrd_repair_inconsistencies | ☑         | ☑        | ☑         || ocrd_tesserocr              | ☑         | ☑        | ☑         || ocrd_wrap                   | ☑         | ☑        | ☑         || tesserocr                   | ☑         | ☑        | ☑         || workflow-configuration      | ☑         | ☑        | ☑         || cor-asv-ann                 | -         | ☑        | ☑         || dinglehopper                | -         | ☑        | ☑         || docstruct                   | -         | ☑        | ☑         || format-converters           | -         | ☑        | ☑         || ocrd_calamari               | -         | ☑        | ☑         || ocrd_keraslm                | -         | ☑        | ☑         || ocrd_olahd_client           | ☑         | ☑        | ☑         || ocrd_olena                  | -         | ☑        | ☑         || ocrd_segment                | -         | ☑        | ☑         || tesseract                   | -         | ☑        | ☑         || ocrd_neat                   | -         | ☑        | ☑         || ocrd_anybaseocr             | -         | -        | ☑         || ocrd_detectron2             | -         | -        | ☑         || ocrd_doxa                   | -         | -        | ☑         || ocrd_kraken                 | -         | -        | ☑         || ocrd_ocropy                 | -         | -        | -         || ocrd_pc_segmentation        | -         | -        | -         || ocrd_typegroups_classifier  | -         | -        | ☑         || sbb_binarization            | -         | -        | ☑         || cor-asv-fst                 | -         | -        | -         |### Fetch Docker imageTo fetch the `maximum` version of the `ocrd/all` Docker image:(replace `maximum` accordingly if you want the `minimum` or `medium` version)```shdocker pull ocrd/all:maximum```      Docker and git imagesIf you want to keep the modules&#39; git repos inside the Docker images – so you can keep makingfast updates, without waiting for a new pre-built image, but also without building an image yourself –then add the suffix `-git` to the image version, e.g. `maximum-git`. This will behave like the native installation,only inside the container. Yes, you can also [commit changes](https://rollout.io/blog/using-docker-commit-to-create-and-change-an-image/)made in containers back to your local Docker image.)  ### Testing the Docker installationTo start, download and extract a document from the [OCR-D GT Repo](https://ola-hd.ocr-d.de/search?q=&amp;fulltextsearch=false&amp;metadatasearch=false&amp;isGT=true&amp;perPageRecords=30):```shwget &quot;https://ola-hd.ocr-d.de/api/export?id=21.11156/BFBAD520-65F4-430A-B4B2-C81A296C9E09&amp;internalId=false&quot; -O wundt_grundriss_1896.ocrd.zipunzip wundt_grundriss_1896.ocrd.zipcd data```Now, spin up the docker container:```shdocker run --workdir /data --volume $PWD:/data --rm -it ocrd/all bash```Your command line should start with something similar to:```shroot@c5d94e852de0:/data#```After spinning up the container, you can use the installation and call the processors the same way as in the native installation.Alternatively, you can [translate each command to a docker call](/en/user_guide#translating-native-commands-to-docker-calls).Let&#39;s segment the images in file group `OCR-D-IMG` from the zip file into regions, thereby creating aMETS file group `OCR-D-SEG-BLOCK-DOCKER`):```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK-DOCKER```When you are finished using OCR-D commands, use this command to stop using docker interactively:```shexit```### Updating Docker imageTo update the Docker image to the latest version, just run the `docker pull` command:(replace `maximum` accordingly if you use the `minimum` or `medium` version)```shdocker pull ocrd/all:maximum```### Further readingWe recommend jumping to the [section about installing models at the bottom of this page](#installing-models) next.Alternatively, for instructions on how to proceed further with the processing of your data, please see the [user guide](/en/user_guide). Make sure to also read [the notes on translating native command line calls to docker calls](/en/user_guide#translating-native-commands-to-docker-calls).## ocrd_all nativelyThe `ocrd_all` project contains a sophisticated Makefile to install or compileprerequisites as necessary, set up a virtualenv, install the core software,install OCR-D modules and more. Detailed documentation [can be found in itsREADME](https://github.com/OCR-D/ocrd_all).### InstallationThere are some [system requirements](https://github.com/OCR-D/ocrd_all#system-packages) for ocrd_all.You need to have `make` installed to make use of `ocrd_all`:```shsudo apt install make```Clone the repository (still without submodules) and change into the `ocrd_all` directory:```shgit clone https://github.com/OCR-D/ocrd_allcd ocrd_all```You should now be in a directory called `ocrd_all`.It is easiest to install all the possible system requirements by calling `make deps-ubuntu` as root:```shsudo make deps-ubuntu```This will install all system requirements.Now you are ready for the final step which will actually install the OCR-D-Software.You can either install  1. all the software at once with the `all` target (equivalent to the [`maximum` Docker version](#mini-medi-maxi)),  2. modules individually by using an executable from that module as the target, or  3. a subset of modules by listing the project names in the `OCRD_MODULES` variable (equivalent to a custom selection of the [`medium` Docker version](#mini-medi-maxi)):```shmake all                       # Installs all the software (recommended)make ocrd-tesserocr-binarize   # Install ocrd_tesserocr which contains ocrd-tesserocr-binarizemake ocrd-cis-ocropy-binarize  # Install ocrd_cis  which contains ocrd-cis-ocropy-binarizemake all OCRD_MODULES=&quot;core ocrd_tesserocr ocrd_cis&quot; # Will install only ocrd_tesserocr and ocrd_cis```(Custom choices for `OCRD_MODULES` and other control variables (cf. `make help`) can also be made permanent by writing them into `local.mk`.)**Note:** Never run `make all` as root unless you know *exactly* what you are doing!Installation is incremental, i.e. failed/interrupted attempts can be continued, and modules can be installed one at a time as needed.Running `make` will also take care of cloning and updating all required submodules.Especially running `make all` will take a while (between 30 and 60 minutes or more on slower machines). In the end, it should say that the last processor was installed successfully.Having installed `ocrd_all` successfully, `ocrd --version` should give you the current version of [OCR-D/core](https://github.com/OCR-D/core).Activate the virtual Python environment, which was created in the directory `venv`, before running any OCR-D command.```shsource venv/bin/activateocrd --versionocrd, version 2.13.2 # your version should be 2.13.2 or later```### Testing the native installationFor example, let&#39;s fetch a document from the [OLA-HD Repo](https://ola-hd.ocr-d.de/):```shwget &quot;https://ola-hd.ocr-d.de/api/export?id=21.11156/BFBAD520-65F4-430A-B4B2-C81A296C9E09&amp;internalId=false&quot; -O wundt_grundriss_1896.ocrd.zipsudo unzip wundt_grundriss_1896.ocrd.zipcd data```If you haven&#39;t done it already, activate your venv:```sh# Activate the venvsource /path/to/ocrd_all/venv/bin/activate```Let&#39;s segment the images in file group `OCR-D-IMG` from the zip file into regions (creating afirst [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) file group`OCR-D-SEG-BLOCK`):```shocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK```### Updating the softwareAs `ocrd_all` is in [activedevelopment](https://github.com/OCR-D/ocrd_all/commits/master), it is wise toregularly update the repository and its submodules:```shgit pull```This will refresh the local clone of ocrd_all with the changes in the official ocrd_all GitHub repository.Now you can install the changes with```shmake all```This will run the installation process for all submodules which have been changed. In the end, it shouldsay that the last processor was installed successfully. `--version` for the processors which have been changedshould give you its current version.### Further readingWe recommend jumping to the [section about installing models at the bottom of this page](#installing-models) next.For instructions on how to process your own data, please see the [user guide](/en/user_guide).## Individual installation (experts only)For developing purposes it might be useful to install modules individually, either with Docker or natively.With all variants of individual module installation, it will be up to you tokeep the repositories up-to-date and installed. We therefore discourageindividual installation of modules and recommend using ocrd_all as outlined above..All [OCR-D modules](https://github.com/topics/ocr-d) follow the same[interface](https://ocr-d.github.io/cli) and common design patterns. So onceyou understand how to install and use one project, you know how to install anduse all of them.### Individual Docker containerThis is the best option if you want full control over which modules youactually intend to use while still profiting from the simple installation ofDocker containers.You need to have [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)Many OCR-D modules are also [published as Docker containers to DockerHub](https://hub.docker.com/u/ocrd). To find the Dockerimage for a module, replace the `ocrd_` prefix with `ocrd/`:```shdocker pull ocrd/tesserocr  # Installs ocrd_tesserocrdocker pull ocrd/olena  # Installs ocrd_olena```Now you can [test your installation](#testing-the-docker-installation).### Native installationInstalling each module into your system natively requires you to know and install all its _dependencies_ first.That can be _system packages_ (or even system package repositories) or _Python packages_.To learn about system dependencies, consult the module&#39;s README files. In contrast, Python dependencies shouldbe resolved automatically by using the Python package manager `pip`.&gt; **NOTE**&gt;&gt; ocrd_tesserocr requires **tesseract-ocr &gt;= 4.1.0**. But the Tesseract packages&gt; bundled with **Ubuntu  please enable [Alexander Pozdnyakov PPA](https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr),&gt; which has up-to-date builds of tesseract and its dependencies:&gt;&gt; ```sh&gt; sudo add-apt-repository ppa:alex-p/tesseract-ocr&gt; sudo apt-get update&gt; ```Next subsections:- For Python you also first need [virtualenv](#virtualenv). Then you have two options:- installing [via PyPI](#from-pypi) or- installing [via local git clone](#from-git).#### virtualenv* **Always install python modules into a virtualenv*** **Never run `pip`/`pip3` as root**First install Python 3 and `venv`:```shsudo apt install python3 python3-venv``````sh# If you haven&#39;t created the venv yet:python3 -m venv ~/venv# Activate the venvsource ~/venv/bin/activate```Once you have activated the virtualenv, you should see `(venv)` prepended toyour shell prompt.#### From PyPIThis is the best option if you want to use the stable, released version of individual modules.However, many modules require a number of non-Python (system) packages. For theexact list of packages you need to look at the README of the module inquestion. (If you are not on Ubuntu &gt;= 18.04, then your requirements maydeviate from that.)For example to install `ocrd_tesserocr` from PyPI:```shsudo apt-get install git python3 python3-pip python3-venv libtesseract-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr wgetpip3 install ocrd_tesserocr```Many ocrd modules conventionally contain a Makefile with a `deps-ubuntu` target that can handle calls to `apt-get` for you:```shsudo make deps-ubuntu```Now you can [test your installation](#testing-the-native-installation).#### From gitThis is the best option if you want to change the source code or install the latest, unpublished changes.```shgit clone https://github.com/OCR-D/ocrd_tesserocrcd ocrd_tesserocrsudo make deps-ubuntu # or manually with apt-getmake deps             # or pip3 install -r requirementsmake install          # or pip3 install .```If you intend to develop a module, it is best to install the module editable:```shpip install -e .```This way, you won&#39;t have to reinstall after making changes.Now you can [test your installation](#testing-the-native-installation).## Installing modelsSeveral processors in OCR-D need pretrained models you have to install beforehand.Please consult our [instruction on models](/en/models) to get more information on how to download and install them.",
      "url": " /en/setup.html"
    },
  

    {
      "slug": "en-start-html",
      "title": "",
      "content"	 : "# OCR-D Quick Start Guide## Open your Ubuntu Terminal**On Ubuntu**, open your Terminal.**On Windows**, install WSL, Ubuntu and Docker Desktop by following these steps:1. Install WSL 2 by opening the PowerShell and running:```shwsl --install```2. [Download and install Ubuntu 22.04.2 LTS from Microsoft App Store.](https://www.microsoft.com/store/productId/9PN20MSR04DW)3. Open Ubuntu 22.04.2 LTS and follow the instructions.4. [Install Docker Desktop and set it up for WSL 2](https://docs.docker.com/desktop/wsl/). 5. Make sure, Docker Desktop is running.## Install and set up DockerIn the Ubuntu shell, run:```shdocker ps```If the command is not found, you may need to [install Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository) first.## Further Requisites1. Install OCR-D via Docker and download example data from Github:```shdocker pull ocrd/all:maximummk dir ocr-dcd ocr-dgit clone https://github.com/OCR-D/assetsmkdir workspacecp -r assets/data/kant_aufklaerung_1784 workspace/kant_aufklaerung_1784cd workspace/kant_aufklaerung_1784```2. Set up Docker : ```shsudo systemctl enable dockersudo usermod -aG docker $USERdocker run --workdir /data --volume $PWD/.config:/.config --volume $PWD:/data --volume $PWD/models:/usr/local/share/ocrd-resources --volume $PWD/models:/usr/local/share/tessdata --volume $PWD/models:/usr/local/share/ocrd-resources -it ocrd/all bash```3. Download some models:```shmkdir -p $PWD/models/ocrd-tesserocr-recognizesudo mkdir -p /usr/local/share/ocrd-resources/ocrd-tesserocr-recognize/configscd dataocrd resmgr download &#39;*&#39;```## First minimal workflow with OCR-D```shocrd-tesserocr-recognize -I OCR-D-IMG -O OCR-D-TESSOCR -P segmentation_level region -P textequiv_level word -P find_tables true -P model ocrd-tesserocr-recognize -I OCR-D-IMG -O OCR-D-TESSOCR -P segmentation_level region -P textequiv_level word -P find_tables true -P model Fraktur_GT4HistOCR```Congratulations! You ran your first (minimal) OCR-D Workflow. You will find the results in the directory`workspace/kant_aufklaerung_1784` under `data`.Consult the [Setup Guide](/en/setup) for more details and other installation methods or jump into the [User Guide](/en/user_guide) to learn more about OCR&amp;#8209;D. Below you find a short explanation for the `ocrd-tesserocr-recognize` command.### ExplanationThe command`ocrd-tesserocr-recognize -I OCR-D-IMG -O OCR-D-TESSOCR -P segmentation_level region -P textequiv_level word -P find_tables true -P model Fraktur_GT4HistOCR`for the recognition contains the following parameters:1. `ocrd-tesserocr-recognize` is the processor used.2. `-I` is followed by the name of the input folder, here images.3. `-O` is followed by the name of the output folder where you will find the results (here binarised images and mets files with the recognised text.4. `-P segmentation_level region` is a parameter for the processor which tells tesserocr to start the segmentation on the level of regions.5. `-P textequiv_level word` is a parameter for the processor which tells tesserocr to stop the segmentation on the level of words (meaning glyphs will not be segmented here).6. `-P find_tables true` is a parameter for the processor which tells tesserocr to recognise tables.7. `-P model Fraktur_GT4HistOCR` is a parameter for the processor which tells tesserocr to use the model `Fraktur_GT4HistOCR` for recognition.",
      "url": " /en/start.html"
    },
  

    {
      "slug": "en-survey-html",
      "title": "Survey concerning usage of OCR texts",
      "content"	 : "# Survey concerning usage of OCR texts## BackgroundIn spring of 2016 the BSB (back then part of the coordinating project) conducted a survey on the usage of OCR texts via the OCR-D-project-website which mainly addressed humanists. In total 139 researchers took part in the poll. 39 of those answers were partly illegible and some of the questions were only answered by a part of the participants. ## Major findingsThe survey shows that the great majority of the participants uses OCR texts for their research (cf. figure 1). Those texts are mainly used as search tools, but also as basis for the analysis of large amounts of text data (cf. figure 2). 60 % of the participants would also use dirty OCR texts for research purposes, whereas 40 % consider it useless data. Interestingly, only historians (87 %) show a significant preference of dirty OCR which is seen especially helpful as finding aid as one can still find information which would have been missed otherwise. Furthermore it facilitates citing by providing an initial text which can then be corrected so that the text doesn’t have to be typed completely manually. Overall however, the original image (61 %) is preferred to the OCR text (39 %) for citing, especially by librarians.Concerning the importance of versioning OCR texts there is much discord among the participating scholars. While almost three quarters of the researchers states that changes in the OCR text are important to their work, only half them wants to have access to earlier versions of OCR texts. These are mainly considered necessary for persistent quoting of the OCR text and for reproducing analyses conducted on those texts – though keeping track of all versions is rather seen as too laborious.**All in all, OCR texts are already widely used for research and also dirty OCR, as is currently the state of most OCRed early modern texts, is regarded as a valuable aid for particular parts of scientific work.**![](/assets/usage_OCR.png)![](/assets/usage_forms.png)",
      "url": " /en/survey.html"
    },
  

    {
      "slug": "de-teststellung-html",
      "title": "Ergebnisse und Erkenntnisse der ersten OCR-D-Teststellung",
      "content"	 : "# Ergebnisse und Erkenntnisse der ersten OCR-D-Teststellung## HintergrundUm die Jahreswende 2019/2020 wurde die OCR-D-Software erstmals in neunPilotbibliotheken getestet. Damit sollte die praktische Akzeptanz der Softwarebei künftigen, potentiellen Nutzer*innen sichergestellt werden, weshalb der Fokus aufderen Funktionalität und Einsetzbarkeit in der Praxis lag. An der Teststellungteilgenommen haben neben den Häusern des Koordinierungsprojekts auch zwei anden Modulprojekten beteiligte Bibliotheken sowie vier weitere Bibliotheken. DieErkenntnisse dieses ersten Testlaufs fließen in die Weiterentwicklung desOCR-D-Prototypen ein.Alle Pilotbibliotheken verfügen über erste Kenntnisse und Erfahrungen zu OCR,da sie zumindest auf Projektebene bzw. über Dienstleister bereits Volltexteerstellt haben. Inwieweit die als wichtig angesehene OCR künftig eigenständigdurchgeführt und fest im Digitalisierungsworkflow verankert werden soll, wirdderzeit noch in den Häusern abgestimmt. Für welche Nutzergruppe Volltexteerstellt werden, wird von den einzelnen Häusern unterschiedlich angegeben.Während ein Drittel allgemein GeisteswissenschaftlerInnen nennt, möchte einweiteres Drittel eine sehr breite Zielgruppe (Geisteswissenschaft, DigitalHumanities, Computerlinguistik und Wirtschaftswissenschaften) bedienen. Dieübrigen Bibliotheken sehen lediglich einen kleinen Nutzerkreis (DigitalHumanities oder Computerlinguistik) als Zielgruppe der OCR-Texte.An eine OCR-Software stellen die Pilotbibliotheken die folgenden Anforderungen:* hohe Erkennungsrate von Layout und Text* kostengünstiger Einsatz* schnelle Adaptierbarkeit/Fehlerbehebung* Modularität* Ausgabe in Standardformate* Anbindung an existierende Workflows* gut dokumentierte Schnittstellen* Wortkoordinaten* Trainierbarkeit* umfangreiche GT KorporaAm wichtigsten ist die hohe Qualität der Texterkennung, die weiteren Merkmalewerden jeweils nur von einem Teil der Pilotbibliotheken angegeben und dürftenals gewünschte, aber untergeordnete fakultative Merkmale angesehen werden.## Auswertung der SoftwaretestsUm die Vergleichbarkeit der einzelnen Tests in den Pilotbibliothekengewährleisten zu können, wurde ein Fragebogen erstellt, der zu Beginn derTeststellung an die Pilotbibliotheken ausgegeben wurde. In diesem werden dieRahmenbedingungen des Testlaufs, bspw. die verwendete technische Ausstattungund die getesteten OCR-D-Prozessoren, sowie die Dokumentation der Software,Schnittstellen, Funktionalität bzw. Benutzbarkeit der Software, derenMöglichkeiten zur Einbindung in existierende Workflows und die jeweilsbenötigten Ausgabeformate erfasst. Mit Erkennungsqualität, Funktionalität bzw.Benutzbarkeit, offenen Anforderungen sowie positiven Merkmalen derOCR-D-Software wurde außerdem nach den Ergebnissen der Teststellung gefragt.In der Teststellung wurden die verschiedenen Möglichkeiten zur Installation derOCR-D-Software mit und ohne Docker-Container genutzt und die Softwareerfolgreich auf einer breiten Auswahl an unterschiedlich leistungsstarken,teils virtuellen Servern installiert. Bei Nicht-Intel-Rechnern (ARM, PowerPC64)war diese komplizierter und zeitaufwändiger, da einzelne Python-Pakete aufdiesen Rechnern nicht ausführbar waren und erst manuell angepasst werdenmussten. Die während der Teststellung entwickelte Gesamt-Installation allerverfügbarer OCR-D-Prozessoren (``ocrd_all``) wurde als einfachste undunaufwändigste Variante dabei als am empfehlenswertesten bestätigt. In eineWorkflow-Software wie bspw. Kitodo wurde die OCR-D-Software an keinerPilotbibliothek integriert, da der Aufwand für eine Einbindung der Software fürden Testlauf zu hoch gewesen wäre. Als Herausforderung wurde die Verwendung der zahlreichen OCR-D-Prozessorenbeschrieben. Hier bereitete weniger deren Aufruf Probleme, als das Verständnisvon deren jeweiligem Einsatzbereich und insbesondere Auswahl sowieZusammenstellung der Prozessoren zu sinnvollen Workflows. Für die ersteTeststellung lag neben der technischen Dokumentation der Software noch keineGesamtdokumentation zu deren Nutzung vor, die sich auch an im OCR-Bereichunerfahrene Anwender richtet. Die Anforderungen und Wünsche der Tester an einesolche Dokumentation wurden noch in der Ausarbeitung der inzwischen [imNutzerbereich der OCR-D-Website eingestelltenAnleitungen](https://ocr-d.de/de/use) berücksichtigt. Die OCR-D-Software läuft insgesamt sehr stabil, Abbrüche wurden von keinerBibliothek gemeldet. Die benötigten Ausgabeformate werden bereits alleangeboten, wohingegen die wenigen noch erforderlichen Arbeiten im Bereich derSchnittstellen für die Weiterentwicklung des Prototyps eingeplant sind.Die Erkennungsqualität wurde von den jeweiligen Pilotbibliotheken nur aneinzelnen Seiten überprüft, da zu den Testbüchern kein Ground Truth vorliegt.Insgesamt sind die Ergebnisse dieses ersten Testlaufs vielversprechend. Die UBMannheim hat OCR-D mit Fokus auf die Tesseract-Prozessoren bspw. an fünfDrucken des 16. bis 19. Jahrhunderts getestet. An Antiqua-Drucken des 17. und 18.Jahrhunderts sowie einem Fraktur-Text aus dem 19. Jahrhundert konntenerwartungsgemäß die besten Ergebnisse von - im Falle der Antiqua deutlich -unter 0,1 CER bei den Rohdaten erzielt werden, wohingen der Frakturdruck ausdem 17. Jahrhundert leicht über 0,1 CER lag. Die größte Herausforderung stelltedie Fraktur aus dem 16. Jahrhundert dar, bei der lediglich ein CER von knappunter 0,16 erreicht werden konnte. Einen umfassenden Einblick in ihreTeststellung gibt die BBAW, deren [Bericht und Daten öffentlicheinsehbar](https://github.com/tboenig/ocrd_bbaw_pilotbibliothek) sind.Desiderate formulieren die OCR-D-Tester insbesondere bei Dokumentation,Qualität bzw. Benutzbarkeit der Prozessoren sowie perspektivisch derenSkalierbarkeit. Die benannten Anforderungen an die Dokumentation derOCR-D-Software wurden bereits weitgehend umgesetzt, wobei Dokumentationinsgesamt als kontinuierliche Aufgabe aufgefasst wird in die sukzessive vorallem auch praktische Erfahrungen in der Anwendung der OCR-D-Softwareeinfließen müssen. Im Bereich der Prozessoren wären v.a. zu Layouterkennung undNachkorrektur noch Verbesserungen wünschenswert. Die entsprechendenEntwicklungen der OCR-D-Modulprojekte konnten durch ihr Entwicklungsstadiumbzw. ihre speziellen technischen Anforderungen (GPU) nur bedingt getestetwerden. Mit deren Ergebnissen oder auch weiteren Modellen können die obengenannten Desiderate hoffentlich bedient werden. Für den Einsatz derOCR-D-Software in der Massendigitalisierung ist die Laufzeit mehrererProzessoren - wie ursprünglich für die dritte Projektphase geplant - noch zuoptimieren, außerdem sollten die Möglichkeiten zur Parallelisierung ausgebautwerden. Positiv von den Testern hervorgehoben wird der modulare und transparente Aufbauder OCR-D-Software, der diese besonders auszeichnet und die Konfigurationoptimaler Workflows für konkrete Anwendungsfälle erlaubt. Außerdem können dieOpen Source verfügbaren Python-Programme bei Bedarf von jeweils daraufspezialisierten Experten weiterentwickelt werden und ohne aufwändigeProgrammierarbeiten für Experimente zum OCR-Workflow genutzt werden. Bei Fragenund Problemen leisten die Entwickler rasch niedrigschwelligen Support.Insgesamt ist es vergleichsweise einfach möglich, die robust laufende, wennauch noch weiter zu optimierende OCR-D-Volltexterstellung anzustoßen, diebereits vielversprechende Ergebnisse liefert.",
      "url": " /de/teststellung.html"
    },
  

    {
      "slug": "de-umfrage-html",
      "title": "Umfrage zur Verwendung von OCR-Texten",
      "content"	 : "# Umfrage zur Verwendung von OCR-Texten## HintergrundIm Frühjahr 2016 führte die BSB (damals Teil des Koordinierungsprojekts) über die OCR-D-Projekt-Website eine Umfrage zur Verwendung von OCR-Texten durch, die sich hauptsächlich an Geisteswissenschaftler richtete. Insgesamt nahmen 139 Forscher an der Umfrage teil. 39 dieser Antworten waren teilweise unleserlich und einige der Fragen wurden nur von einem Teil der Teilnehmer beantwortet. ## HauptergebnisseDie Umfrage zeigt, dass die grosse Mehrheit der Teilnehmerinnen und Teilnehmer OCR-Texte für ihre Forschung verwendet (vgl. Abb. 1). Diese Texte werden hauptsächlich als Suchwerkzeuge, aber auch als Grundlage für die Analyse großer Textdatenmengen verwendet (vgl. Abb. 2). 60 % der Teilnehmerinnen und Teilnehmer würden auch schmutzige OCR-Texte zu Forschungszwecken verwenden, während 40 % sie für nutzlose Daten halten. Interessanterweise zeigen nur die Historiker (87 %) eine signifikante Präferenz für schmutzige OCR, die als besonders hilfreich bei der Suche nach Informationen angesehen wird, die sonst übersehen worden wären. Darüber hinaus erleichtert sie das Zitieren, indem sie einen ersten Text liefert, der dann korrigiert werden kann, so dass der Text nicht vollständig manuell getippt werden muss. Insgesamt wird jedoch das Originalbild (61 %) dem OCR-Text (39 %) zum Zitieren vorgezogen, insbesondere von Bibliothekaren.Hinsichtlich der Bedeutung der Versionierung von OCR-Texten gibt es unter den teilnehmenden Wissenschaftlern große Meinungsverschiedenheiten. Während fast drei Viertel der Forscher angeben, dass Änderungen im OCR-Text für ihre Arbeit wichtig sind, möchte nur die Hälfte von ihnen Zugang zu früheren Versionen von OCR-Texten haben. Diese werden vor allem als notwendig erachtet, um den OCR-Text dauerhaft zitieren und die Analysen, die zu diesen Texten durchgeführt wurden, reproduzieren zu können - obwohl es eher als zu mühsam angesehen wird, alle Versionen im Auge zu behalten.**Insgesamt werden OCR-Texte bereits in großem Umfang für die Forschung verwendet, und auch  schmutzige OCR, wie sie derzeit für die meisten OCR-Texte frühneuzeitlicher Bücher vorliegt, wird als wertvolle Hilfe für bestimmte Teile der wissenschaftlichen Arbeit angesehen.**![](/assets/usage_OCR.png)![](/assets/usage_forms.png)",
      "url": " /de/umfrage.html"
    },
  

    {
      "slug": "en-use-html",
      "title": "How to use OCR-D",
      "content"	 : "# Welcome to the OCR-D User Section!This section contains all information relevant for using the OCR-D-software in libraries and similar institutions. Learn how to install and use the software in your institution.* [Setup Guide](/en/setup)  * How to setup/install the OCR-D stack* [User Guide](/en/user_guide)  * Instructions how to use OCR-D components* [Workflows](/en/workflows)  * Steps of an OCR-D-workflow with sample workflows* [Models](/en/models)  * Overview of models for different OCR-engines* [QUIVER](https://ocr-d.de/quiver-frontend/#/workflows)  * Quality management* [Glossary](/en/spec/glossary)  * Glossary of technical terms used in OCR-D",
      "url": " /en/use.html"
    },
  

    {
      "slug": "de-use-html",
      "title": "Willkommen im Nutzerbereich von OCR-D!",
      "content"	 : "# Willkommen im Nutzerbereich von OCR-D!Dieser Bereich enthält alle Informationen, die für den Einsatz der OCR-D-Software in Bibliotheken und ähnlichen Einrichtungen relevant sind. Erfahren Sie, wie Sie die Software in Ihrer Einrichtung installieren und nutzen können.* [Setup Anleitung](/en/setup)  * Schritt-für-Schritt Anleitung zur Installation von OCR-D *(aktuell nur auf Englisch verfügbar)** [Nutzeranleitung](/en/user_guide)  * Instruktionen zum Arbeiten mit OCR-D *(aktuell nur auf Englisch verfügbar)** [Workflows](/en/workflows)  * Schritte eines OCR-D-Workflows mit Beispielworkflows *(aktuell nur auf Englisch verfügbar)** [Modelle](/en/models)  * Überblick zu Modellen verschiedener OCR-Engines *(aktuell nur auf Englisch verfügbar)** [QUIVER](https://ocr-d.de/quiver-frontend/#/workflows)  * Qualitätsmanagement *(aktuell nur auf Englisch verfügbar)** [Glossar](/de/spec/glossary)  * Fachbegriffe aus dem Bereich der OCR erklärt *(aktuell nur auf Englisch verfügbar)*",
      "url": " /de/use.html"
    },
  

    {
      "slug": "en-user-guide-html",
      "title": "User Guide for Non-IT Users",
      "content"	 : "# User Guide for Non-IT UsersThe following guide provides a detailed description on how to use the OCR-D software after it has been[installed](setup) successfully. As explained in the [Setup Guide](setup), you can either use the[OCR-D Docker solution](https://ocr-d.github.io/en/setup#ocrd_all-via-docker), or you can[install the Software natively](https://ocr-d.github.io/en/setup#ocrd_all-natively) on your OS.Depending on which option you prefer, you will require different steps to run OCR-D, as detailedin the following two paragraphs. (The [third paragraph](#preparing-a-workspace) is obligatoryfor both Docker and native users.)Docker commands need a [extra syntax over native commands](#translating-native-commands-to-docker-calls). This guide always states native calls first, but follows up with the respective command for Docker.## Preparations### Docker installation: Run containerIf you are using the Installation via Docker, we recommend running an interactive shell session in the container:```shdocker run --user $(id -u) --tmpfs /tmp --volume $PWD:/data --volume ocrd-models:/models -it ocrd/all bash```After spinning up the container, you can use the internal installation and call the processorsthe same way as in the native installation. Alternatively, you can[translate each command to a docker call](/en/user_guide#translating-native-commands-to-docker-calls) separately.### Native installation: Activate virtual environmentIf you are using a native installation, you must activate the venv before you can startworking with the OCR-D software. It has either been created automatically if you [installed the software via ocrd_all](setup), or you should have manually[installed it yourself](https://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments)before installing the OCR-D software. To activate, you need to specify the path to your venv. In the automatic `ocrd_all` case,it has simply been created under `venv` in your `ocrd_all` directory:```sh# example with manually created venv:$ source ~/venv/bin/activate# example for automatically created venv:$ source ocrd_all/venv/bin/activate# when the shell loads the venv, the prompt will change:(venv) $```Once you have activated the venv in your shell, you should see its name prepended tothe command prompt.When you are done with your OCR-D work, you can use `deactivate` to deactivatethe venv (or just terminate the shell).### Preparing a workspaceOCR-D processes digitized images in so-called [workspaces](spec/glossary#workspace),i.e. special directories which contain the images to be processed and their corresponding METS file. Any files generated while processing these images with the OCR-D softwarewill also be stored in this directory.How you prepare a workspace **depends** on whether or not you **already** have a METS filewith the paths (or URLs) to the images you want to process. For usage withinOCR-D your METS file should look similar to [this example](example_mets).#### Already existing METSIf you already have a METS file as indicated above, you can create a workspaceand load the pictures to be processed with the following command:```shocrd workspace [-d path/to/workspace] clone URL_OF_METS## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace clone [-d path/to/your/workspace] URL_OF_METS```(Where `path/to/your/workspace` is but an example. You can **omitthe directory argument** if you want to use the current working directoryas target. For repeated use, we recommend a `cd path/to/your/workspace` once,so in subsequent operations, the argument can be omitted.)This will create a file `mets.xml` within the target directory.In most cases, METS files indicate several picture formats. For OCR-D you willonly need one format. We strongly recommend using the format with the highestresolution. Optionally, you can specify to only load the file group needed:List all existing groups:```shocrd workspace [-d /path/to/your/workspace] list-group## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace [-d path/to/your/workspace] list-group```This will provide you with the names of all the different file groups in your METS, e.g. `THUMBNAILS`,`PRESENTATION`, `MAX`.Download all files of one file group:```shocrd workspace [-d path/to/your/workspace] find --file-grp [selected file group] --download## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace [-d path/to/your/workspace] find --file-grp [selected file group] --download```This will download all images in the specified file group and save them in a directory named accordinglyin your workspace. You are now ready to start processing your images with OCR-D.#### Non-existing METSIf you don&#39;t have a METS file or it does not comply with the OCR-D requirements,then you can generate one with the following commands. First, create an emptyworkspace:```shocrd workspace [-d path/to/your/workspace] init## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace [-d path/to/your/workspace] init```(Where `path/to/your/workspace` is but an example. You can **omitthe directory argument** if you want to use the current working directoryas target. For repeated use, we recommend a `cd path/to/your/workspace` once,so in subsequent operations, the argument can be omitted.)This will **create** a file `mets.xml` within the target directory.Then you can set a unique `mods:identifier` …```shocrd workspace [-d path/to/your/workspace] set-id &#39;unique ID&#39;## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace [-d path/to/your/workspace] set-id &#39;unique ID&#39;```… and copy or symlink the directory containing the images to be processed into the workspace directory:```shcp -r path/to/your/images [path/to/your/workspace/].ln -s path/to/your/images [path/to/your/workspace/].```Now you can add those images to the empty METS created above,by **adding references** for their path names. You can do this in a number of ways. Either with the following simple command:```shocrd workspace [-d path/to/your/workspace] add -g {ID of the physical page} -G {name of image fileGrp} -i {ID of the image file} -m image/{MIME format of that image} {path/to/that/image/file/in/workspace}## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace [-d path/to/your/workspace] add -g {ID of the physical page} -G {name of image fileGrp} -i {ID of the image file} -m image/{MIME format of that image} {path/to/that/image/file/in/workspace}```&gt; **Note**: Identifiers in XML must always [start with a letter](https://www.w3.org/TR/REC-xml/#NT-Names).For example, your simple commands could look like this:```shocrd workspace add -g P_00001 -G OCR-D-IMG -i OCR-D-IMG_00001 -m image/tiff images/00001.tifocrd workspace add -g P_00002 -G OCR-D-IMG -i OCR-D-IMG_00001 -m image/tiff images/00002.tif...## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace add -g P_00001 -G OCR-D-IMG -i OCR-D-IMG_00001 -m image/tiff images/00001.tifdocker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace add -g P_00002 -G OCR-D-IMG -i OCR-D-IMG_00002 -m image/tiff images/00002.tif...```Or, if you have lots of images to be added to the METS, you can do this automatically with a `for` loop:&gt; **Note**: For this method, all images must have the same format (tiff, jpeg, ...)```shFILEGRP=&quot;OCR-D-IMG&quot; # name of fileGrp to useEXT=&quot;.tif&quot;  # the actual extension of the image filesMEDIATYPE=&quot;image/tiff&quot;  # the actual MIME type of the imagescd path/to/your/workspacefor path in images/*$EXT; do  base=`basename $path $EXT`;  ## using local ocrd CLI:  ocrd workspace add -G $FILEGRP -i ${FILEGRP}_${base} -g P_$base -m $MEDIATYPE $path  ## alternatively, using Docker:  docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace add -G $FILEGRP -i ${FILEGRP}_${base} -g P_$base -m $MEDIATYPE $pathdone```For example, your `for` loop could look like this:```shfor path in images/*.tif; do base=`basename $path .tif`; ocrd workspace add -G OCR-D-IMG -i OCR-D-IMG_$base -g P_$base -m image/tiff $path; done## alternatively, using Docker:for path in images/*.tif; do base=`basename $path .tif`; docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd workspace add -G OCR-D-IMG -i OCR-D-IMG_$base -g P_$base -m image/tiff $path; done```The log information should inform you about every image which was added to the METS file.In the end, your `mets.xml` should look like this [example METS](example_mets). You are now ready to start processing your images with OCR-D.Finally, the shell script `ocrd-import` from [workflow-configuration](https://github.com/bertsky/workflow-configuration)is a tool which does all of the above (and can also convert arbitrary image formats and extract from PDFs)automatically. For usage options, see:```shocrd-import -h```For example, to search for all files under `path/to/your/images/` recursively, and add all image files under fileGrp `OCR-D-IMG`, keeping their filename stem as page ID,while converting all unsupported image file formats like JPEG2000, XPS or PDF (the latter rendered to bitmap at 300 DPI) to TIFF on the fly, and also add any PAGE-XML file of the same filename stem under fileGrp `OCR-D-SEG-PAGE`,while ignoring other files, and finally write everything to `path/to/your/images/mets.xml`, do:```shocrd-import --nonnum-ids --ignore --render 300 path/to/your/images## alternatively using Dockerdocker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd-import -P -i -r 300 path/to/your/images```You should now have a workspace which contains the aforementioned `mets.xml` that hasa fileGrp `OCR-D-IMG` referencing your local image files.&gt; **Note**: In OCR-D, we typically name the image fileGrp `OCR-D-IMG`, which is used&gt; throughout the documentation. Naming your image fileGrp differently is of course possible,&gt; but you should be aware that you then need to adapt the name of the image or input fileGrp&gt; when copying and pasting from the sample calls provide on this website.## Using the OCR-D processors### OCR-D command-line interface syntaxThere are several ways for invoking the OCR-D processors. Still, all of themmake use of the following syntax:```sh-I Input-Group      # fileGrp of the files to be processed-O Output-Group     # fileGrp of the files results-P parameter value  # (direct assignment of parameters for a particular processor)-p parameter-file   # (file-based assignment of parameters for a particular processor)-g page-range       # (range of physical pages to be processed)```&gt; **Note**: For some processors, all parameters are optional, while other processors such as&gt; `ocrd-tesserocr-recognize` will not work without some parameter specifications.For information on the available processors, and their respective parameters,see [getting more information about processors](#get-more-information-about-processors).### Calling a single processorIf you just want to run a single processor, you can go into your workspace and use the following command:```shocrd-{processor name} -I {Input-Group} -O {Output-Group} [-p {parameter-file}] [-P {parameter} {value}]## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd-{processor name} -I {Input-Group} -O {Output-Group} [-p {parameter-file}] [-P {parameter} {value}]```For example, your processor call command could look like this:```shocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN -P impl sauvola## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN -P impl sauvola```The specified processor will read the files in fileGrp `Input-Group`,binarize them and write the results in fileGrp `Output-Group` in your workspace(i.e. both as files on the filesystem and referenced in the `mets.xml`). It will also add information about this processing step in the METS metadata.&gt; **Note**: For processors using multiple input- or output fileGrps you have to use a comma-separated list.For example:```shocrd-cor-asv-ann-align  -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-OCR4## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd-cor-asv-ann-align  -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-OCR4```&gt; **Note**: If multiple parameter key-value pairs are necessary, each of them has to be preceded by `-P` as in ```sh... -P param1 value1 -P param2 value2 -P param3 value3```&gt; **Note**: If a value consists of several words with whitespaces, they have to be enclosed in quotation marks&gt; (to prevent the shell from splitting them up) as in```sh-P param &quot;value value&quot;```### Calling several processorsRunning several processors one after another on the same data is called a **workflow**.For workflow processing, you need a workflow format and a workflow engine. In the most simple case, you just write a shell script which combines single processorcalls in a command sequence joined by `&amp;&amp;`. The following paragraphs will describe moreadvanced options.#### ocrd processIf you quickly want to specify a particular workflow on the CLI, you can use`ocrd process`, which has a similar syntax as calling single processor CLIs:```shocrd process   &#39;{processor needed without prefix &#39;ocrd-&#39;} -I {Input-Group} -O {Output-Group}&#39;   &#39;{processor needed without prefix &#39;ocrd-&#39;} -I {Input-Group} -O {Output-Group} -P {parameter} {value}&#39;## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd process   &#39;{processor needed without prefix &#39;ocrd-&#39;} -I {Input-Group} -O {Output-Group}&#39;   &#39;{processor needed without prefix &#39;ocrd-&#39;} -I {Input-Group} -O {Output-Group} -P {parameter} {value}&#39;  ```For example, your command could look like this:```shocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -P model Fraktur&#39;## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd process   &#39;cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-SEG-PAGE&#39;   &#39;tesserocr-segment-region -I OCR-D-SEG-PAGE -O OCR-D-SEG-BLOCK&#39;   &#39;tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE&#39;   &#39;tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESSEROCR -P model Fraktur&#39;```Each specified processor will read the files in the respective fileGrp `Input-Group`,process them accordingly, and write the results in the respective fileGrp `Output-Group`in your workspace (i.e. both as files on the filesystem and referenced in the `mets.xml`). It will also add information about this processing step in the METS metadata.The processors work on the files sequentially. So at first, all pages will be processedwith the first processor (e.g. binarized), then (if successful) all pages will be processed by the second processor (e.g. segmented) etc. So in the end, your workspace should contain a directory (and fileGrp) with (intermediate)processing results for each output fileGrp specified in the workflow.&gt; **Note**: In contrast to calling a single processor, for `ocrd process` you leave&gt; out the prefix `ocrd-` before the name of a particular processor.#### ocrd-make`ocrd-make` from [workflow-configuration](https://github.com/bertsky/workflow-configuration)is another tool for specifying OCR-D workflows and running them. It combines GNU `parallel` with GNU `make`as workflow engine, treating document processing like software builds (including incremental and parallelcomputation). Configurations are just makefiles, workspaces and their file groups are just targets.It is included in [ocrd_all](https://github.com/OCR-D/ocrd_all), therefore you most likely already[installed it](setup) along with the other OCR-D processors. &gt; **Note**: The `workflow-configuration` distribution contains several example workflows, which were tested&gt; against the Ground Truth provided by OCR-D. For CER results of those workflows in our tests see&gt; [the table on GitHub](https://github.com/bertsky/workflow-configuration#usage). However, most workflows&gt; are configured for GT data, i.e. they expect preprocessed images which were already segmented&gt; at least down to line level. If you want to run them on naked images, you have to add some preprocessing&gt; and segmentation steps first, otherwise they will fail.In order to run a workflow, change into your data directory (which contains the workspaces) and callthe desired configuration file on your workspace(s):```shocrd-make -f {name_of_your_workflow.mk} [/path/to/your/workspace1] [/path/to/your/workspace2] ...```As indicated in the command above, you can run a workflow on several workspaces by listing them after one another.Or use the special target `all` for all the workspaces in the current directory.The documents in those workspaces will be processed and the respective output along with the log files will be saved into the same workspace(s).For an overview of all available targets and workspaces:```shocrd-make help```For general info on `make` invocation, including the `-j` switch for parallel processing:```shmake --help```When you want to adjust a workflow for better results on your particularimages, you should start off by copying the original `workflow.mk`file:```shcp workflow.mk {name_of_your_new_workflow_configuration.mk}```Then open the new file with an editor which understands `make` syntax like e.g. `nano`,and exchange or add the processors or parameters to your needs:```shnano {name_of_your_new_workflow_configuration.mk}```You can write new rules by using file groups as prerequisites/targets in the normal GNU make syntax.The first target defined must be the default goal that builds the very last file group for that configuration.Alternatively, a variable `.DEFAULT_GOAL` pointing to that target can be set anywhere in the makefile.&gt; **Note**: Also see the [extensive Readme of workflow-configuration](https://bertsky.github.io/workflow-configuration)&gt; on how to write workflows or adjust the preconfigured workflows to your needs.#### Translating native commands to Docker callsThe command calls presented above are easy to translate for use in ourDocker images – simply by prepending the boilerplate telling Docker which image to use,which user to run as, which files to bind to a container path etc.For example a call to[`ocrd-tesserocr-segment`](https://github.com/OCR-D/ocrd_tesserocr) might nativelylook like this …```shocrd-tesserocr-segment -I OCR-D-IMG -O OCR-D-SEG```… to run it with the [`ocrd/all:maximum`](https://hub.docker.com/r/ocrd/all/tags) Docker container …```shdocker run -u $(id -u) -v $PWD:/data -v ocrd-models:/models -- ocrd/all:maximum ocrd-tesserocr-segment -I OCR-D-IMG -O OCR-D-SEG           _________/ ___________/ _____________________/ ________________/ ______________________________________________/               (1)          (2)             (3)                        (4)                         (5)```* (1) tells Docker to run the container as the calling user (who should have write access to the CWD) instead of root* (2) tells Docker to bind-mount the current working directory (CWD) under `/data` in the container* (3) tells Docker to mount `/models` in the container (i.e. the location for all models) under the **named volume** `ocrd-models`* (4) tells Docker which image to spawn a container for* (5) is the unchanged call to the processor&gt; **Note**: You can replace the host-side path in (2) with any absolute directory path.&gt; **Note**: Make sure to keep re-using the same named volume for models and other file resources under (3).&gt; For details, see [models and Docker](models#models-and-docker)&gt; **Note**: It can also be useful to have Docker automatically delete the container after termination&gt; by adding the `--rm` option.&gt; **Note**: It can also be useful to have Docker mount `/tmp` in the container to faster memory,&gt; which can be done via `--tmpfs /tmp` (for a RAM disk) or something like `-v /nvram:/tmp`.### Specifying new OCR-D workflowsWhen you want to specify a new workflow adapted to the features of particularimages, we recommend using an existing workflow as specified in the [Workflow Guide](workflows)as starting point. You can adjust it to your needs by exchanging or adding the specified parametersand/or processors. For an overview on the existing processors, their tasks and features, see the [next section](#get-more-information-about-processors) and our [workflow guide](workflows.html).### Get more information about processorsTo get all available processors you might use the autocomplete in your preferred console.&gt; **Note**: If you installed OCR-D via Docker, make sure you run the interactive bash shell&gt; on the ocrd/all Docker image as described in the section [Preparations](#docker-installation-run-container).&gt; If you installed OCR-D natively, activate the virtual environment first as described in the section&gt; [Preparations](#native-installation-activate-virtual-environment).Type `ocrd-` followed by a tab character (for autocompletion proposals) to get a list of all available processors.To get further information about a particular processor, call it with `--help` or `-h`:```shocrd-{processor name} --help## alternatively, using Docker:docker run --rm -u $(id -u) -v $PWD:/data -- ocrd/all:maximum ocrd-{processor name} --help```### Using modelsSeveral processors rely on models, which usually have to be downloaded beforehand.An overview on the existing model repositories and short descriptions on the most important modelscan be found in our [Models Guide](https://ocr-d.de/en/models).We strongly recommend to use the [OCR-D resource manager](https://ocr-d.de/en/models) to download the models,as this makes it easy to both download and use them.",
      "url": " /en/user_guide.html"
    },
  

    {
      "slug": "en-spec-web-api-html",
      "title": "OCR-D Network",
      "content"	 : "# OCR-D Network## 1. Why do we need OCR-D Network?After having processors running locally via the [CLI](https://ocr-d.de/en/spec/cli), communication over network is thenatural extension. The [OCR-D Network]((https://github.com/OCR-D/core/tree/master/ocrd_network/ocrd_network)) package,which is implemented as part of [OCR-D/core](https://github.com/OCR-D/core), allows users to set up OCR-D in adistributed environment. This setup greatly improves the flexibility, scalability and reliability of OCR-D.## 2. Terminology* **Processing Worker**: a Processing Worker is an [OCR-D Processor](https://ocr-d.de/en/spec/glossary#ocr-d-processor)  running as a worker, i.e. listening to the Process Queue, pulling new jobs when available, processing them, and  pushing the updated job statuses back to the queue if necessary.* **Processor Server**: a Processor Server is an [OCR-D Processor](https://ocr-d.de/en/spec/glossary#ocr-d-processor)  running as a server over HTTP. It accepts requests, execute the processor with parameters provided in the requests,  and return responses.* **Workflow Server**: a Workflow Server is a server which exposes REST endpoints in the `Workflow` section of  the [Web API specification](openapi.yml). In particular, with a `POST /workflow/run` request a workflow can be  executed. The Workflow Server comprises a chain of call to the `POST /processor/run/{executable}` endpoint in an  appropriate order.* **Processing Server**: a Processing Server is a server which exposes REST endpoints in the `Processing` section of  the [Web API specification](openapi.yml). In particular, for each `POST /processor/run/{executable}` request,  either a processing message is added to the respective Job Queue or a request is delegated to the respective Processor  Server.* **Process Queue**: a Process Queue is a queuing system for workflow jobs (i.e. single processor runs on one  workspace) to be executed by Processing Workers and to be enqueued by the Workflow Server via the Processing Server.  In our implementation, it&#39;s [RabbitMQ](https://www.rabbitmq.com/).* **Job queue**: one or many queues in the Process Queue, which contains processing messages. Processing Workers listen  to the job queues.* **Result queue**: one or many queues in the Process Queue, which contains result messages. Depending on the  configuration in the processing messages, Processing Workers might publish result messages to these queues. A  3rd-party service can listen to these queues to get updated about the job status.* **Processing message**: a message published to the job queue. This message contains necessary information for the  Processing Worker to process data and perform actions after the processing has finished. These actions include `POST`  ing the result message to the provided callback URL, or publishing the result message to the result queue. The schema  of processing messages can be found [here](web_api/processing-message.schema.yml).* **Result message**: a message send from a Processing Worker when it has finished processing. This message contains  information about a job (ID, status, etc.). Depending on the configuration in the processing message, a result message  can be `POST`ed to the callback URL, published to the result queue, or both. The schema for result messages can be  found [here](web_api/result-message.schema.yml).* **METS Server**: a METS Server makes a workspace accessible over HTTP or Unix file socket. Thanks to this server, all  operations on a METS file can be executed asynchronously.## 3. The SpecificationWhen having OCR-D running over network, it should expose endpoints to allow users&#39; interactions. Those endpoints aredescribed [here](openapi.yml). It follows the [OpenAPI specification](https://swagger.io/specification/). Most endpointsare already included in [OCR-D/core](https://github.com/OCR-D/core). The rest could be implemented by the organizationwhich uses it. There are 4 parts in the specification: discovery, processing, workflow, and workspace.**Discovery**: The service endpoints in this section provide information about the server. They include, but are notlimited to, hardware configuration, installed processors, and information about each processor.**Processing**: Via the service endpoints in this section, one can get information about aspecific [processor](https://ocr-d.de/en/spec/glossary#ocr-d-processor), trigger a processor run, and check the statusof a running processor. By exposing these endpoints, the server can encapsulate the detailed setup of the system andoffer users a single entry to the processors. The implementation of this section is providedby [OCR-D/core](https://github.com/OCR-D/core).**Workflow**: Beyond single processors, one can manageentire [workflows](https://ocr-d.de/en/spec/glossary#ocr-d-workflow), i.e. a series of connected processorinstances. A workflow is a text file that describes the OCR-D workflow using `ocrd process` syntax.**Workspace**: The service endpoints in this section concern data management, which in OCR-D is handledvia [workspaces](https://ocr-d.de/en/spec/glossary#workspace). (A workspace is the combination ofa [METS](https://ocr-d.de/en/spec/mets) file and any number of referenced files already downloaded, i.e. with locationsrelative to the METS file path.) Processing (via single processors or workflows) always refers to existing workspaces,i.e. workspaces residing in the server&#39;s file system.## 4. Suggested OCR-D System ArchitectureThis document presents two possible architecture setup using OCR-D Network and the technical details behind. In bothsetup, all servers are implemented using [FastAPI](https://fastapi.tiangolo.com/). Behind the scene, itruns [Uvicorn](https://www.uvicorn.org/), an [ASGI](https://asgi.readthedocs.io/en/latest/) web server implementationfor Python. [RabbitMQ](https://www.rabbitmq.com/) is used for the Process Queue, and [MongoDB](https://www.mongodb.com/)is the database system. There are many options for a reverse proxy, such as Nginx, Apache, or HAProxy. From our side, werecommend using [Traefik](https://doc.traefik.io/traefik/).### 4.1 Processors as workers        Fig. 1: A distributed architecture with message queue. In this architecture, processors are deployed as workers.  As shown in Fig. 1, each section in the [Web API specification](#3-the-specification) is implemented by differentservers, which are Discovery Server, Processing Server, Workflow Server, and Workspace Server respectively. Althougheach server in the figure is deployed on its own machine, it is completely up to the implementers to decide whichmachines run which servers. However, having each processor run on its own machine reduces the risk of version andresource conflicts. Furthermore, the machine can be customized to best fit the processor&#39;s hardware requirements andthroughput demand. For example, some processors need GPU computation, while others do not, or some need more CPUcapacity while others need more memory.**Processing**: once a request arrives, it will be pushed to a job queue. A job queue always has the same name as itsrespective processors. For example, `ocrd-olena-binarize`processors listen only to the queuenamed `ocrd-olena-binarize`. A Processing Worker, which isan [OCR-D Processor](https://ocr-d.de/en/spec/glossary#ocr-d-processor) running as a worker, listens to the queue, pullsnew jobs when available, processes them, and push the job statuses back to the queue if necessary. One normally does notrun a Processing Worker directly, but via a Processing Server. Job statuses can be pushed back to the queue, dependingon the [job configuration](#63-process-queue), so that other services get updates and act accordingly.**Database**: in this architecture, a database is required to store information such as users requests, jobs statuses,workspaces, etc. [MongoDB](https://www.mongodb.com/) is required here.**Network File System**: in order to avoid file transfer between different machines, it is highly recommended to havea [Network File System (NFS)](https://en.wikipedia.org/wiki/Network_File_System) set up. With NFS, all ProcessingServers(specifically processors) can work in a shared storage environment and access files as if they are local files.To get data into the NFS, one could use the `POST /workspace` endpoint toupload [OCRD-ZIP](https://ocr-d.de/en/spec/ocrd_zip)files. However, this approach is only appropriate for testing orvery limited data sizes. Usually, Workspace Server should be able to pull data from other storage.### 4.2 Processors as servers        Fig. 2: A distributed architecture where processors are deployed as servers.  The difference between this architecture and the one shown in Fig. 1 is the processors. In this architecture, eachprocessor runs as a server and exposes one endpoint. When the Processing Server receives a request, it will forward thatrequest to the respective Processor Server and wait for the response.This architecture is simpler than the other one, since there is no need to have a Process Queue involved. Without aqueue, all communications are synchronous. It means that clients need to wait for responses from Processing Server. Itmight take a long time, therefore high timeout is recommended.## 5. UsageBoth setups above can be used as follows:1. Retrieve information about the system via endpoints in the `Discovery` section.2. Create a workspace (from an [OCRD-ZIP](https://ocr-d.de/en/spec/ocrd_zip) or METS URL) via the `POST /workspace`   endpoint and get back a workspace ID.3. Create a workflow by uploading a workflow script to the system via the `POST /workflow` endpoint and get back a   workflow ID.4. One can either:    * Trigger a single processor on a workspace by calling the `POST /processor/run/{executable}` endpoint with the      chosen processor name, workspace ID and parameters, or    * Start a workflow on a workspace by calling the `POST /workflow/run` endpoint with the chosen workflow ID      and workspace ID.    * In both cases, a job ID is returned.5. With the given job ID, it is possible to check the job status by calling:    * `GET /processor/job/{job-id}` for a single processor, or    * `GET /workflow/job/{job-id}` for the workflow.6. Download the resulting workspace via the `GET /workspace/{workspace-id}` endpoint and get back an OCRD-ZIP.   Set the request header to `Accept: application/json` in case you only want the meta-data of the workspace but not the   files.## 6. Technical Details### 6.1 How does Processing Server process requests?As one can see from two setups above, the Processing Server needs to go through many steps when it receives a request.These steps are illustrated in Fig. 3 below.        Fig. 3: This activity diagram shows how a Processing Server handles a request.  **Job cache**: there are usually dependencies between jobs, i.e. one job can only run after other jobs are finished. Tosupport this, when the Processing Server receives a job at `/processor/run/{processor-name}` endpoint, it first checksif all dependent jobs are finished or not. If not, the new coming job will be cached and then executed later.**Page lock**: to avoid conflict, only one job is allowed to write to a page group at a time. Therefore, before a job isexecuted, its output file group is locked so that other jobs cannot write to it. The group will then be unlocked whenthe job finished. If a job needs to write to a locked file group, it will be cached and executed later.### 6.2 Processing ServerA Processing Server is a server which exposes REST endpoints in the `Processing` section ofthe [Web API specification](openapi.yml). In the queue-based system architecture, a Processing Server is responsible fordeployment management and enqueuing workflow jobs. For the former, a Processing Server can deploy, re-use, and shutdownProcessing Workers, Process Queue, and Database, depending on the configuration. For the latter, it decodes requests anddelegates them to the Process Queue. Additionally, it is possible to start the needed components externally, withDocker. Therefore `skip_deployment: true` can be set in the `process_queue` and `database` section of the configurationfile.To start a Processing Server, run```shell$ ocrd network processing-server --address=: /path/to/config.yml```This command starts a Processing Server on the provided IP address and port. It accepts only one argument, which is thepath to a configuration file. The schema of a configuration file can be found [here](web_api/config.schema.yml). Belowis a small example of how the file might look like.```yamlprocess_queue:  address: localhost  port: 5672  credentials:    username: admin    password: admin  ssh:    username: cloud    path_to_privkey: /path/to/filedatabase:  address: localhost  port: 27017  credentials:    username: admin    password: admin  ssh:    username: cloud    password: &quot;1234&quot;hosts:  - address: localhost    username: cloud    password: &quot;1234&quot;    workers:      - name: ocrd-cis-ocropy-binarize        number_of_instance: 2        deploy_type: native      - name: ocrd-olena-binarize        number_of_instance: 1        deploy_type: docker  - address: 134.76.1.1    username: tdoan    path_to_privkey: /path/to/file    workers:      - name: ocrd-eynollah-segment        number_of_instance: 1        deploy_type: native```There are three main sections in the configuration file.1. `process_queue`: it contains the `address` and `port`, where the Process Queue is deployed, or will be deployed with   the specified `credentials`. If the `ssh` property is presented, the Processing Server will try to connect to   the `address` via `ssh` with provided `username` and `password` and deploy [RabbitMQ](https://www.rabbitmq.com/) at   the specified `port`. The remote machine must have [Docker](https://www.docker.com/) installed since the deployment   is done via Docker. Make sure that the provided `username` has enough rights to run Docker commands. In case   the `ssh` property is not presented, the Processing Server assumes that RabbitMQ was already deployed and just uses   it.2. `database`: this section also contains the `address` and `port`, where the [MongoDB](https://www.mongodb.com/) is   running, or will run. If `credentials` is presented, it will be used when deploying and connecting to the database.   The `ssh` section behaves exactly the same as described in the `process_queue` section above.3. `hosts`: this section contains a list of hosts, usually virtual machines, where Processing Workers should be   deployed. To be able to connect to a host, an `address` and `username` are required, then comes either `password`   or `path_to_privkey` (path to a private key). All Processing Workers, which should be deployed, must be declared   under   the `workers` property. In case `deploy_type` is `docker`, make sure that [Docker](https://www.docker.com/) is   installed in the target machine and the provided `username` has enough rights to execute Docker commands.Among three sections, `process_queue` and `database` are required, `hosts` is optional. Processing Workers canadditionally be start externally and register themselves to the process_queue`. For more information, please check the[configuration file schema](web_api/config.schema.yml).### 6.3 Process QueueBy using a queuing system for individual per-workspace per-job processor runs, specifically as message queuingwith [RabbitMQ](https://www.rabbitmq.com/), the reliability and flexibility of the Processing Server are greatlyimproved over a system directly coupling the workflow engine and distributed processor instances.In our implementation of the Process Queue, manual acknowledgment mode is used. This means, when a Processing Workerfinishes successfully, it sends a positive ACK signal to RabbitMQ. In case of failure, it tries again three times beforesending a negative ACK signal. When a negative signal is received, RabbitMQ will re-queue the message. If there is notany ACK signal sent for any reason (e.g. consumer crash, power outage, network problem, etc.), RabbitMQ willautomatically re-queue the message after timeout, which is 30 minutes by default. This behavior, however, canbe [overridden](https://www.rabbitmq.com/consumers.html#acknowledgement-timeout) by setting another value forthe `consumer_timeout` property in the [`rabbitmq.conf`](https://www.rabbitmq.com/configure.html#config-file) file.To avoid processing the same input twice (in case of re-queuing), a Processing Worker first checksthe [`redeliver`](https://www.rabbitmq.com/confirms.html#automatic-requeueing) property to see if this message wasre-queued. If yes, and the status of this process in the database is not `SUCCESS`, it will process the data describedin the message again.When a Processing Server receives a request, it creates a message based on the request content, then push it to ajob queue. A job queue always has the same name as its respective processors. For example, `ocrd-olena-binarize`processors listen only to the job queue named `ocrd-olena-binarize`. Below is an example of how a message looks like.For a detailed schema, please check the [message schema](web_api/processing-message.schema.yml).```yamljob_id: uuidprocessor_name: ocrd-cis-ocropy-binarizepath_to_mets: /path/to/mets.xmlinput_file_grps:  - OCR-D-DEFAULToutput_file_grps:  - OCR-D-BINpage_id: PHYS_001,PHYS_002parameters:  params_1: 1  params_2: 2result_queue_name: ocrd-cis-ocropy-binarize-resultcallback_url: https://my.domain.com/callbackinternall_callback_url: http://ocrd-processing-server:8000created_time: 1668782988590```In the message content, `job_id`, `processor_name`, `internal_callback_url` and `created_time` are added by theProcessing Server, while the rest comes from the body of the `POST /processor/run/{executable}` request.Instead of `path_to_mets`, one can also use `workspace_id` to specify a workspace. An ID of a workspace can be obtainedfrom the Workspace Server which is not part of OCR-D core.In case `result_queue_name` property is present, the result of the processing will be pushed to the queue with theprovided name. If the queue does not exist yet, it will be created on the fly. This is useful when there is anotherservice waiting for the results of processing. That service can simply listen to that queue and will be immediatelynotified when the results are available. Below is a simple Python script to demonstrate how a service can listen tothe `result_queue_name` and act accordingly.```pythonimport pika, sys, osdef main():    credentials = pika.PlainCredentials(&#39;username&#39;, &#39;password&#39;)    connection = pika.BlockingConnection(pika.ConnectionParameters(host=&#39;my.domain.name&#39;, port=5672, credentials=credentials))    channel = connection.channel()    # Create the result queue, in case it does not exist yet    result_queue_name = &#39;ocrd-cis-ocropy-binarize-result&#39;    channel.queue_declare(queue=result_queue_name)    def callback(ch, method, properties, body):        print(&#39; [x] Received %r&#39; % body)    channel.basic_consume(queue=result_queue_name, on_message_callback=callback, auto_ack=True)    print(&#39; [*] Waiting for job results.)    channel.start_consuming()```It is important that the result queue exists before one starts listening on it, otherwise an error is thrown. The bestway to ensure this is trying to create the result queue in the listener service, as shown in the Python script above. InRabbitMQ, this action is idempotent, which means that the creation only happens if the queue doesn&#39;t exist yet,otherwise nothing will happen. For more information, please check the[RabbitMQ tutorials](https://www.rabbitmq.com/getstarted.html).If the `callback_url` in the processing message is set, a `POST` request will be made to the provided endpoint when theprocessing is finished. The body of the request is the result message described below.The schema for result messages can be found [here](web_api/result-message.schema.yml). This message is sent to thecallback URL or to the result queue, depending on the configuration in the processing message. An example of the messagelooks like this:```yamljob_id: uuidstatus: SUCCESSpath_to_mets: /path/to/mets.xml```With the returned `job_id`, one can retrieve more information by sending a `GET` request tothe `/processor/job/{job_id}` endpoint, or to `/processor/log/{job_id}` to get all logs of thatjob.### 6.4 Processing WorkerA Processing Worker can be started manually, or it can be managed by a Processing Server viaa [configuration file](#62-processing-server). There are the two ways to start a processing worker:```shell# 1. Use processor name$  worker --queue= --database=# 2. Use ocrd CLI bundled with OCR-D/core$ ocrd network processing-worker  --queue= --database=```* `--queue`: a [Rabbit MQ connection string](https://www.rabbitmq.com/uri-spec.html) to a running instance.* `--database`: a [MongoDB connection string](https://www.mongodb.com/docs/manual/reference/connection-string/) to a  running instance.### 6.5 Processor ServerSame as Processing Worker, there are also two ways to start a Processor Server:```shell# 1. Use processor name$  server --address= --database=# 2. Use ocrd CLI bundled with OCR-D/core$ ocrd network processor-server  --queue= --database=```* `--address`: The URL/address to run the processor server on, format: host:port.* `--database`: a [MongoDB connection string](https://www.mongodb.com/docs/manual/reference/connection-string/) to a  running instance.### 6.6 DatabaseA database is required to store necessary information such as users requests, jobs statuses, workspaces,etc. [MongoDB](https://www.mongodb.com/) is used in this case. To connect to MongoDB via a Graphical UserInterface, [MongoDB Compass](https://www.mongodb.com/products/compass) is recommended.",
      "url": " /en/spec/web_api.html"
    },
  

    {
      "slug": "en-workflows-html",
      "title": "OCR-D Workflow Guide",
      "content"	 : "# WorkflowsThere are several steps necessary to get the fulltext of a scanned print. The whole OCR process is shown in the following figure:The following instructions describe all steps of an OCR workflow. Depending on your particular print (or rather images), not all of thosesteps might be necessary to obtain good results. Whether a step is required or optional is indicated in the description of each step.This guide provides an overview of the available OCR-D processors and their required parameters. For more complex workflows and recommendationssee the [OCR-D-Website-Wiki](https://github.com/OCR-D/ocrd-website/wiki). Feel free to add your own experiences and recommendations in the Wiki!We will regularly amend this guide with valuable contributions from the Wiki.**Note:** In order to be able to run the workflows described in this guide, you need to have prepared your images in an [OCR-D-workspace](https://ocr-d.de/en/user_guide#preparing-a-workspace).We expect that you are familiar with the [OCR-D-user guide](https://ocr-d.de/en/user_guide) which explains all preparatory steps, syntax and differentsolutions for executing whole workflows.## Image Optimization (Page Level)At first, the image should be prepared for OCR.### Step 0.1: Image Enhancement (Page Level, optional)Optionally, you can start off your workflow by enhancing your images, which can be vital for the following binarization. In this processing step,the raw image is taken and enhanced by e.g. grayscale conversion, brightness normalization, noise filtering, etc.**Note:** `ocrd-preprocess-image` can be used to run arbitrary shell commands for preprocessing (original or derived) images, and can be seen as a generic OCR-D wrapper for many of the following workflow steps, provided a matching external tool exists. (The only restriction is that the tool must not change image size or the position/coordinates of its content.)#### Available processors            Processor      Parameter      Remark      Call                  ocrd-im6convert      -P output-format image/tiff      for output-options see IM Documentation      ocrd-im6convert -I OCR-D-IMG -O OCR-D-ENH -P output-format image/tiff              ocrd-preprocess-image            -P input_feature_filter binarized      -P output_feature_added binarized      -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            for parameters and command examples (presets) see the Readme          ocrd-preprocess-image -I OCR-D-IMG -O OCR-D-PREP -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                  ocrd-skimage-normalize                  ocrd-skimage-normalize -I OCR-D-IMG -O OCR-D-NORM            ocrd-skimage-denoise-raw                      ocrd-skimage-denoise-raw -I OCR-D-IMG -O OCR-D-DENOISE          ### Step 0.2: Font detectionOptionally, this processor can determine the font family (e.g. Antiqua, Fraktur,Schwabacher) to help select the right models for text detection.`ocrd-typegroups-classifier` annotates font families on pagelevel, including the confidence value (separated by colon). Supported `fontFamily` values:  * `Antiqua`  * `Bastarda`  * `Fraktur`  * `Gotico-Antiqua`  * `Greek`  * `Hebrew`  * `Italic`  * `Rotunda`  * `Schwabacher`  * `Textura`  * `other_font`  * `not_a_font`**Note:** `ocrd-typegroups-classifier` was trained on a very large and diversedataset, with both geometric and color-space random augmentation (contrast,brightness, hue, even compression artifacts and 2 different binarizationmethods), so it works best on the raw, *non-binarized* RGB image.**Note:** `ocrd-typegroups-classifier` comes with a non-OCR-D CLI that allowsfor the generation of &quot;heatmaps&quot; on the page to visualize which regions of the pageare classified as using a certain font with a certain confidence, see the[project&#39;s README for usage instructions](https://github.com/seuretm/ocrd_typegroups_classifier).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-typegroups-classifier      -P network /path/to/densenet121.tgc      Download densenet121.tgc from GitHub      ocrd-typegroups-classifier -I OCR-D-IMG -O OCR-D-IMG-FONTS      ### Step 1: Binarization (Page Level)All the images should be binarized right at the beginning of your workflow.Many of the following processors require binarized images. Some implementations(for deskewing, segmentation or recognition) may produce better results usingthe original image. But these can always retrieve the raw image instead of thebinarized version automatically.In this processing step, a scanned colored /gray scale document image is takenas input and a black and white binarized image is produced. This step shouldseparate the background from the foreground.**Note:** Binarization tools usually provide a threshold parameter which allowsyou to increase or decrease the weight of the foreground. This is optional andcan be especially useful for images which have not been enhanced.                                                    #### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            -P impl wolf -P k 0.10            Fast              ocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN                      ocrd-cis-ocropy-binarize        -P threshold 0.1        Fast        ocrd-cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN                ocrd-sbb-binarize      -P model      Recommended; pre-trained models can be downloaded from here or via the OCR-D resource manager      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname       ocrd-skimage-binarize      -P k 0.10      Slow      ocrd-skimage-binarize -I OCR-D-IMG -O OCR-D-BIN              ocrd-doxa-binarize      -P algorithm ISauvola      Fast      ocrd-doxa-binarize -I OCR-D-IMG -O OCR-D-BIN      ### Step 2: Cropping (Page Level)In this processing step, a document image is taken as input and the pageis cropped to the content area only (i.e. without noise at the margins or facing pages) by marking the coordinates of the page frame.We strongly recommend to execute this step if your images are not cropped already (i.e. only show the page of a book without a ruler,footer, color scale etc.). Otherwise you might run into severe segmentation problems.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-anybaseocr-crop            The input image has to be binarized and should be deskewed for the module to work.      ocrd-anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP              ocrd-tesserocr-crop            Cannot cope well with facing pages (textual noise is detected as text).      ocrd-tesserocr-crop -I OCR-D-BIN -O OCR-D-CROP      ### Step 3: Binarization (Page Level)For better results, the cropped images can be binarized again at this point or later on (on region level).#### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            Recommended      ocrd-olena-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-sbb-binarize      -P model      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)            ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname            ocrd-skimage-binarize                  ocrd-skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-cis-ocropy-binarize                  ocrd-cis-ocropy-binarize -I OCR-D-CROP -O OCR-D-BIN2      ### Step 4: Denoising (Page Level)In this processing step, artifacts like little specks (both in foreground or background) are removed from the binarized image. (Not to be confused with raw denoising in step 0.)This may not be necessary for all prints, and depends heavily on the selected binarization algorithm.            &amp;nbsp;      &amp;nbsp;                                                            #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-denoise      -P noise_maxsize 3.0            ocrd-cis-ocropy-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE              ocrd-skimage-denoise      -P maxsize 3.0      Slow      ocrd-skimage-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE      ### Step 5: Deskewing (Page Level)In this processing step, a document image is taken as input and the skew ofthat page is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.The input images have to be binarized for this module to work.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                 ocrd-cis-ocropy-deskew      -P level-of-operation page      Recommended      ocrd-cis-ocropy-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P level-of-operation page              ocrd-tesserocr-deskew      -P operation_level page      Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P operation_level page      ### Step 6: Dewarping (Page Level)In this processing step, a document image is taken as input and the text lines are straightened or stretchedif they are curved. The input image has to be binarized for the module to work.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks      Call                  ocrd-anybaseocr-dewarp              -P model_path /path/to/latest_net_G.pth            For available models take a look at this site or use the [OCR-D resource manager](https://ocr-d.de/en/models)  Parameter model_path is optional if the model was installed via ocrd resmgr download ocrd-anybaseocr-dewarp &#39;*&#39;  GPU required!              ocrd-anybaseocr-dewarp -I OCR-D-DESKEW-PAGE -O OCR-D-DEWARP-PAGE            ## Layout AnalysisBy now the image should be well prepared for segmentation.### Step 7: Region segmentationIn this processing step, an (optimized) document image is taken as an input and theimage is segmented into the various regions, including columns.Segments are also classified, either coarse (text, separator, image, table, ...) or fine-grained (paragraph, marginalia, heading, ...).**Note:** The `ocrd-tesserocr-segment`, `ocrd-tesserocr-recognize`, `ocrd-eynollah-segment`, `ocrd-sbb-textline-detector` and`ocrd-cis-ocropy-segment` processors do not only segment the page, butalso the text lines within the detected text regions in onestep. Therefore with those (and only with those!) processors you don&#39;t need tosegment into lines in an extra step and can continue with [step 13 - line-level dewarping](#step-13-dewarping-line-level).**Note:** If you use `ocrd-tesserocr-segment-region`, which uses only boundingboxes instead of polygon coordinates, then you should post-process via`ocrd-segment-repair` with `plausibilize=True` to obtain better results withoutlarge overlaps. _Alternatively_, consider using the all-in-one capabilities of`ocrd-tesserocr-segment` and `ocrd-tesserocr-recognize`, which can do regionsegmentation and line segmentation (and optionally also text recognition) inone step by querying Tesseract&#39;s internal iterator (accessing the more precisepolygon outlines instead of just coarse bounding boxes with lots ofhard-to-recover overlap). _Alternatively_, run with `shrink_polygons=True`(accessing that same iterator to calculate convex hull polygons).**Note:** All the `ocrd-tesserocr-segment*` processors internally delegate to`ocrd-tesserocr-recognize`, so you can replace calls to these task-specificprocessors with calls to `ocrd-tesserocr-recognize` with specific parameters:  processor callocrd-tesserocr-recognize parameters            ocrd-tesserocr-segment-region -P overwrite_regions true      ocrd-tesserocr-recognize -P textequiv_level region -P segmentation_level region -P overwrite_segments true              ocrd-tesserocr-segment-table -P overwrite_cells true      ocrd-tesserocr-recognize -P textequiv_level cell -P segmentation_level cell -P overwrite_segments true              ocrd-tesserocr-segment-line -P overwrite_lines true      ocrd-tesserocr-recognize -P textequiv_level line -P segmentation_level line -P overwrite_segments true              ocrd-tesserocr-segment-word -P overwrite_words true      ocrd-tesserocr-recognize -P textequiv_level word -P segmentation_level word -P overwrite_segments true      **Note:** The three parameters `segmentation_level`, `textequiv_level` and`model` define the behavior of `ocrd-tesserocr-recognize`:* `segmentation_level` determines the *highest level* to segment. Use `&quot;none&quot;` to disable segmentation altogether, i.e. only recognize existing segments.* `textequiv_level` determines the *lowest level* to segment. Use `&quot;none&quot;` to segment until the lowest level (`&quot;glyph&quot;`) and disable recognition altogether, only analyse layout.* `model` determines the model to use for text recognition. Use `&quot;&quot;` or do not set at all to disable recognition, i.e. only analyse layout.Examples:* To segment existing regions into lines (and only lines) only: `segmentation_level=&quot;line&quot;`, `textequiv_level=&quot;line&quot;`, `model=&quot;&quot;`* To segment existing regions into lines (and only lines) and recognize text: `segmentation_level=&quot;line&quot;`, `textequiv_level=&quot;line&quot;`, `model=&quot;Fraktur&quot;`For detailed descriptions of behaviour and options, see [tesserocr&#39;s README](https://github.com/OCR-D/ocrd_tesserocr/blob/master/README.md) and`ocrd-tesserocr-recognize/segment/segment-region/segment-table/segment-line/segment-word --help` help.            &amp;nbsp;      &amp;nbsp;                                                          #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-segment      -P find_tables false -P shrink_polygons true      Recommended. Will reuse internal tesseract iterators to produce a complete segmentation with tight polygons instead of bounding boxes where possible      ocrd-tesserocr-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG -P find_tables false -P shrink_polygons true              ocrd-eynollah-segment      -P models      Models can be found here or downloaded with the OCR-D resource manager;       If you didn&#39;t download the model with the resmgr, for model you need to pass the absolute path on your hard drive as parameter value.      ocrd-eynollah-segment -I OCR-D-IMG -O OCR-D-SEG -P models default              ocrd-sbb-textline-detector      -P model modelname      Models can be found here or downloaded with the OCR-D resource manager;       If you didn&#39;t download the model with resmgr, for model you need to pass the local filesystem path as parameter value.      ocrd-sbb-textline-detector -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P model /path/to/model              ocrd-cis-ocropy-segment      -P level-of-operation page          ocrd-cis-ocropy-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P level-of-operation page              ocrd-tesserocr-segment-region      -P find_tables false      Recommended      ocrd-tesserocr-segment-region -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P find_tables false -P shrink_polygons true              ocrd-segment-repair      -P plausibilize true      Only to be used after ocrd-tesserocr-segment-region      ocrd-segment-repair -I OCR-D-SEG-REG -O OCR-D-SEG-REPAIR -P plausibilize true              ocrd-anybaseocr-block-segmentation      -P block_segmentation_model mrcnn_name -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5      For available models take a look at this site ocr download them via OCR-D resource manager;       If you didn&#39;t use resmgr, you need to pass the local filesystem path as parameter value.      ocrd-anybaseocr-block-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P block_segmentation_model mrcnn_name -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5              ocrd-pc-segmentation                  ocrd-pc-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG              ocrd-detectron2-segment            For available models, any model for Detectron2 forks trained on document layout analysis datasets can be integrated; instructions and examples can be found here      ## Image Optimization (Region Level)In the following steps, the text regions should be optimized for OCR.### Step 8:  Binarization (Region Level)In this processing step, a scanned colored /gray scale document image is taken as input and a blackand white binarized image is produced. This step should separate the background from the foreground.The binarization should be at least executed once (on page or region level). If you already binarizedyour image twice on page level, and have no large images, you can probably skip this step.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-skimage-binarize      -P level-of-operation region            ocrd-skimage-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region              ocrd-sbb-binarize      -P model -P operation_level region      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or with the [OCR-D resource manager](https://ocr-d.de/en/models)      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname -P operation_level region          ocrd-preprocess-image              -P level-of-operation region        -P &quot;output_feature_added&quot; binarized        -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            &amp;nbsp;          ocrd-preprocess-image -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                    ocrd-cis-ocropy-binarize      -P level-of-operation region-P &quot;noise_maxsize&quot;: float            ocrd-cis-ocropy-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region      ### Step 9:  Clipping (Region Level)In this processing step, intrusions of neighbouring non-text (e.g. separator) or text segments (e.g. ascenders/descenders) intotext regions of a page (or text lines or a text region) can be removed. A connected component analysis is run on every segment,as well as its overlapping neighbours. Now for each conflicting binary object,a rule based on majority and proper containment determines whether it belongs to the neighbour, and can thereforebe clipped to the background.This basic text-nontext segmentation ensures that for each text region there is a clean image without interference from separators and neighbouring texts. (On the region level, cleaning via coordinates would be impossible in many common cases.) On the line level, this can be seen as an alternative to _resegmentation_.Note: Clipping must be applied **before** any processor that produces derived images for the same hierarchy level (region/line). Annotations on the next higher level (page/region) are fine of course.#### Available processors            Processor      Parameter      Remarks    Call          &gt;      ocrd-cis-ocropy-clip      -P level-of-operation region      &amp;nbsp;      ocrd-cis-ocropy-clip -I OCR-D-DESKEW-REG -O OCR-D-CLIP-REG -P level-of-operation region      ### Step 10:  Deskewing (Region Level)In this processing step, text region images are taken as input and their skew is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-deskew      -P level-of-operation region            ocrd-cis-ocropy-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG -P level-of-operation region              ocrd-tesserocr-deskew            Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG      ### Step 11: Line segmentationIn this processing step, text regions are segmented into text lines.A line detection algorithm is run on every text region of every PAGE in theinput file group, and a TextLine element with the resulting polygonoutline is added to the annotation of the output PAGE.**Note:** If you use `ocrd-cis-ocropy-segment`, you can directly go on with [Step 13](#step-13-dewarping-on-line-level).**Note:** If you use `ocrd-tesserocr-segment-line`, which uses only boundingboxes instead of polygon coordinates, then you should post-process with theprocessors described in [Step 12](#step-12-resegmentation-line-level)._Alternatively_, consider using the all-in-one capabilities of[`ocrd-tesserocr-recognize`](#step-7-region-segmentation), which can do line segmentationand text recognition in one step by querying Tesseract&#39;s internal iterator(accessing the more precise polygon outlines instead of just coarse boundingboxes with lots of hard-to-recover overlap). _Alternatively_, run with`shrink_polygons=True` (accessing that same iterator to calculate convex hullpolygons)**Note:** As described in [Step 7](#step-7-page-segmentation), `ocrd-eynollah-segment`, `ocrd-sbb-textline-detector` and `ocrd-cis-ocropy-segment` do not only segmentthe page, but also the text lines within the detected text regions in one step. Therefore with those (and only with those!) processors you don’tneed to segment into lines in an extra step.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-segment      -P level-of-operation region      &amp;nbsp;      ocrd-cis-ocropy-segment -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE -P level-of-operation region              ocrd-tesserocr-segment-line      &amp;nbsp;      &amp;nbsp;      ocrd-tesserocr-segment-line -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE      ### Step 12: Resegmentation (Line Level)In this processing step the segmented text lines can be corrected in order to reduce their overlap. This can be done either via coordinates (polygonalizing the bounding boxes tightly around the glyphs) – which is what `ocrd-cis-ocropy-resegment` and `ocrd-segment-project` offer – or via derived images (clipping pixels that do not belong to a text line to the background color) – which is what `ocrd-cis-ocropy-clip` (on the `line` level) offers. The former is usually more accurate, but not always possible (for example, when neighbors intersect heavily, creating non-contiguous contours). The latter is only possible if no preceding workflow step has already annotated derived images (`AlternativeImage` references) on the line level (see also [region-level clipping](../Workflow-Guide-clipping)).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-clip      -P level-of-operation line            ocrd-cis-ocropy-clip -I OCR-D-SEG-LINE -O OCR-D-CLIP-LINE -P level-of-operation line              ocrd-cis-ocropy-resegment                  ocrd-cis-ocropy-resegment -I OCR-D-SEG-LINE -O OCR-D-RESEG          ocrd-segment-project      -P level-of-operation line            ocrd-segment-project -I OCR-D-SEG-LINE -O OCR-D-RESEG -P level-of-operation line      ### Step 13: Dewarping (Line Level)In this processing step, the text line images get vertically aligned if they are curved.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-dewarp      &amp;nbsp;      &amp;nbsp;      ocrd-cis-ocropy-dewarp -I OCR-D-CLIP-LINE -O OCR-D-DEWARP-LINE      ## Text Recognition### Step 14: Text recognitionThis processor recognizes text in segmented lines.An overview on the existing model repositories and short descriptions on the most important models can be found [here](https://ocr-d.de/en/models).We strongly recommend to use the [OCR-D resource manager](https://ocr-d.de/en/models) to download the models, as this way you don&#39;t have to specifythe path to each model.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-recognize      -P model GT4HistOCR_50000000.997_191951            Recommended Model can be found herea faster variant is here      TESSDATA_PREFIX=&quot;/test/data/tesseractmodels/&quot; ocrd-tesserocr-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P model Fraktur+Latin              ocrd-calamari-recognize              if you downloaded your model with the [OCR-D resource manager](https://ocr-d.de/en/models), use-P checkpoint_dir modelname        else use -P checkpoint_dir /path/to/models                    RecommendedModel can be found here;        For checkpoint you need to pass the local path on your hard drive as parameter value, and keep the verbatim asterisk (*).            ocrd-calamari-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P checkpoint_dir qurator-gt4histocr-1.0      **Note:** For `ocrd-tesserocr` the environment variable `TESSDATA_PREFIX` hasto be set to point to the directory where the used models are stored unlessthe default directory (normally $VIRTUAL_ENV/share/tessdata) is used.The directory should at least contain the following models:`deu.traineddata`, `eng.traineddata`, `osd.traineddata`.**Note:** Faster models for `tesserocr-recognize` are available fromhttps://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/.A good and currently the fastest model is[Fraktur-fast](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/Fraktur-fast.traineddata).UB Mannheim provides many more [models online](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/)which were trained on different GT data sets, for example from[Austrian Newspapers](https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/ONB/tessdata_fast/).**Note:** If you want to go on with the optional post correction, you should also set the `textequiv_level` to `glyph` or in the case of`ocrd-calamari-recognize` at least `word` (which is already the default for `ocrd-tesserocr-recognize`).### Step 14.1: Font style annotationThis processor can determine the font style (e.g. *italic*, **bold**,underlined) and font family text recognition results.`ocrd-tesserocr-fontshape` can either use existing segmentation orsegment on-demand. It can detect the following font styles:  * `fontSize`  * `fontFamily`  * `bold`  * `italic`  * `underlined`  * `monospace`  * `serif`**Note:** `ocrd-tesserocr-fontshape` needs the old, pre-LSTM models to work atall. You can use the pre-installed `osd` (which is purely rule-based), butthere might be better alternatives for your language and script. You can stillget the old models from Tesseract&#39;s Github repo at the [lastrevision](https://github.com/tesseract-ocr/tessdata/commit/3cf1e2df1fe1d1da29295c9ef0983796c7958b7d)before the [LSTMmodels](https://github.com/tesseract-ocr/tessdata/commit/4592b8d453889181e01982d22328b5846765eaad)replaced them, usually under the same name. (Thus, `deu.traineddata` used to bea rule-based model but now is an LSTM model. `deu-frak.traineddata` is stillonly available as rule-based model and was complemented by the new LSTM models`deu_latf.traineddata` and `script/Fraktur.traineddata`.) If you do need one of themodels that was replaced completely, then you should at least rename the oldone (e.g. to `deu3.traineddata`).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-tesserocr-fontshape      -P model osd -P padding 2      Download other pre-LSTM models from GitHub      ocrd-tesserocr-fontshape -I OCR-D-OCR -O OCR-D-OCR-FONT      ## Post Correction (Optional)### Step 15: Text alignmentIn this processing step, text results from multiple OCR engines (in different annotations sharing the same line segmentation) are alignedinto one annotation.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cor-asv-ann-align            -P method majority                ocrd-cor-asv-ann-align -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-ALIGN              ocrd-cis-align                ocrd-cis-align -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-ALIGN      #### Comparison| | ocrd-cor-asv-ann-align | ocrd-cis-align || --- | --- | --- || **goal** | optimal aligned string (i.e. _as_ post-correction) | candidates for input _for_ ocrd-cis-postcorrect || **input arity** | N fileGrps | N fileGrps (first as &quot;master&quot;) || **input constraints** | textlines must have common IDs | regions and textlines must be in same order || **input level** | textline (+ optionally words or glyphs for confidence) | textline (for strings) and word (for resegmentation) || **output** | PAGE with single-best TextEquiv per textline | PAGE with multiple aligned TextEquivs per textline || **alignment library** | `difflib.SequenceMatcher` | [`de.lmu.cis.ocrd.align`](https://github.com/cisocrgroup/ocrd-postcorrection/tree/master/src/main/java/de/lmu/cis/ocrd/align) || **alignment method** | true n-ary multi-alignment (closest pairs first), including lower level confidences | 1:n alignment with master also restricting allowable word-segmentation || **decision** | majority voting, confidence voting, or combination | no decision |### Step 16: Post-correctionIn this processing step, the recognized text is corrected by statistical error modelling, language modelling, and word modelling (dictionaries, morphology and orthography).**Note:** Most tools benefit strongly from input which includes alternative OCR hypotheses. Currently, models for `ocrd-cor-asv-ann-process`are optimised for input from specific OCR models, whereas `ocrd-cis-postcorrect` expects input from multi-OCR alignment. For more information, see [this presentation](https://vdhd2021.hypotheses.org/176) at vDHd 2021 (held on 23rd May 2021) ([slides](https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z3.pdf) / [video](https://meet.gwdg.de/playback/presentation/2.0/playback.html?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680) in German)**Note:** There is some overlap with [text alignment](https://github.com/OCR-D/ocrd-website/wiki/Workflow-Guide-text-alignment) here, which can also be used (or contribute to) post-correction.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cor-asv-ann-process      -P textequiv_level word -P model_file modelname      Pre-trained models can be found here and here or downloaded via the OCR-D resource manager;      If you didn&#39;t download the model with resmgr, for model_file you need to pass the local filesystem path      as parameter value.     (Relative paths are resolved from the workspace directory or the environment variable CORASVANN_DATA.)     There is no default model_file.      ocrd-cor-asv-ann-process -I OCR-D-OCR -O OCR-D-PROCESS -P textequiv_level word -P model_file /path/to/model/model.h5              ocrd-cis-postcorrect      -P profilerPath /path/to/profiler.bash -P profilerConfig ignored -P nOCR 2 -P model /path/to/model/model.zip              The profilerConfig parameters can be specified in a JSON file. If you do not want to use a profiler, you can set the value for profilerConfig to ignored.      In this case, your profiler.bash should look like this:#!/bin/bashcat &gt; /dev/nullecho &#39;{}&#39;      For model you need to pass the local filesystem path as parameter value.      There is no default model.            ocrd-cis-postcorrect -I OCR-D-ALIGN -O OCR-D-CORRECT -p postcorrect.json      ## Evaluation (Optional)If Ground Truth data is available, the OCR and layout recognition can be evaluated.### Step 17: Layout EvaluationIn this processing step, GT annotation and segmentation results are matchedand evaluated.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-segment-evaluate            -P level-of-operation region      -P only-fg true      -P ignore-subtype true      -P for-categories TextRegion,TableRegion            alpha    ocrd-segment-evaluate -I OCR-D-GT-SEG,OCR-D-SEG -O OCR-D-SEG-EVAL      ### Step 18: OCR EvaluationIn this processing step, the text output of the OCR or post-correction can be evaluated by aligning with ground truth text and measuring the error rates.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-dinglehopper            -P textequiv_level region            For page-wise visual comparison (2 file groups). First input group should point to the ground truth.      ocrd-dinglehopper -I OCR-D-GT,OCR-D-OCR -O OCR-D-EVAL              ocrd-cor-asv-ann-evaluate            -P metric historic-latin      -P gt_level 2            -P confusion 20      -P histogram true            For document-wide aggregation (N file groups). First input group should point to the ground truth.      ocrd-cor-asv-ann-evaluate -I OCR-D-GT,OCR-D-OCR -O OCR-D-EVAL      #### Comparison| | ocrd-dinglehopper | ocrd-cor-asv-ann-evaluate || --- | ----------------| -------------------- || **goal** | CER/WER and visualization | CER/WER (mean+stddev) || **granularity** | only single pages | single-page + aggregated || **input arity** | 2 fileGrps | N fileGrps || **input constraints** | segmentations may deviate | segments must have same IDs || **input level** | region or textline | textline || **output** | HTML + JSON report per page | JSON report per page+all || **alignment** | `rapidfuzz.string_metric.levenshtein_editops` | `difflib.SequenceMatcher` || **Unicode** | `uniseg.graphemeclusters` to get distances on graphemes | calculates alignment on codepoints, but post-processes combining characters || **charset** | NFC + a set of normalizations that (roughly) target OCR-D GT transcription guidelines level 3 to level 2 | NFC or NFKC or a custom normalization (called `historic_latin`) with setting `gt_level` 1/2/3 |## Generic Data Management (Optional)OCR-D produces PAGE XML files which contain the recognized text as well as detailedinformation on the structure of the processed pages, the coordinates of the recognizedelements etc. Optionally, the output can be converted to other formats, or copied verbatim (re-generating PAGE-XML)### Step 19: Adaptation of CoordinatesAll OCR-D processors are required to relate coordinates to the original image for each page, and to keep the original image reference (`Page/@imageFilename`). However, sometimes it may be necessary to deviate from that strict requirement in order to get the overall workflow to function properly.For example, if you have a page-level dewarping step, it is currently impossible to correctly relate to the original image&#39;s coordinates for any segments annotated after that, because there is no descriptive annotation of the underlying coordinate transform in PAGE-XML. Therefore, it is better to _replace the original image_ of the output PAGE-XML by the dewarped image before proceeding with the workflow. (If the dewarped image has also been cropped or deskewed, then of course all existing coordinates are re-calculated accordingly as well.)Another use case is exporting PAGE-XML for tools that cannot apply cropping or deskewing, like [LAREX](https://github.com/OCR4all/LAREX) or Transkribus.Conversely, you might want to align two PAGE-XML files for the same page that have different original image references, projecting all segments below the page level from the one to the other (transforming all coordinates according to the page-level annotation, or keeping them unchanged).#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-segment-replace-original      &amp;nbsp;      &amp;nbsp;    ocrd-segment-replace-original -I OCR-D-CROP-DESK -O OCR-D-CROP-DESK-SUBST              ocrd-segment-replace-page      &amp;nbsp;      &amp;nbsp;    ocrd-segment-replace-page -I OCR-D-CROP-DESK,OCR-D-CROP-DESK-SUBST-SEG -O OCR-D-CROP-DESK-SEG -P transform_coordinates true      ### Step 20: Format ConversionIn this processing step the produced PAGE XML files can be converted to ALTO,PDF, hOCR or text files. Note that ALTO and hOCR can also be converted intodifferent formats whereas the PDF version of PAGE XML OCR results is a widelyaccessible format that can be used as-is by expert and layman alike.#### Available processors          Processor      Parameter      Remarks      Call              ocrd-fileformat-transform      -P from-to &quot;alto2.0 alto3.0&quot;      # or &quot;alto2.0 alto3.1&quot;      # or &quot;alto2.0 hocr&quot;      # or &quot;alto2.1 alto3.0&quot;      # or &quot;alto2.1 alto3.1&quot;      # or &quot;alto2.1 hocr&quot;      # or &quot;alto page&quot;      # or &quot;alto text&quot;      # or &quot;gcv hocr&quot;      # or &quot;hocr alto2.0&quot;      # or &quot;hocr alto2.1&quot;      # or &quot;hocr text&quot;      # or &quot;page alto&quot;      # or &quot;page hocr&quot;      # or &quot;page text&quot;                  As the value consists of two words, when using -P form it has to be enclosed in quotation marks.      If you want to save all OCR results in one file, you can use the following command: cat OCR* &gt; full.txt          ocrd-fileformat-transform -I OCR-D-OCR -O OCR-D-ALTO             mets-mods2tei      --ocr -T FULLTEXT -I OCR-D-IMG      Not a processor CLI, processes the workspace METS, generating a single TEI (DTABf-formatted) for the whole document. Only takes ALTO input, so usually needs a prior ocrd-fileformat-transform -I OCR-D-OCR -O FULLTEXT -P from-to &quot;page alto&quot;.          mm2tei --ocr -T FULLTEXT -I OCR-D-IMG -O TEI.xml             ocrd-page2tei            generates a single TEI (using page2tei XSLT with Saxon) for the whole document.          ocrd-page2tei -I OCR-D-OCR -O OCR-D-TEI              ocrd-pagetopdf      {  # font file name to use for rendering text  &quot;font&quot;: &quot;AletheiaSans.ttf&quot;,  # fix (invalid) negative coordinates  &quot;negative2zero&quot;: true,  # concatenate to multi-page PDF (empty for none)  &quot;multipage&quot;: &quot;name_of_pdf&quot;,  # multi-page PDF page labels  &quot;pagelabel&quot;: &quot;pageId&quot;,  # render text on this hierarchy level  &quot;textequiv_level&quot;: &quot;word&quot;,  # draw polygon outlines in the PDF (empty for none)  &quot;outlines&quot;: &quot;line&quot;}                  ocrd-pagetopdf -I OCR-D-OCR -O OCR-D-PDF -P textequiv_level word              ocrd-segment-extract-pages      -P mimetype image/png -P transparency true      Get page images (cropped and deskewed as annotated; raw and binarized) and mask images (color-coded for regions) along with JSON files for region annotations (custom and COCO format).      ocrd-segment-extract-pages -I OCR-D-SEG-REGION -O OCR-D-IMG-PAGE,OCR-D-IMG-PAGE-BIN,OCR-D-IMG-PAGE-MASK              ocrd-segment-extract-regions      -P mimetype image/png -P transparency true      Get region images (cropped, masked and deskewed as annotated) along with JSON files for region annotations (custom format).      ocrd-segment-extract-regions -I OCR-D-SEG-REGION -O OCR-D-IMG-REGION              ocrd-segment-extract-lines      -P mimetype image/png -P transparency true      Get text line images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for line annotations (custom format).      ocrd-segment-extract-lines -I OCR-D-SEG-LINE -O OCR-D-IMG-LINE              ocrd-segment-extract-words      -P mimetype image/png -P transparency true      Get word images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for word annotations (custom format).      ocrd-segment-extract-words -I OCR-D-SEG-WORD -O OCR-D-IMG-WORD              ocrd-segment-extract-glyphs      -P mimetype image/png -P transparency true      Get glyph images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for glyph annotations (custom format).      ocrd-segment-extract-glyphs -I OCR-D-SEG-GLYPH -O OCR-D-IMG-GLYPH              ocrd-segment-from-masks      -P colordict &#39;{  &quot;#969696&quot;: &quot;TableRegion&quot;,   &quot;#00FF00&quot;: &quot;TextRegion:page-number&quot;,   &quot;#FFFF00&quot;: &quot;TextRegion:heading&quot;,   &quot;#00FFFF&quot;: &quot;GraphicRegion:logo&quot;,   &quot;#0000FF&quot;: &quot;TextRegion:subject&quot;,   &quot;#FF0000&quot;: &quot;TextRegion:catch-word&quot;,   &quot;#FF00FF&quot;: &quot;TextRegion:footnote&quot;,   &quot;#646464&quot;: &quot;TextRegion:paragraph&quot; }&#39;      Import mask images as region segmentation. If colordict is empty, defaults to PageViewer color scheme (also written by ocrd-segment-extract-pages).      ocrd-segment-from-masks -I OCR-D-SEG-PAGE,OCR-D-IMG-PAGE-MASK -O OCR-D-SEG-REGION              ocrd-segment-from-coco            Import COCO format region segmentation (also written by ocrd-segment-extract-pages).      ocrd-segment-from-coco -I OCR-D-SEG-PAGE,OCR-D-SEG-COCO -O OCR-D-SEG-REGION    ### Step 20.1: Generic transformationsSometimes PAGE-XML annotations need to be processed specially to make a workflow&#39;s processors interoperate properly. For example, a text producing processor might forget to make `TextEquiv` consistent between hierarchy levels, or it might be necessary to remove specific region types. Also, repairing minor syntactic or semantic deficiencies is usually required for export or visualization, like removing empty `ReadingOrder` and dead `@regionRef`s, ensuring each `TextEquiv` has a `Unicode`, or fixing negative or floating-point coordinates. While it is always possible to do that ad-hoc via scripts, it might help formulate this as a proper workflow step via processor CLI.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-page-transform      -P xsl page-remove-regions.xsl -P xslt-params &quot;-s type=ImageRegion&quot;      Many useful XSLTs come as preinstalled resources, but can be passed any XSL file. Specify mimetype if the output is not PAGE-XML anymore    ocrd-page-transform      ### Step 21: ArchivingAfter you have successfully processed your images, the results should be saved and archived. OLA-HD isa longterm archive system which works as a mixture between an archive system and a repository. For furtherdetails on OLA-HD see the extensive [concept paper](https://github.com/subugoe/OLA-HD-IMPL/blob/master/docs/OLA-HD_Konzept.pdf).You can also check out the [prototype](http://141.5.98.232/) to make sure, OLA-HD meets your needs and requirements.To use the prototype, specify http://141.5.98.232/api as the endpoint parameter in your call.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-olahd-client      {  &quot;endpoint&quot;: &quot;URL of your OLA-HD instance&quot;,  &quot;username&quot;: &quot;X&quot;,  &quot;password&quot;: &quot;*&quot;}      the parameters should be written to a json file:    echo &#39;{  &quot;endpoint&quot;: &quot;URL of your OLA-HD instance&quot;,  &quot;username&quot;: &quot;X&quot;,  &quot;password&quot;: &quot;*&quot;}&#39; &gt; olahd.json        ocrd-olahd-client -I OCR-D-OCR -p olahd.json      ### Step 22: Dummy ProcessingSometimes it can be useful to have a dummy processor, which takes the files in an Input fileGrp andcopies them the a new Output fileGrp, re-generating the PAGE XML from the current namespace schema/model.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-dummy      &amp;nbsp;      &amp;nbsp;    ocrd-dummy -I OCR-D-FILEGRP -O OCR-D-DUMMY      # RecommendationsIn order to facilitate the usage of OCR-D and the configuration of workflows, we provide two workflowswhich can be used as a start for your OCR-D-tests. They were determined by testing the processors listedabove on selected pages of some prints from the 17th and 18th century.The results vary quite a lot from page to page. In most cases, segmentation is a problem.Note that for our test pages, not all steps described above werde needed to obtain the best results.Depending on your particular images, you might want to include those processors again for better results.We are currently working on regression tests with the help of which we will be able to provide more profoundworkflows soon, which will replace those interim solutions. ## Minimal workflowSince `ocrd-tesserocr-recognize` can do binarization (Otsu), regionsegmentation, table recognition, line segmentation and text recognition at once, just like theupstream `tesseract` command line tool, it&#39;s a good single-step workflow to geta baseline result to compare to granular workflows.**Note:** Be aware that you will most likely obtain significantly betterresults by configuring a more granular workflow like e.g. the[workflows](#best-results-for-selected-pages)[below](#good-results-for-slower-processors).            Step      Processor      Parameter                  1      ocrd-tesserocr-recognize      -P segmentation_level region -P textequiv_level word -P find_tables true -P model Fraktur_GT4HistOCR      ### Example with ocrd-process```shocrd process &quot;tesserocr-recognize -P segmentation_level region -P textequiv_level word -P find_tables true -P model GT4HistOCR_50000000.997_191951&quot;```",
      "url": " /en/workflows.html"
    },
  

    {
      "slug": "en-workflows-src-html",
      "title": "OCR-D Workflow Guide",
      "content"	 : "# WorkflowsThere are several steps necessary to get the fulltext of a scanned print. The whole OCR process is shown in the following figure:The following instructions describe all steps of an OCR workflow. Depending on your particular print (or rather images), not all of thosesteps might be necessary to obtain good results. Whether a step is required or optional is indicated in the description of each step.This guide provides an overview of the available OCR-D processors and their required parameters. For more complex workflows and recommendationssee the [OCR-D-Website-Wiki](https://github.com/OCR-D/ocrd-website/wiki). Feel free to add your own experiences and recommendations in the Wiki!We will regularly amend this guide with valuable contributions from the Wiki.**Note:** In order to be able to run the workflows described in this guide, you need to have prepared your images in an [OCR-D-workspace](https://ocr-d.de/en/user_guide#preparing-a-workspace).We expect that you are familiar with the [OCR-D-user guide](https://ocr-d.de/en/user_guide) which explains all preparatory steps, syntax and differentsolutions for executing whole workflows.## Image Optimization (Page Level)At first, the image should be prepared for OCR.### Step 0.1: Image Enhancement (Page Level, optional)### Step 0.2: Font detection### Step 1: Binarization (Page Level)### Step 2: Cropping (Page Level)### Step 3: Binarization (Page Level)For better results, the cropped images can be binarized again at this point or later on (on region level).#### Available processors            Processor      Parameter      Remark      Call                ocrd-olena-binarize            Recommended      ocrd-olena-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-sbb-binarize      -P model      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)            ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname            ocrd-skimage-binarize                  ocrd-skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2            ocrd-cis-ocropy-binarize                  ocrd-cis-ocropy-binarize -I OCR-D-CROP -O OCR-D-BIN2      ### Step 4: Denoising (Page Level)### Step 5: Deskewing (Page Level)### Step 6: Dewarping (Page Level)## Layout AnalysisBy now the image should be well prepared for segmentation.### Step 7: Region segmentation## Image Optimization (Region Level)In the following steps, the text regions should be optimized for OCR.### Step 8:  Binarization (Region Level)In this processing step, a scanned colored /gray scale document image is taken as input and a blackand white binarized image is produced. This step should separate the background from the foreground.The binarization should be at least executed once (on page or region level). If you already binarizedyour image twice on page level, and have no large images, you can probably skip this step.#### Available processors            Processor      Parameter      Remarks    Call                  ocrd-skimage-binarize      -P level-of-operation region            ocrd-skimage-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region              ocrd-sbb-binarize      -P model -P operation_level region      pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or with the [OCR-D resource manager](https://ocr-d.de/en/models)      ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname -P operation_level region          ocrd-preprocess-image              -P level-of-operation region        -P &quot;output_feature_added&quot; binarized        -P command &quot;scribo-cli sauvola-ms-split &#39;@INFILE&#39; &#39;@OUTFILE&#39; --enable-negate-output&quot;            &amp;nbsp;          ocrd-preprocess-image -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region -P output_feature_added binarized -P command &quot;scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output&quot;                    ocrd-cis-ocropy-binarize      -P level-of-operation region-P &quot;noise_maxsize&quot;: float            ocrd-cis-ocropy-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region      ### Step 9:  Clipping (Region Level)### Step 10:  Deskewing (Region Level)In this processing step, text region images are taken as input and their skew is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-deskew      -P level-of-operation region            ocrd-cis-ocropy-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG -P level-of-operation region              ocrd-tesserocr-deskew            Fast, also performs a decent orientation correction      ocrd-tesserocr-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG      ### Step 11: Line segmentation### Step 12: Resegmentation (Line Level)### Step 13: Dewarping (Line Level)In this processing step, the text line images get vertically aligned if they are curved.            &amp;nbsp;      &amp;nbsp;                                                      #### Available processors            Processor      Parameter      Remarks    Call                  ocrd-cis-ocropy-dewarp      &amp;nbsp;      &amp;nbsp;      ocrd-cis-ocropy-dewarp -I OCR-D-CLIP-LINE -O OCR-D-DEWARP-LINE      ## Text Recognition### Step 14: Text recognition### Step 14.1: Font style annotation## Post Correction (Optional)### Step 15: Text alignment### Step 16: Post-correction## Evaluation (Optional)If Ground Truth data is available, the OCR and layout recognition can be evaluated.### Step 17: Layout Evaluation### Step 18: OCR Evaluation## Generic Data Management (Optional)OCR-D produces PAGE XML files which contain the recognized text as well as detailedinformation on the structure of the processed pages, the coordinates of the recognizedelements etc. Optionally, the output can be converted to other formats, or copied verbatim (re-generating PAGE-XML)### Step 19: Adaptation of Coordinates### Step 20: Format Conversion### Step 20.1: Generic transformations### Step 21: Archiving### Step 22: Dummy Processing# Recommendations",
      "url": " /en/workflows.src.html"
    },
  

    {
      "slug": "assets-main-css",
      "title": "",
      "content"	 : "@import &quot;minima&quot;;",
      "url": " /assets/main.css"
    },
  

    {
      "slug": "feed-xml",
      "title": "",
      "content"	 : "{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != &quot;posts&quot; %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: &quot; | &quot; | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: &quot; | &quot; | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: &quot;categories&quot;, page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: &quot;post&quot;, &quot;post.draft != true&quot; %}{% endunless %}{% assign posts = posts | sort: &quot;date&quot; | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: &quot;&quot; | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains &quot;://&quot; %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}",
      "url": " /feed.xml"
    },
  

    {
      "slug": "sitemap-xml",
      "title": "",
      "content"	 : "{% if page.xsl %}{% endif %}{% assign collections = site.collections | where_exp:&#39;collection&#39;,&#39;collection.output != false&#39; %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:&#39;doc&#39;,&#39;doc.sitemap != false&#39; %}{% for doc in docs %}{{ doc.url | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }}{% endif %}{% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:&#39;doc&#39;,&#39;doc.sitemap != false&#39; | where_exp:&#39;doc&#39;,&#39;doc.url != &quot;/404.html&quot;&#39; %}{% for page in pages %}{{ page.url | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }}{% endif %}{% endfor %}{% assign static_files = page.static_files | where_exp:&#39;page&#39;,&#39;page.sitemap != false&#39; | where_exp:&#39;page&#39;,&#39;page.name != &quot;404.html&quot;&#39; %}{% for file in static_files %}{{ file.path | replace:&#39;/index.html&#39;,&#39;/&#39; | absolute_url | xml_escape }}{{ file.modified_time | date_to_xmlschema }}{% endfor %}",
      "url": " /sitemap.xml"
    },
  

    {
      "slug": "robots-txt",
      "title": "",
      "content"	 : "Sitemap: {{ &quot;sitemap.xml&quot; | absolute_url }}",
      "url": " /robots.txt"
    },
  
  
  {
      "slug": "en-2021-08-06-kick-off-phase3-html",
      "title": "OCR-D Phase III started",
      "content"	 : "On 30 July, our kick-off workshop took place, heralding phase III of OCR-D.The day before, the project participants met internally to get to know each other and coordinate their work. On the public workshop day, the team of the Coordination Project gave an introduction into the objectives in phase III and public communication channels of OCR-D, the current status and plans of the OCR-D software, the Web API and the handling of Ground Truth Data in OCR-D. Also, the Coordination Project gave an insight into Best Practices of Software Developing in the past phase of OCR-D, as well as ideas for the community, how to contribute.In addition, the implementation and module projects presented themselves in short presentations to the interested community and our cooperation partnersUB Braunschweig, SLUB Dresden UB Mannheim are extending both OCR-D and Kitodo for productive mass digitisation; SUB Göttingen and GWDG are working on Performance Optimisation and Integration, deploying OCR-D on a High Performance Cluster; GEI Braunschweig, HCI and ZPD of the University of Würzburg will implement OCR-D features in OCR4all, making OCR-D available via their software; the ULB Sachsen-Anhalt will implement OCR-D in their Open Source mass digitization infrastructure .While these project partners will work on four implementation scenarios, we have three module projects, improving OCR-D processors: UB Mannheim enabling work-specific training with Tesseract and Calamari; JGU Mainz and FAU Erlangen-Nürnberg improving font group recognition for better fitting OCR-models; and OLA-HD by SUB Göttingen and GWDG, optimising reliability, searchability and fine-grained referencing of the OLA-HD long-term archiving repository.In our chat channel, the gitter lobby, we always keep you informed about public OCR-D events. Further information about how to stay in touch and contribute to OCR-D can be found in our overview of platforms.",
      "url": " /en/2021/08/06/kick-off-phase3.html"
    }
    ,
  
  {
      "slug": "de-2021-08-06-kick-off-phase3-html",
      "title": "OCR-D Phase III gestartet",
      "content"	 : "Am 30. Juli fand unser Kick-off-Workshop statt, der die Phase III von OCR-D einläutete.Das Team gab eine Einführung in die Ziele und öffentlichen Kommunikationskanäle von OCR-D in Phase III, in Status und Pläne der OCR-Software und der Web-API und in den Umgang mit Ground Truth Daten in OCR-D. Zudem gab das Koordinierungsprojekt einen Einblick in die bisherige Praxis der Softwareentwicklung in OCR-D mit Möglichkeiten, mitzuwirken.Darüber hinaus stellten sich die Implementierungs- und Modulprojekte der interessierten Community und unseren Kooperationspartnern in kurzen Vorträgen vor.Die UB Braunschweig, die SLUB Dresden und die UB Mannheim erweitern OCR-D und Kitodo für die produktive Massendigitalisierung; die SUB Göttingen und die GWDG arbeiten an der Performance-Optimierung und Integration, indem sie OCR-D auf einem Hochleistungscluster einsetzen; das GEI Braunschweig, das HCI und das ZPD der Universität Würzburg werden OCR-D-Funktionen in OCR4all implementieren und damit OCR-D über ihre Software verfügbar machen; die ULB Sachsen-Anhalt wird OCR-D in ihre Open-Source-Massendigitalisierungsinfrastruktur implementieren.Während diese Projektpartner an vier Implementierungsszenarien arbeiten werden, haben wir drei Modulprojekte, die OCR-D-Prozessoren verbessern: die UB Mannheim, die ein werkspezifisches Training mit Tesseract und Calamari ermöglicht; JGU Mainz und FAU Erlangen-Nürnberg, die die Erkennung von Schriftgruppen für besser passende OCR-Modelle vorantreiben; und das Projekt der SUB Göttingen und der GWDG, das die Zuverlässigkeit, Durchsuchbarkeit und feinkörnige Referenzierung des Langzeitarchivs OLA-HD optimiert.In unserem Chat-Kanal, der Gitter-Lobby, halten wir Sie stets über öffentliche OCR-D-Veranstaltungen auf dem Laufenden. Weitere Informationen darüber, wie Sie mit OCR-D in Kontakt treten und zu OCR-D beitragen können, finden Sie in unserer Seite über Plattformen.",
      "url": " /de/2021/08/06/kick-off-phase3.html"
    }
    ,
  
  {
      "slug": "en-2021-06-11-bibtag-html",
      "title": "OCR-D at the Bibliothekartag 2021",
      "content"	 : "OCR-D will also be present at this year’s Bibliothekartag, which will take place virtually from 16-18 June 2021 and on twodays in Bremen. The OCR-D project is participating with two presentations on the current status of the funding initiativeand on the collaborative creation of training materials.Elisabeth Engl describes the current status ofthe OCR-D software and gives an outlook on the many planned application scenarios, which are at the centre of the third project phase. Kay-Michael Würzner and Robert Sachunskyreport on their experiences with setting up and conducting a collaborative transcription initiative at SLUB Dresden to create OCR training material and models by means of OCR-D and low-threshold annotation tools.",
      "url": " /en/2021/06/11/bibtag.html"
    }
    ,
  
  {
      "slug": "de-2021-06-11-bibtag-html",
      "title": "OCR-D auf dem Bibliothekartag 2021",
      "content"	 : "OCR-D ist auch auf dem diesjährigen Bibliothekartag vertreten, der vom 16.-18. Juni 2021 virtuell sowie an zwei Tagen auch vor Ort in Bremen stattfindet. Das OCR-D-Projekt beteiligt sich mit zwei Vorträgen zum aktuellen Stand in der Förderinitiative sowie zur kollaborativenErstellung von Trainingsmaterialien.Elisabeth Engl beschreibt den derzeitigen Stand der OCR-D-Softwareund gibt einen Ausblick auf die vielfältigen geplanten Anwendungsszenarien in bestandshaltenden und -verarbeitenden Einrichtungen,die im Zentrum der dritten Projektphase stehen. Kay-Michael Würzner und Robert Sachunskyberichten von ihren Erfahrungen bei Aufbau und Begleitung einer kollaborativen Transkriptionsinitiative an der SLUB Dresden zur Erstellung von OCR-Trainingsmaterial und -Modellen mithilfe von OCR-D und niedrigschwelliger Annotationswerkzeuge.",
      "url": " /de/2021/06/11/bibtag.html"
    }
    ,
  
  {
      "slug": "en-2021-06-10-projects-html",
      "title": "Implementation and module projects granted",
      "content"	 : "In addition to the coordination project, the DFG also approved seven implementation and module projects that will begin their work in the coming months.Of the eleven proposals submitted, which were developed in the course of extensive piloting of the OCR-D software in the summer of 2020, four implementation and three module projects will be funded by the DFG.The module projects will further develop selected OCR-D tools.  Workflow for work-specific training based on generic models with OCR-D as well as ground truth enhancement (UB Mannheim).  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen)  OLA-HD Service - A Generic Service for Long-Term Archiving of Historical Prints (SUB Göttingen, GWDG)Together with the ca. 65 other OCR-D tools,  these form the basis for the work of the four implementation projects, which will prepare the existing OCR-D software for its productive use in mass digitisation.  Integration of Kitodo and OCR-D for productive mass digitisation (UB Braunschweig, SLUB Dresden, UB Mannheim).  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR-D Software in Modular Mass Digitisation Workflows (ULB Halle)  OCR4all libraries full text recognition of historical collections (GEI Braunschweig, HCI and ZPD of the University of Würzburg)We are very much looking forward to this next project phase and the (further) cooperation with the implementation and module projects, which will prepare the use of the OCR-D software in different usage scenarios.",
      "url": " /en/2021/06/10/projects.html"
    }
    ,
  
  {
      "slug": "de-2021-06-10-projects-html",
      "title": "Implementierungs- und Modulprojekte bewilligt",
      "content"	 : "Neben dem Antrag des Koordinierungsprojekts wurden von der DFG sieben Implementierungs- und Modulprojekte bewilligt, die in den kommenden Monaten ihre Arbeit aufnehmen werden.Von den elf eingereichten Anträgen, die im Zuge einer umfangreichen Pilotierung der OCR-D-Software im Sommer 2020 erarbeitet wurden, werden vier Implementierungs- und drei Modulprojekte von der DFG gefördert.Die Modulprojekte werden ausgewählte OCR-D-Werkzeuge weiterentwickeln.  Workflow für werkspezifisches Training auf Basis generischer Modelle mit OCR-D sowie Ground Truth Aufwertung (UB Mannheim)  Font Group Recognition for Improved OCR (JGU Mainz, FAU Erlangen)  OLA-HD Service - Ein generischer Dienst für die Langzeitarchivierung historischer Drucke (SUB Göttingen, GWDG)Zusammen mit den übrigen derzeit ca. 65 OCR-D-Werkzeugen bilden diese die Basis für dieArbeiten der vier Implementierungsprojekte, die die OCR-D-Software fürden produktiven Einsatz in der Massendigitalisierung vorbereiten werden.  Integration von Kitodo und OCR-D zur produktiven Massendigitalisierung (UB Braunschweig, SLUB Dresden, UB Mannheim)  OPERANDI: OCR-D Performance Optimisation and Integration (SUB Göttingen, GWDG)  OCR-D Software in modularen Massendigitalisierungs Workflows (ULB Halle)  OCR4all libraries Volltexterkennung historischer Sammlungen (GEI Braunschweig, HCI und ZPD der Universität Würzburg)Wir freuen uns sehr auf diese nächste Projektphase und die (weitere) Zusammenarbeit mit den Implementierungs- und Modulprojekten, die den Einsatz der OCR-D-Software in verschiedenen Nutzungsszenarienvorbereiten werden.",
      "url": " /de/2021/06/10/projects.html"
    }
    ,
  
  {
      "slug": "en-2021-04-26-barcamp-html",
      "title": "OCR(-D) &amp; Co starting in May",
      "content"	 : "On 7 May 2021 will be the inaugural session of our new barcamp-like monthly event OCR(-D) &amp;amp; Co. barcamp format, developers, users and all other interested persons will be will be given the opportunity to talkabout OCR(-D).OCR(-D) &amp;amp; Co will take place every first Friday of the month from May 7 onwards at 10-11 am CET in a BBB room.At the beginning of each meeting, participants have the opportunity to suggest topics of interest, which can thenbe discussed by small groups in breakout rooms. In addition to the open TechCall,where the OCR-D community discusses technical topics every second Wednesday, OCR(-D) &amp;amp; Co also offers participants without in-depthOCR-D knowledge the opportunity to contribute their own questions and ideas to the discussion and to openly exchange ideaswith other OCR-interested people. We look forward to your participation!",
      "url": " /en/2021/04/26/barcamp.html"
    }
    ,
  
  {
      "slug": "de-2021-04-26-barcamp-html",
      "title": "OCR(-D) &amp; Co startet im Mai",
      "content"	 : "Am 7. Mai 2021 startet OCR(-D) &amp;amp; Co, zu dem OCR-D alle Interessierten einlädt. Bei dem offenen Treffen im Barcamp-Format können sich Entwickler, Nutzer und alle weiteren Interessenten niedrigschwellig überOCR(-D) austauschen.OCR(-D) &amp;amp; Co wird ab Mai jeden ersten Freitag im Monat 10-11 Uhr in einem BBB-Raum stattfinden.Zu Beginn jedes Treffens haben die TeilnehmerInnen die Möglichkeit, für sie interessante Themen vorzuschlagen, die dannin Kleingruppen diskutiert und besprochen werden können. In Ergänzung zum offenen TechCall,bei dem sich die OCR-D-Community jeden zweiten Mittwoch über technische Themen austauscht, bietet OCR(-D) &amp;amp; Co auch TeilnehmerInnenohne tiefergehende OCR-D-Kenntnisse die Möglichkeit, ihre eigenen Fragen und Ideen in die Diskussion einzubringen und sichoffen mit anderen OCR-Interessierten auszutauschen. Wir freuen uns auf Ihre Teilnahme!",
      "url": " /de/2021/04/26/barcamp.html"
    }
    ,
  
  {
      "slug": "en-2021-01-19-phase3-html",
      "title": "Phase III of the OCR-D-coordination project granted",
      "content"	 : "The coordination project’s application for the third phase of the OCR-D funding initiative was approved by the DFG in January 2021.In phase III, we will optimise the results of the previous module project phase and we will initiate the productive use of theOCR-D software in mass digitisation both technically and organisationally.The previous project partners from BBAW, HAB and SBB will now be joined by SUB Göttingen and GWDG, whereas the KIT has left the project. Together, the partners can continue to support and coordinate the work of the other OCR-D projects in Phase III.In addition, the OCR-D software will be optimised for its use in mass digitisation and the functioning of theoverall OCR-D workflow will be ensured. Great importance will be put on ensuring the permanent support andfurther development of the OCR-D software and on communicating the results of the implementation work to abroad circle of users who will use it for the efficient full-text digitisation of VD materials.The coordination project is delighted to continue the work of the two previous projectphases and looks forward to working with the other OCR-D projects.",
      "url": " /en/2021/01/19/phase3.html"
    }
    ,
  
  {
      "slug": "de-2021-01-19-phase3-html",
      "title": "Phase III der OCR-D-Koordinierung bewilligt",
      "content"	 : "Der Antrag des Koordinierungsprojekts für die dritte Phase der OCR-D-Förderinitiative wurde im Januar 2021 von der DFG bewilligt.In Phase III werden die Ergebnisse der vorangegangenen Modulprojekt-Phase optimiert und der produktive Einsatz derOCR-D-Software in der Massendigitalisierung technisch und organisatorisch eingeleitet.Die bisherigen Projektpartner aus BBAW, HAB und SBB werden nach dem Ausscheiden des KIT durch SUB Göttingen und GWDG verstärkt. Gemeinschaftlich können die Partner auch in Phase III die Arbeiten der weiteren OCR-D-Projekte unterstützen undkoordinierend begleiten. Zudem soll die OCR-D-Software für ihren Einsatz in der Massendigitalisierung optimiertund die Funktionsweise des OCR-D-Gesamtworkflows sichergestellt werden. Großer Wert wird darauf gelegt werden, die dauerhafte Betreuung und Weiterentwicklung der OCR-D-Software sicherzustellen und die Ergebnisse derImplementierungsarbeiten an einen breiten Kreis an NutzerInnen zu vermitteln, die diese zur effizientenVolltextdigitalisierung der VD-Materialien einsetzen.Das Koordinierungsprojekt freut sich auf die gemeinsame Fortsetzung der Arbeiten aus den beiden vorangegangenenProjektphasen mit den weiteren OCR-D-Projekten.",
      "url": " /de/2021/01/19/phase3.html"
    }
    ,
  
  {
      "slug": "en-2020-10-02-elag-html",
      "title": "OCR-D at the Mini-ELAG",
      "content"	 : "On October 20, 2020 the Mini-ELAG (European Library Automation Group)takes place, where librarians and IT professionals discuss new information technologiesand their application in libraries and documentation centers. OCR-D will be represented at virtual conference with a lecture by Clemens Neudecker (SBB) onOCR-D: An open ecosystem for improving OCR on historical documents.The talk will present the OCR-D software and its functions and disckuss the open, participativedevelopment strategy of the DFG project.",
      "url": " /en/2020/10/02/elag.html"
    }
    ,
  
  {
      "slug": "de-2020-10-02-elag-html",
      "title": "OCR-D bei der Mini-ELAG",
      "content"	 : "Am 20. Oktober 2020 findet die Mini-ELAG (European Library Automation Group)statt, bei der der Bibliothekare und IT-Fachleute neue Informationstechnologien und ihre Anwendung in Bibliotheken und Dokumentationsszentren diskutieren. OCR-D ist bei der virtuellen Konferenz mit einem Vortrag von Clemens Neudecker (SBB) zuOCR-D: An open ecosystem for improving OCR on historical documents vertreten.Die OCR-D-Software wird mit ihren Funktionen vorgestellt und die offene, partizipativeEntwicklungsstrategie des DFG-Projekts diskutiert.",
      "url": " /de/2020/10/02/elag.html"
    }
    ,
  
  {
      "slug": "en-2020-09-22-fair-html",
      "title": "OCR-D at the virtual workshop FAIR &amp; Co",
      "content"	 : "From October 7 to 8, the eHumanities working group of the Union of German Academies of Sciences and Humanities,in cooperation with the Göttingen Academy of Sciences and Humanities,is organizing the workshop FAIR &amp;amp; Co: Visibility and Availability of Digital Academy Research in a Networked Scientific Landscape.The OCR-D project will be represented with a lecture on the topic Digital Transformation: OCR-D, Offer and Vision by Matthias Boenig (BBAW).Using the example of the German Text Archive, it will be shown how the application spectrum of this referencecorpus can be extended by the area of machine learning to improve character and structurerecognition. For the whole program of the workshop see the website of this workshop.",
      "url": " /en/2020/09/22/fair.html"
    }
    ,
  
  {
      "slug": "de-2020-09-22-fair-html",
      "title": "OCR-D beim virtuellen Workshop FAIR &amp; Co",
      "content"	 : "Vom 7. bis 8. Oktober veranstaltet die AG eHumanities der Union der deutschen Akademien der Wissenschaftenin Zusammenarbeit mit der Akademie der Wissenschaften zu Göttingenden Workshop FAIR &amp;amp; Co.: Sicht- und Verfügbarkeit der digitalen Akademieforschung in einer vernetzten Wissenschaftslandschaft.Das OCR-D-Projekt ist mit einem Vortrag von Matthias Boenig (BBAW) zum Thema: Digitale Transformation: OCR-D, Angebot und Vision vertreten. Am Beispiel desDeutschen Textarchivs wird dargestellt, wie das Anwendungsspektrum dieses Referenzkorpus um den Bereichdes maschinellen Lernens zur Verbesserung der Zeichen- und Strukturerkennung erweitertwerden kann. Das ganze Programm kann auf der Website des Workshopeingesehen werden.",
      "url": " /de/2020/09/22/fair.html"
    }
    ,
  
  {
      "slug": "en-2020-08-01-implementation-workshop-html",
      "title": "Workshop for the implementation plans",
      "content"	 : "Following the successful first (virtual) meeting of those interested in the DFG-call for theimplementation of the OCR-D-Software, a further workshop will be held on 7 August, 9-13 p.m.to prepare the OCR-D grant proposals.At this second meeting the applicants can inform each other about their previous tests ofthe OCR-D software and exchange experiences. In addition, the project plans,which have been further developed in the meantime, will be discussed in order to identifypossible synergies between the grant proposals.We are looking forward to this new exchange and a continuing successful pilot phase!",
      "url": " /en/2020/08/01/implementation-workshop.html"
    }
    ,
  
  {
      "slug": "de-2020-08-01-antragsworkshop-html",
      "title": "Workshop der Implementierungsvorhaben",
      "content"	 : "Im Anschluss an das erfolgreiche erste (virtuelle) Treffen der Implementierungsinteressentenfindet am 7. August, 9-13 Uhr ein weiterer Workshop zur Vorbereitung der OCR-D-Anträge statt.Bei dem erneuten Treffen können sich die Antragsteller gegenseitig über ihre bisherigenTests der OCR-D-Software informieren und Erfahrungen dazu austauschen. Außerdem werden die inzwischen weiterentwickelten Anträgspläne diskutiert, um mögliche Synergien zwischenden Anträgen erkennen zu können.Wir freuen uns auf diesen neuen Austausch und eine weiterhin erfolgreiche Pilotierungsphase!",
      "url": " /de/2020/08/01/antragsworkshop.html"
    }
    ,
  
  {
      "slug": "en-2020-06-04-pilot-html",
      "title": "Kick-off pilot phase",
      "content"	 : "We are very happy about the great interest in the DFG call for proposals for the implementation of the OCR-D software. As OCR-D coordination projectwe will support the planned projects from the pilot phase onwards and promote the exchange of information among interested parties as desired by the DFG.To kick off the pilot phase, we are organising a large video conference on 19 June, 9-13 o’clock, at which all interested parties can get to know each other and the pilot tests can be coordinated.Interested parties who have not submitted a letter of intent themselves and who are still looking for a suitable partner with whom they could engage in the third phase of OCR-D are also welcome to the video conference.If you are interested, please register for the video conference by 12 June at engl@hab.de.We look forward to working with you and to a successful pilot phase!",
      "url": " /en/2020/06/04/pilot.html"
    }
    ,
  
  {
      "slug": "de-2020-06-04-pilotphase-html",
      "title": "Auftakt der Pilotierungsphase",
      "content"	 : "Wir freuen uns sehr über das große Interesse an der DFG-Ausschreibung zur Implementierung der OCR-D-Software. Als OCR-D-Koordinierungsprojektwerden wir die geplanten Vorhaben von der Pilotierung an begleiten und den von der DFG gewünschten Austausch unter den Interessenten befördern.Zum Auftakt der Pilotierungsphase veranstalten wir am 19. Juni, 9-13 Uhr eine große Videokonferenz, bei der sich alle Interessentenkennenlernen und die Pilotierungsarbeiten abgestimmt werden können.Zu der Videokonferenz sind auch Interessenten willkommen, die selbst keine Absichtserklärung eingereicht haben und derzeit noch auf der Suchenach einem geeigneten Partner sind, mit dem sie sich in die dritte OCR-D-Phase einbringen könnten.Bei Interesse melden Sie sich bitte bis zum 12. Juni unter engl@hab.de zur Videokonferenz an.Wir freuen uns auf die Zusammenarbeit und eine erfolgreiche Pilotierungsphase!",
      "url": " /de/2020/06/04/pilotphase.html"
    }
    ,
  
  {
      "slug": "en-2020-02-25-dfg-call-html",
      "title": "Call for OCR-D Implementation online!",
      "content"	 : "The call for the implementation of the OCR-D software for the full text digitisation of historical printsis now available on the website of the German Research Association (DFG).The aim of the OCR-D coordination project, which was launched in autumn 2015,is to describe procedures and develop guidelines in order to achieve an optimalworkflow and the greatest possible standardisation of OCR-related processes andmetadata. Furthermore, the complete transformation of the written Germancultural heritage into a machine-readable form (structured full text) is to beprepared conceptually. Primarily, works from the Union Catalogue of BooksPrinted in German Speaking Countries in the 16th-18th century (VD) as well asbooks published in the 19th century in the German language area will beconsidered. The VD projects comprise about 1 million titles that are currentlybeing digitized and are to be processed by means of OCR in the future.The work done so far in the OCR-D initiative has led to significantimprovements in the optical character and layout recognition of historicalprints.  The Software prototype promises flexible integration into existing(librarian) workflow and digitisation systems due to its modular design. Theimplementation of the OCR-D software in libraries, archives and othercollection holding institutions is the next necessary step to ensure thathigh-quality full texts can be produced.The central goal of the new call is the development of (generic) implementationpackages with acceptable performance for different requirements. It isprimarily aimed at collection holding institutions, the application is due onOctober 7, 2020. The DFG expects applicants to hand in a letter of intent by May 5 22. If you are interested in participating, please don’t hesitate tocontact us.",
      "url": " /en/2020/02/25/dfg-call.html"
    }
    ,
  
  {
      "slug": "de-2020-02-25-dfg-ausschreibung-html",
      "title": "Ausschreibung zur OCR-D-Implementierung online!",
      "content"	 : "Die Ausschreibung für die Implementierung der OCR-D-Software zur Volltextdigitalisierung historischer Drucke ist ab sofort auf den Seiten der Deutschen Forschungsgemeinschaft (DFG) zugänglich.Das im Herbst 2015 gestartete Koordinierungsprojekt OCR-D hat zum Ziel, zumeinen Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einenoptimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCRbezogenen Prozessen und Metadaten zu erzielen. Zum anderen soll dievollständige Transformation des schriftlichen deutschen Kulturerbes in einemaschinenlesbare Form (strukturierter Volltext) konzeptionell vorbereitetwerden. Vornehmlich betrachtet werden Werke aus den Verzeichnissen der imdeutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD) sowiedes 19. Jhs. Die VD-Projekte umfassen ca. 1 Mio. Titel, die derzeitdigitalisiert und zukünftig mittels einer OCR prozessiert werden sollen.Die bisherigen Arbeiten in der OCR-D-Initiative haben zu wesentlichenVerbesserungen der Verfahren zur automatischen Text- und Strukturerfassunghistorischer Drucke geführt. Der Software-Prototyp verspricht durch seinenmodularen Aufbau eine flexible Integration in bestehende (bibliothekarische)Workflow- und Digitalisierungssysteme. Die Implementierung der OCR-D-Softwarein Bibliotheken, Archiven sowie anderen bestandshaltenden bzw.bestandsverarbeitenden Einrichtungen ist folglich der nächste notwendigeSchritt, damit die Erzeugung hochqualitativer Volltexte stattfinden kann.Zentrales Ziel der neuen Ausschreibung ist die Entwicklung (generischer)Implementierungspakete mit akzeptabler Performanz für unterschiedlicheAnforderungen. Sie richtet sich vorwiegend an bestandshaltende Einrichtungen,Anträge können bis zum 7. Oktober 2020 bei der DFG eingereicht werden. Die DFG erwartet von Antragstellern vorab eine Absichtserklärung, die bis zum 5. 22. Mai abzugeben ist. BeiInteresse an der Ausschreibung nehmen Sie gerne mit uns Kontakt auf.",
      "url": " /de/2020/02/25/dfg-ausschreibung.html"
    }
    ,
  
  {
      "slug": "en-2020-02-19-dhd-html",
      "title": "OCR-D at the DHd in Paderborn",
      "content"	 : "The OCR-D funding initiative will be represented with several presentations at the DHd 2020, which will take place in Paderborn from 2-6 March.In V15, on March 5, the OCR-D coordination project will discuss the results and perspectives of the funding initiative on the basis of four theses on full text transformation. In the same section, the colleagues of the module project for the development of a model repository and automatic font recognition for OCR-D will report on their findings on the use of fracture in German-language books. For further information on the presentations see the DHd program.",
      "url": " /en/2020/02/19/dhd.html"
    }
    ,
  
  {
      "slug": "de-2020-02-19-dhd-html",
      "title": "OCR-D bei der DHd in Paderborn",
      "content"	 : "Die OCR-D-Förderinitiative ist mit mehreren Vorträgen auf der DHd 2020 vertreten, die vom 2.-6. März in Paderborn stattfindet.In V15, am 5. März, stellt das OCR-D-Koordinierungsprojekt anhand von vier Thesen zur Volltexttransformation Ergebnisse und Perspektiven der Förderinitiative zur Diskussion. In der gleichen Sektion berichten die Kollegen des Modulprojekts zur Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D über ihre Erkenntnisse zur Verwendung von Fraktur in deutschsprachigen Büchern. Weitere Informationen zu den Vorträgen finden Sie im Programm der DHd.",
      "url": " /de/2020/02/19/dhd.html"
    }
    ,
  
  {
      "slug": "en-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Full texts - the future of old prints: OCR-D-Workshop in Bonn",
      "content"	 : "The OCR-D workshop “Full texts - the future of old prints” will take place inBonn on 12 February. The findings and desiderata of the DFG project will bepresented and discussed there with a broad audience of developers, OCR experts,users and funding agencies.The first morning lectures will focus on the OCR-D software itself. It will bedemonstrated with its range of functions and technical possibilities,specifications and documentation will be described as a basis for its creationand use. The experiences and insights that the software developers have gainedon the creation of OCR in libraries are of special interest with regard tofuture OCR projects. The morning section will be concluded in a paneldiscussion with the developers of the OCR-D module projects.The afternoon section is reserved for the discussion about the future of theOCR-D software. The workshop participants will discuss various ways andpossibilities for implementing the software at libraries and transferring it topractical use. Subsequently, possible further funding measures of the DFGproject will be discussed, with the help of which the last steps can be takentowards the mass processing of the VD titles, which is ultimately targeted.",
      "url": " /en/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "de-2020-02-03-volltexte-die-zukunft-alter-drucke-html",
      "title": "Volltexte – die Zukunft alter Drucke: OCR-D-Workshop in Bonn",
      "content"	 : "Am 12. Februar findet in Bonn der OCR-D-Workshop “Volltexte – die Zukunft alterDrucke” statt. Die Erkenntnisse und Desiderate des DFG-Projekts werden dorteinem bereiten Publikum aus Entwicklern, OCR-Experten, Anwendern undFördergebern vorgestellt und diskutiert.Die ersten Vorträge am Vormittag stellen die OCR-D-Software selbst in denMittelpunkt. Diese wird mit ihrem Funktionsumfang und ihren technischenMöglichkeiten vorgeführt sowie Spezifikationen und Dokumentationen als Basisfür deren Erstellung und Verwendung beschrieben. Die Erfahrungen undErkenntnisse, die die Software- Entwickler zur Praxis der OCR-Erstellung inbestandshaltenden Einrichtungen gewonnen haben, sind mit Blick auf künftigeOCR-Projekte von übergeordnetem Interesse. Beschlossen wird der Vormittagsteilin einer Podiumsdiskussion mit den Entwicklern aus den einzelnenOCR-D-Modulprojekten.Die Nachmittagssektion ist der Diskussion über die Zukunft der OCR-D-Softwarevorbehalten. Mit den Workshop-Teilnehmern sollen verschiedene Wege undMöglichkeiten zur Implementierung der Software an bestandshaltendenEinrichtungen und deren Überführung in den Praxiseinsatz erörtert werden. Darananschließend werden mögliche weitere Fördermaßnahmen des DFG-Projektsbesprochen, mit deren Hilfe die letzten Schritte zur letztlich anvisiertenmassenhaften Prozessierung der VD-Titel gegangen werden können.",
      "url": " /de/2020/02/03/volltexte-die-zukunft-alter-drucke.html"
    }
    ,
  
  {
      "slug": "en-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Cooperation with Kitodo signed",
      "content"	 : "Kitodo and OCR-D have signed a Letter of Intent to cooperate on the coordinated and sustainable development and provision of OCR software solutions for mass full text digitization. Kitodo is an open source software suite for the digitisation of cultural property and widely used in the library sector.The basic aim of the cooperation is to make use of overlaps between Kitodo and OCR-D to achieve synergies in technical and organizational aspects. It will be perspectively aspired, to integrate OCR-D as a separate functional area in Kitodo.Production.. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and upcoming developments. In the coming years, OCR-D intends to develop the resulting software solution to a stable executable, widely used and extensively documented software solution. In mutual agreement and in view of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on Interfaces to Kitodo.Production and current and upcoming developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. willcoordinating institutions of the OCR-D project and the In the coming years, OCR-D intends to expand the resulting software solution into a stable, widely used and extensively documented software solution. In mutual agreement and against the background of the upcoming implementation phase of OCR-D, the cooperation partners will exchange information on interfaces to Kitodo.Production and current and future developments of the OCR mass full text digitization. With regard to the technical integration of OCR-D in Kitodo.Production, Kitodo e.V. will support the coordination project and the implementation partners with its extensive experience.",
      "url": " /en/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "de-2019-11-20-kooperation-mit-kitodo-html",
      "title": "Kooperation mit Kitodo vereinbart",
      "content"	 : "Kitodo und OCR-D haben einen Letter of Intent zur Zusammenarbeit bei derkoordinierten und nachhaltigen Entwicklung und Bereitstellung vonOCR-Softwarelösungen zur Massenvolltextdigitalisierung unterzeichnet. Kitodoist eine quelloffene Softwaresuite für die Digitalisierung von Kulturgut inbestandshaltenden Einrichtungen und im Bibliotheksbereich weit verbreitet.Grundlegendes Ziel der Kooperation ist es, inhaltliche und konzeptionelleÜberschneidungen zwischen Kitodo und OCR-D zum Erzielen von Synergien intechnischer und organisatorischer Hinsicht zu nutzen. Es wird perspektivischangestrebt, OCR-D als eigenen Funktionsbereich in Kitodo.Productionaufzunehmen. OCR-D beabsichtigt in den kommenden Jahren, die entstandeneSoftwarelösung zu einer stabil lauffähigen, breit eingesetzten sowieausführlich dokumentierten Softwarelösung auszubauen. Im gegenseitigenEinverständnis und vor dem Hintergrund der anstehenden Implementierungsphasevon OCR-D tauschen sich die Kooperationspartner dazu vor allem überSchnittstellen zu Kitodo.Production und aktuelle sowie kommende Entwicklungender OCR-Massenvolltextdigitalisierung aus. Mit Blick auf die technischeIntegration von OCR-D in Kitodo.Production, wird Kitodo e.V. diekoordinierenden Einrichtungen des OCR-D-Projektes und dieImplementierungspartner mit seinen Erfahrungen umfassend unterstützen.",
      "url": " /de/2019/11/20/kooperation-mit-kitodo.html"
    }
    ,
  
  {
      "slug": "en-2019-08-23-icdar-html",
      "title": "OCR-D at the ICDAR 2019 in Sydney",
      "content"	 : "The OCR-D paper “okralact - a multi-engine Open Source OCR training system (Konstantin Baierer, Rui Dong, Clemens Neudecker) was accepted for the 5th International Workshop on Historical Document Imaging and Processing HIP 2019 (https://www.primaresearch.org/hip2019/) in the context of the ICDAR Conference 2019 in Sydney (https://icdar2019.org/).The source code for the prototype for a multi-engine OCR Training infrastructure presented there is available at https://github.com/Doreenruirui/okralact. Training infrastructure is available at https://github.com/Doreenruirui/okralact for find. With “Dataset of Pages from Early Printed Books with Multiple Font Groups” the module project for the development of a model repository and an automatic font recognition for OCR-D is also at the same workshop with a paper of Mathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier and Vincent Christlein.",
      "url": " /en/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "de-2019-08-23-icdar-html",
      "title": "OCR-D bei der ICDAR 2019 in Sydney",
      "content"	 : "Das OCR-D Paper „okralact – a multi-engine Open Source OCR training system“(Konstantin Baierer, Rui Dong, Clemens Neudecker) wurde für den 5thInternational Workshop on Historical Document Imaging and Processing HIP 2019(https://www.primaresearch.org/hip2019/) im Rahmen der ICDAR Konferenz 2019 inSydney (https://icdar2019.org/) angenommen.Der Quellcode zum dort vorgestellten Prototypen für eine multi-Enginge OCRTrainingsinfrastruktur ist unter https://github.com/Doreenruirui/okralact zufinden. Mit „Dataset of Pages from Early Printed Books with Multiple FontGroups” ist zudem das Modulprojekt zur Entwicklung eines Modellrepositoriumsund einer automatischen Schriftarterkennung für OCR-D mit einem paper vonMathias Seuret, Saskia Limbach, Nikolaus Weichselbaumer, Andreas Maier andVincent Christlein auf dem gleichen Workshop vertreten.",
      "url": " /de/2019/08/23/icdar.html"
    }
    ,
  
  {
      "slug": "en-2019-08-09-europeanatech-html",
      "title": "OCR-D in EuropeanaTech Insights",
      "content"	 : "On the occasion of DATeCH 2019, the online journal “EuropeanaTech Insights” dedicated its last issue to the topic of optical character recognition.In addition to articles on OCR in Bengali and Arabic texts from the British Library and on automatic article recognition in Finnish journals, the OCR-D project and its first results are presented in detail under the title “OCR-D: An End-to-end open source OCR framework for historical documents”. Authors of the paper are C. Neudecker, K. Baierer, M. Federbusch (Berlin State Library), K.M. Würzner, M. Boenig (Berlin-Brandenburg Academy of Sciences and Humanities), E. Herrmann (Duke-August Library Wolfenbüttel), V. Hartmann (Karlsruhe Institute of Technology).",
      "url": " /en/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "de-2019-08-09-europeanatech-html",
      "title": "OCR-D auf EuropeanaTech Insights",
      "content"	 : "Das Online Journal „EuropeanaTech Insights“ widmete aus Anlass der DATeCH 2019 sein letztes Heft dem Thema Optical Character Recognition.Neben Artikeln über OCR an bengalischen und arabischen Texten der British Library und über automatische Artikelerkennung in finnischen Zeitschriften werden das OCR-D Projekt und erste Ergebnisse daraus ausführlich unter dem Titel „OCR-D: An End-to-end open source OCR framework for historical documents“ vorgestellt. Verfasser des Papers sind C. Neudecker, K. Baierer, M. Federbusch (Staatsbibliothek Berlin), K.M. Würzner, M. Boenig (Berlin-Brandenburgische Akademie der Wissenschaften), E. Herrmann (Herzog-August Bibliothek Wolfenbüttel), V. Hartmann (Karlsruher Institut für Technologie).",
      "url": " /de/2019/08/09/europeanatech.html"
    }
    ,
  
  {
      "slug": "en-2019-05-13-datech-best-paper-html",
      "title": "Best Paper Award for OCR-D at the DATeCH 2019",
      "content"	 : "At the international conference “Digital Access to Textual Cultural Heritage 2019” (DATeCH2019) held in Brussels, the paper on OCR-D entitled “OCR-D: An end-to-end open-source OCR framework for historical documents” was awarded the Best Paper Award. The authors are Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-Michael Würzner, Matthias Boenig, Elisa Herrmann and Volker Hartmann.",
      "url": " /en/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "de-2019-05-13-datech-best-paper-html",
      "title": "Best Paper Award bei der DATeCH 2019 für OCR-D",
      "content"	 : "Bei der in Brüssel veranstalteten internationalen Konferenz “Digital Access toTextual Cultural Heritage 2019” (DATeCH2019) wurde der Beitrag über OCR-D mitdem Titel “OCR-D: An end-to-end open-source OCR framework for historicaldocuments” mit dem Best Paper Award ausgezeichnet. Als Autorinnen und Autorensind Clemens Neudecker, Konstantin Baierer, Maria Federbusch, Kay-MichaelWürzner, Matthias Boenig, Elisa Herrmann und Volker Hartmann verantwortlich.",
      "url": " /de/2019/05/13/datech-best-paper.html"
    }
    ,
  
  {
      "slug": "en-2018-08-31-ocrd-verlaengert-html",
      "title": "OCR-D extended",
      "content"	 : "The German Research Foundation (DFG) has approved an extension of the OCR-D project for another 18 months. The new funding phase will start in October 2018 and will therefore end in March 2020. This good news allows us to continue supporting the module projects and to consolidate the results. On the other hand, it will also allow the coordination committee’s own work packages to be continued and deepened.The current funding phase would end in the 3rd quarter of 2018 and project-immanent tasks such as combining the individual module project results into a software package or updating the DFG’s “Digitisation” rules of practice based on the findings of this project would not have been possible. During the extension period, the Coordinating Committee will work particularly on the OCR-D framework, the continuous development of which you can follow via the GitHub site (http://www.github.de/ocr-d).In addition, we are working on the later distribution of the software, be it by evaluating the modules on the basis of application examples, by contacts to OCR service providers or a pilot program in spring 2019. Within the pilot programme, the requirements already identified should be even better tailored to the needs of the future users.The third focus is on the extension of the ground truth data and the format maintenance and, of course, the coordination and management of the Communication with the module projects.  Besides the internal workshops the coordinating body will hold a final workshop in summer 2019, for which we will invite the professional public.",
      "url": " /en/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "de-2018-08-31-ocrd-verlaengert-html",
      "title": "OCR-D bis 2020 verlängert",
      "content"	 : "Die Deutsche Forschungsgemeinschaft (DFG) hat eine Verlängerung des ProjektsOCR-D für weitere 18 Monate bewilligt. Die neue Förderphase beginnt im Oktober2018 und endet demnach im März 2020. Diese erfreuliche Nachricht ermöglicht unszum einen die weitere Unterstützung der Modulprojekte sowie die Zusammenführungder Ergebnisse. Zum anderen können dadurch auch die eigenen Arbeitspakete desKoordinierungsgremiums weitergeführt und vertieft werden.Die aktuelle Förderphase wäre im 3. Quartal 2018 geendet und projektimmanenteAufgaben, wie die Zusammenführung der einzelnen Modulprojektergebnisse zu einemSoftwarepaket oder die Aktualisierung der DFG-Praxisregeln “Digitalisierung”auf Grundlage der Erkenntnisse aus diesem Vorhaben, wären nicht möglichgewesen. Das Koordinierungsgremium wird in der Verlängerung besonders amOCR-D-Framework arbeiten, dessen stetige Entwicklung Sie über die GitHub-Seite(http://www.github.de/ocr-d) verfolgen können.Daneben arbeiten wir an der späteren Verbreitung der Software, sei es durch dieEvaluierung der Module anhand von Anwendungsbeispielen, durch Kontakte zuOCR-Dienstleistern oder ein Pilotprogramm im Frühjahr 2019. Im Rahmen diesesPilotprogramms sollen die schon ermittelten Anforderungen noch besser auf dieBedürfnisse der späteren Anwender abgestimmt werden.Der dritte Schwerpunkt liegt auf der Erweiterung der Ground-Truth-Daten und derdazugehörigen Formatpflege sowie naturgemäß auf der Koordinierung undKommunikation mit den Modulprojekten.  Neben den internen Workshops organisiertdas Koordinierungsgremium einen Abschlussworkshop im Sommer 2019, zu dem wirdie Fachöffentlichkeit einladen werden.",
      "url": " /de/2018/08/31/ocrd-verlaengert.html"
    }
    ,
  
  {
      "slug": "en-2018-03-28-start-der-modulprojekte-html",
      "title": "OCR-D Kick-Off Meeting",
      "content"	 : "From March 5th to 6th the big kick-off meeting of the module projects took place in the Herzog August Bibliothek in Wolfenbüttel, which officially heralds the second phase of OCR-D.The coordination committee of OCR-D met for the first time with the ten module project participants to get to know each other and to present the work done and planned on both sides.In short presentations, the project application contents were presented and connections between the module projects and the coordination committee were sought. During the personal exchange in the World Café as well as during the joint dinner, first project-strategic discussions could be initiated, which will accompany us in the near future.Of the more than 20 project proposals received in response to the DFG call for proposals in March 2017, eight module projects were approved at the end of the year:Scalable methods of text and structure recognition for the full text digitisation of historical prints: Image optimisationGerman Research Center for Artificial Intelligence (DFKI)Scalable methods of text and structure recognition for the full text digitisation of historical prints: layout recognitionDFKIFurther development of a semi-automatic open-source tool for layout analysis and region extraction and classification (LAREX) of early printed booksUniversity of WürzburgNN/FST - Unsupervised OCR Postcorrection based on Neural Networks and Finite-state TransducersLeipzig UniversityOptimized use of OCR processes - Tesseract as a component in the OCR-D workflowUniversity of MannheimAutomatic and semi-automatic post-correction of OCR-recorded historical printsMunich UniversityDevelopment of a model repository and automatic font recognition for OCR-DLeipzig University, Erlangen University, Mainz UniversityOLA-HD - An OCR-D long-term archive for historical printsSUB Göttingen, GWDG GöttingenThe module projects usually have a duration of 18 months, a public final workshop is planned for June 2019. Until then, the coordination committee will organise two project-internal developer meetings to discuss the previous versions of the work. An insight into the development can also be found on the GitHub page of OCR-D: https://github.com//ocr-dIn addition, the coordinating committee and the module projects are in regular exchange via video conferences during this intensive work phase.Further details on the individual module projects and later on the developer meetings will follow on the OCR-D website.We are looking forward to working together!For questions and suggestions: contact",
      "url": " /en/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "de-2018-03-28-start-der-modulprojekte-html",
      "title": "Start der 8 Modulprojekte - OCR-D geht in die zweite Projektphase",
      "content"	 : "Vom 05.-06. März fand in der Herzog August Bibliothek in Wolfenbüttel das große Kick-Off-Treffen der Modulprojekte statt, welches offiziell die zweite Phase von OCR-D einläutet.Das Koordinierunsggremium von OCR-D traf sich zum ersten Mal mit den zehn Modulprojektnehmern zum gegenseitigen Kennenlernen und zum Vorstellen der geleisteten und geplanten Arbeiten auf beiden Seiten.In kurzen Präsentationen wurden die Projektantragsinhalte dargestellt und nach Anknüpfungspunkten zwischen den Modulprojekten sowie zum Koordinierungsgremium gesucht. Beim persönlichen Austausch im World Café sowie beim gemeinsamen Abendessen konnten zudem erste projektstrategische Diskussionen angestoßen werden, die uns in naher Zukunft noch begleiten werden.Von den über 20 eingegangenen Projektanträgen auf die DFG-Ausschreibung im März 2017 wurden Ende des Jahres acht Modulprojekte bewilligt:Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: BildoptimierungDeutsches Forschungszentrum für Künstliche Intelligenz (DFKI)Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: LayouterkennungDFKIWeiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken,Universität WürzburgNN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state TransducersUniversität LeipzigOptimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-WorkflowUniversität MannheimAutomatische und semi-automatische Nachkorrektur OCR-erfasster historischer DruckeUniversität MünchenEntwicklung eines Modellrepositoriums und einer automatischen Schriftarterkennung für OCR-DUniversität Leipzig, Universität Erlangen, Universität MainzOLA-HD – Ein OCR-D-Langzeitarchiv für historische DruckeSUB Göttingen, GWDG GöttingenDie Modulprojekte haben in der Regel eine Laufzeit von 18 Monaten, ein öffentlicher Abschlussworkshop ist für Juni 2019 geplant. Bis dahin organisiert das Koordinierungsgremium zwei projektinterne Entwicklertreffen, auf denen die bisherigen Arbeitsversionen besprochen werden. Einen Einblick in die Entwicklung bietet auch die GitHub-Seite von OCR-D: https://github.com//ocr-dZudem stehen das Koordinierungsgremium und die Modulprojekte in dieser intensiven Arbeitsphase per Videokonferenzen im regelmäßigen Austausch.Weitere Details zu den einzelnen Modulprojekten und später zu den Entwicklertreffen folgen auf der Seite von OCR-D.Wir freuen uns auf die gemeinsame Zusammenarbeit!Für Fragen und Anregungen: Kontakt",
      "url": " /de/2018/03/28/start-der-modulprojekte.html"
    }
    ,
  
  {
      "slug": "en-2017-03-06-modulprojektausschreibung-html",
      "title": "Call for OCR-D module project proposals",
      "content"	 : "The call for module projects within the framework of OCR-D can now be found online on the website of the German Research Foundation (DFG) (link to the call)The aim of the OCR-D coordination project, which was launched in autumn 2015, is to describe procedures and develop guidelines in order to achieve an optimal workflow and the greatest possible standardisation of OCR-related processes and metadata. Furthermore, the complete transformation of the written German cultural heritage into a machine-readable form (structured full text) is to be prepared conceptually. Primarily, works from the Union Catalogue of Books Printed in German Speaking Countries in the 16th-18th century (VD) as well as books published in the 19th century in the German language area will be considered. The VD projects comprise about 1 million titles that are currently being digitized and are to be processed by means of OCR in the future.In the first project phase of OCR-D, development needs for automatic text recognition processes were identified. Based on this, the DFG is now issuing calls for proposals for six module project topics, which will be managed in terms of content and technology by the OCR-D coordination project. The following topics are announced:Image presortingLayout recognitionText optimizationModel trainingLong-term archiving and persistenceQuality assuranceIn order to get an impression of the material to be treated, we provide Ground-Truth data (link to the data).",
      "url": " /en/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "de-2017-03-06-modulprojektausschreibung-html",
      "title": "Modulprojektausschreibungen online!",
      "content"	 : "Die Ausschreibung für Modulprojekte im Rahmen von OCR-D ist ab sofort online auf der Seite der Deutschen Forschungsgemeinschaft (DFG) zu finden (Link zur Ausschreibung)Das im Herbst 2015 gestartete Koordinierungsprojekt OCR-D hat zum Ziel, zum einen Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR bezogenen Prozessen und Metadaten zu erzielen. Zum anderen soll die vollständige Transformation des schriftlichen deutschen Kulturerbes in eine maschinenlesbare Form (strukturierter Volltext) konzeptionell vorbereitet werden. Vornehmlich betrachtet werden Werke aus den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD) sowie des 19. Jhs. Die VD-Projekte umfassen ca. 1 Mio. Titel die derzeit digitalisiert und zukünftig mittels einer OCR prozessiert werden sollen.In der ersten Projektphase von OCR-D wurden Entwicklungsbedarfe für Verfahren der automatischen Texterkennung ermittelt. Darauf aufbauend erfolgt nun die Ausschreibungen der DFG zu sechs Modulprojektthemen, die inhaltlich und technisch durch das Koordinierungsgremium von OCR-D betreut. Folgende Themen sind ausgeschrieben:BildvorsortierungLayouterkennungTextoptimierungModelltrainingLangzeitarchivierung und PersistenzQualitätssicherungUm einen Eindruck des zu behandelnden Materials zu bekommen stellen wir Ground-Truth-Daten zur Verfügung (Link zu den Daten).",
      "url": " /de/2017/03/06/modulprojektausschreibung.html"
    }
    ,
  
  {
      "slug": "en-2016-12-06-staatsbibliothek-html",
      "title": "State Library in Berlin is New Project Partner of OCR-D",
      "content"	 : "The coordinating body of OCR-D consists of the Herzog August Bibliothek Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities Berlin, in particular the German Text Archive, and now the Staatsbibliothek zu Berlin. In future, the SBB will take over the work of the Bayerische Staatsbibliothek, which withdrew from the project on 31 August 2016.The work packages include long-term archiving and persistence, the Conception of workflows and use cases as well as the composition of training corpora. OCR-D investigates further development possibilities for Optical Character Recognition (OCR) process. The project sees itself as a coordinating body and network at the same time, bringing together developers, researchers and users to combine current research findings with practical requirements in a practicable solution. During the first project phase, development needs were identified on the basis of which module project calls will follow. In these, solutions for the development needs will be worked out and thus the current state of research on OCR will be brought together with the requirements from practice. The calls for proposals are planned for the first half of 2017. The results from OCR-D will have far-reaching changes for digitisation projects. In addition to the goal of preparing the transformation of the titles from the VD projects into machine-readable form, proposals for the DFG’s “Digitisation” practice rules will also be drawn up in response to the new findings in order to complete the media conversion of the entire written cultural heritage published in the German-speaking world in the medium to long term in the spirit of European and national agendas.",
      "url": " /en/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "de-2016-12-06-staatsbibliothek-html",
      "title": "Staatsbibliothek zu Berlin ist neuer Partner von OCR-D",
      "content"	 : "Das Koordinierungsgremium von OCR-D setzt sich aus der Herzog August BibliothekWolfenbüttel, der Berlin-Brandenburgischen Akademie der Wissenschaften Berlin,dort insbesondere dem Deutschen Textarchiv, sowie nun der Staatsbibliothek zuBerlin zusammen. Die SBB übernimmt dabei zukünftig die Arbeiten der BayerischenStaatsbibliothek, die zum 31.08.2016 aus dem Projekt ausgeschieden ist.DieArbeitspakete umfassen u.a. die Langzeitarchivierung und Persistenz, dieKonzeption von Workflows und Use Cases sowie die Zusammenstellung vonTrainingskorpora. OCR-D untersucht Weiterentwicklungsmöglichkeiten fürVerfahren der Optical Character Recognition (OCR). Das Projekt versteht sichdabei als Koordinierungsgremium und Netzwerk zugleich, bringt Entwickler,Forscher und Anwender zusammen um aktuelle Erkenntnisse aus der Forschung mitden Anforderungen aus der Praxis in einer praktikablen Lösung zu vereinen. Inder ersten Projektphase wurden Entwicklungsbedarfe aufgedeckt auf Basis dererModulprojektausschreibungen folgen. In diesen werden für dieEntwicklungsbedarfe Lösungen erarbeitet und so der aktuelle Forschungsstand zurOCR mit den Anforderungen aus der Praxis zusammen gebracht. Die Ausschreibungensind für das erste Halbjahr 2017 geplant. Die Ergebnisse aus OCR-D werdenweitreichende Veränderungen für Digitalisierungsprojekte haben. Neben dem Ziel,die Transformation der Titel aus den VD-Projekten in maschinenlesbare Formvorzubereiten, werden auch Vorschläge für die DFG-Praxisregeln„Digitalisierung“ an die neuen Erkenntnisse erarbeitet, um im Geisteeuropäischer und nationaler Agenden die Medienkonversion des gesamten imdeutschen Sprachraum erschienenen schriftlichen kulturellen Erbes mittel- bislangfristig zu vollenden.",
      "url": " /de/2016/12/06/staatsbibliothek.html"
    }
    ,
  
  {
      "slug": "en-2016-06-01-ocrd-html",
      "title": "The OCR-D project",
      "content"	 : "OCR-D is a coordination project that is aimed at the further development of Optical Character Recognition (OCR) processes for historical prints.Workflow and methods of automatic text recognition are investigated, described and, if necessary, optimized. A major goal is to conceptually prepare the transformation of prints of the German-speaking countries from the 16th to 19th century into electronic full text.The Herzog August Bibliothek Wolfenbüttel, the Berlin-Brandenburg Academy of Sciences and Humanities in Berlin, the Staatsbibliothek zu Berlin Preußischer Kulturbesitz and the Karlsruhe Institute of Technology are participating in this project. The Bayerische Staatsbibliothek was also involved until 31 August 2016. The project is supported by experts, scientists and libraries.In recent years, scientific libraries in particular have digitised extensive collections of images. Searchable full texts can be automatically generated from these image data using OCR procedures. The added value provided by the use of digital full texts is indispensable in many scientific disciplines today, especially in the field of humanities research.So far, however, access to the electronic full text is often not possible or only possible in an insufficient form. Many historical holdings are available in digitalised form through the “Verzeichnisse der im deutschen Sprachbereich erschienenen Drucke” (VD). Results from common OCR procedures have so far been insufficient. In particular, old print types, especially fracture, are hardly recognized.There is a need for development here, which we are uncovering in OCR-D. We build on the already existing tools and investigations. By a new combination, in rare cases also by new development, the OCR process for VD prints shall be specialized. Thereby we are looking for answers to current technical, information scientific and organisational problems.The project is funded by the German Research Foundation (DFG) and will run for three years until September 2018. In the first phase, needs will be identified and concepts for the further development will be developed. The cooperation structure will be consolidated and continued in the second phase.  In this phase, calls for proposals for pilot projects will be issued, which will enable other institutions to participate. In all steps we welcome a lively exchange with colleagues from related projects and institutions as well as service providers.At the end of the overall project, a consolidated procedure for the OCR processing of digitised material from the printed German cultural heritage of the 16th to 19th centuries will be developed.",
      "url": " /en/2016/06/01/ocrd.html"
    }
    ,
  
  {
      "slug": "de-2016-06-01-ocrd-html",
      "title": "Das OCR-D-Projekt",
      "content"	 : "OCR-D ist ein Koordinierungsprojekt, welches auf die Weiterentwicklung vonVerfahren der Optical Character Recognition (OCR) für historische Druckeausgerichtet ist.Dabei werden Workflow und Verfahren der automatischen Texterkennung untersucht,beschrieben und ggf. optimiert. Ein wesentliches Ziel ist es, dieTransformation von Drucke des deutschsprachigen Raums aus dem 16.-19.Jahrhundert in elektronischen Volltext konzeptuell vorzubereiten.An diesem Vorhaben beteiligen sich die Herzog August Bibliothek Wolfenbüttel,die Berlin-Brandenburgische Akademie der Wissenschaften in Berlin sowie dieStaatsbibliothek zu Berlin Preußischer Kulturbesitz und dem Karlsruher Institutfür Technologie. Ebenfalls beteiligt war bis zum 31.08.2016 die BayerischeStaatsbibliothek. Unterstütz wird das Projekt durch Experten, Wissenschaftlerund Bibliotheken.In den letzten Jahren haben vor allem wissenschaftliche Bibliothekenumfangreiche Bestände bilddigitalisiert. Mit Hilfe von OCR-Verfahren können ausdiesen Bilddaten durchsuchbare Volltexte automatisch generiert werden. DerMehrwert durch die Nutzung von digitalen Volltexten ist in vielenWissenschaftsdisziplinen, insbesondere im Bereich der geisteswissenschaftlichenForschung heute unverzichtbar.Bislang ist der  Zugriff auf den elektronischen Volltext jedoch oft nicht odernur in unzureichender Form möglich. Viele historische Bestände liegen in digitalisierter Form durch die „Verzeichnisse der im deutschen Sprachbereich erschienenen Drucke“ (kurz VD) vor. Resultate aus gängigen OCR-Verfahren waren bislang ungenügend. Insbesondere werden alte Drucktypen, vor allem Fraktur, nur schwerlich erkannt.Hier besteht Entwicklungsbedarf, den wir in OCR-D  aufdecken. Wir bauen dabei auf die bereits bestehende Tools und Untersuchungen auf. Durch eine Neu-Kombination, in seltenen Fällen auch durch Neuentwicklung, soll der OCR-Prozess für die VD-Drucke spezialisiert werden. Dabei wird nach Antworten auf aktuelle technische, informationswissenschaftliche und organisatorische Probleme gesucht.Das Projekt wird durch die Deutsche Forschungsgemeinschaft (DFG) gefördert und hat eine Laufzeit von drei Jahren bis September 2018. In der ersten Phase  werden Bedarfe aufgedeckt und Konzepte für den weiteren Verlauf erarbeitet. Die Kooperationsstruktur wird gefestigt und in der zweiten Phase fortgeführt.  In dieser werden Ausschreibungen für Pilotprojekte erfolgen, die eine Beteiligung weiterer Einrichtungen ermöglicht. In allen Schritten begrüßen wir einen regen Austausch mit Kolleginnen und Kollegen aus  artverwandten Projekten und Einrichtungen sowie Dienstleistern.Am Ende des Gesamtvorhabens soll ein konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des gedruckten deutschen Kulturerbes des 16. bis 19. Jh. erarbeitet sein.",
      "url": " /de/2016/06/01/ocrd.html"
    }
    
  
]
