<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8"/>
  <title>OCR-D Workflow Guide - OCR-D</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="https://avatars0.githubusercontent.com/u/26362587?s=200&amp;v=4" />
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous" />
  <link rel="alternate" type="application/atom+xml" title="OCR-D Blog" href="/feed.xml" />
  <link rel="stylesheet" href="/assets/bulma.css" />
  <link rel="stylesheet" href="/assets/bulma-switch.min.css" />
  <link rel="stylesheet" href="/assets/syntax-highlight.css" />
  <link rel="stylesheet" href="/assets/ocrd.css" />
</head>
<body>
<script async src="https://cse.google.com/cse.js?cx=e9e3f7148e57ed66c"></script>


<script>
function ToggleSearchActive2() {
    var T = document.getElementById("button-header")
	var A = document.getElementById("google-search-header");
    T.style.display = "none";  // <-- Set it to none
	A.style.visibility = "visible";  // <-- Set it to visible
}
</script>


<nav class="navbar is-transparent is-fixed-top">

  <div class="navbar-brand">
    <a class="navbar-item" href="/">
      <img src="/assets/ocrd-logo-small.png" height="28"/>
    </a>
    <div class="navbar-burger burger" data-target="ocrd-navbar-menu">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>

  <div id="ocrd-navbar-menu" class="navbar-menu">
    <div class="navbar-start">
      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/">About</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/en/blog">News</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/about">About the OCR-D Project</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/phase2">Phase II: Projects</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/phase3">Phase III: Projects</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/community">Community</a>

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/publications">Publications and Presentations</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/data">Data</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/initial-tests">Pilot Study</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/user_survey">User Survey</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/contact">Contacts</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/imprint">Imprint</a>
            

          

          
        </div>
      </div>
      

      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/dev">Technical Resources</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/en/decisions">Decision Log</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/gt-guidelines/trans">Ground Truth Guidelines</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/gt-guidelines/trans/trPage">PAGE-XML format documentation</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/dev-best-practice">OCR-D development best practices</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/spec">Specifications</a>

          

          

          
          

            
            <a class="navbar-item" href="/core">OCR-D/core API Documentation</a>

          

          
        </div>
      </div>
      

      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/use">User Guides & Info</a>
        <div class="navbar-dropdown">
          

          
          
            
            
              
              <a class="navbar-item" href="/en/setup">Setup Guide</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/user_guide">User Guide</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/workflows">Workflows</a>
            

          

          

          
          

            
            <a class="navbar-item" href="/en/models">Models</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/spec/glossary">Glossary</a>

          

          
        </div>
      </div>
      

      
      
        <a class="navbar-item" href="/en/faq">FAQ</a>
      

      
	 <span class="navbar-item">
	 
            <a href="" title="View in German"></a>
				<div class="navbar-item has-dropdown is-hoverable" style="padding-right:10px">Search
				<div class="navbar-dropdown" style="font-size: 0.75em; padding:5px">For this feature, we implemented Google Programmable Search Engine. 
					If you use it, please note that cookies may be stored and Privacy Policy by Google LLC applies: 
					<a href="https://policies.google.com/privacy">https://policies.google.com/privacy</a> 
					<button onclick="ToggleSearchActive2()" id="button-header">Agree, show me Google Search!</button></div>
				</div>
				<div id="google-search-header" style="visibility: hidden">
					<div class="gcse-search"></div>
				</div>
			
	</span>
    </div>

    <div class="navbar-end">

      <span class="navbar-item">
        
          <a href="https://translate.google.com/translate?hl=&sl=en&tl=de&u=https%3A%2F%2Focr-d.de%2Fen%2Fworkflows" title="View in German">
            de
          </a>
        </span>


    </div> </div> </nav>
<div class="columns">
      
      <aside id="toc-sidebar-content" class="column is-one-third menu is-hidden-mobile">
        <ul class="menu-list column is-one-third">
  <li><a href="#workflows">Workflows</a>
    <ul>
      <li><a href="#image-optimization-page-level">Image Optimization (Page Level)</a>
        <ul>
          <li><a href="#step-01-image-enhancement-page-level-optional">Step 0.1: Image Enhancement (Page Level, optional)</a>
            <ul>
              <li><a href="#available-processors">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-02-font-detection">Step 0.2: Font detection</a>
            <ul>
              <li><a href="#available-processors-1">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-1-binarization-page-level">Step 1: Binarization (Page Level)</a>
            <ul>
              <li><a href="#available-processors-2">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-2-cropping-page-level">Step 2: Cropping (Page Level)</a>
            <ul>
              <li><a href="#available-processors-3">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-3-binarization-page-level">Step 3: Binarization (Page Level)</a>
            <ul>
              <li><a href="#available-processors-4">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-4-denoising-page-level">Step 4: Denoising (Page Level)</a>
            <ul>
              <li><a href="#available-processors-5">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-5-deskewing-page-level">Step 5: Deskewing (Page Level)</a>
            <ul>
              <li><a href="#available-processors-6">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-6-dewarping-page-level">Step 6: Dewarping (Page Level)</a>
            <ul>
              <li><a href="#available-processors-7">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#layout-analysis">Layout Analysis</a>
        <ul>
          <li><a href="#step-7-region-segmentation">Step 7: Region segmentation</a>
            <ul>
              <li><a href="#available-processors-8">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#image-optimization-region-level">Image Optimization (Region Level)</a>
        <ul>
          <li><a href="#step-8--binarization-region-level">Step 8:  Binarization (Region Level)</a>
            <ul>
              <li><a href="#available-processors-9">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-9--clipping-region-level">Step 9:  Clipping (Region Level)</a>
            <ul>
              <li><a href="#available-processors-10">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-10--deskewing-region-level">Step 10:  Deskewing (Region Level)</a>
            <ul>
              <li><a href="#available-processors-11">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-11-line-segmentation">Step 11: Line segmentation</a>
            <ul>
              <li><a href="#available-processors-12">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-12-resegmentation-line-level">Step 12: Resegmentation (Line Level)</a>
            <ul>
              <li><a href="#available-processors-13">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-13-dewarping-line-level">Step 13: Dewarping (Line Level)</a>
            <ul>
              <li><a href="#available-processors-14">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#text-recognition">Text Recognition</a>
        <ul>
          <li><a href="#step-14-text-recognition">Step 14: Text recognition</a>
            <ul>
              <li><a href="#available-processors-15">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-141-font-style-annotation">Step 14.1: Font style annotation</a>
            <ul>
              <li><a href="#available-processors-16">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#post-correction-optional">Post Correction (Optional)</a>
        <ul>
          <li><a href="#step-15-text-alignment">Step 15: Text alignment</a>
            <ul>
              <li><a href="#available-processors-17">Available processors</a></li>
              <li><a href="#comparison">Comparison</a></li>
            </ul>
          </li>
          <li><a href="#step-16-post-correction">Step 16: Post-correction</a>
            <ul>
              <li><a href="#available-processors-18">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#evaluation-optional">Evaluation (Optional)</a>
        <ul>
          <li><a href="#step-17-layout-evaluation">Step 17: Layout Evaluation</a>
            <ul>
              <li><a href="#available-processors-19">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-18-ocr-evaluation">Step 18: OCR Evaluation</a>
            <ul>
              <li><a href="#available-processors-20">Available processors</a></li>
              <li><a href="#comparison-1">Comparison</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#generic-data-management-optional">Generic Data Management (Optional)</a>
        <ul>
          <li><a href="#step-19-adaptation-of-coordinates">Step 19: Adaptation of Coordinates</a>
            <ul>
              <li><a href="#available-processors-21">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-20-format-conversion">Step 20: Format Conversion</a>
            <ul>
              <li><a href="#available-processors-22">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-201-generic-transformations">Step 20.1: Generic transformations</a>
            <ul>
              <li><a href="#available-processors-23">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-21-archiving">Step 21: Archiving</a>
            <ul>
              <li><a href="#available-processors-24">Available processors</a></li>
            </ul>
          </li>
          <li><a href="#step-22-dummy-processing">Step 22: Dummy Processing</a>
            <ul>
              <li><a href="#available-processors-25">Available processors</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#recommendations">Recommendations</a>
    <ul>
      <li><a href="#minimal-workflow">Minimal workflow</a>
        <ul>
          <li><a href="#example-with-ocrd-process">Example with ocrd-process</a></li>
        </ul>
      </li>
      <li><a href="#best-results-for-selected-pages">Best results for selected pages</a>
        <ul>
          <li><a href="#example-with-ocrd-process-1">Example with ocrd-process</a></li>
        </ul>
      </li>
      <li><a href="#good-results-for-slower-processors">Good results for slower processors</a>
        <ul>
          <li><a href="#example-with-ocrd-process-2">Example with ocrd-process</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

      </aside>
      <div id="toc-sidebar-toggle">&lt;&gt;</div>
      

      <main class="container content column is-two-thirds" aria-label="Content">
        <h1 id="workflows">Workflows</h1>
<p>There are several steps necessary to get the fulltext of a scanned print. The whole OCR process is shown in the following figure:</p>

<p class="figure img" style="max-width: 100%">
	<a href="/assets/Funktionsmodell.svg" style="max-width: 100%">
		<img src="/assets/Funktionsmodell.svg" style="max-width: 100%" />
	</a>
</p>

<p>The following instructions describe all steps of an OCR workflow. Depending on your particular print (or rather images), not all of those
steps might be necessary to obtain good results. Whether a step is required or optional is indicated in the description of each step.
This guide provides an overview of the available OCR-D processors and their required parameters. For more complex workflows and recommendations
see the <a href="https://github.com/OCR-D/ocrd-website/wiki">OCR-D-Website-Wiki</a>. Feel free to add your own experiences and recommendations in the Wiki!
We will regularly amend this guide with valuable contributions from the Wiki.</p>

<p><strong>Note:</strong> In order to be able to run the workflows described in this guide, you need to have prepared your images in an <a href="https://ocr-d.de/en/user_guide#preparing-a-workspace">OCR-D-workspace</a>.
We expect that you are familiar with the <a href="https://ocr-d.de/en/user_guide">OCR-D-user guide</a> which explains all preparatory steps, syntax and different
solutions for executing whole workflows.</p>

<h2 id="image-optimization-page-level">Image Optimization (Page Level)</h2>
<p>At first, the image should be prepared for OCR.</p>

<h3 id="step-01-image-enhancement-page-level-optional">Step 0.1: Image Enhancement (Page Level, optional)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-preprocessing.md|sed '$d' -->
<p>Optionally, you can start off your workflow by enhancing your images, which can be vital for the following binarization. In this processing step,
the raw image is taken and enhanced by e.g. grayscale conversion, brightness normalization, noise filtering, etc.</p>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">ocrd-preprocess-image</code> can be used to run arbitrary shell commands for preprocessing (original or derived) images, and can be seen as a generic OCR-D wrapper for many of the following workflow steps, provided a matching external tool exists. (The only restriction is that the tool must not change image size or the position/coordinates of its content.)</p>

<h4 id="available-processors">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remark</th>
      <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-im6convert">
      <td>ocrd-im6convert</td>
      <td><code>-P output-format image/tiff</code></td>
      <td>for <code>output-options</code> see <a href="https://imagemagick.org/script/command-line-options.php">IM Documentation</a></td>
      <td><code>ocrd-im6convert -I OCR-D-IMG -O OCR-D-ENH -P output-format image/tiff</code></td>
    </tr>
    <tr data-processor="ocrd-preprocess-image">
      <td>ocrd-preprocess-image</td>
      <td>
      <code>-P input_feature_filter binarized</code><br />
      <code>-P output_feature_added binarized</code><br />
      <code>-P command "scribo-cli sauvola-ms-split '@INFILE' '@OUTFILE' --enable-negate-output"</code>
      </td>
      <td>for parameters and command examples (presets) see <a href="https://github.com/bertsky/ocrd_wrap#ocr-d-processor-interface-ocrd-preprocess-image">the Readme</a></td>
      <td><code>
    ocrd-preprocess-image -I OCR-D-IMG -O OCR-D-PREP -P output_feature_added binarized -P command "scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output"
    </code></td>
    </tr>
    <tr data-processor="ocrd-skimage-normalize">
      <td>ocrd-skimage-normalize</td>
      <td></td>
      <td></td>
      <td><code>ocrd-skimage-normalize -I OCR-D-IMG -O OCR-D-NORM</code></td>
    </tr>
  <tr data-processor="ocrd-skimage-denoise-raw">
      <td>ocrd-skimage-denoise-raw</td>
      <td></td>
      <td></td>
      <td><code>
    ocrd-skimage-denoise-raw -I OCR-D-IMG -O OCR-D-DENOISE
    </code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-02-font-detection">Step 0.2: Font detection</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-font-detection.md|sed '$d' -->
<p>Optionally, this processor can determine the font family (e.g. Antiqua, Fraktur,
Schwabacher) to help select the right models for text detection.</p>

<p><code class="language-plaintext highlighter-rouge">ocrd-typegroups-classifier</code> annotates font families on page
level, including the confidence value (separated by colon). Supported <code class="language-plaintext highlighter-rouge">fontFamily</code> values:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Antiqua</code></li>
  <li><code class="language-plaintext highlighter-rouge">Bastarda</code></li>
  <li><code class="language-plaintext highlighter-rouge">Fraktur</code></li>
  <li><code class="language-plaintext highlighter-rouge">Gotico-Antiqua</code></li>
  <li><code class="language-plaintext highlighter-rouge">Greek</code></li>
  <li><code class="language-plaintext highlighter-rouge">Hebrew</code></li>
  <li><code class="language-plaintext highlighter-rouge">Italic</code></li>
  <li><code class="language-plaintext highlighter-rouge">Rotunda</code></li>
  <li><code class="language-plaintext highlighter-rouge">Schwabacher</code></li>
  <li><code class="language-plaintext highlighter-rouge">Textura</code></li>
  <li><code class="language-plaintext highlighter-rouge">other_font</code></li>
  <li><code class="language-plaintext highlighter-rouge">not_a_font</code></li>
</ul>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">ocrd-typegroups-classifier</code> was trained on a very large and diverse
dataset, with both geometric and color-space random augmentation (contrast,
brightness, hue, even compression artifacts and 2 different binarization
methods), so it works best on the raw, <em>non-binarized</em> RGB image.</p>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">ocrd-typegroups-classifier</code> comes with a non-OCR-D CLI that allows
for the generation of “heatmaps” on the page to visualize which regions of the page
are classified as using a certain font with a certain confidence, see the
<a href="https://github.com/seuretm/ocrd_typegroups_classifier">project’s README for usage instructions</a>.</p>

<h4 id="available-processors-1">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-typegroups-classifier">
      <td>ocrd-typegroups-classifier</td>
      <td><code>-P network /path/to/densenet121.tgc</code></td>
      <td>Download <a href="https://github.com/seuretm/ocrd_typegroups_classifier/raw/master/ocrd_typegroups_classifier/models/densenet121.tgc"><code>densenet121.tgc</code> from GitHub</a></td>
      <td><code>ocrd-typegroups-classifier -I OCR-D-IMG -O OCR-D-IMG-FONTS</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-1-binarization-page-level">Step 1: Binarization (Page Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-binarization.md|sed '$d' -->
<p>All the images should be binarized right at the beginning of your workflow.
Many of the following processors require binarized images. Some implementations
(for deskewing, segmentation or recognition) may produce better results using
the original image. But these can always retrieve the raw image instead of the
binarized version automatically.</p>

<p>In this processing step, a scanned colored /gray scale document image is taken
as input and a black and white binarized image is produced. This step should
separate the background from the foreground.</p>

<p><strong>Note:</strong> Binarization tools usually provide a threshold parameter which allows
you to increase or decrease the weight of the foreground. This is optional and
can be especially useful for images which have not been enhanced.</p>

<table class="before-after">
  <tbody>
    <tr>
      <td>
        <a href="https://ocr-d.de/assets/workflow/Original.png"><img src="https://ocr-d.de/assets/workflow/Original.png" /></a>
      </td>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-BIN_0001-BIN_sauvola.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-BIN_0001-BIN_sauvola.png" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-2">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remark</th>
      <th>Call</th>
  </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-olena-binarize">
      <td>ocrd-olena-binarize
      </td>
      <td><code>-P impl wolf -P k 0.10</code>
      </td>
      <td>Fast</td>
      <td class="processor-call">
        <code>ocrd-olena-binarize -I OCR-D-IMG -O OCR-D-BIN</code>
      </td>
    </tr>
    <tr data-processor="ocrd-cis-ocropy-binarize">
        <td>ocrd-cis-ocropy-binarize</td>
        <td><code>-P threshold 0.1</code></td>
        <td>Fast</td>
        <td><code>ocrd-cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN</code></td>
      </tr>
    <tr data-processor="ocrd-sbb-binarize">
      <td>ocrd-sbb-binarize</td>
      <td><code>-P model</code></td>
      <td>Recommended; pre-trained models can be downloaded from <a href="https://qurator-data.de/sbb_binarization/">here</a> or via the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a></td>
      <td><code>ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname</code></td>
 </tr>
	<tr data-processor="ocrd-skimage-binarize">
      <td>ocrd-skimage-binarize</td>
      <td><code>-P k 0.10</code></td>
      <td>Slow</td>
      <td><code>ocrd-skimage-binarize -I OCR-D-IMG -O OCR-D-BIN</code></td>
    </tr>
    <tr data-processor="ocrd-doxa-binarize">
      <td>ocrd-doxa-binarize</td>
      <td><code>-P algorithm ISauvola</code></td>
      <td>Fast</td>
      <td><code>ocrd-doxa-binarize -I OCR-D-IMG -O OCR-D-BIN</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-2-cropping-page-level">Step 2: Cropping (Page Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-cropping.md|sed '$d' -->
<p>In this processing step, a document image is taken as input and the page
is cropped to the content area only (i.e. without noise at the margins or facing pages) by marking the coordinates of the page frame.
We strongly recommend to execute this step if your images are not cropped already (i.e. only show the page of a book without a ruler,
footer, color scale etc.). Otherwise you might run into severe segmentation problems.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <a href="https://ocr-d.de/assets/workflow/denoised.png"><img src="https://ocr-d.de/assets/workflow/denoised.png" alt="" /></a>
      </td>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-CROP_0001.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-CROP_0001.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-3">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-anybaseocr-crop">
      <td>ocrd-anybaseocr-crop</td>
      <td></td>
      <td>The input image has to be binarized and <br />should be deskewed for the module to work.</td>
      <td><code>ocrd-anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP</code></td>
    </tr>
    <tr data-processor="ocrd-tesserocr-crop">
      <td>ocrd-tesserocr-crop</td>
      <td></td>
      <td>Cannot cope well with facing pages (textual noise is detected as text).</td>
      <td><code>ocrd-tesserocr-crop -I OCR-D-BIN -O OCR-D-CROP</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-3-binarization-page-level">Step 3: Binarization (Page Level)</h3>

<p>For better results, the cropped images can be binarized again at this point or later on (on region level).</p>

<h4 id="available-processors-4">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remark</th>
      <th>Call</th>
  </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-olena-binarize">
      <td>ocrd-olena-binarize</td>
      <td></td>
      <td>Recommended</td>
      <td><code>ocrd-olena-binarize -I OCR-D-CROP -O OCR-D-BIN2</code></td>
    </tr>
  <tr data-processor="ocrd-sbb-binarize">
      <td>ocrd-sbb-binarize</td>
      <td><code>-P model</code></td>
      <td>pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or via the [OCR-D resource manager](https://ocr-d.de/en/models)</td>
      <td></td>
      <td><code>ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname</code></td>
    </tr>
  <tr data-processor="ocrd-skimage-binarize">
      <td>ocrd-skimage-binarize</td>
      <td></td>
      <td></td>
      <td><code>ocrd-skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2</code></td>
    </tr>
  <tr data-processor="ocrd-cis-ocropy-binarize">
      <td>ocrd-cis-ocropy-binarize</td>
      <td></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-binarize -I OCR-D-CROP -O OCR-D-BIN2</code></td>
    </tr>
  </tbody>
</table>

<h3 id="step-4-denoising-page-level">Step 4: Denoising (Page Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-denoising.md|sed '$d' -->
<p>In this processing step, artifacts like little specks (both in foreground or background) are removed from the binarized image. (Not to be confused with raw denoising in step 0.)</p>

<p>This may not be necessary for all prints, and depends heavily on the selected binarization algorithm.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-BIN_0001-BIN_sauvola.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-CROP-ALTERNATE_0009.png" alt="" /></a>
      </td>
      <td>
        <a href="https://ocr-d.de/assets/workflow/denoise.PNG"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DENOISE-ALTERNATE_0009.png" alt="" /></a>
        </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-5">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cis-ocropy-denoise">
      <td>ocrd-cis-ocropy-denoise</td>
      <td><code>-P noise_maxsize 3.0</code></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE</code></td>
    </tr>
    <tr data-processor="ocrd-skimage-denoise">
      <td>ocrd-skimage-denoise</td>
      <td><code>-P maxsize 3.0</code></td>
      <td>Slow</td>
      <td><code>ocrd-skimage-denoise -I OCR-D-BIN2 -O OCR-D-DENOISE</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-5-deskewing-page-level">Step 5: Deskewing (Page Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-deskewing.md|sed '$d' -->
<p>In this processing step, a document image is taken as input and the skew of
that page is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.
The input images have to be binarized for this module to work.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESPECK_0001.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESPECK_0001.png" alt="" /></a>
      </td>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-6">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
   <tr data-processor="ocrd-cis-ocropy-deskew">
      <td>ocrd-cis-ocropy-deskew</td>
      <td><code>-P level-of-operation page</code></td>
      <td>Recommended</td>
      <td><code>ocrd-cis-ocropy-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P level-of-operation page</code></td>
    </tr>
    <tr data-processor="ocrd-tesserocr-deskew">
      <td>ocrd-tesserocr-deskew</td>
      <td><code>-P operation_level page</code></td>
      <td>Fast, also performs a decent orientation correction</td>
      <td><code>ocrd-tesserocr-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE -P operation_level page</code></td>
    </tr>
    <tr data-processor="ocrd-anybaseocr-deskew">
      <td>ocrd-anybaseocr-deskew</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>ocrd-anybaseocr-deskew -I OCR-D-DENOISE -O OCR-D-DESKEW-PAGE</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-6-dewarping-page-level">Step 6: Dewarping (Page Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-dewarping.md|sed '$d' -->
<p>In this processing step, a document image is taken as input and the text lines are straightened or stretched
if they are curved. The input image has to be binarized for the module to work.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-TO-DEWARP_0005.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-TO-DEWARP_0005.png" alt="" /></a>
      </td>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARPEP_0005.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARPEP_0005.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-7">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
      <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-anybaseocr-dewarp">
      <td>ocrd-anybaseocr-dewarp</td>
      <td>
        <code>-P model_path /path/to/latest_net_G.pth</code>
      </td>
      <td>For available models take a look at this <a href="https://github.com/OCR-D/ocrd_anybaseocr/tree/master/ocrd_anybaseocr/models">site</a> or use the [OCR-D resource manager](https://ocr-d.de/en/models) <br /> Parameter <code>model_path</code> is optional if the model was installed via <code>ocrd resmgr download ocrd-anybaseocr-dewarp '*'</code> <br /> <strong>GPU required!</strong></td>
      <td>
        <code>ocrd-anybaseocr-dewarp -I OCR-D-DESKEW-PAGE -O OCR-D-DEWARP-PAGE</code>
      </td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h2 id="layout-analysis">Layout Analysis</h2>

<p>By now the image should be well prepared for segmentation.</p>

<h3 id="step-7-region-segmentation">Step 7: Region segmentation</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-region-segmentation.md|sed '$d' -->
<p>In this processing step, an (optimized) document image is taken as an input and the
image is segmented into the various regions, including columns.
Segments are also classified, either coarse (text, separator, image, table, …) or fine-grained (paragraph, marginalia, heading, …).</p>

<p><strong>Note:</strong> The <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-segment</code>, <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>, <code class="language-plaintext highlighter-rouge">ocrd-eynollah-segment</code>, <code class="language-plaintext highlighter-rouge">ocrd-sbb-textline-detector</code> and
<code class="language-plaintext highlighter-rouge">ocrd-cis-ocropy-segment</code> processors do not only segment the page, but
also the text lines within the detected text regions in one
step. Therefore with those (and only with those!) processors you don’t need to
segment into lines in an extra step and can continue with <a href="#step-13-dewarping-line-level">step 13 - line-level dewarping</a>.</p>

<p><strong>Note:</strong> If you use <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-segment-region</code>, which uses only bounding
boxes instead of polygon coordinates, then you should post-process via
<code class="language-plaintext highlighter-rouge">ocrd-segment-repair</code> with <code class="language-plaintext highlighter-rouge">plausibilize=True</code> to obtain better results without
large overlaps. <em>Alternatively</em>, consider using the all-in-one capabilities of
<code class="language-plaintext highlighter-rouge">ocrd-tesserocr-segment</code> and <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>, which can do region
segmentation and line segmentation (and optionally also text recognition) in
one step by querying Tesseract’s internal iterator (accessing the more precise
polygon outlines instead of just coarse bounding boxes with lots of
hard-to-recover overlap). <em>Alternatively</em>, run with <code class="language-plaintext highlighter-rouge">shrink_polygons=True</code>
(accessing that same iterator to calculate convex hull polygons).</p>

<p><strong>Note:</strong> All the <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-segment*</code> processors internally delegate to
<code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>, so you can replace calls to these task-specific
processors with calls to <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code> with specific parameters:</p>

<table>
  <thead><tr><th>processor call</th><th><code>ocrd-tesserocr-recognize</code> parameters</th></tr></thead>
  <tbody>
    <tr>
      <td>ocrd-tesserocr-segment-region -P overwrite_regions true</td>
      <td>ocrd-tesserocr-recognize -P textequiv_level region -P segmentation_level region -P overwrite_segments true</td>
    </tr>
    <tr>
      <td>ocrd-tesserocr-segment-table -P overwrite_cells true</td>
      <td>ocrd-tesserocr-recognize -P textequiv_level cell -P segmentation_level cell -P overwrite_segments true</td>
    </tr>
    <tr>
      <td>ocrd-tesserocr-segment-line -P overwrite_lines true</td>
      <td>ocrd-tesserocr-recognize -P textequiv_level line -P segmentation_level line -P overwrite_segments true</td>
    </tr>
    <tr>
      <td>ocrd-tesserocr-segment-word -P overwrite_words true</td>
      <td>ocrd-tesserocr-recognize -P textequiv_level word -P segmentation_level word -P overwrite_segments true</td>
    </tr>
  </tbody>
</table>

<p><strong>Note:</strong> The three parameters <code class="language-plaintext highlighter-rouge">segmentation_level</code>, <code class="language-plaintext highlighter-rouge">textequiv_level</code> and
<code class="language-plaintext highlighter-rouge">model</code> define the behavior of <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">segmentation_level</code> determines the <em>highest level</em> to segment. Use <code class="language-plaintext highlighter-rouge">"none"</code> to disable segmentation altogether, i.e. only recognize existing segments.</li>
  <li><code class="language-plaintext highlighter-rouge">textequiv_level</code> determines the <em>lowest level</em> to segment. Use <code class="language-plaintext highlighter-rouge">"none"</code> to segment until the lowest level (<code class="language-plaintext highlighter-rouge">"glyph"</code>) and disable recognition altogether, only analyse layout.</li>
  <li><code class="language-plaintext highlighter-rouge">model</code> determines the model to use for text recognition. Use <code class="language-plaintext highlighter-rouge">""</code> or do not set at all to disable recognition, i.e. only analyse layout.</li>
</ul>

<p>Examples:</p>
<ul>
  <li>To segment existing regions into lines (and only lines) only: <code class="language-plaintext highlighter-rouge">segmentation_level="line"</code>, <code class="language-plaintext highlighter-rouge">textequiv_level="line"</code>, <code class="language-plaintext highlighter-rouge">model=""</code></li>
  <li>To segment existing regions into lines (and only lines) and recognize text: <code class="language-plaintext highlighter-rouge">segmentation_level="line"</code>, <code class="language-plaintext highlighter-rouge">textequiv_level="line"</code>, <code class="language-plaintext highlighter-rouge">model="Fraktur"</code></li>
</ul>

<p>For detailed descriptions of behaviour and options, see <a href="https://github.com/OCR-D/ocrd_tesserocr/blob/master/README.md">tesserocr’s README</a> and
<code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize/segment/segment-region/segment-table/segment-line/segment-word --help</code> help.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-CROP_0001.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-CROP_0001.png" alt="" /></a>
      </td>
      <td>
        <a href="https://ocr-d.de/assets/workflow/seg-page.PNG"><img src="https://ocr-d.de/assets/workflow/seg-page.PNG" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-8">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-tesserocr-segment">
      <td>ocrd-tesserocr-segment</td>
      <td><code>-P find_tables false -P shrink_polygons true</code></td>
      <td>Recommended. Will reuse internal tesseract iterators to produce a complete segmentation with tight polygons instead of bounding boxes where possible</td>
      <td><code>ocrd-tesserocr-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG -P find_tables false -P shrink_polygons true</code></td>
    </tr>
    <tr data-processor="ocrd-eynollah-segment">
      <td>ocrd-eynollah-segment</td>
      <td><code>-P models</code></td>
      <td>Models can be found <a href="https://qurator-data.de/eynollah/models_eynollah.tar.gz)">here</a> or downloaded with the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>; <br />
      If you didn't download the model with the <code>resmgr</code>, for <code>model</code> you need to <strong>pass the absolute path on your hard drive</strong> as parameter value.</td>
      <td><code>ocrd-eynollah-segment -I OCR-D-IMG -O OCR-D-SEG -P models default</code></td>
    </tr>
    <tr data-processor="ocrd-sbb-textline-detector">
      <td>ocrd-sbb-textline-detector</td>
      <td><code>-P model modelname</code></td>
      <td>Models can be found <a href="https://qurator-data.de/sbb_textline_detector/">here</a> or downloaded with the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>; <br />
      If you didn't download the model with <code>resmgr</code>, for <code>model</code> you need to <strong>pass the local filesystem path</strong> as parameter value.</td>
      <td><code>ocrd-sbb-textline-detector -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P model /path/to/model</code></td>
    </tr>
    <tr data-processor="ocrd-cis-ocropy-segment">
      <td>ocrd-cis-ocropy-segment</td>
      <td><code>-P level-of-operation page</code></td>
      <td></td>
    <td><code>ocrd-cis-ocropy-segment -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-LINE -P level-of-operation page</code></td>
    </tr>
    <tr data-processor="ocrd-tesserocr-segment-region">
      <td>ocrd-tesserocr-segment-region</td>
      <td><code>-P find_tables false</code></td>
      <td>Recommended</td>
      <td><code>ocrd-tesserocr-segment-region -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P find_tables false -P shrink_polygons true</code></td>
    </tr>
    <tr data-processor="ocrd-segment-repair">
      <td>ocrd-segment-repair</td>
      <td><code>-P plausibilize true</code></td>
      <td>Only to be used after <code>ocrd-tesserocr-segment-region</code></td>
      <td><code>ocrd-segment-repair -I OCR-D-SEG-REG -O OCR-D-SEG-REPAIR -P plausibilize true</code></td>
    </tr>
    <tr data-processor="ocrd-anybaseocr-block-segmentation">
      <td>ocrd-anybaseocr-block-segmentation</td>
      <td><code>-P block_segmentation_model mrcnn_name</code> -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5&lt;/code&gt;</td>
      <td>For available models take a look at <a href="https://github.com/OCR-D/ocrd_anybaseocr/tree/master/ocrd_anybaseocr/models">this site</a> ocr download them via <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>; 
      If you didn't use <code>resmgr</code>, you need to <strong>pass the local filesystem path</strong> as parameter value.</td>
      <td><code>ocrd-anybaseocr-block-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG -P block_segmentation_model mrcnn_name -P block_segmentation_weights /path/to/model/block_segmentation_weights.h5</code></td>
    </tr>
    <tr data-processor="ocrd-pc-segmentation">
      <td>ocrd-pc-segmentation</td>
      <td></td>
      <td></td>
      <td><code>ocrd-pc-segmentation -I OCR-D-DEWARP-PAGE -O OCR-D-SEG-REG</code></td>
    </tr>
    <tr data-processor="ocrd-detectron2-segment">
      <td>ocrd-detectron2-segment</td>
      <td></td>
      <td>For available models, any model for <a href="https://github.com/facebookresearch/detectron2">Detectron2</a> forks trained on document layout analysis datasets can be integrated; instructions and examples can be found <a href="https://github.com/bertsky/ocrd_detectron2#models">here</a></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h2 id="image-optimization-region-level">Image Optimization (Region Level)</h2>

<p>In the following steps, the text regions should be optimized for OCR.</p>

<h3 id="step-8--binarization-region-level">Step 8:  Binarization (Region Level)</h3>

<p>In this processing step, a scanned colored /gray scale document image is taken as input and a black
and white binarized image is produced. This step should separate the background from the foreground.</p>

<p>The binarization should be at least executed once (on page or region level). If you already binarized
your image twice on page level, and have no large images, you can probably skip this step.</p>

<h4 id="available-processors-9">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-skimage-binarize">
      <td>ocrd-skimage-binarize</td>
      <td><code>-P level-of-operation region</code></td>
      <td></td>
      <td><code>ocrd-skimage-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region</code></td>
    </tr>
    <tr data-processor="ocrd-sbb-binarize">
      <td>ocrd-sbb-binarize</td>
      <td><code>-P model -P operation_level region</code></td>
      <td>pre-trained models can be downloaded from [here](https://qurator-data.de/sbb_binarization/) or with the [OCR-D resource manager](https://ocr-d.de/en/models)</td>
      <td><code>ocrd-sbb-binarize -I OCR-D-IMG -O OCR-D-BIN -P model modelname -P operation_level region</code></td>
    </tr>
	<tr data-processor="ocrd-preprocess-image">
      <td>ocrd-preprocess-image</td>
      <td>
        <code>-P level-of-operation region</code><br />
        <code>-P "output_feature_added" binarized</code><br />
        <code>-P command "scribo-cli sauvola-ms-split '@INFILE' '@OUTFILE' --enable-negate-output"</code>
      </td>
      <td>&nbsp;</td>
      <td><code>
    ocrd-preprocess-image -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region -P output_feature_added binarized -P command "scribo-cli sauvola-ms-split @INFILE @OUTFILE --enable-negate-output"
      </code></td>
    </tr>
    <tr data-processor="ocrd-cis-ocropy-binarize">
      <td>ocrd-cis-ocropy-binarize</td>
      <td><code>-P level-of-operation region</code><br /><code>-P "noise_maxsize": float</code></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-binarize -I OCR-D-SEG-REG -O OCR-D-BIN-REG -P level-of-operation region</code></td>
    </tr>
  </tbody>
</table>

<h3 id="step-9--clipping-region-level">Step 9:  Clipping (Region Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-clipping.md|sed '$d' -->
<p>In this processing step, intrusions of neighbouring non-text (e.g. separator) or text segments (e.g. ascenders/descenders) into
text regions of a page (or text lines or a text region) can be removed. A connected component analysis is run on every segment,
as well as its overlapping neighbours. Now for each conflicting binary object,
a rule based on majority and proper containment determines whether it belongs to the neighbour, and can therefore
be clipped to the background.</p>

<p>This basic text-nontext segmentation ensures that for each text region there is a clean image without interference from separators and neighbouring texts. (On the region level, cleaning via coordinates would be impossible in many common cases.) On the line level, this can be seen as an alternative to <em>resegmentation</em>.</p>

<p>Note: Clipping must be applied <strong>before</strong> any processor that produces derived images for the same hierarchy level (region/line). Annotations on the next higher level (page/region) are fine of course.</p>

<!-- TODO: add images -->

<h4 id="available-processors-10">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
  <tr data-processor="ocrd-cis-ocropy-clip">&gt;
      <td>ocrd-cis-ocropy-clip</td>
      <td><code>-P level-of-operation region</code></td>
      <td>&nbsp;</td>
      <td><code>ocrd-cis-ocropy-clip -I OCR-D-DESKEW-REG -O OCR-D-CLIP-REG -P level-of-operation region</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-10--deskewing-region-level">Step 10:  Deskewing (Region Level)</h3>

<p>In this processing step, text region images are taken as input and their skew is corrected by annotating the detected angle (-45° .. 45°) and rotating the image. Optionally, also the orientation is corrected by annotating the detected angle (multiples of 90°) and transposing the image.</p>

<table class="before-after">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
      <a href="https://ocr-d.de/assets/workflow/seg-page.PNG"><img src="https://ocr-d.de/assets/workflow/seg-page.PNG" alt="" /></a>
      </td>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001_region0002.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001_region0002.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-11">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cis-ocropy-deskew">
      <td>ocrd-cis-ocropy-deskew</td>
      <td><code>-P level-of-operation region</code></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG -P level-of-operation region</code></td>
    </tr>
    <tr data-processor="ocrd-tesserocr-deskew">
      <td>ocrd-tesserocr-deskew</td>
      <td></td>
      <td>Fast, also performs a decent orientation correction</td>
      <td><code>ocrd-tesserocr-deskew -I OCR-D-BIN-REG -O OCR-D-DESKEW-REG</code></td>
    </tr>
  </tbody>
</table>

<h3 id="step-11-line-segmentation">Step 11: Line segmentation</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-line-segmentation.md|sed '$d' -->
<p>In this processing step, text regions are segmented into text lines.
A line detection algorithm is run on every text region of every PAGE in the
input file group, and a TextLine element with the resulting polygon
outline is added to the annotation of the output PAGE.</p>

<p><strong>Note:</strong> If you use <code class="language-plaintext highlighter-rouge">ocrd-cis-ocropy-segment</code>, you can directly go on with <a href="#step-13-dewarping-on-line-level">Step 13</a>.</p>

<p><strong>Note:</strong> If you use <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-segment-line</code>, which uses only bounding
boxes instead of polygon coordinates, then you should post-process with the
processors described in <a href="#step-12-resegmentation-line-level">Step 12</a>.
<em>Alternatively</em>, consider using the all-in-one capabilities of
<a href="#step-7-region-segmentation"><code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code></a>, which can do line segmentation
and text recognition in one step by querying Tesseract’s internal iterator
(accessing the more precise polygon outlines instead of just coarse bounding
boxes with lots of hard-to-recover overlap). <em>Alternatively</em>, run with
<code class="language-plaintext highlighter-rouge">shrink_polygons=True</code> (accessing that same iterator to calculate convex hull
polygons)</p>

<p><strong>Note:</strong> As described in <a href="#step-7-page-segmentation">Step 7</a>, <code class="language-plaintext highlighter-rouge">ocrd-eynollah-segment</code>, <code class="language-plaintext highlighter-rouge">ocrd-sbb-textline-detector</code> and <code class="language-plaintext highlighter-rouge">ocrd-cis-ocropy-segment</code> do not only segment
the page, but also the text lines within the detected text regions in one step. Therefore with those (and only with those!) processors you don’t
need to segment into lines in an extra step.</p>

<table class="">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001_region0002.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DESKEW_0001_region0002.png" alt="" /></a>
      </td>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-12">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cis-ocropy-segment">
      <td>ocrd-cis-ocropy-segment</td>
      <td><code>-P level-of-operation region</code></td>
      <td>&nbsp;</td>
      <td><code>ocrd-cis-ocropy-segment -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE -P level-of-operation region</code></td>
    </tr>
    <tr data-processor="ocrd-tesserocr-segment-line">
      <td>ocrd-tesserocr-segment-line</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>ocrd-tesserocr-segment-line -I OCR-D-CLIP-REG -O OCR-D-SEG-LINE</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-12-resegmentation-line-level">Step 12: Resegmentation (Line Level)</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-resegmentation.md|sed '$d' -->
<p>In this processing step the segmented text lines can be corrected in order to reduce their overlap.</p>

<p>This can be done either via coordinates (polygonalizing the bounding boxes tightly around the glyphs) – which is what <code class="language-plaintext highlighter-rouge">ocrd-cis-ocropy-resegment</code> and <code class="language-plaintext highlighter-rouge">ocrd-segment-project</code> offer – 
or via derived images (clipping pixels that do not belong to a text line to the background color) – which is what <code class="language-plaintext highlighter-rouge">ocrd-cis-ocropy-clip</code> (on the <code class="language-plaintext highlighter-rouge">line</code> level) offers. 
The former is usually more accurate, but not always possible (for example, when neighbors intersect heavily, creating non-contiguous contours). The latter is only possible if no preceding workflow 
step has already annotated derived images (<code class="language-plaintext highlighter-rouge">AlternativeImage</code> references) on the line level (see also <a href="../Workflow-Guide-clipping">region-level clipping</a>).</p>

<!-- TODO: add images -->

<h4 id="available-processors-13">Available processors</h4>
<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cis-ocropy-clip">
      <td>ocrd-cis-ocropy-clip</td>
      <td><code>-P level-of-operation line</code></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-clip -I OCR-D-SEG-LINE -O OCR-D-CLIP-LINE -P level-of-operation line</code></td>
    </tr>
    <tr data-processor="ocrd-cis-ocropy-resegment">
      <td>ocrd-cis-ocropy-resegment</td>
      <td></td>
      <td></td>
      <td><code>ocrd-cis-ocropy-resegment -I OCR-D-SEG-LINE -O OCR-D-RESEG</code></td>
    </tr>
	<tr data-processor="ocrd-segment-project">
      <td>ocrd-segment-project</td>
      <td><code>-P level-of-operation line</code></td>
      <td></td>
      <td><code>ocrd-segment-project -I OCR-D-SEG-LINE -O OCR-D-RESEG -P level-of-operation line</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-13-dewarping-line-level">Step 13: Dewarping (Line Level)</h3>

<p>In this processing step, the text line images get vertically aligned if they are curved.</p>

<table class="">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png" alt="" /></a>
      </td>
      <td>
      <a href="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png"><img src="https://ocr-d.de/assets/workflow/OCR-D-IMG-DEWARP_0001_region0002_region0002_line0005.png" alt="" /></a>
      </td>
    </tr>
  </tbody>
</table>

<h4 id="available-processors-14">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cis-ocropy-dewarp">
      <td>ocrd-cis-ocropy-dewarp</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>ocrd-cis-ocropy-dewarp -I OCR-D-CLIP-LINE -O OCR-D-DEWARP-LINE</code></td>
    </tr>
  </tbody>
</table>

<h2 id="text-recognition">Text Recognition</h2>

<h3 id="step-14-text-recognition">Step 14: Text recognition</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-text-recognition.md|sed '$d' -->
<p>This processor recognizes text in segmented lines.</p>

<p>An overview on the existing model repositories and short descriptions on the most important models can be found <a href="https://ocr-d.de/en/models">here</a>.</p>

<p>We strongly recommend to use the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a> to download the models, as this way you don’t have to specify
the path to each model.</p>

<h4 id="available-processors-15">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-tesserocr-recognize">
      <td>ocrd-tesserocr-recognize</td>
      <td><code>-P model GT4HistOCR_50000000.997_191951</code>
      </td>
      <td>Recommended <br />Model can be found <a href="https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/GT4HistOCR_5000000/tessdata_best/GT4HistOCR_50000000.997_191951.traineddata">here</a><br />a faster variant is <a href="https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/GT4HistOCR_5000000/tessdata_fast/">here</a></td>
      <td><code>TESSDATA_PREFIX="/test/data/tesseractmodels/" ocrd-tesserocr-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P model Fraktur+Latin</code></td>
    </tr>
    <tr data-processor="ocrd-calamari-recognize">
      <td>ocrd-calamari-recognize</td>
      <td>
        if you downloaded your model with the [OCR-D resource manager](https://ocr-d.de/en/models), use<code>-P checkpoint_dir modelname</code><br />
        else use <code>-P checkpoint_dir /path/to/models</code>
      </td>
      <td>
        Recommended<br />Model can be found <a href="https://qurator-data.de/calamari-models/GT4HistOCR/2019-12-11T11_10+0100/model.tar.xz">here</a>;
        <br />For <code>checkpoint</code> you need to <b>pass the local path on your hard drive</b> as parameter value, and <b>keep the verbatim asterisk (<code>*</code>)</b>.
      </td>
      <td><code>ocrd-calamari-recognize -I OCR-D-DEWARP-LINE -O OCR-D-OCR -P checkpoint_dir qurator-gt4histocr-1.0</code></td>
    </tr>
  </tbody>
</table>

<p><strong>Note:</strong> For <code class="language-plaintext highlighter-rouge">ocrd-tesserocr</code> the environment variable <code class="language-plaintext highlighter-rouge">TESSDATA_PREFIX</code> has
to be set to point to the directory where the used models are stored unless
the default directory (normally $VIRTUAL_ENV/share/tessdata) is used.
The directory should at least contain the following models:
<code class="language-plaintext highlighter-rouge">deu.traineddata</code>, <code class="language-plaintext highlighter-rouge">eng.traineddata</code>, <code class="language-plaintext highlighter-rouge">osd.traineddata</code>.</p>

<p><strong>Note:</strong> Faster models for <code class="language-plaintext highlighter-rouge">tesserocr-recognize</code> are available from
https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/.
A good and currently the fastest model is
<a href="https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/Fraktur_5000000/tessdata_fast/Fraktur-fast.traineddata">Fraktur-fast</a>.
UB Mannheim provides many more <a href="https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/">models online</a>
which were trained on different GT data sets, for example from
<a href="https://ub-backup.bib.uni-mannheim.de/~stweil/ocrd-train/data/ONB/tessdata_fast/">Austrian Newspapers</a>.</p>

<p><strong>Note:</strong> If you want to go on with the optional post correction, you should also set the <code class="language-plaintext highlighter-rouge">textequiv_level</code> to <code class="language-plaintext highlighter-rouge">glyph</code> or in the case of
<code class="language-plaintext highlighter-rouge">ocrd-calamari-recognize</code> at least <code class="language-plaintext highlighter-rouge">word</code> (which is already the default for <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>).</p>

<!-- END-EVAL -->

<h3 id="step-141-font-style-annotation">Step 14.1: Font style annotation</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-font-style-annotation.md|sed '$d' -->
<p>This processor can determine the font style (e.g. <em>italic</em>, <strong>bold</strong>,
<ins>underlined</ins>) and font family text recognition results.</p>

<p><code class="language-plaintext highlighter-rouge">ocrd-tesserocr-fontshape</code> can either use existing segmentation or
segment on-demand. It can detect the following font styles:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">fontSize</code></li>
  <li><code class="language-plaintext highlighter-rouge">fontFamily</code></li>
  <li><code class="language-plaintext highlighter-rouge">bold</code></li>
  <li><code class="language-plaintext highlighter-rouge">italic</code></li>
  <li><code class="language-plaintext highlighter-rouge">underlined</code></li>
  <li><code class="language-plaintext highlighter-rouge">monospace</code></li>
  <li><code class="language-plaintext highlighter-rouge">serif</code></li>
</ul>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-fontshape</code> needs the old, pre-LSTM models to work at
all. You can use the pre-installed <code class="language-plaintext highlighter-rouge">osd</code> (which is purely rule-based), but
there might be better alternatives for your language and script. You can still
get the old models from Tesseract’s Github repo at the <a href="https://github.com/tesseract-ocr/tessdata/commit/3cf1e2df1fe1d1da29295c9ef0983796c7958b7d">last
revision</a>
before the <a href="https://github.com/tesseract-ocr/tessdata/commit/4592b8d453889181e01982d22328b5846765eaad">LSTM
models</a>
replaced them, usually under the same name. (Thus, <code class="language-plaintext highlighter-rouge">deu.traineddata</code> used to be
a rule-based model but now is an LSTM model. <code class="language-plaintext highlighter-rouge">deu-frak.traineddata</code> is still
only available as rule-based model and was complemented by the new LSTM models
<code class="language-plaintext highlighter-rouge">frk.traineddata</code> and <code class="language-plaintext highlighter-rouge">script/Fraktur.traineddata</code>.) If you do need one of the
models that was replaced completely, then you should at least rename the old
one (e.g. to <code class="language-plaintext highlighter-rouge">deu3.traineddata</code>).</p>

<h4 id="available-processors-16">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-tesserocr-fontshape">
      <td>ocrd-tesserocr-fontshape</td>
      <td><code>-P model osd -P padding 2</code></td>
      <td>Download other pre-LSTM models <a href="https://github.com/tesseract-ocr/tessdata/commit/3cf1e2df1fe1d1da29295c9ef0983796c7958b7d">from GitHub</a></td>
      <td><code>ocrd-tesserocr-fontshape -I OCR-D-OCR -O OCR-D-OCR-FONT</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h2 id="post-correction-optional">Post Correction (Optional)</h2>

<h3 id="step-15-text-alignment">Step 15: Text alignment</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-text-alignment.md|sed '$d' -->
<p>In this processing step, text results from multiple OCR engines (in different annotations sharing the same line segmentation) are aligned
into one annotation.</p>

<h4 id="available-processors-17">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ocrd-cor-asv-ann-align</td>
      <td>
      <code>-P method majority</code>
      </td>
      <td></td>
    <td><code>ocrd-cor-asv-ann-align -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-ALIGN</code></td>
    </tr>
    <tr>
      <td>ocrd-cis-align</td>
      <td></td>
      <td></td>
    <td><code>ocrd-cis-align -I OCR-D-OCR1,OCR-D-OCR2,OCR-D-OCR3 -O OCR-D-ALIGN</code></td>
    </tr>
  </tbody>
</table>

<h4 id="comparison">Comparison</h4>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>ocrd-cor-asv-ann-align</th>
      <th>ocrd-cis-align</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>goal</strong></td>
      <td>optimal aligned string (i.e. <em>as</em> post-correction)</td>
      <td>candidates for input <em>for</em> ocrd-cis-postcorrect</td>
    </tr>
    <tr>
      <td><strong>input arity</strong></td>
      <td>N fileGrps</td>
      <td>N fileGrps (first as “master”)</td>
    </tr>
    <tr>
      <td><strong>input constraints</strong></td>
      <td>textlines must have common IDs</td>
      <td>regions and textlines must be in same order</td>
    </tr>
    <tr>
      <td><strong>input level</strong></td>
      <td>textline (+ optionally words or glyphs for confidence)</td>
      <td>textline (for strings) and word (for resegmentation)</td>
    </tr>
    <tr>
      <td><strong>output</strong></td>
      <td>PAGE with single-best TextEquiv per textline</td>
      <td>PAGE with multiple aligned TextEquivs per textline</td>
    </tr>
    <tr>
      <td><strong>alignment library</strong></td>
      <td><code class="language-plaintext highlighter-rouge">difflib.SequenceMatcher</code></td>
      <td><a href="https://github.com/cisocrgroup/ocrd-postcorrection/tree/master/src/main/java/de/lmu/cis/ocrd/align"><code class="language-plaintext highlighter-rouge">de.lmu.cis.ocrd.align</code></a></td>
    </tr>
    <tr>
      <td><strong>alignment method</strong></td>
      <td>true n-ary multi-alignment (closest pairs first), including lower level confidences</td>
      <td>1:n alignment with master also restricting allowable word-segmentation</td>
    </tr>
    <tr>
      <td><strong>decision</strong></td>
      <td>majority voting, confidence voting, or combination</td>
      <td>no decision</td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-16-post-correction">Step 16: Post-correction</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-post-correction.md|sed '$d' -->
<p>In this processing step, the recognized text is corrected by statistical error modelling, language modelling, and word modelling (dictionaries, morphology and orthography).</p>

<p><strong>Note:</strong> Most tools benefit strongly from input which includes alternative OCR hypotheses. Currently, models for <code class="language-plaintext highlighter-rouge">ocrd-cor-asv-ann-process</code>
are optimised for input from specific OCR models, whereas <code class="language-plaintext highlighter-rouge">ocrd-cis-postcorrect</code> expects input from multi-OCR alignment. For more information, see <a href="https://vdhd2021.hypotheses.org/176">this presentation</a> at vDHd 2021 (held on 23rd May 2021) (<a href="https://dhd-ag-ocr.github.io/slides/OCR@vDHd-Z3.pdf">slides</a> / <a href="https://meet.gwdg.de/playback/presentation/2.0/playback.html?meetingId=db36b9cd45a79838b121a8b68270a85734c8f026-1621428290680">video</a> in German)</p>

<p><strong>Note:</strong> There is some overlap with <a href="https://github.com/OCR-D/ocrd-website/wiki/Workflow-Guide-text-alignment">text alignment</a> here, which can also be used (or contribute to) post-correction.</p>

<h4 id="available-processors-18">Available processors</h4>
<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-cor-asv-ann-process">
      <td>ocrd-cor-asv-ann-process</td>
      <td><code>-P textequiv_level word -P model_file modelname</code></td>
      <td>Pre-trained models can be found <a href="https://github.com/ASVLeipzig/cor-asv-ann-models">here</a> and <a href="https://git.informatik.uni-leipzig.de/ocr-d/cor-asv-ann-models">here</a> or downloaded via the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>;
      <br />If you didn't download the model with <code>resmgr</code>, for <code>model_file</code> you need to <b>pass the local filesystem path</b>
      as parameter value.
     (Relative paths are resolved from the workspace directory or the environment variable <code>CORASVANN_DATA</code>.)
     There is no default <code>model_file</code>.</td>
      <td><code>ocrd-cor-asv-ann-process -I OCR-D-OCR -O OCR-D-PROCESS -P textequiv_level word -P model_file /path/to/model/model.h5</code></td>
    </tr>
    <tr data-processor="ocrd-cis-postcorrect">
      <td>ocrd-cis-postcorrect</td>
      <td><code>-P profilerPath /path/to/profiler.bash -P profilerConfig ignored -P nOCR 2 -P model /path/to/model/model.zip</code></td>
        <td>
      The <code>profilerConfig</code> parameters can be specified in a JSON file. If you do not want to use a profiler, you can set the value for <code>profilerConfig</code> to <code>ignored</code>.
      In this case, your <code>profiler.bash</code> should look like this:<pre><code>
#!/bin/bash
cat &gt; /dev/null
echo '{}'
</code></pre>
      For <code>model</code> you need to <b>pass the local filesystem path</b> as parameter value.
      There is no default <code>model</code>.
      </td>
      <td><code>ocrd-cis-postcorrect -I OCR-D-ALIGN -O OCR-D-CORRECT -p postcorrect.json</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h2 id="evaluation-optional">Evaluation (Optional)</h2>

<p>If Ground Truth data is available, the OCR and layout recognition can be evaluated.</p>

<h3 id="step-17-layout-evaluation">Step 17: Layout Evaluation</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-OLR-evaluation.md|sed '$d' -->
<p>In this processing step, GT annotation and segmentation results are matched
and evaluated.</p>

<h4 id="available-processors-19">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ocrd-segment-evaluate</td>
      <td>
      <code>-P level-of-operation region</code>
      <code>-P only-fg true</code>
      <code>-P ignore-subtype true</code>
      <code>-P for-categories TextRegion,TableRegion</code>
      </td>
      <td>alpha</td>
    <td><code>ocrd-segment-evaluate -I OCR-D-GT-SEG,OCR-D-SEG -O OCR-D-SEG-EVAL</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-18-ocr-evaluation">Step 18: OCR Evaluation</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-ocr-evaluation.md|sed '$d' -->
<p>In this processing step, the text output of the OCR or post-correction can be evaluated by aligning with ground truth text and measuring the error rates.</p>

<h4 id="available-processors-20">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-dinglehopper">
      <td>ocrd-dinglehopper</td>
      <td>
      <code>-P textequiv_level region</code>
      </td>
      <td>For page-wise visual comparison (2 file groups). First input group should point to the ground truth.</td>
      <td><code>ocrd-dinglehopper -I OCR-D-GT,OCR-D-OCR -O OCR-D-EVAL</code></td>
    </tr>
    <tr data-processor="ocrd-cor-asv-ann-evaluate">
      <td>ocrd-cor-asv-ann-evaluate</td>
      <td>
      <code>-P metric historic-latin</code>
      <code>-P gt_level 2</code>      
      <code>-P confusion 20</code>
      <code>-P histogram true</code>
      </td>
      <td>For document-wide aggregation (N file groups). First input group should point to the ground truth.</td>
      <td><code>ocrd-cor-asv-ann-evaluate -I OCR-D-GT,OCR-D-OCR -O OCR-D-EVAL</code></td>
    </tr>
  </tbody>
</table>

<h4 id="comparison-1">Comparison</h4>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>ocrd-dinglehopper</th>
      <th>ocrd-cor-asv-ann-evaluate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>goal</strong></td>
      <td>CER/WER and visualization</td>
      <td>CER/WER (mean+stddev)</td>
    </tr>
    <tr>
      <td><strong>granularity</strong></td>
      <td>only single pages</td>
      <td>single-page + aggregated</td>
    </tr>
    <tr>
      <td><strong>input arity</strong></td>
      <td>2 fileGrps</td>
      <td>N fileGrps</td>
    </tr>
    <tr>
      <td><strong>input constraints</strong></td>
      <td>segmentations may deviate</td>
      <td>segments must have same IDs</td>
    </tr>
    <tr>
      <td><strong>input level</strong></td>
      <td>region or textline</td>
      <td>textline</td>
    </tr>
    <tr>
      <td><strong>output</strong></td>
      <td>HTML + JSON report per page</td>
      <td>JSON report per page+all</td>
    </tr>
    <tr>
      <td><strong>alignment</strong></td>
      <td><code class="language-plaintext highlighter-rouge">rapidfuzz.string_metric.levenshtein_editops</code></td>
      <td><code class="language-plaintext highlighter-rouge">difflib.SequenceMatcher</code></td>
    </tr>
    <tr>
      <td><strong>Unicode</strong></td>
      <td><code class="language-plaintext highlighter-rouge">uniseg.graphemeclusters</code> to get distances on graphemes</td>
      <td>calculates alignment on codepoints, but post-processes combining characters</td>
    </tr>
    <tr>
      <td><strong>charset</strong></td>
      <td>NFC + a set of normalizations that (roughly) target OCR-D GT transcription guidelines level 3 to level 2</td>
      <td>NFC or NFKC or a custom normalization (called <code class="language-plaintext highlighter-rouge">historic_latin</code>) with setting <code class="language-plaintext highlighter-rouge">gt_level</code> 1/2/3</td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h2 id="generic-data-management-optional">Generic Data Management (Optional)</h2>

<p>OCR-D produces PAGE XML files which contain the recognized text as well as detailed
information on the structure of the processed pages, the coordinates of the recognized
elements etc. Optionally, the output can be converted to other formats, or copied verbatim (re-generating PAGE-XML)</p>

<h3 id="step-19-adaptation-of-coordinates">Step 19: Adaptation of Coordinates</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-adaptation-of-coordinates.md|sed '$d' -->
<p>All OCR-D processors are required to relate coordinates to the original image for each page, and to keep the original image reference (<code class="language-plaintext highlighter-rouge">Page/@imageFilename</code>). However, sometimes it may be necessary to deviate from that strict requirement in order to get the overall workflow to function properly.</p>

<p>For example, if you have a page-level dewarping step, it is currently impossible to correctly relate to the original image’s coordinates for any segments annotated after that, because there is no descriptive annotation of the underlying coordinate transform in PAGE-XML. Therefore, it is better to <em>replace the original image</em> of the output PAGE-XML by the dewarped image before proceeding with the workflow. (If the dewarped image has also been cropped or deskewed, then of course all existing coordinates are re-calculated accordingly as well.)</p>

<p>Another use case is exporting PAGE-XML for tools that cannot apply cropping or deskewing, like <a href="https://github.com/OCR4all/LAREX">LAREX</a> or Transkribus.</p>

<p>Conversely, you might want to align two PAGE-XML files for the same page that have different original image references, projecting all segments below the page level from the one to the other (transforming all coordinates according to the page-level annotation, or keeping them unchanged).</p>

<h4 id="available-processors-21">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-segment-replace-original">
      <td>ocrd-segment-replace-original</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    <td><code>ocrd-segment-replace-original -I OCR-D-CROP-DESK -O OCR-D-CROP-DESK-SUBST</code></td>
    </tr>
    <tr data-processor="ocrd-segment-replace-page">
      <td>ocrd-segment-replace-page</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    <td><code>ocrd-segment-replace-page -I OCR-D-CROP-DESK,OCR-D-CROP-DESK-SUBST-SEG -O OCR-D-CROP-DESK-SEG -P transform_coordinates true</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-20-format-conversion">Step 20: Format Conversion</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-format-conversion.md|sed '$d' -->
<p>In this processing step the produced PAGE XML files can be converted to ALTO,
PDF, hOCR or text files. Note that ALTO and hOCR can also be converted into
different formats whereas the PDF version of PAGE XML OCR results is a widely
accessible format that can be used as-is by expert and layman alike.</p>

<h4 id="available-processors-22">Available processors</h4>

<table class="processor-table">
<thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
      <th>Call</th>
    </tr>

</thead>
<tbody>
    <tr data-processor="ocrd-fileformat-transform">
      <td>ocrd-fileformat-transform</td>
      <td><pre><code>-P from-to "alto2.0 alto3.0"
      # or "alto2.0 alto3.1"
      # or "alto2.0 hocr"
      # or "alto2.1 alto3.0"
      # or "alto2.1 alto3.1"
      # or "alto2.1 hocr"
      # or "alto page"
      # or "alto text"
      # or "gcv hocr"
      # or "hocr alto2.0"
      # or "hocr alto2.1"
      # or "hocr text"
      # or "page alto"
      # or "page hocr"
      # or "page text"
      </code></pre>
      </td>
      <td>As the value consists of two words, when using <code>-P</code> form it has to be enclosed in quotation marks.<br />
      If you want to save all OCR results in one file, you can use the following command: <pre>cat OCR* &gt; full.txt</pre>
    </td>
      <td><code>ocrd-fileformat-transform -I OCR-D-OCR -O OCR-D-ALTO</code></td>
    </tr>
   <tr data-processor="mets-mods2tei">
      <td><a href="https://github.com/slub/mets-mods2tei">mets-mods2tei</a></td>
      <td><pre>--ocr -T FULLTEXT -I OCR-D-IMG</pre></td>
      <td>Not a processor CLI, processes the workspace METS, generating a single TEI (<a href="https://deutschestextarchiv.de/doku/basisformat/">DTABf-formatted</a>) for the whole document. Only takes ALTO input, so usually needs a prior <code>ocrd-fileformat-transform -I OCR-D-OCR -O FULLTEXT -P from-to "page alto"</code>.
    </td>
      <td><code>mm2tei --ocr -T FULLTEXT -I OCR-D-IMG -O TEI.xml</code></td>
    </tr>
   <tr data-processor="ocrd-page2tei">
      <td><a href="https://github.com/bertsky/ocrd_page2tei">ocrd-page2tei</a></td>
      <td></td>
      <td>generates a single TEI (using <a href="https://github.com/dariok/page2tei">page2tei</a> XSLT with Saxon) for the whole document.
    </td>
      <td><code>ocrd-page2tei -I OCR-D-OCR -O OCR-D-TEI</code></td>
    </tr>
    <tr data-processor="ocrd-pagetopdf">
      <td>ocrd-pagetopdf</td>
      <td><pre><code>{
  # font file name to use for rendering text
  "font": "AletheiaSans.ttf",
  # fix (invalid) negative coordinates
  "negative2zero": true,
  # concatenate to multi-page PDF (empty for none)
  "multipage": "name_of_pdf",
  # multi-page PDF page labels
  "pagelabel": "pageId",
  # render text on this hierarchy level
  "textequiv_level": "word",
  # draw polygon outlines in the PDF (empty for none)
  "outlines": "line"
}</code></pre>
      </td>
      <td></td>
      <td><code>ocrd-pagetopdf -I OCR-D-OCR -O OCR-D-PDF -P textequiv_level word</code></td>
    </tr>
    <tr data-processor="ocrd-segment-extract-pages">
      <td>ocrd-segment-extract-pages</td>
      <td><code>-P mimetype image/png -P transparency true</code></td>
      <td>Get page images (cropped and deskewed as annotated; raw and binarized) and mask images (color-coded for regions) along with JSON files for region annotations (custom and <a href="https://cocodataset.org/#format-data">COCO</a> format).</td>
      <td><code>ocrd-segment-extract-pages -I OCR-D-SEG-REGION -O OCR-D-IMG-PAGE,OCR-D-IMG-PAGE-BIN,OCR-D-IMG-PAGE-MASK</code></td>
    </tr>
    <tr data-processor="ocrd-segment-extract-regions">
      <td>ocrd-segment-extract-regions</td>
      <td><code>-P mimetype image/png -P transparency true</code></td>
      <td>Get region images (cropped, masked and deskewed as annotated) along with JSON files for region annotations (custom format).</td>
      <td><code>ocrd-segment-extract-regions -I OCR-D-SEG-REGION -O OCR-D-IMG-REGION</code></td>
    </tr>
    <tr data-processor="ocrd-segment-extract-lines">
      <td>ocrd-segment-extract-lines</td>
      <td><code>-P mimetype image/png -P transparency true</code></td>
      <td>Get text line images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for line annotations (custom format).</td>
      <td><code>ocrd-segment-extract-lines -I OCR-D-SEG-LINE -O OCR-D-IMG-LINE</code></td>
    </tr>
    <tr data-processor="ocrd-segment-extract-words">
      <td>ocrd-segment-extract-words</td>
      <td><code>-P mimetype image/png -P transparency true</code></td>
      <td>Get word images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for word annotations (custom format).</td>
      <td><code>ocrd-segment-extract-words -I OCR-D-SEG-WORD -O OCR-D-IMG-WORD</code></td>
    </tr>
    <tr data-processor="ocrd-segment-extract-glyphs">
      <td>ocrd-segment-extract-glyphs</td>
      <td><code>-P mimetype image/png -P transparency true</code></td>
      <td>Get glyph images (cropped, masked and deskewed as annotated) along with text files (Ocropus convention) and JSON files for glyph annotations (custom format).</td>
      <td><code>ocrd-segment-extract-glyphs -I OCR-D-SEG-GLYPH -O OCR-D-IMG-GLYPH</code></td>
    </tr>
    <tr data-processor="ocrd-segment-from-masks">
      <td>ocrd-segment-from-masks</td>
      <td><pre><code>-P colordict '{
  "#969696": "TableRegion", 
  "#00FF00": "TextRegion:page-number", 
  "#FFFF00": "TextRegion:heading", 
  "#00FFFF": "GraphicRegion:logo", 
  "#0000FF": "TextRegion:subject", 
  "#FF0000": "TextRegion:catch-word", 
  "#FF00FF": "TextRegion:footnote", 
  "#646464": "TextRegion:paragraph" }'</code></pre></td>
      <td>Import mask images as region segmentation. If <code>colordict</code> is empty, defaults to PageViewer color scheme (also written by <code>ocrd-segment-extract-pages</code>).</td>
      <td><code>ocrd-segment-from-masks -I OCR-D-SEG-PAGE,OCR-D-IMG-PAGE-MASK -O OCR-D-SEG-REGION</code></td>
    </tr>
    <tr data-processor="ocrd-segment-from-coco">
      <td>ocrd-segment-from-coco</td>
      <td></td>
      <td>Import <a href="https://cocodataset.org/#format-data">COCO</a> format region segmentation (also written by <code>ocrd-segment-extract-pages</code>).</td>
      <td><code>ocrd-segment-from-coco -I OCR-D-SEG-PAGE,OCR-D-SEG-COCO -O OCR-D-SEG-REGION</code></td>
    </tr>

</tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-201-generic-transformations">Step 20.1: Generic transformations</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-generic-transformations.md|sed '$d' -->
<p>Sometimes PAGE-XML annotations need to be processed specially to make a workflow’s processors interoperate properly. For example, a text producing processor might forget to make <code class="language-plaintext highlighter-rouge">TextEquiv</code> consistent between hierarchy levels, or it might be necessary to remove specific region types. Also, repairing minor syntactic or semantic deficiencies is usually required for export or visualization, like removing empty <code class="language-plaintext highlighter-rouge">ReadingOrder</code> and dead <code class="language-plaintext highlighter-rouge">@regionRef</code>s, ensuring each <code class="language-plaintext highlighter-rouge">TextEquiv</code> has a <code class="language-plaintext highlighter-rouge">Unicode</code>, or fixing negative or floating-point coordinates. While it is always possible to do that ad-hoc via scripts, it might help formulate this as a proper workflow step via processor CLI.</p>

<h4 id="available-processors-23">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-page-transform">
      <td>ocrd-page-transform</td>
      <td><code>-P xsl page-remove-regions.xsl -P xslt-params "-s type=ImageRegion"</code></td>
      <td>Many <a href="https://bertsky.github.io/workflow-configuration/#usage">useful XSLTs</a> come as preinstalled resources, but can be passed any XSL file. Specify <code>mimetype</code> if the output is not PAGE-XML anymore</td>
    <td><code>ocrd-page-transform</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-21-archiving">Step 21: Archiving</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-archiving.md|sed '$d' -->
<p>After you have successfully processed your images, the results should be saved and archived. OLA-HD is
a longterm archive system which works as a mixture between an archive system and a repository. For further
details on OLA-HD see the extensive <a href="https://github.com/subugoe/OLA-HD-IMPL/blob/master/docs/OLA-HD_Konzept.pdf">concept paper</a>.
You can also check out the <a href="http://141.5.98.232/">prototype</a> to make sure, OLA-HD meets your needs and requirements.
To use the prototype, specify http://141.5.98.232/api as the endpoint parameter in your call.</p>

<h4 id="available-processors-24">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-olahd-client">
      <td>ocrd-olahd-client</td>
      <td>{
  "endpoint": "URL of your OLA-HD instance",
  "username": "X",
  "password": "*"
}</td>
      <td>the parameters should be written to a json file:<br />
    echo '{  "endpoint": "URL of your OLA-HD instance",
  "username": "X",
  "password": "*"}' &gt; olahd.json
    </td>
    <td><code>ocrd-olahd-client -I OCR-D-OCR -p olahd.json</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h3 id="step-22-dummy-processing">Step 22: Dummy Processing</h3>

<!-- BEGIN-EVAL sed -n '0,/^## Notes/ p' ./repo/ocrd-website.wiki/Workflow-Guide-dummy-processing.md|sed '$d' -->
<p>Sometimes it can be useful to have a dummy processor, which takes the files in an Input fileGrp and
copies them the a new Output fileGrp, re-generating the PAGE XML from the current namespace schema/model.</p>

<h4 id="available-processors-25">Available processors</h4>

<table class="processor-table">
  <thead>
    <tr>
      <th>Processor</th>
      <th>Parameter</th>
      <th>Remarks</th>
    <th>Call</th>
    </tr>
  </thead>
  <tbody>
    <tr data-processor="ocrd-dummy">
      <td>ocrd-dummy</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    <td><code>ocrd-dummy -I OCR-D-FILEGRP -O OCR-D-DUMMY</code></td>
    </tr>
  </tbody>
</table>

<!-- END-EVAL -->

<h1 id="recommendations">Recommendations</h1>

<!-- BEGIN-INCLUDE ./repo/ocrd-website.wiki/Workflow-Guide-recommendations.md -->
<p>In order to facilitate the usage of OCR-D and the configuration of workflows, we provide two workflows
which can be used as a start for your OCR-D-tests. They were determined by testing the processors listed
above on selected pages of some prints from the 17th and 18th century.</p>

<p>The results vary quite a lot from page to page. In most cases, segmentation is a problem.</p>

<p>Note that for our test pages, not all steps described above werde needed to obtain the best results.
Depending on your particular images, you might want to include those processors again for better results.</p>

<p>We are currently working on regression tests with the help of which we will be able to provide more profound
workflows soon, which will replace those interim solutions.</p>

<h2 id="minimal-workflow">Minimal workflow</h2>

<p>Since <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code> can do binarization (Otsu), region
segmentation, table recognition, line segmentation and text recognition at once, just like the
upstream <code class="language-plaintext highlighter-rouge">tesseract</code> command line tool, it’s a good single-step workflow to get
a baseline result to compare to granular workflows.</p>

<p><strong>Note:</strong> Be aware that you will most likely obtain significantly better
results by configuring a more granular workflow like e.g. the
<a href="#best-results-for-selected-pages">workflows</a>
<a href="#good-results-for-slower-processors">below</a>.</p>

<table class="processor-table">
  <thead>
    <tr>
      <th>Step</th>
      <th>Processor</th>
      <th>Parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>ocrd-tesserocr-recognize</td>
      <td>-P segmentation_level region -P textequiv_level word -P find_tables true -P model Fraktur_GT4HistOCR</td>
    </tr>
  </tbody>
</table>

<h3 id="example-with-ocrd-process">Example with ocrd-process</h3>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ocrd process <span class="s2">"tesserocr-recognize -P segmentation_level region -P textequiv_level word -P find_tables true -P model GT4HistOCR_50000000.997_191951"</span>
</code></pre></div></div>

<h2 id="best-results-for-selected-pages">Best results for selected pages</h2>

<p>The following workflow has produced best results for ‘simple’ pages (e.g. <a href="https://ocr-d-repo.scc.kit.edu/api/v1/dataresources/dda89351-7596-46eb-9736-593a5e9593d3/data/bagit/data/OCR-D-IMG/OCR-D-IMG_0004.tif">this
page</a>)  (CER ~1%).</p>

<table class="processor-table">
  <thead>
    <tr>
      <th>Step</th>
      <th>Processor</th>
      <th>Parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="#step-1-binarization-page-level">1</a></td>
      <td>ocrd-cis-ocropy-binarize</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-2-cropping-page-level">2</a></td>
      <td>ocrd-anybaseocr-crop</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-3-binarization-page-level">3</a></td>
      <td>ocrd-skimage-binarize</td>
      <td>-P method li</td>
    </tr>
    <tr>
      <td><a href="#step-4-denoising-page-level">4</a></td>
      <td>ocrd-skimage-denoise</td>
      <td>P level-of-operation page</td>
    </tr>
    <tr>
      <td><a href="#step-5-deskewing-page-level">5</a></td>
      <td>ocrd-tesserocr-deskew</td>
      <td>-P level-of-operation page</td>
    </tr>
    <tr>
      <td><a href="#step-7-region-segmentation">7</a></td>
      <td>ocrd-cis-ocropy-segment</td>
      <td>-P level-of-operation page</td>
    </tr>
    <tr>
      <td><a href="#step-13-dewarping-line-level">13</a></td>
      <td>ocrd-cis-ocropy-dewarp</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-14-text-recognition">14</a></td>
      <td>ocrd-calamari-recognize</td>
      <td>-P checkpoint_dir qurator-gt4histocr-1.0</td>
    </tr>
  </tbody>
</table>

<h3 id="example-with-ocrd-process-1">Example with ocrd-process</h3>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ocrd process <span class="se">\</span>
  <span class="s2">"cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN"</span> <span class="se">\</span>
  <span class="s2">"anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP"</span> <span class="se">\</span>
  <span class="s2">"skimage-binarize -I OCR-D-CROP -O OCR-D-BIN2 -P method li"</span> <span class="se">\</span>
  <span class="s2">"skimage-denoise -I OCR-D-BIN2 -O OCR-D-BIN-DENOISE -P level-of-operation page"</span> <span class="se">\</span>
  <span class="s2">"tesserocr-deskew -I OCR-D-BIN-DENOISE -O OCR-D-BIN-DENOISE-DESKEW -P operation_level page"</span> <span class="se">\</span>
  <span class="s2">"cis-ocropy-segment -I OCR-D-BIN-DENOISE-DESKEW -O OCR-D-SEG -P level-of-operation page"</span> <span class="se">\</span>
  <span class="s2">"cis-ocropy-dewarp -I OCR-D-SEG -O OCR-D-SEG-LINE-RESEG-DEWARP"</span> <span class="se">\</span>
  <span class="s2">"calamari-recognize -I OCR-D-SEG-LINE-RESEG-DEWARP -O OCR-D-OCR -P checkpoint_dir qurator-gt4histocr-1.0"</span>
</code></pre></div></div>

<p><strong>Note:</strong>
(1) This workflow expects your images to be stored in a folder called <code class="language-plaintext highlighter-rouge">OCR-D-IMG</code>. If your images are saved in a different folder,
you need to adjust <code class="language-plaintext highlighter-rouge">-I OCR-D-IMG</code> in the second line of the call above with the name of your folder, e.g. <code class="language-plaintext highlighter-rouge">-I MAX</code>
(2) For the last processor in this workflow, <code class="language-plaintext highlighter-rouge">ocrd-calamari-recognize</code>, you need to specify the model which is to be used. 
If you didn’t download it via the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>, you have to use the <code class="language-plaintext highlighter-rouge">checkpoint</code> parameter
and pass your local path to the model on your hard drive as parameter value! In this case, the last line of the <code class="language-plaintext highlighter-rouge">ocrd-process</code> call above could e.g. look like this:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="s2">"calamari-recognize -I OCR-D-SEG-LINE-RESEG-DEWARP -O OCR-D-OCR -P checkpoint /test/data/calamari_models/</span><span class="se">\*</span><span class="s2">.ckpt.json"</span>
</code></pre></div></div>
<p>All the other lines can just be copied and pasted.</p>

<h2 id="good-results-for-slower-processors">Good results for slower processors</h2>

<p>If your computer is not that powerful you may try this workflow. It works fine for simple pages and produces also good results in shorter time.</p>

<table class="processor-table">
  <thead>
    <tr>
      <th>Step</th>
      <th>Processor</th>
      <th>Parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="#step-1-binarization-page-level">1</a></td>
      <td>ocrd-cis-ocropy-binarize</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-2-cropping-page-level">2</a></td>
      <td>ocrd-anybaseocr-crop</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-3-binarization-page-level">3</a></td>
      <td>ocrd-skimage-denoise</td>
      <td>-P level-of-operation page</td>
    </tr>
    <tr>
      <td><a href="#step-5-deskewing-page-level">5</a></td>
      <td>ocrd-tesserocr-deskew</td>
      <td>-P level-of-operation page</td>
    </tr>
    <tr>
      <td><a href="#step-7-region-segmentation">7</a></td>
      <td>ocrd-tesserocr-segment</td>
      <td>-P shrink_polygons true</td>
    </tr>
    <tr>
      <td><a href="#step-13-dewarping-line-level">13</a></td>
      <td>ocrd-cis-ocropy-dewarp</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="#step-14-text-recognition">14</a></td>
      <td>ocrd-tesserocr-recognize</td>
      <td>-P textequiv_level glyph -P overwrite_segments true -P model GT4HistOCR_50000000.997_191951</td>
    </tr>
  </tbody>
</table>

<h3 id="example-with-ocrd-process-2">Example with ocrd-process</h3>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ocrd process <span class="se">\</span>
  <span class="s2">"cis-ocropy-binarize -I OCR-D-IMG -O OCR-D-BIN"</span> <span class="se">\</span>
  <span class="s2">"anybaseocr-crop -I OCR-D-BIN -O OCR-D-CROP"</span> <span class="se">\</span>
  <span class="s2">"skimage-denoise -I OCR-D-CROP -O OCR-D-BIN-DENOISE -P level-of-operation page"</span> <span class="se">\</span>
  <span class="s2">"tesserocr-deskew -I OCR-D-BIN-DENOISE -O OCR-D-BIN-DENOISE-DESKEW -P operation_level page"</span> <span class="se">\</span>
  <span class="s2">"tesserocr-segment -I OCR-D-BIN-DENOISE-DESKEW -O OCR-D-SEG -P shrink_polygons true"</span> <span class="se">\</span>
  <span class="s2">"cis-ocropy-dewarp -I OCR-D-SEG -O OCR-D-SEG-DEWARP"</span> <span class="se">\</span>
  <span class="s2">"tesserocr-recognize -I OCR-D-SEG-DEWARP -O OCR-D-OCR -P textequiv_level glyph -P overwrite_segments true -P model GT4HistOCR_50000000.997_191951"</span>
</code></pre></div></div>

<p><strong>Note:</strong>
(1) This workflow expects your images to be stored in a folder called <code class="language-plaintext highlighter-rouge">OCR-D-IMG</code>. If your images are saved in a different folder,
you need to adjust <code class="language-plaintext highlighter-rouge">-I OCR-D-IMG</code> in the second line of the call above with the name of your folder, e.g. <code class="language-plaintext highlighter-rouge">-I my_images</code>
(2) For the last processor in this workflow, <code class="language-plaintext highlighter-rouge">ocrd-tesserocr-recognize</code>, the environment variable TESSDATA_PREFIX has to be
set to point to the directory where the used models are stored if they are not in the default location. If you downloaded your models
with the <a href="https://ocr-d.de/en/models">OCR-D resource manager</a>, this is already taken care of.</p>

<!-- END-INCLUDE -->

<script src="/js/workflows.js"></script>


      </main>
    </div><footer class="footer" style="padding: 1rem">
    <div class="content has-text-centered">
      <img class="footer-logo" src="/assets/dfg_logo_eng.jpg" alt="DFG logo"/>
    </div>
    <!-- <div class="content has-text-centered"> -->
    <!--   <img class="footer-logo" src="/assets/logo-bbaw.png" alt="BBAW logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-hab.gif" alt="HAB logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-kit.png" alt="KIT logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-sbb.png" alt="SBB logo"/> -->
    <!-- </div> -->
    <div class="content has-text-centered">
		<a href="https://github.com/OCR-D">GitHub</a>
		|
		<a href="https://gitter.im/OCR-D/Lobby">Gitter</a>
		|
		<a href="https://twitter.com/OCR_D_community">Twitter</a>
		|
		<a href="https://github.com/OCR-D/ocrd-website/wiki">Wiki</a>
		|
		<a href="https://hub.docker.com/u/ocrd">Docker Hub</a>
		|
		<a href="https://www.zotero.org/groups/418719/ocr-d">Technology Watch</a>
		|
		<a href="/sitemap.xml">sitemap.xml</a>
		|
		
			<a href="/en/imprint">Imprint</a>
		
    </div>

<script src="/assets/script.js"></script>
</footer>
</body>

</html>
