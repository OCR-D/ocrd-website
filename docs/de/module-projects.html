<!DOCTYPE html>
<html lang="de"><head>
  <meta charset="utf-8"/>
  <title></title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="https://avatars0.githubusercontent.com/u/26362587?s=200&amp;v=4" />
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous" />
  <link rel="alternate" type="application/atom+xml" title="OCR-D Blog" href="/feed.xml" />
  <link rel="stylesheet" href="/assets/bulma.css" />
  <link rel="stylesheet" href="/assets/bulma-switch.min.css" />
  <link rel="stylesheet" href="/assets/syntax-highlight.css" />
  <link rel="stylesheet" href="/assets/ocrd.css" />
</head>
<body><nav class="navbar is-transparent is-fixed-top">

  <div class="navbar-brand">
    <a class="navbar-item" href="/">
      <img src="/assets/ocrd-logo-small.png" height="28"/>
    </a>
    <div class="navbar-burger burger" data-target="ocrd-navbar-menu">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>

  <div id="ocrd-navbar-menu" class="navbar-menu">
    <div class="navbar-start">
      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/de/">Über</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/de/blog">Aktuelles</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/about">Das OCR-D-Projekt</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/phase3">OCR-D Phase III</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/contact">Kontaktieren Sie uns!</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/platforms">Plattformen</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/publications">Publikationen und Vorträge</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/module-projects">Modulprojekte</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/data">Daten</a>

          

          

          
          
            
            
              
              <a class="navbar-item" href="/de/teststellung">Erste Teststellung</a>
            

          

          

          
          

            
            <a class="navbar-item" href="/de/user_survey">Nutzer*innenumfrage</a>

          

          

          
          
            
            
              
              <a class="navbar-item" href="/de/impressum">Impressum</a>
            

          

          
        </div>
      </div>
      
      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/de/dev">Entwickler*innen</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/de/gt-guidelines/trans">Ground Truth Richtlinien</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/gt-guidelines/trans/trPage">PAGE-XML Formatdokumentation</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/dev-best-practice">Best Practices für Softwareentwicklung in OCR-D</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/spec">Spezifikationen</a>

          

          

          
          

            
            <a class="navbar-item" href="/core">OCR-D/core API Dokumentation</a>

          

          
        </div>
      </div>
      
      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/de/use">Anwender*innen</a>
        <div class="navbar-dropdown">
          

          
          
            
            
              <a class="navbar-item" href="https://translate.google.com/translate?hl=&sl=en&tl=de&u=https%3A%2F%2Focr-d.de%2Fen%2Fsetup">Setup Anleitung</a>
            

          

          

          
          
            
            
              <a class="navbar-item" href="https://translate.google.com/translate?hl=&sl=en&tl=de&u=https%3A%2F%2Focr-d.de%2Fen%2Fuser_guide">Nutzeranleitung</a>
            

          

          

          
          
            
            
              <a class="navbar-item" href="https://translate.google.com/translate?hl=&sl=en&tl=de&u=https%3A%2F%2Focr-d.de%2Fen%2Fworkflows">Workflows</a>
            

          

          

          
          

            
            <a class="navbar-item" href="/en/models">Modelle</a>

          

          

          
          

            
            <a class="navbar-item" href="/de/spec/glossary">Glossar</a>

          

          
        </div>
      </div>
      
      
      
        <a class="navbar-item" href="/de/faq">FAQ</a>
      
      

    </div>

    <div class="navbar-end">

      <span class="navbar-item">
         <a href="/en/module-projects.html" title="Auf Englisch lesen">
              en
            </a> 
      </span>

    </div> </div> </nav>
<div class="columns">
      
      <aside id="toc-sidebar-content" class="column is-one-third menu is-hidden-mobile">
        <ul class="menu-list column is-one-third">
  <li><a href="#modulprojekte">Modulprojekte</a>
    <ul>
      <li><a href="#skalierbare-verfahren-der-text--und-strukturerkennung-für-die-volltextdigitalisierung-historischer-drucke-bildoptimierung">Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Bildoptimierung</a></li>
      <li><a href="#skalierbare-verfahren-der-text--und-strukturerkennung-für-die-volltextdigitalisierung-historischer-drucke-layouterkennung">Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Layouterkennung</a></li>
      <li><a href="#weiterentwicklung-eines-semi-automatischen-open-source-tools-zur-layout-analyse-und-regionen-extraktion-und--klassifikation-larex-von-frühen-buchdrucken">Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken</a></li>
      <li><a href="#nnfst--unsupervised-ocr-postcorrection-based-on-neural-networks-and-finite-state-transducers">NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers</a></li>
      <li><a href="#optimierter-einsatz-von-ocr-verfahren--tesseract-als-komponente-im-ocr-d-workflow">Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow</a></li>
      <li><a href="#automatische-nachkorrektur-historischer-ocr-erfasster-drucke-mit-integrierter-optionaler-interaktiver-korrektur">Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur</a></li>
      <li><a href="#entwicklung-eines-modellrepositoriums-und-einer-automatischen-schriftarterkennung-für-ocr-d">Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D</a></li>
      <li><a href="#ola-hd--ein-ocr-d-langzeitarchiv-für-historische-drucke">OLA-HD – Ein OCR-D-Langzeitarchiv für historische Drucke</a></li>
    </ul>
  </li>
</ul>

      </aside>
      <div id="toc-sidebar-toggle">&lt;&gt;</div>
      

      <main class="container content column is-two-thirds" aria-label="Content">
        <h1 id="modulprojekte">Modulprojekte</h1>

<p>Aus den Projektanträgen für die Modulprojektausschreibung der DFG im März 2017 wurden acht Projekte bewilligt:</p>

<h2 id="skalierbare-verfahren-der-text--und-strukturerkennung-für-die-volltextdigitalisierung-historischer-drucke-bildoptimierung">Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Bildoptimierung</h2>

<p class="poster-image">
  <a href="/assets/poster/DFKI.pdf">
    <img src="/assets/poster/DFKI.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)</em></p>

<p>Das DFKI war als Projektpartner im OCR-D Projekt mit zwei Modulen vertreten:
Bildoptimierung und Layouterkennung. In beiden Modulen wurden mehrere
Prozessoren entwickelt und in das OCR-D-Softwaresystem integriert.</p>

<p>Das erste Modul-Projekt <em>Bildoptimierung</em> fokussierte sich auf die
Vorverarbeitung der Digitalisate mit dem Ziel, die Bildqualität und somit auch
die Performanz der nachfolgenden OCR-Module zu verbessern. Dafür wurden
Werkzeuge für die Binarisierung, das Deskewing, das Cropping und das Dewarping
implementiert.</p>

<p>Das auf Computer Vision basierte Cropping-Werkzeug ist als besonders performant
hervorzuheben. Es erzielt auf den gesamten Projektdaten vorwiegend sehr gute
Ergebnisse. Auch das Dewarping-Werkzeug ist aufgrund seiner neuartigen
Architektur interessant. Mit Hilfe generativer neuronaler Netze werden
entzerrte Varianten von Bildern generiert, anstatt explizite Transformationen
für die Entzerrung zu bestimmen.</p>

<h2 id="skalierbare-verfahren-der-text--und-strukturerkennung-für-die-volltextdigitalisierung-historischer-drucke-layouterkennung">Skalierbare Verfahren der Text- und Strukturerkennung für die Volltextdigitalisierung historischer Drucke: Layouterkennung</h2>

<p class="poster-image">
  <a href="/assets/poster/DFKI.pdf">
    <img src="/assets/poster/DFKI.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>DFKI</em></p>

<p>GitHub: <a href="https://github.com/mjenckel/OCR-D-LAYoutERkennung/tree/master">mjenckel/OCR-D-LAYoutERkennung/tree/master</a></p>

<p>Im zweiten Modul-Projekt des DFKI <em>Layouterkennung</em> galt es, die
Dokumentstruktur, sowohl einzelner Dokumentseiten als auch im Gesamtdokument,
zu extrahieren. Die dabei gewonnenen Metadaten helfen zum einen, das Dokument
als Ganzes zu digitalisieren, zum anderen ist das Extrahieren bestimmter
Dokumentstrukturen notwendig. Die meisten OCR-Methoden können z.B. nur einzelne
Textzeilen verarbeiten. Die entwickelten Werkzeuge dienen der
Text-Nicht-Text-Segmentierung, der Blocksegmentierung und -klassifizierung, der
Textzeilenerkennung sowie der Strukturanalyse.</p>

<p>Ein Entwicklungsschwerpunkt war die kombinierte Blocksegmentierung und
-klassifizierung, welche auf der, aus der Video- und Bildsegmentierung
bekannten, MaskRCNN-Architektur basiert. Dieses Werkzeug arbeitet mit den
unbearbeiteten Rohdaten, sodass einerseits keine Vorverarbeitung notwendig ist
und andererseits das volle Informationsspektrum ausgenutzt werden kann.</p>

<h2 id="weiterentwicklung-eines-semi-automatischen-open-source-tools-zur-layout-analyse-und-regionen-extraktion-und--klassifikation-larex-von-frühen-buchdrucken">Weiterentwicklung eines semi-automatischen Open-Source-Tools zur Layout-Analyse und Regionen-Extraktion und -Klassifikation (LAREX) von frühen Buchdrucken</h2>

<p class="poster-image">
  <a href="/assets/poster/Wuerzburg.pdf">
    <img src="/assets/poster/Wuerzburg.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Julius-Maximilians-Universität Würzburg</em> <br />
<em>Institut für Informatik: Lehrstuhl für Künstliche Intelligenz und angewandte Informatik</em></p>

<p>GitHub: <a href="https://github.com/ocr-d-modul-2-segmentierung">ocr-d-modul-2-segmentierung</a></p>

<p>Am Lehrstuhl für Informatik VI der Uni Würzburg wurde in den Vorarbeiten LAREX
entwickelt, ein komfortabler Editor zur Annotation von Regionen und
Layout-Elementen auf Buchseiten. Bei der Weiterentwicklung im
OCR-D-Modulprojekt lag der Schwerpunkt neben der Verbesserung der effizienten
Bedienbarkeit vor allem auch in dem Ausbau der automatischen Verfahren.</p>

<p>Hierzu wurde ein Convolutional-Neural-Net (CNN) implementiert und trainiert,
welches jedem Pixel eines Seitenscans eine Einordnung in verschiedene Klassen
zuweist, um so Bild und Text zu trennen. Unter Betrachtung der Pixel je nur
einer Klasse wird anschließend mit klassischen Verfahren eine Segmentierung der
Seite durchgeführt. Ein weiterer getesteter Ansatz nutzte zuerst klassische
Segmentierungsverfahren und ordnete die Segmente anschließend ein.</p>

<p>Das auf der CNN-Ausgabe basierende Segmentierungsverfahren wurde an die
OCR-D-Schnittstellen angepasst. Auf reinen Textseiten oder Seiten mit deutlich
abgetrennten Bildern wurden gute Ergebnisse erzielt. Verbesserungspotential
besteht vor allem bei der Erkennung von Zierinitialen älterer Drucke und
weiteren nah am Text liegenden Bildern sowie mehrspaltigen Layouts.</p>

<div class="is-clearfix"></div>
<h2 id="nnfst--unsupervised-ocr-postcorrection-based-on-neural-networks-and-finite-state-transducers">NN/FST – Unsupervised OCR-Postcorrection based on Neural Networks and Finite-state Transducers</h2>

<p class="poster-image">
  <a href="/assets/poster/Leipzig.pdf">
    <img src="/assets/poster/Leipzig.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Universität Leipzig</em>  <br />
 Institut für Informatik: Abteilung Automatische Sprachverarbeitung_</p>

<p>GitHub: <a href="https://github.com/ASVLeipzig/cor-asv-fst">ASVLeipzig/cor-asv-fst</a></p>

<p>Eine vollautomatische Nachkorrektur separat von der eigentlichen OCR ist immer
nur dann sinnvoll, wenn dabei statistisches Wissen über “richtigen Text” und
über typische OCR-Fehler <em>a priori</em> hinzukommt. Dafür eignen sich neuronale
Netze (NN) ebenso wie gewichtete endliche Transduktoren (WFST), die auf
entsprechenden zusätzlichen Daten trainiert werden können.</p>

<p>Für die Umsetzung einer kombinierten Architektur aus NN und FST wurde
entschieden, drei Module zu implementieren:</p>
<ol>
  <li>eine reine NN-Lösung mit durchgehend (<em>end-to-end</em>) trainiertem Modell
  allein auf Zeichenebene – als tiefes (mehrschichtiges), bidirektionales
  rekurrentes Netzwerk nach dem Encoder-Decoder-Schema (für verschiedene
  Eingabe- und Ausgabelänge) mit Attention-Mechanismus und A*-Beamsearch mit
  einstellbarer Rückweisungsschwelle (gegen Überkorrektur), d.h. die
  Nachkorrektur von Textzeilen wird wie maschinelle Übersetzung behandelt,</li>
  <li>ein NN-Sprachmodell (LM) auf Zeichenebene – als tiefes (mehrschichtiges),
  bidirektionales rekurrentes Netzwerk mit Schnittstelle für Graph-Eingabe
  und inkrementeller Dekodierung,</li>
  <li>eine WFST-Komponente mit explizit zu trainierendem Fehlermodell auf
  Zeichenebene und Wortmodell/Lexikon, sowie Anbindung an 2. – per
  WFST-Komposition von Eingabegraph mit Fehler- und Wortmodell nach
  Sliding-Window-Prinzip, Konversion der Einzelfenster zu einem
  Hypothesengraph pro Textzeile, und Kombination der jeweiligen
  Ausgabegewichte mit LM-Bewertungen in einer effizienten Suche nach dem
  besten Pfad.</li>
</ol>

<p>Die Kombination von 3. mit 2. stellt also eine hybride Lösung dar. Aber auch 1.
kann von 2. profitieren (sofern die gleiche Netzwerk-Topologie benutzt wird),
indem die Gewichte aus einem auf größeren Mengen reinem Text trainierten
Sprachmodell initialisiert werden (Transfer-Learning).</p>

<p>Beide Ansätze profitieren von einer engen Anbindung an den OCR-Suchraum, d.h.
eine Übergabe alternativer Zeichen-Hypothesen und ihrer Konfidenz (wie bisher
nur mit Tesseract möglich und in Zusammenarbeit mit dem Modulprojekt der UB
Mannheim realisiert). Sie liefern aber auch auf reinem Volltext bereits gute
Ergebnisse (mit CER-Reduktion von bis zu 5%), sofern genügend passende
Trainingsdaten zur Verfügung stehen und die OCR ihrerseits brauchbare
Ergebnisse (unterhalb 10% CER) liefert.</p>

<p>Für alle Module stehen Kommandozeilen-Schnittstellen für Training und
Evaluierung, sowie volle OCR-D-Schnittstellen für Prozessierung und Evaluierung
zur Verfügung.</p>

<div class="is-clearfix"></div>

<h2 id="optimierter-einsatz-von-ocr-verfahren--tesseract-als-komponente-im-ocr-d-workflow">Optimierter Einsatz von OCR-Verfahren – Tesseract als Komponente im OCR-D-Workflow</h2>

<p class="poster-image">
  <a href="/assets/poster/Mannheim.pdf">
    <img src="/assets/poster/Mannheim.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Universität Mannheim</em><br />
<em>Universitätsbibliothek Mannheim</em></p>

<p>GitHub: <a href="http://github.com/tesseract-ocr/tesseract/" title="http://github.com/tesseract-ocr/tesseract/">tesseract-ocr/tesseract/</a></p>

<p>Im Fokus des Modulprojekts stand die OCR-Software Tesseract, die seit 1985 von
Ray Smith entwickelt wurde, seit 2005 als Open Source unter einer freien
Lizenz.</p>

<p>Das Projekt umfasste zwei Hauptziele: Die Einbindung von Tesseract in den
OCR-D-Workflow inklusive Unterstützung der anderen Modulprojekte durch die
Bereitstellung von Schnittstellen, sowie die allgemeine Verbesserung der
Stabilität, Codequalität und Performance von Tesseract.</p>

<p>Die Einbindung in den OCR-D-Workflow erforderte wesentlich weniger Aufwand als
ursprünglich geplant; hauptsächlich, weil die meiste Arbeit bereits außerhalb
des Modulprojekts geleistet war und dabei die schon vorhandene
Python-Schnittstelle tesserocr genutzt werden konnte.</p>

<p>Für das OCR-D-Modulprojekt der Universität Leipzig wurde Tesseract um die
Generierung von alternativen OCR-Ergebnissen für die Einzelzeichen erweitert.
Als Eingabedaten für ein OCR-Postkorrektur-Modell lässt sich damit die
Texterkennung weiter verbessern. Ein wertvoller Nebeneffekt des neuen Codes
sind genauere Zeichen- und Wortkoordinaten.</p>

<p>Mit mehreren hundert Korrekturen konnte die Codequalität signifikant gesteigert
und ein deutlich stabilerer Programmfluss erreicht werden. Tesseract ist jetzt
wartbarer, braucht weniger Speicher und ist schneller als zuvor.</p>

<p>Eine wesentliche Verbesserung der Erkennungsgenauigkeit für die meisten der für
OCR-D relevanten Druckwerke konnte durch neue generische Modelle für Tesseract
erreicht werden. Diese wurden ab Septem-ber 2019 bis Januar 2020 auf Basis der
Datensammlung <a href="https://zenodo.org/record/1344132"><em>GT4HistOCR</em></a> trainiert.</p>

<div class="is-clearfix"></div>

<h2 id="automatische-nachkorrektur-historischer-ocr-erfasster-drucke-mit-integrierter-optionaler-interaktiver-korrektur">Automatische Nachkorrektur historischer OCR-erfasster Drucke mit integrierter optionaler interaktiver Korrektur</h2>

<p class="poster-image">
  <a href="/assets/poster/München.pdf">
    <img src="/assets/poster/München.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Ludwig-Maximilians-Universität München</em><br />
<em>Centrum für Informations- und Sprachverarbeitung (CIS)</em></p>

<p>GitHub: cisocrgroup/ocrd-postcorrection](https://github.com/cisocrgroup/ocrd-postcorrection), <a href="https://github.com/cisocrgroup/cis-ocrd-py">cisocrgroup/cis-ocrd-py</a></p>

<p>Das Ergebnis des Projekts ist ein in den OCR-D-Workflow integriertes System
<em>A-I-PoCoTo</em> zur vollautomati-schen Nachkorrektur OCR-erfasster historischer
Drucke. Das System beinhaltet zudem eine optional nachge-schaltete interaktive
Nachkorrektur (<em>I-PoCoTo</em>), die in das interaktive Nachkorrektursystem
<em>PoCoWeb</em> einge-bunden ist. Das System kann damit auch alternativ als
Stand-Alone-Tool zur gemeinschaftlichen webbasierten Nachkorrektur von
OCR-Dokumenten eingesetzt werden.</p>

<p>Die Grundlage der vollautomatischen Nachkorrektur ist ein flexibles,
featurebasiertes Machine-Learning (ML) Verfahren zur vollautomatischen
OCR-Nachkorrektur mit einem besonderen Fokus auf die Vermeidung der
Verschlimmbesserungsproblematik. Zur Erkennung von Fehlern und für die
Erzeugung von Korrekturkandida-ten verwendet das System die am CIS entwickelte
dokumentenabhängige Profilierungstechnologie. Die Fea-tures des Systems
verwenden neben verschiedenen Konfidenzwerten insbesondere auch Informationen
aus zusätzlichen Hilfs-OCRs.</p>

<p>Das System protokolliert sämtliche Korrekturentscheidungen. Über diesen
Protokollmechanismus kann die automatische Postkorrektur in <em>PoCoWeb</em>
interaktiv überprüft werden. Dabei können sowohl einzelne getätigte
Korrekturentscheidungen manuell rückgängig gemacht werden, als auch nicht
getätigte Korrekturentschei-dungen nachträglich ausgeführt werden.</p>

<p>Das gesamte System ist in den OCR-D-Workflow eingebunden und folgt den dort
gültigen Konventionen.</p>

<div class="is-clearfix"></div>

<h2 id="entwicklung-eines-modellrepositoriums-und-einer-automatischen-schriftarterkennung-für-ocr-d">Entwicklung eines Modellrepositoriums und einer Automatischen Schriftarterkennung für OCR-D</h2>

<p class="poster-image">
  <a href="/assets/poster/Mainz.pdf">
    <img src="/assets/poster/Mainz.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Universität Leipzig</em><br />
<em>Institut für Informatik: Lehrstuhl für Digital Humanities</em><br />
<em>Friedrich-Alexander-Universität Erlangen-Nürnberg</em> <br />
<em>Department Informatik: Lehrstuhl für Informatik 5: Mustererkennung</em><br />
<em>Johannes Gutenberg-Universität Mainz</em><br />
 Gutenberg-Institut für Weltliteratur und schriftorientierte Medien: Abteilung Buchwissenschaft_</p>

<p>GitHub: <a href="https://github.com/OCR-D/okralact">OCR-D/okralact</a>, <a href="https://github.com/seuretm/ocrd_typegroups_classifier">seuretm/ocrd_typegroups_classifier</a></p>

<p>Die Erkennungsquoten von OCR für Drucke, die vor 1800 produziert wurden,
variieren sehr stark, da die Diversität historischer Schriftarten in den
Trainingsdaten entweder gar nicht oder nur unzureichend berücksichtigt wird.
Daher hat sich dieses Modulprojekt, bestehend aus Informatiker<em>innen und
Buchhistoriker</em>innen, drei Ziele gesteckt:</p>

<p>Zum einen haben wir ein Tool zur automatischen Erkennung von Schriftarten in
Bilddigitalisaten entwickelt. Hier haben wir uns besonders auf gebrochene
Schriften neben der Fraktur konzentriert, die bisher wenig Beachtung gefunden
haben, jedoch im 15. und 16. Jahrhundert weit verbreitet waren: Bastarda,
Rotunda, Textura und Schwabacher. Das Tool wurde mit 35.000 Bildern trainiert
und erreicht eine Genauigkeit von 98% bei der Bestimmung von Schriftarten.
Insgesamt kann es nicht nur zwischen den o.g. Schriftarten differenzieren,
sondern auch Hebräisch, Griechisch, Fraktur, Antiqua und Kursiv unterscheiden.</p>

<p>In einem zweiten Schritt wurde eine Online-Trainingsinfrastruktur geschaffen
(Okralact). Sie vereinfacht die Benutzung verschiedener OCR-engines (Tesseract,
Ocropus, Kraken, Calamari) und ermöglicht es zugleich, spezifische Modelle für
bestimmte Schriftarten zu trainieren.</p>

<p>Zum Abschluss wurde ein Modellrepositorium eingerichtet, das bereits
erarbeitete schriftartspezifische OCR-Modelle enthält. Um hier einen Grundstock
zu legen, haben wir insgesamt ca. 2.500 Zeilen für Bastarda, Textura und
Schwabacher aus einer Vielzahl verschiedener Bücher transkribiert.</p>

<p>Die hohe Genauigkeit des Tools zur Erkennung der Schriftarten eröffnet die
Möglichkeit, in Zukunft durch weitere Trainingsdaten das Tool sogar zwischen
den Schriften einzelner Drucker unterscheiden zu lassen, was mehrere Desiderate
der historischen Forschung adressieren würde.</p>

<div class="is-clearfix"></div>

<h2 id="ola-hd--ein-ocr-d-langzeitarchiv-für-historische-drucke">OLA-HD – Ein OCR-D-Langzeitarchiv für historische Drucke</h2>

<p class="poster-image">
  <a href="/assets/poster/Göttingen.pdf">
    <img src="/assets/poster/Göttingen.png" style="height: 400px" title="Klicken für PDF-Version in Originalgröße" />
  </a>
</p>

<p><em>Georg-August-Universität Göttingen</em>  <br />
<em>Niedersächsische Staats- und Universitätsbibliothek</em>   <br />
<em>Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen</em> <br /></p>

<p>GitHub: <a href="https://github.com/subugoe/OLA-HD-IMPL">subugoe/OLA-HD-IMPL</a></p>

<p>Im September 2018 starteten die Abteilung Digitale Bibliothek der
Niedersächsischen Staats- und Universi-tätsbibliothek und die Gesellschaft für
wissenschaftliche Datenverarbeitung Göttingen das DFG-Projekt <a href="https://www.sub.uni-goettingen.de/projekte-forschung/projektdetails/projekt/ola-hd-ein-ocr-d-langzeitarchiv-fuer-historische-drucke/"><em>OLA-HD – Ein
OCR-D Langzeitarchiv für historische
Drucke</em></a>.</p>

<p>Ziel von OLA-HD ist die Entwicklung eines integrierten Konzepts für die
Langzeitarchivierung und persistente Identifizierung von OCR-Objekten, sowie
eine prototypische Implementierung.</p>

<p>Im regelmäßigen Austausch mit den Projektpartnern wurden die
Basis-Anforderungen für die Langzeitarchivierung und persistente Identifikation
ermittelt und in Form einer Spezifikation zur technischen und
wirtschaftlich-organisatorischen Umsetzung festgehalten.</p>

<p>Mit dem Prototypen kann der Anwender OCR-Ergebnisse eines Werkes als OCRD-ZIP
in das System laden. Das System validiert die Zip-Datei, vergibt eine PID und
schickt die Datei an den Archiv-Manager <a href="https://cdstar.gwdg.de/">(CDSTAR – GWDG Common Data Storage
Architecture)</a>. Dieser schreibt die Zip-Datei in das
Archiv (Bandspeicher). Abhängig von der Konfiguration (Datei-Typ, Datei-Größe
etc.) werden Dateien zusätzlich in ein Online Storage geschrieben (Festplatte),
um einen schnellen Zugriff zu ermöglichen. Der Nutzer hat Zugriff auf alle
OCR-Versionen und kann Versionen als BagIt-Zip Dateien herunterladen. Alle
Werke und Versionen haben eigene PIDs. Die PIDs werden vom European Persistent
Identifier Consortium <a href="https://www.pidconsortium.net/">(ePIC)</a> Service
generiert. Die verschiedenen OCR-Versionen eines Werkes sind über die PID
verknüpft, sodass das System die Versionierung in einer Baumstruktur abbilden
kann.</p>

<p>Nicht angemeldete Anwender können den Bestand durchsuchen und in der
Dateistruktur eine Vorschau von Text und – sofern vorhanden – Bild erhalten
bzw. über die verschiedenen Versionen navigieren. Die Anwender können sich über
das GWDG-Portal registrieren und anmelden und können über ein Dashboard ihre
Dateien verwalten.</p>

<p>Bis März 2020 werden kleinere Optimierungen am User-Interface vorgenommen und
das Konzept finalisiert. Im Konzept werden weitere Ausbaustufen beschrieben,
die sinnvoll sein können, um die prototypische Soft-ware in ein Produkt zu
überführen.</p>

      </main>
    </div><footer class="footer" style="padding: 1rem">
    <div class="content has-text-centered">
      <img class="footer-logo" src="/assets/dfg_logo_ger.jpg" alt="DFG logo"/>
    </div>
    <!-- <div class="content has-text-centered"> -->
    <!--   <img class="footer-logo" src="/assets/logo-bbaw.png" alt="BBAW logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-hab.gif" alt="HAB logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-kit.png" alt="KIT logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-sbb.png" alt="SBB logo"/> -->
    <!-- </div> -->
    <div class="content has-text-centered">
      <a href="https://github.com/OCR-D">GitHub</a>
      |
      <a href="https://gitter.im/OCR-D/Lobby">gitter</a>
      |
      <a href="https://github.com/OCR-D/ocrd-website/wiki">Wiki</a>
      |
      <a href="https://hub.docker.com/u/ocrd">Docker Hub</a>
      |
      <a href="https://www.zotero.org/groups/418719/ocr-d">Technology Watch</a>
      |
      <a href="/sitemap.xml">sitemap.xml</a>
    </div>

<script src="/assets/script.js"></script>
</footer>
</body>

</html>
