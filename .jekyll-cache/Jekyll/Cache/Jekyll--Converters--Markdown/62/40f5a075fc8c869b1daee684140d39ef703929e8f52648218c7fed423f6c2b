I"©K<h1 id="ocr-d-glossary">OCR-D Glossary</h1>

<blockquote>
  <p>Glossary of terms from the domain of image processing/OCR and how they are used within the OCR-D framework</p>
</blockquote>

<p>This section is non-normative.</p>

<h2 id="layout-and-typography">Layout and Typography</h2>

<h3 id="block">Block</h3>

<p>See <a href="#region">Region</a></p>

<h3 id="border">Border</h3>

<p>From the <a href="https://ocr-d.de/de/gt-guidelines/pagexml/pagecontent_xsd_Complex_Type_pc_BorderType.html">PAGE-XML content schema documentation</a></p>

<blockquote>
  <p>Border of the actual page (if the scanned image contains parts not belonging to the page).</p>
</blockquote>

<h3 id="font-family">Font family</h3>

<p>Within OCR-D, <em>font family</em> refers to grouping elements by font similarity. The
semantics of a <em>font family</em> are up to the data producer.</p>

<h3 id="glyph">Glyph</h3>

<p>Within OCR-D, a glyph is the atomic unit within a <a href="#word">word</a>.</p>

<h3 id="grapheme-cluster">Grapheme Cluster</h3>

<p>See <a href="#glyph">Glyph</a></p>

<h3 id="line">Line</h3>

<p>See <a href="#textline">TextLine</a></p>

<h3 id="reading-order">Reading Order</h3>

<p>Reading order describes the logical sequence of <a href="#region">regions</a> within a document.</p>

<h3 id="region">Region</h3>

<p>A region is described by a polygon inside a page.</p>

<h3 id="region-type">Region type</h3>

<p>The semantics or function of a <a href="#region">region</a> such as heading, page number, column, table‚Ä¶</p>

<h3 id="symbol">Symbol</h3>

<p>See <a href="#glyph">Glyph</a></p>

<h3 id="textline">TextLine</h3>

<p>A text line is a single row of <a href="#word">words</a> within a text <a href="#region">region</a>. (Depending on the region‚Äôs or page‚Äôs orientation, and the script‚Äôs writing direction, it can be horizontal or vertical.)</p>

<h3 id="print-space">Print space</h3>

<p>From the <a href="https://ocr-d.de/de/gt-guidelines/pagexml/pagecontent_xsd_Complex_Type_pc_PrintSpaceType.html">PAGE-XML content schema documentation</a></p>

<blockquote>
  <p>Determines the effective area on the paper of a printed page. Its size is equal for all pages of a book (exceptions: titlepage, multipage pictures).</p>

  <p>It contains all living elements (except marginalia) like paragraphs and headings, as well as footnotes, headings, running titles.</p>

  <p>It does not contain pagenumber (if not part of running title), marginalia, signature mark, preview words.</p>
</blockquote>

<h3 id="word">Word</h3>

<p>A word is a sequence of <a href="#glyph">glyphs</a> within a <a href="#textline">line</a> which does not contain any word-bounding whitespace. (That is, it includes punctuation and is synonym to <em>token</em> in NLP.)</p>

<h2 id="data">Data</h2>

<h3 id="ground-truth">Ground Truth</h3>

<p>Ground truth (GT) <a href="http://ocr-d.de/daten">in the context of OCR-D</a> are
transcriptions, specific structure descriptions and word lists. These are
essentially available in PAGE XML format in combination with the original
image. Essential parts of the GT were created manually.</p>

<p>We distinguish different usage scenarios for GT:</p>

<h4 id="reference-data">Reference data</h4>

<p>With the term <em>reference data</em>, we refer to data that illustrates
different stages of an OCR/OLR process on representative materials. They are
supposed to support the assessment of commonly encountered difficulties and challenges when
running certain analysis operations and are therefore manually annotated
at all levels.</p>

<h4 id="evaluation-data">Evaluation data</h4>

<p><em>Evaluation data</em> are used to quantitatively evaluate the performance of OCR tools
and/or algorithms. Parts of these data which correspond to the tool(s) under consideration
are guaranteed to be recorded manually.</p>

<h4 id="training-data">Training data</h4>

<p>Many OCR-related tools need to be adapted to the specific domain of the works which are to
be processed. This domain adaptation is called <em>training</em>. Data used to guide this process
are called <em>training data</em>. It is essential that those parts of these data which are fed
to the training algorithm are captured manually.</p>

<h2 id="activities">Activities</h2>

<h3 id="binarization">Binarization</h3>

<p>Binarization means converting all color or grayscale pixels in an image to either black or white.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">binarized</code> (<code class="language-plaintext highlighter-rouge">comments</code> of a mets:file), <code class="language-plaintext highlighter-rouge">preprocessing/optimization/binarization</code> (<code class="language-plaintext highlighter-rouge">step</code> in ocrd-tool.json)</p>

<p>See <a href="http://felixniklas.com/imageprocessing/binarization">Felix‚Äô Niklas interactive demo</a></p>

<h3 id="dewarping">Dewarping</h3>

<p>Manipulate an image in such a way that all text lines are
straightened and any geometrical distortions have been corrected.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">preprocessing/optimization/dewarping</code></p>

<p>See <a href="https://mzucker.github.io/2016/08/15/page-dewarping.html">Matt Zucker‚Äôs entry on Dewarping</a>.</p>

<h3 id="despeckling">Despeckling</h3>

<p>Remove artifacts such as smudges, ink blots, underlinings etc. from an image. Typically applied to 
remove ‚Äúsalt-and-pepper‚Äù noise resulting from <a href="#Binarization">Binarization</a>.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">preprocessing/optimization/despeckling</code></p>

<h3 id="deskewing">Deskewing</h3>

<p>Rotate an image so that all text lines are horizontal.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">preprocessing/optimization/deskewing</code></p>

<h3 id="font-identification">Font identification</h3>

<p>Detect the font type(s) used in the document, either before or after an OCR run.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">recognition/font-identification</code></p>

<h3 id="grayscale-normalization">Grayscale normalization</h3>

<blockquote>
  <p>ISSUE: https://github.com/OCR-D/spec/issues/41</p>
</blockquote>

<p>Controlled term:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">gray_normalized</code> (<code class="language-plaintext highlighter-rouge">comments</code> in file)</li>
  <li><code class="language-plaintext highlighter-rouge">preprocessing/optimization/cropping</code> (step)</li>
</ul>

<p>Gray normalization is similar to binarization but instead of a purely bitonal
image, the output can also contain shades of gray to avoid inadvertently
combining glyphs when they are very close together.</p>

<h3 id="document-analysis">Document analysis</h3>

<p>Document analysis is the detection of structure on the document level to e.g. create a table of contents.</p>

<h3 id="reading-order-detection">Reading order detection</h3>

<p>Detect the <a href="#reading-order">reading order</a> of <a href="#region">regions</a>.</p>

<h3 id="cropping">Cropping</h3>

<p>Detecting the print space in a page, as opposed to the margins. It is a form of
<a href="#region-segmentation">region segmentation</a>.</p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">preprocessing/optimization/cropping</code>.</p>

<h3 id="border-removal">Border removal</h3>

<p>‚Äì&gt; <a href="#cropping">Cropping</a></p>

<h3 id="segmentation">Segmentation</h3>

<p>Segmentation means detecting areas within an image.</p>

<p>Specific segmentation algorithms are labelled by the semantics of the regions
they detect not the semantics of the input, i.e. an algorithm that detects
regions is called <a href="#region-segmentation">region segmentation</a>.</p>

<h3 id="region-segmentation">Region segmentation</h3>

<p>Segment an image into <a href="#region">regions</a>. Also determines whether this is a text
or non-text region (e.g. images).</p>

<p>Controlled term:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">SEG-REGION</code> (<code class="language-plaintext highlighter-rouge">USE</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">layout/segmentation/region</code> (step)</li>
</ul>

<h3 id="region-classification">Region classification</h3>

<p>Determine the <a href="#region-type">type</a> of a detected region.</p>

<h3 id="line-segmentation">Line segmentation</h3>
<p>Segment text <a href="#region">regions</a> into <a href="#textline">textlines</a>.</p>

<p>Controlled term:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">SEG-LINE</code> (<code class="language-plaintext highlighter-rouge">USE</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">layout/segmentation/line</code> (step)</li>
</ul>

<h3 id="line-recognition">Line recognition</h3>

<p>See <a href="#ocr">OCR</a>.</p>

<h3 id="ocr">OCR</h3>

<p>Map pixel areas to <a href="#glyph">glyphs</a> and <a href="#words">words</a>.</p>

<h3 id="word-segmentation">Word segmentation</h3>

<p>Segment a <a href="#textline">textline</a> into <a href="#word">words</a></p>

<p>Controlled term:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">SEG-LINE</code> (<code class="language-plaintext highlighter-rouge">USE</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">layout/segmentation/word</code> (step)</li>
</ul>

<h3 id="glyph-segmentation">Glyph segmentation</h3>

<p>Segment a <a href="#textline">textline</a> into <a href="#glyph">glyphs</a></p>

<p>Controlled term: <code class="language-plaintext highlighter-rouge">SEG-GLYPH</code></p>

<h3 id="text-recognition">Text recognition</h3>

<p>See <a href="#ocr">OCR</a>.</p>

<h3 id="text-optimization">Text optimization</h3>

<p>Text optimization encompasses the manipulations to the text based on the steps
up to and including text recognition. This includes (semi-)automatically correcting
recognition errors, orthographical harmonization, fixing segmentation errors etc.</p>

<h2 id="data-persistence">Data Persistence</h2>

<h3 id="software-repository">Software repository</h3>

<p>The software repository contains all OCR-D algorithms and tools developed
during the project including tests. It will also contain the documentation and
installation instructions for deploying a document analysis workflow.</p>

<h3 id="ground-truth-repository">Ground Truth repository</h3>

<p>Contains all the <a href="#ground-truth">ground truth</a> data.</p>

<h3 id="research-data-repository">Research data repository</h3>

<p>The research data repository may contain the results of all
<a href="#activities">activities</a> during document analysis. At least it contains the
end results of every processed document and its full provenance. The research
data repository must be available locally.</p>

<h3 id="model-repository">Model repository</h3>

<p>Contains all trained (OCR) models for text recognition. The model repository
has to be available at least locally. Ideally, a publicly available model repository will
be developed.</p>

<h3 id="workspace">Workspace</h3>

<p>A workspace is a representation for some document in the local file system. Minimally it consists of a directory with a copy of the <a href="https://ocr-d.de/en/spec/mets">METS</a> file. Additionally, that directory may contain physical data files and sub-directories belonging to the document (required or generated by run-time OCR-D processing), as referenced by the METS via <code class="language-plaintext highlighter-rouge">mets:file/mets:FLocat/@href</code> and <code class="language-plaintext highlighter-rouge">mets:fileGrp/@USE</code>. Files and sub-directories without reference (like log or config files) are not part of the workspace, as are references to remote locations. They can be added to the workspace by referencing them in the METS via their relative local path names.</p>

<h2 id="workflow-modules">Workflow modules</h2>

<p>The <a href="https://ocr-d.de">OCR-D project</a> divided the various elements of an OCR
workflow into six abstract modules.</p>

<h3 id="image-preprocessing">Image preprocessing</h3>

<p>Manipulating the input images for subsequent layout analysis and text recognition.</p>

<h3 id="layout-analysis">Layout analysis</h3>

<p>Detection of structure within the page.</p>

<h3 id="text-recognition-and-optimization">Text recognition and optimization</h3>

<p>Recognition of text and post-correction of recognition errors.</p>

<h3 id="model-training">Model training</h3>

<p>Generating data files from aligned ground truth text and images to configure
the prediction of text and layout recognition engines.</p>

<h3 id="long-term-preservation-and-persistence">Long-term preservation and persistence</h3>

<p>Storing results of OCR and OLR indefinitely, taking into account versioning,
multiple runs, provenance/parametrization and providing access to these saved
snapshots in a granular fashion.</p>

<h3 id="quality-assurance">Quality assurance</h3>

<p>Providing measures, algorithms and software to estimate the quality of the <a href="#activities">individual processes</a> within the OCR-D domain.</p>

<h2 id="component-architecture">Component architecture</h2>

<h3 id="ocr-d-application">(OCR-D) Application</h3>

<p>Application composed of various servers that can execute processors; can be a desktop computer or workstation, a distributed system comprising a controller and multiple processing servers, or an HPC cluster.</p>

<h3 id="ocr-d-web-api">OCR-D Web API</h3>

<p>As proposed in <a href="https://github.com/OCR-D/spec/pull/173">OCR-D/spec#173</a>, the OCR-D Web API defines uniform and interdependent services that can be distributed across network components, depending on the use case.</p>

<h3 id="ocr-d-service">(OCR-D) Service</h3>

<p>Group of endpoints of the OCR-D Web API; discovery/workspace/processing/workflow/‚Ä¶</p>

<h3 id="ocr-d-server">(OCR-D) Server</h3>

<p>Concrete implementation of a subset of OCR-D services, or the network host providing it.</p>

<h3 id="ocr-d-controller">(OCR-D) Controller</h3>

<p>OCR-D Server (implementing at least <em>discovery</em>, <em>workspace</em> and <em>workflow</em> services) executing workflows (a single workflow or multiple workflows simultaneously), distributing tasks to configured processing servers, managing workspace data management. Should also manage load balancing.</p>

<h3 id="ocr-d-processing-server">(OCR-D) Processing Server</h3>

<p>OCR-D server (implementing at least <em>discovery</em> and <em>processing</em> services) that can execute one or more (locally installed) processors or evaluators, manages workspace data; implementor should consider whether a single OCR-D processing server (with page-parallel processing) best fits the use case, or multiple OCR-D processing servers (with document-parallel processing), or even dedicated OCR-D processing servers with GPU/CUDA support.</p>

<h3 id="ocr-d-backend">(OCR-D) Backend</h3>

<p>Software component of a server concerned with network operations; e.g. Python library with request handlers, implementing service discovery and network-capable workspace data management.</p>

<h3 id="ocr-d-workflow-runtime-library">(OCR-D) Workflow Runtime Library</h3>

<p>Software component of a server or processor concerned with OCR systems modelling; e.g. Python library in <a href="https://github.com/OCR-D/core">OCR-D/core</a> providing classes for all essential functional components (<code class="language-plaintext highlighter-rouge">OcrdPage</code>, <code class="language-plaintext highlighter-rouge">OcrdMets</code>, <code class="language-plaintext highlighter-rouge">Workspace</code>, <code class="language-plaintext highlighter-rouge">Resolver</code>, <code class="language-plaintext highlighter-rouge">Processor</code>, <code class="language-plaintext highlighter-rouge">ProcessorTask</code>, <code class="language-plaintext highlighter-rouge">Workflow</code>, <code class="language-plaintext highlighter-rouge">WorkflowTask</code> ‚Ä¶), including mechanisms for signalling and orchestration of workflows, on top of which components (from processor to controller) can be implemented.</p>

<h3 id="ocr-d-workflow-engine">(OCR-D) Workflow Engine</h3>

<p>Central software component of the controller, executing workflows, including control structures (in a linear/parallel/incremental way). Also needed in single-host CLI deployments (where it can be based on inter-process communication and file system I/O alone), like <code class="language-plaintext highlighter-rouge">ocrd process</code>.</p>

<h3 id="ocr-d-processor">(OCR-D) Processor</h3>

<p>A processor is a tool that implements the uniform <a href="https://ocr-d.de/en/spec/cli">OCR-D command-line-interface</a> for run-time data processing. That is, it executes a single <a href="#activities">workflow step</a>, or a combination of multiple workflow steps, on the <a href="https://ocr-d.de/en/user_guide#preparing-a-workspace">workspace</a> (represented by local <a href="https://ocr-d.de/en/spec/mets">METS</a>), reading input files for all or requested physical pages of the input fileGrp(s), and writing output files for them into the output fileGrp(s). It may take a number of optional or mandatory <a href="https://ocr-d.de/en/spec/ocrd_tool">parameters</a>.</p>

<p>‚Üí <a href="https://ocr-d.de/en/workflows">OCR-D Workflow Guide</a></p>

<h3 id="ocr-d-evaluator">(OCR-D) Evaluator</h3>

<p>An evaluator is a tool that implements the uniform OCR-D CLI for run-time quality estimation, assessing an <a href="#activities">activity‚Äôs</a> annotation (i.e. a <a href="#processor">processor‚Äôs</a> output) with some quality metric to yield a score and applying a given threshold against it to signal full or partial success/failure.</p>

<h3 id="ocr-d-module">(OCR-D) Module</h3>

<p>Software package/repository providing one or more processors or evaluators, possibly encompassing additional areas of functionality (training, format conversion, creation of GT, visualization)</p>

<p>Modules can comprise multiple methods/activities that are called <a href="#processor"><em>processors</em></a>
for OCR-D. There were <a href="https://ocr-d.de/en/module-projects">eight MP</a> in the
second phase of OCR-D (2018-2020).</p>

<h3 id="messaging">Messaging</h3>

<p>Messaging service on the basis of Publish/Subscribe architecture (or similar) to coordinate network components, in particular for the distribution of tasks and load balancing, as well as signalling processor/evaluator results.</p>

<h3 id="ocr-d-workflow">OCR-D Workflow</h3>

<p>Combination of <a href="#activities">activities</a> via concrete <a href="#processor">processors</a> and <a href="#evaluator">evaluators</a> and their parameterization configured as a sequence or lattice, depending on their success or failure. Implemented in the <a href="#ocr-d-workflow-runtime-library">OCR-D Workflow Runtime Library</a> and serializable in a yet-to-specifcy format (as of 2020/10).</p>

<p>The term <em>Workflow</em> is understood to encompass more features in other contexts, such as manual intervention by the user. In contrast to the terminology in workflow engines like Taverna or digitization frameworks like Kitodo, an OCR-D workflow is a fully automatic process.</p>
:ET